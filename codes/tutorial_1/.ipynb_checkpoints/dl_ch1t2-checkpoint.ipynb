{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "valuable-baltimore",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "decimal-brooklyn",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = [4.209015  , 6.0251656 , 6.586659  , 1.0785204 , 5.323591,   2.9644287, 8.885769  , 9.895647  ,  6.464806  , 0.18034637, 1.2534696]\n",
    "x = [34.552039 , 74.45411  , 80.987488 ,  3.458197 , 56.4778655, 26.98163  , 95.79415  , 106.228316 , 61.169422 , 1.089516 , 8.962632]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "peaceful-andrew",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 34.5520,  74.4541,  80.9875,   3.4582,  56.4779,  26.9816,  95.7942,\n",
       "        106.2283,  61.1694,   1.0895,   8.9626])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = torch.tensor(y)\n",
    "x = torch.tensor(x)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "played-suspect",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(x, w, b):\n",
    "    return w * x + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "beautiful-carol",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn(yy, y):\n",
    "    squared_diffs = (yy - y)**2\n",
    "    return squared_diffs.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "competitive-actor",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dloss_fn(yy, y):\n",
    "    dsq_diffs = 2 * (yy - y) / yy.size(0)\n",
    "    return dsq_diffs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aggressive-regard",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dmodel_dw(x, w, b):\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "incredible-cardiff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dmodel_db(x, w, b):\n",
    "    return 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "faced-breakdown",
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad_fn(x, y, yy, w, b):\n",
    "    dloss_dyy = dloss_fn(yy,y)\n",
    "    dloss_dw = dloss_dyy * dmodel_dw(x, w, b)\n",
    "    dloss_db = dloss_dyy * dmodel_db(x, w, b)\n",
    "    return torch.stack([dloss_dw.sum(), dloss_db.sum()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "electoral-domestic",
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_loop(n_epochs, learning_rate, params, x, y):\n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        w, b = params\n",
    "        yy = model(x, w, b)\n",
    "        loss = loss_fn(yy, y)                \n",
    "        grad = grad_fn(x, y, yy, w, b)        \n",
    "        params = params - learning_rate * grad\n",
    "        print('Epoch %d, Loss %f' % (epoch, float(loss)))\n",
    "        print('\\t Params: ', params)\n",
    "        print('\\t Grad: ', grad)\n",
    "        \n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "attractive-tactics",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 3.4552,  7.4454,  8.0987,  0.3458,  5.6478,  2.6982,  9.5794, 10.6228,\n",
       "         6.1169,  0.1090,  0.8963])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = x * 0.1\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "devoted-reception",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss 0.622568\n",
      "\t Params:  tensor([ 0.9400, -0.0039])\n",
      "\t Grad:  tensor([5.9979, 0.3906])\n",
      "Epoch 2, Loss 0.399406\n",
      "\t Params:  tensor([ 0.9257, -0.0017])\n",
      "\t Grad:  tensor([ 1.4320, -0.2172])\n",
      "Epoch 3, Loss 0.385859\n",
      "\t Params:  tensor([0.9220, 0.0018])\n",
      "\t Grad:  tensor([ 0.3730, -0.3561])\n",
      "Epoch 4, Loss 0.383605\n",
      "\t Params:  tensor([0.9207, 0.0057])\n",
      "\t Grad:  tensor([ 0.1271, -0.3863])\n",
      "Epoch 5, Loss 0.381978\n",
      "\t Params:  tensor([0.9200, 0.0096])\n",
      "\t Grad:  tensor([ 0.0698, -0.3913])\n",
      "Epoch 6, Loss 0.380404\n",
      "\t Params:  tensor([0.9194, 0.0135])\n",
      "\t Grad:  tensor([ 0.0563, -0.3904])\n",
      "Epoch 7, Loss 0.378853\n",
      "\t Params:  tensor([0.9189, 0.0174])\n",
      "\t Grad:  tensor([ 0.0529, -0.3883])\n",
      "Epoch 8, Loss 0.377323\n",
      "\t Params:  tensor([0.9184, 0.0212])\n",
      "\t Grad:  tensor([ 0.0518, -0.3858])\n",
      "Epoch 9, Loss 0.375813\n",
      "\t Params:  tensor([0.9179, 0.0251])\n",
      "\t Grad:  tensor([ 0.0513, -0.3832])\n",
      "Epoch 10, Loss 0.374323\n",
      "\t Params:  tensor([0.9174, 0.0289])\n",
      "\t Grad:  tensor([ 0.0509, -0.3807])\n",
      "Epoch 11, Loss 0.372852\n",
      "\t Params:  tensor([0.9169, 0.0327])\n",
      "\t Grad:  tensor([ 0.0506, -0.3782])\n",
      "Epoch 12, Loss 0.371401\n",
      "\t Params:  tensor([0.9164, 0.0364])\n",
      "\t Grad:  tensor([ 0.0502, -0.3757])\n",
      "Epoch 13, Loss 0.369969\n",
      "\t Params:  tensor([0.9159, 0.0402])\n",
      "\t Grad:  tensor([ 0.0499, -0.3732])\n",
      "Epoch 14, Loss 0.368557\n",
      "\t Params:  tensor([0.9154, 0.0439])\n",
      "\t Grad:  tensor([ 0.0496, -0.3707])\n",
      "Epoch 15, Loss 0.367162\n",
      "\t Params:  tensor([0.9149, 0.0475])\n",
      "\t Grad:  tensor([ 0.0492, -0.3683])\n",
      "Epoch 16, Loss 0.365786\n",
      "\t Params:  tensor([0.9144, 0.0512])\n",
      "\t Grad:  tensor([ 0.0489, -0.3658])\n",
      "Epoch 17, Loss 0.364429\n",
      "\t Params:  tensor([0.9139, 0.0548])\n",
      "\t Grad:  tensor([ 0.0486, -0.3634])\n",
      "Epoch 18, Loss 0.363089\n",
      "\t Params:  tensor([0.9134, 0.0585])\n",
      "\t Grad:  tensor([ 0.0483, -0.3610])\n",
      "Epoch 19, Loss 0.361767\n",
      "\t Params:  tensor([0.9129, 0.0620])\n",
      "\t Grad:  tensor([ 0.0480, -0.3586])\n",
      "Epoch 20, Loss 0.360462\n",
      "\t Params:  tensor([0.9125, 0.0656])\n",
      "\t Grad:  tensor([ 0.0476, -0.3562])\n",
      "Epoch 21, Loss 0.359175\n",
      "\t Params:  tensor([0.9120, 0.0691])\n",
      "\t Grad:  tensor([ 0.0473, -0.3539])\n",
      "Epoch 22, Loss 0.357905\n",
      "\t Params:  tensor([0.9115, 0.0727])\n",
      "\t Grad:  tensor([ 0.0470, -0.3515])\n",
      "Epoch 23, Loss 0.356651\n",
      "\t Params:  tensor([0.9111, 0.0761])\n",
      "\t Grad:  tensor([ 0.0467, -0.3492])\n",
      "Epoch 24, Loss 0.355414\n",
      "\t Params:  tensor([0.9106, 0.0796])\n",
      "\t Grad:  tensor([ 0.0464, -0.3469])\n",
      "Epoch 25, Loss 0.354193\n",
      "\t Params:  tensor([0.9101, 0.0831])\n",
      "\t Grad:  tensor([ 0.0461, -0.3446])\n",
      "Epoch 26, Loss 0.352988\n",
      "\t Params:  tensor([0.9097, 0.0865])\n",
      "\t Grad:  tensor([ 0.0458, -0.3423])\n",
      "Epoch 27, Loss 0.351800\n",
      "\t Params:  tensor([0.9092, 0.0899])\n",
      "\t Grad:  tensor([ 0.0455, -0.3400])\n",
      "Epoch 28, Loss 0.350627\n",
      "\t Params:  tensor([0.9088, 0.0933])\n",
      "\t Grad:  tensor([ 0.0452, -0.3378])\n",
      "Epoch 29, Loss 0.349469\n",
      "\t Params:  tensor([0.9083, 0.0966])\n",
      "\t Grad:  tensor([ 0.0449, -0.3355])\n",
      "Epoch 30, Loss 0.348327\n",
      "\t Params:  tensor([0.9079, 0.0999])\n",
      "\t Grad:  tensor([ 0.0446, -0.3333])\n",
      "Epoch 31, Loss 0.347200\n",
      "\t Params:  tensor([0.9074, 0.1033])\n",
      "\t Grad:  tensor([ 0.0443, -0.3311])\n",
      "Epoch 32, Loss 0.346088\n",
      "\t Params:  tensor([0.9070, 0.1065])\n",
      "\t Grad:  tensor([ 0.0440, -0.3289])\n",
      "Epoch 33, Loss 0.344990\n",
      "\t Params:  tensor([0.9065, 0.1098])\n",
      "\t Grad:  tensor([ 0.0437, -0.3267])\n",
      "Epoch 34, Loss 0.343907\n",
      "\t Params:  tensor([0.9061, 0.1131])\n",
      "\t Grad:  tensor([ 0.0434, -0.3246])\n",
      "Epoch 35, Loss 0.342838\n",
      "\t Params:  tensor([0.9057, 0.1163])\n",
      "\t Grad:  tensor([ 0.0431, -0.3224])\n",
      "Epoch 36, Loss 0.341784\n",
      "\t Params:  tensor([0.9053, 0.1195])\n",
      "\t Grad:  tensor([ 0.0428, -0.3203])\n",
      "Epoch 37, Loss 0.340743\n",
      "\t Params:  tensor([0.9048, 0.1227])\n",
      "\t Grad:  tensor([ 0.0425, -0.3182])\n",
      "Epoch 38, Loss 0.339716\n",
      "\t Params:  tensor([0.9044, 0.1258])\n",
      "\t Grad:  tensor([ 0.0423, -0.3161])\n",
      "Epoch 39, Loss 0.338703\n",
      "\t Params:  tensor([0.9040, 0.1290])\n",
      "\t Grad:  tensor([ 0.0420, -0.3140])\n",
      "Epoch 40, Loss 0.337703\n",
      "\t Params:  tensor([0.9036, 0.1321])\n",
      "\t Grad:  tensor([ 0.0417, -0.3119])\n",
      "Epoch 41, Loss 0.336716\n",
      "\t Params:  tensor([0.9032, 0.1352])\n",
      "\t Grad:  tensor([ 0.0414, -0.3098])\n",
      "Epoch 42, Loss 0.335742\n",
      "\t Params:  tensor([0.9027, 0.1383])\n",
      "\t Grad:  tensor([ 0.0412, -0.3078])\n",
      "Epoch 43, Loss 0.334781\n",
      "\t Params:  tensor([0.9023, 0.1413])\n",
      "\t Grad:  tensor([ 0.0409, -0.3057])\n",
      "Epoch 44, Loss 0.333833\n",
      "\t Params:  tensor([0.9019, 0.1444])\n",
      "\t Grad:  tensor([ 0.0406, -0.3037])\n",
      "Epoch 45, Loss 0.332897\n",
      "\t Params:  tensor([0.9015, 0.1474])\n",
      "\t Grad:  tensor([ 0.0403, -0.3017])\n",
      "Epoch 46, Loss 0.331974\n",
      "\t Params:  tensor([0.9011, 0.1504])\n",
      "\t Grad:  tensor([ 0.0401, -0.2997])\n",
      "Epoch 47, Loss 0.331063\n",
      "\t Params:  tensor([0.9007, 0.1534])\n",
      "\t Grad:  tensor([ 0.0398, -0.2977])\n",
      "Epoch 48, Loss 0.330164\n",
      "\t Params:  tensor([0.9003, 0.1563])\n",
      "\t Grad:  tensor([ 0.0395, -0.2957])\n",
      "Epoch 49, Loss 0.329276\n",
      "\t Params:  tensor([0.8999, 0.1592])\n",
      "\t Grad:  tensor([ 0.0393, -0.2938])\n",
      "Epoch 50, Loss 0.328401\n",
      "\t Params:  tensor([0.8995, 0.1622])\n",
      "\t Grad:  tensor([ 0.0390, -0.2918])\n",
      "Epoch 51, Loss 0.327537\n",
      "\t Params:  tensor([0.8992, 0.1651])\n",
      "\t Grad:  tensor([ 0.0388, -0.2899])\n",
      "Epoch 52, Loss 0.326684\n",
      "\t Params:  tensor([0.8988, 0.1679])\n",
      "\t Grad:  tensor([ 0.0385, -0.2880])\n",
      "Epoch 53, Loss 0.325843\n",
      "\t Params:  tensor([0.8984, 0.1708])\n",
      "\t Grad:  tensor([ 0.0383, -0.2861])\n",
      "Epoch 54, Loss 0.325013\n",
      "\t Params:  tensor([0.8980, 0.1736])\n",
      "\t Grad:  tensor([ 0.0380, -0.2842])\n",
      "Epoch 55, Loss 0.324194\n",
      "\t Params:  tensor([0.8976, 0.1765])\n",
      "\t Grad:  tensor([ 0.0377, -0.2823])\n",
      "Epoch 56, Loss 0.323385\n",
      "\t Params:  tensor([0.8973, 0.1793])\n",
      "\t Grad:  tensor([ 0.0375, -0.2804])\n",
      "Epoch 57, Loss 0.322587\n",
      "\t Params:  tensor([0.8969, 0.1821])\n",
      "\t Grad:  tensor([ 0.0372, -0.2786])\n",
      "Epoch 58, Loss 0.321800\n",
      "\t Params:  tensor([0.8965, 0.1848])\n",
      "\t Grad:  tensor([ 0.0370, -0.2767])\n",
      "Epoch 59, Loss 0.321023\n",
      "\t Params:  tensor([0.8962, 0.1876])\n",
      "\t Grad:  tensor([ 0.0368, -0.2749])\n",
      "Epoch 60, Loss 0.320257\n",
      "\t Params:  tensor([0.8958, 0.1903])\n",
      "\t Grad:  tensor([ 0.0365, -0.2731])\n",
      "Epoch 61, Loss 0.319500\n",
      "\t Params:  tensor([0.8954, 0.1930])\n",
      "\t Grad:  tensor([ 0.0363, -0.2712])\n",
      "Epoch 62, Loss 0.318754\n",
      "\t Params:  tensor([0.8951, 0.1957])\n",
      "\t Grad:  tensor([ 0.0360, -0.2694])\n",
      "Epoch 63, Loss 0.318018\n",
      "\t Params:  tensor([0.8947, 0.1984])\n",
      "\t Grad:  tensor([ 0.0358, -0.2677])\n",
      "Epoch 64, Loss 0.317291\n",
      "\t Params:  tensor([0.8944, 0.2010])\n",
      "\t Grad:  tensor([ 0.0356, -0.2659])\n",
      "Epoch 65, Loss 0.316574\n",
      "\t Params:  tensor([0.8940, 0.2037])\n",
      "\t Grad:  tensor([ 0.0353, -0.2641])\n",
      "Epoch 66, Loss 0.315866\n",
      "\t Params:  tensor([0.8936, 0.2063])\n",
      "\t Grad:  tensor([ 0.0351, -0.2624])\n",
      "Epoch 67, Loss 0.315167\n",
      "\t Params:  tensor([0.8933, 0.2089])\n",
      "\t Grad:  tensor([ 0.0348, -0.2606])\n",
      "Epoch 68, Loss 0.314478\n",
      "\t Params:  tensor([0.8930, 0.2115])\n",
      "\t Grad:  tensor([ 0.0346, -0.2589])\n",
      "Epoch 69, Loss 0.313798\n",
      "\t Params:  tensor([0.8926, 0.2141])\n",
      "\t Grad:  tensor([ 0.0344, -0.2572])\n",
      "Epoch 70, Loss 0.313127\n",
      "\t Params:  tensor([0.8923, 0.2166])\n",
      "\t Grad:  tensor([ 0.0342, -0.2555])\n",
      "Epoch 71, Loss 0.312465\n",
      "\t Params:  tensor([0.8919, 0.2192])\n",
      "\t Grad:  tensor([ 0.0339, -0.2538])\n",
      "Epoch 72, Loss 0.311811\n",
      "\t Params:  tensor([0.8916, 0.2217])\n",
      "\t Grad:  tensor([ 0.0337, -0.2521])\n",
      "Epoch 73, Loss 0.311166\n",
      "\t Params:  tensor([0.8913, 0.2242])\n",
      "\t Grad:  tensor([ 0.0335, -0.2505])\n",
      "Epoch 74, Loss 0.310530\n",
      "\t Params:  tensor([0.8909, 0.2267])\n",
      "\t Grad:  tensor([ 0.0333, -0.2488])\n",
      "Epoch 75, Loss 0.309902\n",
      "\t Params:  tensor([0.8906, 0.2292])\n",
      "\t Grad:  tensor([ 0.0330, -0.2471])\n",
      "Epoch 76, Loss 0.309282\n",
      "\t Params:  tensor([0.8903, 0.2316])\n",
      "\t Grad:  tensor([ 0.0328, -0.2455])\n",
      "Epoch 77, Loss 0.308671\n",
      "\t Params:  tensor([0.8899, 0.2341])\n",
      "\t Grad:  tensor([ 0.0326, -0.2439])\n",
      "Epoch 78, Loss 0.308067\n",
      "\t Params:  tensor([0.8896, 0.2365])\n",
      "\t Grad:  tensor([ 0.0324, -0.2423])\n",
      "Epoch 79, Loss 0.307472\n",
      "\t Params:  tensor([0.8893, 0.2389])\n",
      "\t Grad:  tensor([ 0.0322, -0.2407])\n",
      "Epoch 80, Loss 0.306884\n",
      "\t Params:  tensor([0.8890, 0.2413])\n",
      "\t Grad:  tensor([ 0.0320, -0.2391])\n",
      "Epoch 81, Loss 0.306305\n",
      "\t Params:  tensor([0.8887, 0.2436])\n",
      "\t Grad:  tensor([ 0.0318, -0.2375])\n",
      "Epoch 82, Loss 0.305732\n",
      "\t Params:  tensor([0.8883, 0.2460])\n",
      "\t Grad:  tensor([ 0.0315, -0.2359])\n",
      "Epoch 83, Loss 0.305168\n",
      "\t Params:  tensor([0.8880, 0.2483])\n",
      "\t Grad:  tensor([ 0.0313, -0.2343])\n",
      "Epoch 84, Loss 0.304611\n",
      "\t Params:  tensor([0.8877, 0.2507])\n",
      "\t Grad:  tensor([ 0.0311, -0.2328])\n",
      "Epoch 85, Loss 0.304061\n",
      "\t Params:  tensor([0.8874, 0.2530])\n",
      "\t Grad:  tensor([ 0.0309, -0.2312])\n",
      "Epoch 86, Loss 0.303519\n",
      "\t Params:  tensor([0.8871, 0.2553])\n",
      "\t Grad:  tensor([ 0.0307, -0.2297])\n",
      "Epoch 87, Loss 0.302983\n",
      "\t Params:  tensor([0.8868, 0.2576])\n",
      "\t Grad:  tensor([ 0.0305, -0.2282])\n",
      "Epoch 88, Loss 0.302455\n",
      "\t Params:  tensor([0.8865, 0.2598])\n",
      "\t Grad:  tensor([ 0.0303, -0.2267])\n",
      "Epoch 89, Loss 0.301933\n",
      "\t Params:  tensor([0.8862, 0.2621])\n",
      "\t Grad:  tensor([ 0.0301, -0.2252])\n",
      "Epoch 90, Loss 0.301419\n",
      "\t Params:  tensor([0.8859, 0.2643])\n",
      "\t Grad:  tensor([ 0.0299, -0.2237])\n",
      "Epoch 91, Loss 0.300911\n",
      "\t Params:  tensor([0.8856, 0.2665])\n",
      "\t Grad:  tensor([ 0.0297, -0.2222])\n",
      "Epoch 92, Loss 0.300411\n",
      "\t Params:  tensor([0.8853, 0.2688])\n",
      "\t Grad:  tensor([ 0.0295, -0.2207])\n",
      "Epoch 93, Loss 0.299916\n",
      "\t Params:  tensor([0.8850, 0.2709])\n",
      "\t Grad:  tensor([ 0.0293, -0.2193])\n",
      "Epoch 94, Loss 0.299428\n",
      "\t Params:  tensor([0.8847, 0.2731])\n",
      "\t Grad:  tensor([ 0.0291, -0.2178])\n",
      "Epoch 95, Loss 0.298947\n",
      "\t Params:  tensor([0.8844, 0.2753])\n",
      "\t Grad:  tensor([ 0.0289, -0.2164])\n",
      "Epoch 96, Loss 0.298472\n",
      "\t Params:  tensor([0.8841, 0.2774])\n",
      "\t Grad:  tensor([ 0.0287, -0.2149])\n",
      "Epoch 97, Loss 0.298003\n",
      "\t Params:  tensor([0.8839, 0.2796])\n",
      "\t Grad:  tensor([ 0.0285, -0.2135])\n",
      "Epoch 98, Loss 0.297541\n",
      "\t Params:  tensor([0.8836, 0.2817])\n",
      "\t Grad:  tensor([ 0.0284, -0.2121])\n",
      "Epoch 99, Loss 0.297085\n",
      "\t Params:  tensor([0.8833, 0.2838])\n",
      "\t Grad:  tensor([ 0.0282, -0.2107])\n",
      "Epoch 100, Loss 0.296634\n",
      "\t Params:  tensor([0.8830, 0.2859])\n",
      "\t Grad:  tensor([ 0.0280, -0.2093])\n",
      "Epoch 101, Loss 0.296190\n",
      "\t Params:  tensor([0.8827, 0.2880])\n",
      "\t Grad:  tensor([ 0.0278, -0.2079])\n",
      "Epoch 102, Loss 0.295751\n",
      "\t Params:  tensor([0.8825, 0.2900])\n",
      "\t Grad:  tensor([ 0.0276, -0.2065])\n",
      "Epoch 103, Loss 0.295318\n",
      "\t Params:  tensor([0.8822, 0.2921])\n",
      "\t Grad:  tensor([ 0.0274, -0.2052])\n",
      "Epoch 104, Loss 0.294891\n",
      "\t Params:  tensor([0.8819, 0.2941])\n",
      "\t Grad:  tensor([ 0.0273, -0.2038])\n",
      "Epoch 105, Loss 0.294470\n",
      "\t Params:  tensor([0.8816, 0.2962])\n",
      "\t Grad:  tensor([ 0.0271, -0.2025])\n",
      "Epoch 106, Loss 0.294054\n",
      "\t Params:  tensor([0.8814, 0.2982])\n",
      "\t Grad:  tensor([ 0.0269, -0.2011])\n",
      "Epoch 107, Loss 0.293644\n",
      "\t Params:  tensor([0.8811, 0.3002])\n",
      "\t Grad:  tensor([ 0.0267, -0.1998])\n",
      "Epoch 108, Loss 0.293239\n",
      "\t Params:  tensor([0.8808, 0.3021])\n",
      "\t Grad:  tensor([ 0.0265, -0.1985])\n",
      "Epoch 109, Loss 0.292839\n",
      "\t Params:  tensor([0.8806, 0.3041])\n",
      "\t Grad:  tensor([ 0.0264, -0.1971])\n",
      "Epoch 110, Loss 0.292445\n",
      "\t Params:  tensor([0.8803, 0.3061])\n",
      "\t Grad:  tensor([ 0.0262, -0.1958])\n",
      "Epoch 111, Loss 0.292056\n",
      "\t Params:  tensor([0.8800, 0.3080])\n",
      "\t Grad:  tensor([ 0.0260, -0.1945])\n",
      "Epoch 112, Loss 0.291672\n",
      "\t Params:  tensor([0.8798, 0.3100])\n",
      "\t Grad:  tensor([ 0.0258, -0.1933])\n",
      "Epoch 113, Loss 0.291293\n",
      "\t Params:  tensor([0.8795, 0.3119])\n",
      "\t Grad:  tensor([ 0.0257, -0.1920])\n",
      "Epoch 114, Loss 0.290919\n",
      "\t Params:  tensor([0.8793, 0.3138])\n",
      "\t Grad:  tensor([ 0.0255, -0.1907])\n",
      "Epoch 115, Loss 0.290550\n",
      "\t Params:  tensor([0.8790, 0.3157])\n",
      "\t Grad:  tensor([ 0.0253, -0.1894])\n",
      "Epoch 116, Loss 0.290186\n",
      "\t Params:  tensor([0.8788, 0.3176])\n",
      "\t Grad:  tensor([ 0.0252, -0.1882])\n",
      "Epoch 117, Loss 0.289827\n",
      "\t Params:  tensor([0.8785, 0.3194])\n",
      "\t Grad:  tensor([ 0.0250, -0.1869])\n",
      "Epoch 118, Loss 0.289472\n",
      "\t Params:  tensor([0.8783, 0.3213])\n",
      "\t Grad:  tensor([ 0.0248, -0.1857])\n",
      "Epoch 119, Loss 0.289122\n",
      "\t Params:  tensor([0.8780, 0.3231])\n",
      "\t Grad:  tensor([ 0.0247, -0.1845])\n",
      "Epoch 120, Loss 0.288777\n",
      "\t Params:  tensor([0.8778, 0.3250])\n",
      "\t Grad:  tensor([ 0.0245, -0.1832])\n",
      "Epoch 121, Loss 0.288436\n",
      "\t Params:  tensor([0.8775, 0.3268])\n",
      "\t Grad:  tensor([ 0.0243, -0.1820])\n",
      "Epoch 122, Loss 0.288100\n",
      "\t Params:  tensor([0.8773, 0.3286])\n",
      "\t Grad:  tensor([ 0.0242, -0.1808])\n",
      "Epoch 123, Loss 0.287769\n",
      "\t Params:  tensor([0.8771, 0.3304])\n",
      "\t Grad:  tensor([ 0.0240, -0.1796])\n",
      "Epoch 124, Loss 0.287441\n",
      "\t Params:  tensor([0.8768, 0.3322])\n",
      "\t Grad:  tensor([ 0.0239, -0.1784])\n",
      "Epoch 125, Loss 0.287118\n",
      "\t Params:  tensor([0.8766, 0.3339])\n",
      "\t Grad:  tensor([ 0.0237, -0.1773])\n",
      "Epoch 126, Loss 0.286800\n",
      "\t Params:  tensor([0.8763, 0.3357])\n",
      "\t Grad:  tensor([ 0.0235, -0.1761])\n",
      "Epoch 127, Loss 0.286485\n",
      "\t Params:  tensor([0.8761, 0.3375])\n",
      "\t Grad:  tensor([ 0.0234, -0.1749])\n",
      "Epoch 128, Loss 0.286174\n",
      "\t Params:  tensor([0.8759, 0.3392])\n",
      "\t Grad:  tensor([ 0.0232, -0.1738])\n",
      "Epoch 129, Loss 0.285868\n",
      "\t Params:  tensor([0.8756, 0.3409])\n",
      "\t Grad:  tensor([ 0.0231, -0.1726])\n",
      "Epoch 130, Loss 0.285566\n",
      "\t Params:  tensor([0.8754, 0.3426])\n",
      "\t Grad:  tensor([ 0.0229, -0.1715])\n",
      "Epoch 131, Loss 0.285268\n",
      "\t Params:  tensor([0.8752, 0.3443])\n",
      "\t Grad:  tensor([ 0.0228, -0.1703])\n",
      "Epoch 132, Loss 0.284974\n",
      "\t Params:  tensor([0.8750, 0.3460])\n",
      "\t Grad:  tensor([ 0.0226, -0.1692])\n",
      "Epoch 133, Loss 0.284683\n",
      "\t Params:  tensor([0.8747, 0.3477])\n",
      "\t Grad:  tensor([ 0.0225, -0.1681])\n",
      "Epoch 134, Loss 0.284396\n",
      "\t Params:  tensor([0.8745, 0.3494])\n",
      "\t Grad:  tensor([ 0.0223, -0.1670])\n",
      "Epoch 135, Loss 0.284114\n",
      "\t Params:  tensor([0.8743, 0.3510])\n",
      "\t Grad:  tensor([ 0.0222, -0.1659])\n",
      "Epoch 136, Loss 0.283835\n",
      "\t Params:  tensor([0.8741, 0.3527])\n",
      "\t Grad:  tensor([ 0.0220, -0.1648])\n",
      "Epoch 137, Loss 0.283559\n",
      "\t Params:  tensor([0.8739, 0.3543])\n",
      "\t Grad:  tensor([ 0.0219, -0.1637])\n",
      "Epoch 138, Loss 0.283287\n",
      "\t Params:  tensor([0.8736, 0.3559])\n",
      "\t Grad:  tensor([ 0.0217, -0.1626])\n",
      "Epoch 139, Loss 0.283019\n",
      "\t Params:  tensor([0.8734, 0.3576])\n",
      "\t Grad:  tensor([ 0.0216, -0.1615])\n",
      "Epoch 140, Loss 0.282755\n",
      "\t Params:  tensor([0.8732, 0.3592])\n",
      "\t Grad:  tensor([ 0.0215, -0.1604])\n",
      "Epoch 141, Loss 0.282494\n",
      "\t Params:  tensor([0.8730, 0.3608])\n",
      "\t Grad:  tensor([ 0.0213, -0.1594])\n",
      "Epoch 142, Loss 0.282236\n",
      "\t Params:  tensor([0.8728, 0.3623])\n",
      "\t Grad:  tensor([ 0.0212, -0.1583])\n",
      "Epoch 143, Loss 0.281981\n",
      "\t Params:  tensor([0.8726, 0.3639])\n",
      "\t Grad:  tensor([ 0.0210, -0.1573])\n",
      "Epoch 144, Loss 0.281731\n",
      "\t Params:  tensor([0.8724, 0.3655])\n",
      "\t Grad:  tensor([ 0.0209, -0.1562])\n",
      "Epoch 145, Loss 0.281483\n",
      "\t Params:  tensor([0.8722, 0.3670])\n",
      "\t Grad:  tensor([ 0.0207, -0.1552])\n",
      "Epoch 146, Loss 0.281239\n",
      "\t Params:  tensor([0.8720, 0.3686])\n",
      "\t Grad:  tensor([ 0.0206, -0.1542])\n",
      "Epoch 147, Loss 0.280998\n",
      "\t Params:  tensor([0.8717, 0.3701])\n",
      "\t Grad:  tensor([ 0.0205, -0.1531])\n",
      "Epoch 148, Loss 0.280760\n",
      "\t Params:  tensor([0.8715, 0.3716])\n",
      "\t Grad:  tensor([ 0.0203, -0.1521])\n",
      "Epoch 149, Loss 0.280525\n",
      "\t Params:  tensor([0.8713, 0.3731])\n",
      "\t Grad:  tensor([ 0.0202, -0.1511])\n",
      "Epoch 150, Loss 0.280293\n",
      "\t Params:  tensor([0.8711, 0.3746])\n",
      "\t Grad:  tensor([ 0.0201, -0.1501])\n",
      "Epoch 151, Loss 0.280065\n",
      "\t Params:  tensor([0.8709, 0.3761])\n",
      "\t Grad:  tensor([ 0.0199, -0.1491])\n",
      "Epoch 152, Loss 0.279839\n",
      "\t Params:  tensor([0.8707, 0.3776])\n",
      "\t Grad:  tensor([ 0.0198, -0.1481])\n",
      "Epoch 153, Loss 0.279616\n",
      "\t Params:  tensor([0.8705, 0.3791])\n",
      "\t Grad:  tensor([ 0.0197, -0.1472])\n",
      "Epoch 154, Loss 0.279397\n",
      "\t Params:  tensor([0.8704, 0.3805])\n",
      "\t Grad:  tensor([ 0.0195, -0.1462])\n",
      "Epoch 155, Loss 0.279180\n",
      "\t Params:  tensor([0.8702, 0.3820])\n",
      "\t Grad:  tensor([ 0.0194, -0.1452])\n",
      "Epoch 156, Loss 0.278966\n",
      "\t Params:  tensor([0.8700, 0.3834])\n",
      "\t Grad:  tensor([ 0.0193, -0.1442])\n",
      "Epoch 157, Loss 0.278755\n",
      "\t Params:  tensor([0.8698, 0.3849])\n",
      "\t Grad:  tensor([ 0.0192, -0.1433])\n",
      "Epoch 158, Loss 0.278547\n",
      "\t Params:  tensor([0.8696, 0.3863])\n",
      "\t Grad:  tensor([ 0.0190, -0.1423])\n",
      "Epoch 159, Loss 0.278341\n",
      "\t Params:  tensor([0.8694, 0.3877])\n",
      "\t Grad:  tensor([ 0.0189, -0.1414])\n",
      "Epoch 160, Loss 0.278138\n",
      "\t Params:  tensor([0.8692, 0.3891])\n",
      "\t Grad:  tensor([ 0.0188, -0.1405])\n",
      "Epoch 161, Loss 0.277938\n",
      "\t Params:  tensor([0.8690, 0.3905])\n",
      "\t Grad:  tensor([ 0.0187, -0.1395])\n",
      "Epoch 162, Loss 0.277741\n",
      "\t Params:  tensor([0.8688, 0.3919])\n",
      "\t Grad:  tensor([ 0.0185, -0.1386])\n",
      "Epoch 163, Loss 0.277546\n",
      "\t Params:  tensor([0.8686, 0.3933])\n",
      "\t Grad:  tensor([ 0.0184, -0.1377])\n",
      "Epoch 164, Loss 0.277353\n",
      "\t Params:  tensor([0.8685, 0.3946])\n",
      "\t Grad:  tensor([ 0.0183, -0.1368])\n",
      "Epoch 165, Loss 0.277164\n",
      "\t Params:  tensor([0.8683, 0.3960])\n",
      "\t Grad:  tensor([ 0.0182, -0.1359])\n",
      "Epoch 166, Loss 0.276976\n",
      "\t Params:  tensor([0.8681, 0.3973])\n",
      "\t Grad:  tensor([ 0.0180, -0.1350])\n",
      "Epoch 167, Loss 0.276791\n",
      "\t Params:  tensor([0.8679, 0.3987])\n",
      "\t Grad:  tensor([ 0.0179, -0.1341])\n",
      "Epoch 168, Loss 0.276609\n",
      "\t Params:  tensor([0.8677, 0.4000])\n",
      "\t Grad:  tensor([ 0.0178, -0.1332])\n",
      "Epoch 169, Loss 0.276429\n",
      "\t Params:  tensor([0.8676, 0.4013])\n",
      "\t Grad:  tensor([ 0.0177, -0.1323])\n",
      "Epoch 170, Loss 0.276251\n",
      "\t Params:  tensor([0.8674, 0.4027])\n",
      "\t Grad:  tensor([ 0.0176, -0.1314])\n",
      "Epoch 171, Loss 0.276076\n",
      "\t Params:  tensor([0.8672, 0.4040])\n",
      "\t Grad:  tensor([ 0.0175, -0.1306])\n",
      "Epoch 172, Loss 0.275903\n",
      "\t Params:  tensor([0.8670, 0.4053])\n",
      "\t Grad:  tensor([ 0.0173, -0.1297])\n",
      "Epoch 173, Loss 0.275733\n",
      "\t Params:  tensor([0.8669, 0.4065])\n",
      "\t Grad:  tensor([ 0.0172, -0.1288])\n",
      "Epoch 174, Loss 0.275564\n",
      "\t Params:  tensor([0.8667, 0.4078])\n",
      "\t Grad:  tensor([ 0.0171, -0.1280])\n",
      "Epoch 175, Loss 0.275398\n",
      "\t Params:  tensor([0.8665, 0.4091])\n",
      "\t Grad:  tensor([ 0.0170, -0.1271])\n",
      "Epoch 176, Loss 0.275234\n",
      "\t Params:  tensor([0.8664, 0.4104])\n",
      "\t Grad:  tensor([ 0.0169, -0.1263])\n",
      "Epoch 177, Loss 0.275072\n",
      "\t Params:  tensor([0.8662, 0.4116])\n",
      "\t Grad:  tensor([ 0.0168, -0.1255])\n",
      "Epoch 178, Loss 0.274913\n",
      "\t Params:  tensor([0.8660, 0.4129])\n",
      "\t Grad:  tensor([ 0.0167, -0.1246])\n",
      "Epoch 179, Loss 0.274755\n",
      "\t Params:  tensor([0.8659, 0.4141])\n",
      "\t Grad:  tensor([ 0.0166, -0.1238])\n",
      "Epoch 180, Loss 0.274600\n",
      "\t Params:  tensor([0.8657, 0.4153])\n",
      "\t Grad:  tensor([ 0.0164, -0.1230])\n",
      "Epoch 181, Loss 0.274446\n",
      "\t Params:  tensor([0.8655, 0.4165])\n",
      "\t Grad:  tensor([ 0.0163, -0.1222])\n",
      "Epoch 182, Loss 0.274295\n",
      "\t Params:  tensor([0.8654, 0.4178])\n",
      "\t Grad:  tensor([ 0.0162, -0.1214])\n",
      "Epoch 183, Loss 0.274145\n",
      "\t Params:  tensor([0.8652, 0.4190])\n",
      "\t Grad:  tensor([ 0.0161, -0.1205])\n",
      "Epoch 184, Loss 0.273998\n",
      "\t Params:  tensor([0.8651, 0.4202])\n",
      "\t Grad:  tensor([ 0.0160, -0.1197])\n",
      "Epoch 185, Loss 0.273853\n",
      "\t Params:  tensor([0.8649, 0.4214])\n",
      "\t Grad:  tensor([ 0.0159, -0.1190])\n",
      "Epoch 186, Loss 0.273709\n",
      "\t Params:  tensor([0.8647, 0.4225])\n",
      "\t Grad:  tensor([ 0.0158, -0.1182])\n",
      "Epoch 187, Loss 0.273567\n",
      "\t Params:  tensor([0.8646, 0.4237])\n",
      "\t Grad:  tensor([ 0.0157, -0.1174])\n",
      "Epoch 188, Loss 0.273427\n",
      "\t Params:  tensor([0.8644, 0.4249])\n",
      "\t Grad:  tensor([ 0.0156, -0.1166])\n",
      "Epoch 189, Loss 0.273290\n",
      "\t Params:  tensor([0.8643, 0.4260])\n",
      "\t Grad:  tensor([ 0.0155, -0.1158])\n",
      "Epoch 190, Loss 0.273153\n",
      "\t Params:  tensor([0.8641, 0.4272])\n",
      "\t Grad:  tensor([ 0.0154, -0.1151])\n",
      "Epoch 191, Loss 0.273019\n",
      "\t Params:  tensor([0.8640, 0.4283])\n",
      "\t Grad:  tensor([ 0.0153, -0.1143])\n",
      "Epoch 192, Loss 0.272886\n",
      "\t Params:  tensor([0.8638, 0.4295])\n",
      "\t Grad:  tensor([ 0.0152, -0.1135])\n",
      "Epoch 193, Loss 0.272756\n",
      "\t Params:  tensor([0.8637, 0.4306])\n",
      "\t Grad:  tensor([ 0.0151, -0.1128])\n",
      "Epoch 194, Loss 0.272627\n",
      "\t Params:  tensor([0.8635, 0.4317])\n",
      "\t Grad:  tensor([ 0.0150, -0.1120])\n",
      "Epoch 195, Loss 0.272499\n",
      "\t Params:  tensor([0.8634, 0.4328])\n",
      "\t Grad:  tensor([ 0.0149, -0.1113])\n",
      "Epoch 196, Loss 0.272374\n",
      "\t Params:  tensor([0.8632, 0.4339])\n",
      "\t Grad:  tensor([ 0.0148, -0.1106])\n",
      "Epoch 197, Loss 0.272250\n",
      "\t Params:  tensor([0.8631, 0.4350])\n",
      "\t Grad:  tensor([ 0.0147, -0.1098])\n",
      "Epoch 198, Loss 0.272127\n",
      "\t Params:  tensor([0.8629, 0.4361])\n",
      "\t Grad:  tensor([ 0.0146, -0.1091])\n",
      "Epoch 199, Loss 0.272006\n",
      "\t Params:  tensor([0.8628, 0.4372])\n",
      "\t Grad:  tensor([ 0.0145, -0.1084])\n",
      "Epoch 200, Loss 0.271887\n",
      "\t Params:  tensor([0.8626, 0.4383])\n",
      "\t Grad:  tensor([ 0.0144, -0.1077])\n",
      "Epoch 201, Loss 0.271770\n",
      "\t Params:  tensor([0.8625, 0.4394])\n",
      "\t Grad:  tensor([ 0.0143, -0.1070])\n",
      "Epoch 202, Loss 0.271654\n",
      "\t Params:  tensor([0.8623, 0.4404])\n",
      "\t Grad:  tensor([ 0.0142, -0.1062])\n",
      "Epoch 203, Loss 0.271539\n",
      "\t Params:  tensor([0.8622, 0.4415])\n",
      "\t Grad:  tensor([ 0.0141, -0.1055])\n",
      "Epoch 204, Loss 0.271426\n",
      "\t Params:  tensor([0.8621, 0.4425])\n",
      "\t Grad:  tensor([ 0.0140, -0.1048])\n",
      "Epoch 205, Loss 0.271315\n",
      "\t Params:  tensor([0.8619, 0.4436])\n",
      "\t Grad:  tensor([ 0.0139, -0.1041])\n",
      "Epoch 206, Loss 0.271205\n",
      "\t Params:  tensor([0.8618, 0.4446])\n",
      "\t Grad:  tensor([ 0.0138, -0.1035])\n",
      "Epoch 207, Loss 0.271096\n",
      "\t Params:  tensor([0.8616, 0.4456])\n",
      "\t Grad:  tensor([ 0.0137, -0.1028])\n",
      "Epoch 208, Loss 0.270989\n",
      "\t Params:  tensor([0.8615, 0.4466])\n",
      "\t Grad:  tensor([ 0.0137, -0.1021])\n",
      "Epoch 209, Loss 0.270883\n",
      "\t Params:  tensor([0.8614, 0.4477])\n",
      "\t Grad:  tensor([ 0.0136, -0.1014])\n",
      "Epoch 210, Loss 0.270779\n",
      "\t Params:  tensor([0.8612, 0.4487])\n",
      "\t Grad:  tensor([ 0.0135, -0.1007])\n",
      "Epoch 211, Loss 0.270676\n",
      "\t Params:  tensor([0.8611, 0.4497])\n",
      "\t Grad:  tensor([ 0.0134, -0.1001])\n",
      "Epoch 212, Loss 0.270574\n",
      "\t Params:  tensor([0.8610, 0.4507])\n",
      "\t Grad:  tensor([ 0.0133, -0.0994])\n",
      "Epoch 213, Loss 0.270474\n",
      "\t Params:  tensor([0.8608, 0.4516])\n",
      "\t Grad:  tensor([ 0.0132, -0.0988])\n",
      "Epoch 214, Loss 0.270375\n",
      "\t Params:  tensor([0.8607, 0.4526])\n",
      "\t Grad:  tensor([ 0.0131, -0.0981])\n",
      "Epoch 215, Loss 0.270277\n",
      "\t Params:  tensor([0.8606, 0.4536])\n",
      "\t Grad:  tensor([ 0.0130, -0.0974])\n",
      "Epoch 216, Loss 0.270181\n",
      "\t Params:  tensor([0.8605, 0.4546])\n",
      "\t Grad:  tensor([ 0.0129, -0.0968])\n",
      "Epoch 217, Loss 0.270086\n",
      "\t Params:  tensor([0.8603, 0.4555])\n",
      "\t Grad:  tensor([ 0.0129, -0.0962])\n",
      "Epoch 218, Loss 0.269992\n",
      "\t Params:  tensor([0.8602, 0.4565])\n",
      "\t Grad:  tensor([ 0.0128, -0.0955])\n",
      "Epoch 219, Loss 0.269899\n",
      "\t Params:  tensor([0.8601, 0.4574])\n",
      "\t Grad:  tensor([ 0.0127, -0.0949])\n",
      "Epoch 220, Loss 0.269808\n",
      "\t Params:  tensor([0.8599, 0.4584])\n",
      "\t Grad:  tensor([ 0.0126, -0.0943])\n",
      "Epoch 221, Loss 0.269718\n",
      "\t Params:  tensor([0.8598, 0.4593])\n",
      "\t Grad:  tensor([ 0.0125, -0.0936])\n",
      "Epoch 222, Loss 0.269629\n",
      "\t Params:  tensor([0.8597, 0.4602])\n",
      "\t Grad:  tensor([ 0.0124, -0.0930])\n",
      "Epoch 223, Loss 0.269541\n",
      "\t Params:  tensor([0.8596, 0.4612])\n",
      "\t Grad:  tensor([ 0.0124, -0.0924])\n",
      "Epoch 224, Loss 0.269455\n",
      "\t Params:  tensor([0.8594, 0.4621])\n",
      "\t Grad:  tensor([ 0.0123, -0.0918])\n",
      "Epoch 225, Loss 0.269369\n",
      "\t Params:  tensor([0.8593, 0.4630])\n",
      "\t Grad:  tensor([ 0.0122, -0.0912])\n",
      "Epoch 226, Loss 0.269285\n",
      "\t Params:  tensor([0.8592, 0.4639])\n",
      "\t Grad:  tensor([ 0.0121, -0.0906])\n",
      "Epoch 227, Loss 0.269202\n",
      "\t Params:  tensor([0.8591, 0.4648])\n",
      "\t Grad:  tensor([ 0.0120, -0.0900])\n",
      "Epoch 228, Loss 0.269119\n",
      "\t Params:  tensor([0.8590, 0.4657])\n",
      "\t Grad:  tensor([ 0.0119, -0.0894])\n",
      "Epoch 229, Loss 0.269038\n",
      "\t Params:  tensor([0.8588, 0.4666])\n",
      "\t Grad:  tensor([ 0.0119, -0.0888])\n",
      "Epoch 230, Loss 0.268958\n",
      "\t Params:  tensor([0.8587, 0.4675])\n",
      "\t Grad:  tensor([ 0.0118, -0.0882])\n",
      "Epoch 231, Loss 0.268879\n",
      "\t Params:  tensor([0.8586, 0.4683])\n",
      "\t Grad:  tensor([ 0.0117, -0.0876])\n",
      "Epoch 232, Loss 0.268802\n",
      "\t Params:  tensor([0.8585, 0.4692])\n",
      "\t Grad:  tensor([ 0.0116, -0.0870])\n",
      "Epoch 233, Loss 0.268725\n",
      "\t Params:  tensor([0.8584, 0.4701])\n",
      "\t Grad:  tensor([ 0.0116, -0.0865])\n",
      "Epoch 234, Loss 0.268649\n",
      "\t Params:  tensor([0.8583, 0.4709])\n",
      "\t Grad:  tensor([ 0.0115, -0.0859])\n",
      "Epoch 235, Loss 0.268574\n",
      "\t Params:  tensor([0.8582, 0.4718])\n",
      "\t Grad:  tensor([ 0.0114, -0.0853])\n",
      "Epoch 236, Loss 0.268500\n",
      "\t Params:  tensor([0.8580, 0.4726])\n",
      "\t Grad:  tensor([ 0.0113, -0.0848])\n",
      "Epoch 237, Loss 0.268427\n",
      "\t Params:  tensor([0.8579, 0.4735])\n",
      "\t Grad:  tensor([ 0.0113, -0.0842])\n",
      "Epoch 238, Loss 0.268355\n",
      "\t Params:  tensor([0.8578, 0.4743])\n",
      "\t Grad:  tensor([ 0.0112, -0.0836])\n",
      "Epoch 239, Loss 0.268285\n",
      "\t Params:  tensor([0.8577, 0.4751])\n",
      "\t Grad:  tensor([ 0.0111, -0.0831])\n",
      "Epoch 240, Loss 0.268215\n",
      "\t Params:  tensor([0.8576, 0.4760])\n",
      "\t Grad:  tensor([ 0.0110, -0.0825])\n",
      "Epoch 241, Loss 0.268145\n",
      "\t Params:  tensor([0.8575, 0.4768])\n",
      "\t Grad:  tensor([ 0.0110, -0.0820])\n",
      "Epoch 242, Loss 0.268077\n",
      "\t Params:  tensor([0.8574, 0.4776])\n",
      "\t Grad:  tensor([ 0.0109, -0.0814])\n",
      "Epoch 243, Loss 0.268010\n",
      "\t Params:  tensor([0.8573, 0.4784])\n",
      "\t Grad:  tensor([ 0.0108, -0.0809])\n",
      "Epoch 244, Loss 0.267943\n",
      "\t Params:  tensor([0.8572, 0.4792])\n",
      "\t Grad:  tensor([ 0.0107, -0.0804])\n",
      "Epoch 245, Loss 0.267878\n",
      "\t Params:  tensor([0.8571, 0.4800])\n",
      "\t Grad:  tensor([ 0.0107, -0.0798])\n",
      "Epoch 246, Loss 0.267813\n",
      "\t Params:  tensor([0.8569, 0.4808])\n",
      "\t Grad:  tensor([ 0.0106, -0.0793])\n",
      "Epoch 247, Loss 0.267749\n",
      "\t Params:  tensor([0.8568, 0.4816])\n",
      "\t Grad:  tensor([ 0.0105, -0.0788])\n",
      "Epoch 248, Loss 0.267687\n",
      "\t Params:  tensor([0.8567, 0.4824])\n",
      "\t Grad:  tensor([ 0.0105, -0.0783])\n",
      "Epoch 249, Loss 0.267624\n",
      "\t Params:  tensor([0.8566, 0.4832])\n",
      "\t Grad:  tensor([ 0.0104, -0.0777])\n",
      "Epoch 250, Loss 0.267563\n",
      "\t Params:  tensor([0.8565, 0.4839])\n",
      "\t Grad:  tensor([ 0.0103, -0.0772])\n",
      "Epoch 251, Loss 0.267503\n",
      "\t Params:  tensor([0.8564, 0.4847])\n",
      "\t Grad:  tensor([ 0.0103, -0.0767])\n",
      "Epoch 252, Loss 0.267443\n",
      "\t Params:  tensor([0.8563, 0.4855])\n",
      "\t Grad:  tensor([ 0.0102, -0.0762])\n",
      "Epoch 253, Loss 0.267384\n",
      "\t Params:  tensor([0.8562, 0.4862])\n",
      "\t Grad:  tensor([ 0.0101, -0.0757])\n",
      "Epoch 254, Loss 0.267326\n",
      "\t Params:  tensor([0.8561, 0.4870])\n",
      "\t Grad:  tensor([ 0.0101, -0.0752])\n",
      "Epoch 255, Loss 0.267269\n",
      "\t Params:  tensor([0.8560, 0.4877])\n",
      "\t Grad:  tensor([ 0.0100, -0.0747])\n",
      "Epoch 256, Loss 0.267212\n",
      "\t Params:  tensor([0.8559, 0.4885])\n",
      "\t Grad:  tensor([ 0.0099, -0.0742])\n",
      "Epoch 257, Loss 0.267156\n",
      "\t Params:  tensor([0.8558, 0.4892])\n",
      "\t Grad:  tensor([ 0.0099, -0.0737])\n",
      "Epoch 258, Loss 0.267101\n",
      "\t Params:  tensor([0.8557, 0.4899])\n",
      "\t Grad:  tensor([ 0.0098, -0.0732])\n",
      "Epoch 259, Loss 0.267047\n",
      "\t Params:  tensor([0.8556, 0.4907])\n",
      "\t Grad:  tensor([ 0.0097, -0.0727])\n",
      "Epoch 260, Loss 0.266993\n",
      "\t Params:  tensor([0.8555, 0.4914])\n",
      "\t Grad:  tensor([ 0.0097, -0.0723])\n",
      "Epoch 261, Loss 0.266940\n",
      "\t Params:  tensor([0.8554, 0.4921])\n",
      "\t Grad:  tensor([ 0.0096, -0.0718])\n",
      "Epoch 262, Loss 0.266888\n",
      "\t Params:  tensor([0.8553, 0.4928])\n",
      "\t Grad:  tensor([ 0.0095, -0.0713])\n",
      "Epoch 263, Loss 0.266836\n",
      "\t Params:  tensor([0.8552, 0.4935])\n",
      "\t Grad:  tensor([ 0.0095, -0.0708])\n",
      "Epoch 264, Loss 0.266785\n",
      "\t Params:  tensor([0.8552, 0.4942])\n",
      "\t Grad:  tensor([ 0.0094, -0.0704])\n",
      "Epoch 265, Loss 0.266735\n",
      "\t Params:  tensor([0.8551, 0.4949])\n",
      "\t Grad:  tensor([ 0.0093, -0.0699])\n",
      "Epoch 266, Loss 0.266685\n",
      "\t Params:  tensor([0.8550, 0.4956])\n",
      "\t Grad:  tensor([ 0.0093, -0.0694])\n",
      "Epoch 267, Loss 0.266636\n",
      "\t Params:  tensor([0.8549, 0.4963])\n",
      "\t Grad:  tensor([ 0.0092, -0.0690])\n",
      "Epoch 268, Loss 0.266588\n",
      "\t Params:  tensor([0.8548, 0.4970])\n",
      "\t Grad:  tensor([ 0.0092, -0.0685])\n",
      "Epoch 269, Loss 0.266541\n",
      "\t Params:  tensor([0.8547, 0.4977])\n",
      "\t Grad:  tensor([ 0.0091, -0.0681])\n",
      "Epoch 270, Loss 0.266494\n",
      "\t Params:  tensor([0.8546, 0.4983])\n",
      "\t Grad:  tensor([ 0.0090, -0.0676])\n",
      "Epoch 271, Loss 0.266447\n",
      "\t Params:  tensor([0.8545, 0.4990])\n",
      "\t Grad:  tensor([ 0.0090, -0.0672])\n",
      "Epoch 272, Loss 0.266401\n",
      "\t Params:  tensor([0.8544, 0.4997])\n",
      "\t Grad:  tensor([ 0.0089, -0.0667])\n",
      "Epoch 273, Loss 0.266356\n",
      "\t Params:  tensor([0.8543, 0.5003])\n",
      "\t Grad:  tensor([ 0.0089, -0.0663])\n",
      "Epoch 274, Loss 0.266312\n",
      "\t Params:  tensor([0.8542, 0.5010])\n",
      "\t Grad:  tensor([ 0.0088, -0.0658])\n",
      "Epoch 275, Loss 0.266268\n",
      "\t Params:  tensor([0.8542, 0.5017])\n",
      "\t Grad:  tensor([ 0.0087, -0.0654])\n",
      "Epoch 276, Loss 0.266224\n",
      "\t Params:  tensor([0.8541, 0.5023])\n",
      "\t Grad:  tensor([ 0.0087, -0.0650])\n",
      "Epoch 277, Loss 0.266182\n",
      "\t Params:  tensor([0.8540, 0.5030])\n",
      "\t Grad:  tensor([ 0.0086, -0.0645])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 278, Loss 0.266139\n",
      "\t Params:  tensor([0.8539, 0.5036])\n",
      "\t Grad:  tensor([ 0.0086, -0.0641])\n",
      "Epoch 279, Loss 0.266098\n",
      "\t Params:  tensor([0.8538, 0.5042])\n",
      "\t Grad:  tensor([ 0.0085, -0.0637])\n",
      "Epoch 280, Loss 0.266056\n",
      "\t Params:  tensor([0.8537, 0.5049])\n",
      "\t Grad:  tensor([ 0.0085, -0.0633])\n",
      "Epoch 281, Loss 0.266016\n",
      "\t Params:  tensor([0.8536, 0.5055])\n",
      "\t Grad:  tensor([ 0.0084, -0.0628])\n",
      "Epoch 282, Loss 0.265976\n",
      "\t Params:  tensor([0.8536, 0.5061])\n",
      "\t Grad:  tensor([ 0.0083, -0.0624])\n",
      "Epoch 283, Loss 0.265936\n",
      "\t Params:  tensor([0.8535, 0.5067])\n",
      "\t Grad:  tensor([ 0.0083, -0.0620])\n",
      "Epoch 284, Loss 0.265897\n",
      "\t Params:  tensor([0.8534, 0.5074])\n",
      "\t Grad:  tensor([ 0.0082, -0.0616])\n",
      "Epoch 285, Loss 0.265859\n",
      "\t Params:  tensor([0.8533, 0.5080])\n",
      "\t Grad:  tensor([ 0.0082, -0.0612])\n",
      "Epoch 286, Loss 0.265821\n",
      "\t Params:  tensor([0.8532, 0.5086])\n",
      "\t Grad:  tensor([ 0.0081, -0.0608])\n",
      "Epoch 287, Loss 0.265783\n",
      "\t Params:  tensor([0.8532, 0.5092])\n",
      "\t Grad:  tensor([ 0.0081, -0.0604])\n",
      "Epoch 288, Loss 0.265746\n",
      "\t Params:  tensor([0.8531, 0.5098])\n",
      "\t Grad:  tensor([ 0.0080, -0.0600])\n",
      "Epoch 289, Loss 0.265710\n",
      "\t Params:  tensor([0.8530, 0.5104])\n",
      "\t Grad:  tensor([ 0.0080, -0.0596])\n",
      "Epoch 290, Loss 0.265674\n",
      "\t Params:  tensor([0.8529, 0.5110])\n",
      "\t Grad:  tensor([ 0.0079, -0.0592])\n",
      "Epoch 291, Loss 0.265638\n",
      "\t Params:  tensor([0.8528, 0.5116])\n",
      "\t Grad:  tensor([ 0.0079, -0.0588])\n",
      "Epoch 292, Loss 0.265603\n",
      "\t Params:  tensor([0.8528, 0.5121])\n",
      "\t Grad:  tensor([ 0.0078, -0.0584])\n",
      "Epoch 293, Loss 0.265569\n",
      "\t Params:  tensor([0.8527, 0.5127])\n",
      "\t Grad:  tensor([ 0.0078, -0.0580])\n",
      "Epoch 294, Loss 0.265534\n",
      "\t Params:  tensor([0.8526, 0.5133])\n",
      "\t Grad:  tensor([ 0.0077, -0.0576])\n",
      "Epoch 295, Loss 0.265501\n",
      "\t Params:  tensor([0.8525, 0.5139])\n",
      "\t Grad:  tensor([ 0.0077, -0.0573])\n",
      "Epoch 296, Loss 0.265467\n",
      "\t Params:  tensor([0.8524, 0.5144])\n",
      "\t Grad:  tensor([ 0.0076, -0.0569])\n",
      "Epoch 297, Loss 0.265435\n",
      "\t Params:  tensor([0.8524, 0.5150])\n",
      "\t Grad:  tensor([ 0.0076, -0.0565])\n",
      "Epoch 298, Loss 0.265402\n",
      "\t Params:  tensor([0.8523, 0.5156])\n",
      "\t Grad:  tensor([ 0.0075, -0.0561])\n",
      "Epoch 299, Loss 0.265370\n",
      "\t Params:  tensor([0.8522, 0.5161])\n",
      "\t Grad:  tensor([ 0.0075, -0.0558])\n",
      "Epoch 300, Loss 0.265339\n",
      "\t Params:  tensor([0.8521, 0.5167])\n",
      "\t Grad:  tensor([ 0.0074, -0.0554])\n",
      "Epoch 301, Loss 0.265308\n",
      "\t Params:  tensor([0.8521, 0.5172])\n",
      "\t Grad:  tensor([ 0.0074, -0.0550])\n",
      "Epoch 302, Loss 0.265277\n",
      "\t Params:  tensor([0.8520, 0.5178])\n",
      "\t Grad:  tensor([ 0.0073, -0.0547])\n",
      "Epoch 303, Loss 0.265247\n",
      "\t Params:  tensor([0.8519, 0.5183])\n",
      "\t Grad:  tensor([ 0.0073, -0.0543])\n",
      "Epoch 304, Loss 0.265217\n",
      "\t Params:  tensor([0.8519, 0.5188])\n",
      "\t Grad:  tensor([ 0.0072, -0.0539])\n",
      "Epoch 305, Loss 0.265187\n",
      "\t Params:  tensor([0.8518, 0.5194])\n",
      "\t Grad:  tensor([ 0.0072, -0.0536])\n",
      "Epoch 306, Loss 0.265158\n",
      "\t Params:  tensor([0.8517, 0.5199])\n",
      "\t Grad:  tensor([ 0.0071, -0.0532])\n",
      "Epoch 307, Loss 0.265129\n",
      "\t Params:  tensor([0.8516, 0.5204])\n",
      "\t Grad:  tensor([ 0.0071, -0.0529])\n",
      "Epoch 308, Loss 0.265101\n",
      "\t Params:  tensor([0.8516, 0.5210])\n",
      "\t Grad:  tensor([ 0.0070, -0.0525])\n",
      "Epoch 309, Loss 0.265073\n",
      "\t Params:  tensor([0.8515, 0.5215])\n",
      "\t Grad:  tensor([ 0.0070, -0.0522])\n",
      "Epoch 310, Loss 0.265045\n",
      "\t Params:  tensor([0.8514, 0.5220])\n",
      "\t Grad:  tensor([ 0.0069, -0.0518])\n",
      "Epoch 311, Loss 0.265018\n",
      "\t Params:  tensor([0.8514, 0.5225])\n",
      "\t Grad:  tensor([ 0.0069, -0.0515])\n",
      "Epoch 312, Loss 0.264991\n",
      "\t Params:  tensor([0.8513, 0.5230])\n",
      "\t Grad:  tensor([ 0.0068, -0.0511])\n",
      "Epoch 313, Loss 0.264965\n",
      "\t Params:  tensor([0.8512, 0.5235])\n",
      "\t Grad:  tensor([ 0.0068, -0.0508])\n",
      "Epoch 314, Loss 0.264939\n",
      "\t Params:  tensor([0.8512, 0.5240])\n",
      "\t Grad:  tensor([ 0.0067, -0.0505])\n",
      "Epoch 315, Loss 0.264913\n",
      "\t Params:  tensor([0.8511, 0.5246])\n",
      "\t Grad:  tensor([ 0.0067, -0.0501])\n",
      "Epoch 316, Loss 0.264887\n",
      "\t Params:  tensor([0.8510, 0.5250])\n",
      "\t Grad:  tensor([ 0.0067, -0.0498])\n",
      "Epoch 317, Loss 0.264862\n",
      "\t Params:  tensor([0.8510, 0.5255])\n",
      "\t Grad:  tensor([ 0.0066, -0.0495])\n",
      "Epoch 318, Loss 0.264837\n",
      "\t Params:  tensor([0.8509, 0.5260])\n",
      "\t Grad:  tensor([ 0.0066, -0.0491])\n",
      "Epoch 319, Loss 0.264813\n",
      "\t Params:  tensor([0.8508, 0.5265])\n",
      "\t Grad:  tensor([ 0.0065, -0.0488])\n",
      "Epoch 320, Loss 0.264789\n",
      "\t Params:  tensor([0.8508, 0.5270])\n",
      "\t Grad:  tensor([ 0.0065, -0.0485])\n",
      "Epoch 321, Loss 0.264765\n",
      "\t Params:  tensor([0.8507, 0.5275])\n",
      "\t Grad:  tensor([ 0.0064, -0.0482])\n",
      "Epoch 322, Loss 0.264741\n",
      "\t Params:  tensor([0.8506, 0.5280])\n",
      "\t Grad:  tensor([ 0.0064, -0.0478])\n",
      "Epoch 323, Loss 0.264718\n",
      "\t Params:  tensor([0.8506, 0.5284])\n",
      "\t Grad:  tensor([ 0.0064, -0.0475])\n",
      "Epoch 324, Loss 0.264695\n",
      "\t Params:  tensor([0.8505, 0.5289])\n",
      "\t Grad:  tensor([ 0.0063, -0.0472])\n",
      "Epoch 325, Loss 0.264672\n",
      "\t Params:  tensor([0.8504, 0.5294])\n",
      "\t Grad:  tensor([ 0.0063, -0.0469])\n",
      "Epoch 326, Loss 0.264650\n",
      "\t Params:  tensor([0.8504, 0.5299])\n",
      "\t Grad:  tensor([ 0.0062, -0.0466])\n",
      "Epoch 327, Loss 0.264628\n",
      "\t Params:  tensor([0.8503, 0.5303])\n",
      "\t Grad:  tensor([ 0.0062, -0.0463])\n",
      "Epoch 328, Loss 0.264606\n",
      "\t Params:  tensor([0.8503, 0.5308])\n",
      "\t Grad:  tensor([ 0.0061, -0.0460])\n",
      "Epoch 329, Loss 0.264585\n",
      "\t Params:  tensor([0.8502, 0.5312])\n",
      "\t Grad:  tensor([ 0.0061, -0.0457])\n",
      "Epoch 330, Loss 0.264564\n",
      "\t Params:  tensor([0.8501, 0.5317])\n",
      "\t Grad:  tensor([ 0.0061, -0.0454])\n",
      "Epoch 331, Loss 0.264543\n",
      "\t Params:  tensor([0.8501, 0.5321])\n",
      "\t Grad:  tensor([ 0.0060, -0.0451])\n",
      "Epoch 332, Loss 0.264522\n",
      "\t Params:  tensor([0.8500, 0.5326])\n",
      "\t Grad:  tensor([ 0.0060, -0.0448])\n",
      "Epoch 333, Loss 0.264502\n",
      "\t Params:  tensor([0.8500, 0.5330])\n",
      "\t Grad:  tensor([ 0.0059, -0.0445])\n",
      "Epoch 334, Loss 0.264482\n",
      "\t Params:  tensor([0.8499, 0.5335])\n",
      "\t Grad:  tensor([ 0.0059, -0.0442])\n",
      "Epoch 335, Loss 0.264462\n",
      "\t Params:  tensor([0.8498, 0.5339])\n",
      "\t Grad:  tensor([ 0.0059, -0.0439])\n",
      "Epoch 336, Loss 0.264443\n",
      "\t Params:  tensor([0.8498, 0.5343])\n",
      "\t Grad:  tensor([ 0.0058, -0.0436])\n",
      "Epoch 337, Loss 0.264423\n",
      "\t Params:  tensor([0.8497, 0.5348])\n",
      "\t Grad:  tensor([ 0.0058, -0.0433])\n",
      "Epoch 338, Loss 0.264404\n",
      "\t Params:  tensor([0.8497, 0.5352])\n",
      "\t Grad:  tensor([ 0.0058, -0.0430])\n",
      "Epoch 339, Loss 0.264385\n",
      "\t Params:  tensor([0.8496, 0.5356])\n",
      "\t Grad:  tensor([ 0.0057, -0.0427])\n",
      "Epoch 340, Loss 0.264367\n",
      "\t Params:  tensor([0.8496, 0.5361])\n",
      "\t Grad:  tensor([ 0.0057, -0.0425])\n",
      "Epoch 341, Loss 0.264349\n",
      "\t Params:  tensor([0.8495, 0.5365])\n",
      "\t Grad:  tensor([ 0.0056, -0.0422])\n",
      "Epoch 342, Loss 0.264331\n",
      "\t Params:  tensor([0.8494, 0.5369])\n",
      "\t Grad:  tensor([ 0.0056, -0.0419])\n",
      "Epoch 343, Loss 0.264313\n",
      "\t Params:  tensor([0.8494, 0.5373])\n",
      "\t Grad:  tensor([ 0.0056, -0.0416])\n",
      "Epoch 344, Loss 0.264295\n",
      "\t Params:  tensor([0.8493, 0.5377])\n",
      "\t Grad:  tensor([ 0.0055, -0.0413])\n",
      "Epoch 345, Loss 0.264278\n",
      "\t Params:  tensor([0.8493, 0.5381])\n",
      "\t Grad:  tensor([ 0.0055, -0.0411])\n",
      "Epoch 346, Loss 0.264261\n",
      "\t Params:  tensor([0.8492, 0.5385])\n",
      "\t Grad:  tensor([ 0.0055, -0.0408])\n",
      "Epoch 347, Loss 0.264244\n",
      "\t Params:  tensor([0.8492, 0.5390])\n",
      "\t Grad:  tensor([ 0.0054, -0.0405])\n",
      "Epoch 348, Loss 0.264227\n",
      "\t Params:  tensor([0.8491, 0.5394])\n",
      "\t Grad:  tensor([ 0.0054, -0.0403])\n",
      "Epoch 349, Loss 0.264211\n",
      "\t Params:  tensor([0.8491, 0.5398])\n",
      "\t Grad:  tensor([ 0.0053, -0.0400])\n",
      "Epoch 350, Loss 0.264195\n",
      "\t Params:  tensor([0.8490, 0.5402])\n",
      "\t Grad:  tensor([ 0.0053, -0.0397])\n",
      "Epoch 351, Loss 0.264179\n",
      "\t Params:  tensor([0.8490, 0.5405])\n",
      "\t Grad:  tensor([ 0.0053, -0.0395])\n",
      "Epoch 352, Loss 0.264163\n",
      "\t Params:  tensor([0.8489, 0.5409])\n",
      "\t Grad:  tensor([ 0.0052, -0.0392])\n",
      "Epoch 353, Loss 0.264147\n",
      "\t Params:  tensor([0.8489, 0.5413])\n",
      "\t Grad:  tensor([ 0.0052, -0.0389])\n",
      "Epoch 354, Loss 0.264132\n",
      "\t Params:  tensor([0.8488, 0.5417])\n",
      "\t Grad:  tensor([ 0.0052, -0.0387])\n",
      "Epoch 355, Loss 0.264117\n",
      "\t Params:  tensor([0.8487, 0.5421])\n",
      "\t Grad:  tensor([ 0.0051, -0.0384])\n",
      "Epoch 356, Loss 0.264102\n",
      "\t Params:  tensor([0.8487, 0.5425])\n",
      "\t Grad:  tensor([ 0.0051, -0.0382])\n",
      "Epoch 357, Loss 0.264087\n",
      "\t Params:  tensor([0.8486, 0.5429])\n",
      "\t Grad:  tensor([ 0.0051, -0.0379])\n",
      "Epoch 358, Loss 0.264072\n",
      "\t Params:  tensor([0.8486, 0.5432])\n",
      "\t Grad:  tensor([ 0.0050, -0.0377])\n",
      "Epoch 359, Loss 0.264058\n",
      "\t Params:  tensor([0.8485, 0.5436])\n",
      "\t Grad:  tensor([ 0.0050, -0.0374])\n",
      "Epoch 360, Loss 0.264044\n",
      "\t Params:  tensor([0.8485, 0.5440])\n",
      "\t Grad:  tensor([ 0.0050, -0.0372])\n",
      "Epoch 361, Loss 0.264030\n",
      "\t Params:  tensor([0.8484, 0.5444])\n",
      "\t Grad:  tensor([ 0.0049, -0.0369])\n",
      "Epoch 362, Loss 0.264016\n",
      "\t Params:  tensor([0.8484, 0.5447])\n",
      "\t Grad:  tensor([ 0.0049, -0.0367])\n",
      "Epoch 363, Loss 0.264002\n",
      "\t Params:  tensor([0.8483, 0.5451])\n",
      "\t Grad:  tensor([ 0.0049, -0.0364])\n",
      "Epoch 364, Loss 0.263989\n",
      "\t Params:  tensor([0.8483, 0.5454])\n",
      "\t Grad:  tensor([ 0.0048, -0.0362])\n",
      "Epoch 365, Loss 0.263975\n",
      "\t Params:  tensor([0.8483, 0.5458])\n",
      "\t Grad:  tensor([ 0.0048, -0.0360])\n",
      "Epoch 366, Loss 0.263962\n",
      "\t Params:  tensor([0.8482, 0.5462])\n",
      "\t Grad:  tensor([ 0.0048, -0.0357])\n",
      "Epoch 367, Loss 0.263949\n",
      "\t Params:  tensor([0.8482, 0.5465])\n",
      "\t Grad:  tensor([ 0.0047, -0.0355])\n",
      "Epoch 368, Loss 0.263937\n",
      "\t Params:  tensor([0.8481, 0.5469])\n",
      "\t Grad:  tensor([ 0.0047, -0.0352])\n",
      "Epoch 369, Loss 0.263924\n",
      "\t Params:  tensor([0.8481, 0.5472])\n",
      "\t Grad:  tensor([ 0.0047, -0.0350])\n",
      "Epoch 370, Loss 0.263912\n",
      "\t Params:  tensor([0.8480, 0.5476])\n",
      "\t Grad:  tensor([ 0.0046, -0.0348])\n",
      "Epoch 371, Loss 0.263899\n",
      "\t Params:  tensor([0.8480, 0.5479])\n",
      "\t Grad:  tensor([ 0.0046, -0.0345])\n",
      "Epoch 372, Loss 0.263887\n",
      "\t Params:  tensor([0.8479, 0.5483])\n",
      "\t Grad:  tensor([ 0.0046, -0.0343])\n",
      "Epoch 373, Loss 0.263875\n",
      "\t Params:  tensor([0.8479, 0.5486])\n",
      "\t Grad:  tensor([ 0.0046, -0.0341])\n",
      "Epoch 374, Loss 0.263863\n",
      "\t Params:  tensor([0.8478, 0.5489])\n",
      "\t Grad:  tensor([ 0.0045, -0.0339])\n",
      "Epoch 375, Loss 0.263852\n",
      "\t Params:  tensor([0.8478, 0.5493])\n",
      "\t Grad:  tensor([ 0.0045, -0.0336])\n",
      "Epoch 376, Loss 0.263840\n",
      "\t Params:  tensor([0.8477, 0.5496])\n",
      "\t Grad:  tensor([ 0.0045, -0.0334])\n",
      "Epoch 377, Loss 0.263829\n",
      "\t Params:  tensor([0.8477, 0.5499])\n",
      "\t Grad:  tensor([ 0.0044, -0.0332])\n",
      "Epoch 378, Loss 0.263818\n",
      "\t Params:  tensor([0.8477, 0.5503])\n",
      "\t Grad:  tensor([ 0.0044, -0.0330])\n",
      "Epoch 379, Loss 0.263807\n",
      "\t Params:  tensor([0.8476, 0.5506])\n",
      "\t Grad:  tensor([ 0.0044, -0.0328])\n",
      "Epoch 380, Loss 0.263796\n",
      "\t Params:  tensor([0.8476, 0.5509])\n",
      "\t Grad:  tensor([ 0.0044, -0.0325])\n",
      "Epoch 381, Loss 0.263785\n",
      "\t Params:  tensor([0.8475, 0.5512])\n",
      "\t Grad:  tensor([ 0.0043, -0.0323])\n",
      "Epoch 382, Loss 0.263775\n",
      "\t Params:  tensor([0.8475, 0.5516])\n",
      "\t Grad:  tensor([ 0.0043, -0.0321])\n",
      "Epoch 383, Loss 0.263764\n",
      "\t Params:  tensor([0.8474, 0.5519])\n",
      "\t Grad:  tensor([ 0.0043, -0.0319])\n",
      "Epoch 384, Loss 0.263754\n",
      "\t Params:  tensor([0.8474, 0.5522])\n",
      "\t Grad:  tensor([ 0.0042, -0.0317])\n",
      "Epoch 385, Loss 0.263744\n",
      "\t Params:  tensor([0.8474, 0.5525])\n",
      "\t Grad:  tensor([ 0.0042, -0.0315])\n",
      "Epoch 386, Loss 0.263733\n",
      "\t Params:  tensor([0.8473, 0.5528])\n",
      "\t Grad:  tensor([ 0.0042, -0.0313])\n",
      "Epoch 387, Loss 0.263724\n",
      "\t Params:  tensor([0.8473, 0.5531])\n",
      "\t Grad:  tensor([ 0.0042, -0.0311])\n",
      "Epoch 388, Loss 0.263714\n",
      "\t Params:  tensor([0.8472, 0.5534])\n",
      "\t Grad:  tensor([ 0.0041, -0.0309])\n",
      "Epoch 389, Loss 0.263704\n",
      "\t Params:  tensor([0.8472, 0.5538])\n",
      "\t Grad:  tensor([ 0.0041, -0.0307])\n",
      "Epoch 390, Loss 0.263695\n",
      "\t Params:  tensor([0.8471, 0.5541])\n",
      "\t Grad:  tensor([ 0.0041, -0.0304])\n",
      "Epoch 391, Loss 0.263685\n",
      "\t Params:  tensor([0.8471, 0.5544])\n",
      "\t Grad:  tensor([ 0.0040, -0.0302])\n",
      "Epoch 392, Loss 0.263676\n",
      "\t Params:  tensor([0.8471, 0.5547])\n",
      "\t Grad:  tensor([ 0.0040, -0.0300])\n",
      "Epoch 393, Loss 0.263667\n",
      "\t Params:  tensor([0.8470, 0.5550])\n",
      "\t Grad:  tensor([ 0.0040, -0.0298])\n",
      "Epoch 394, Loss 0.263658\n",
      "\t Params:  tensor([0.8470, 0.5553])\n",
      "\t Grad:  tensor([ 0.0040, -0.0296])\n",
      "Epoch 395, Loss 0.263649\n",
      "\t Params:  tensor([0.8470, 0.5556])\n",
      "\t Grad:  tensor([ 0.0039, -0.0295])\n",
      "Epoch 396, Loss 0.263640\n",
      "\t Params:  tensor([0.8469, 0.5558])\n",
      "\t Grad:  tensor([ 0.0039, -0.0293])\n",
      "Epoch 397, Loss 0.263631\n",
      "\t Params:  tensor([0.8469, 0.5561])\n",
      "\t Grad:  tensor([ 0.0039, -0.0291])\n",
      "Epoch 398, Loss 0.263623\n",
      "\t Params:  tensor([0.8468, 0.5564])\n",
      "\t Grad:  tensor([ 0.0039, -0.0289])\n",
      "Epoch 399, Loss 0.263614\n",
      "\t Params:  tensor([0.8468, 0.5567])\n",
      "\t Grad:  tensor([ 0.0038, -0.0287])\n",
      "Epoch 400, Loss 0.263606\n",
      "\t Params:  tensor([0.8468, 0.5570])\n",
      "\t Grad:  tensor([ 0.0038, -0.0285])\n",
      "Epoch 401, Loss 0.263598\n",
      "\t Params:  tensor([0.8467, 0.5573])\n",
      "\t Grad:  tensor([ 0.0038, -0.0283])\n",
      "Epoch 402, Loss 0.263590\n",
      "\t Params:  tensor([0.8467, 0.5576])\n",
      "\t Grad:  tensor([ 0.0038, -0.0281])\n",
      "Epoch 403, Loss 0.263582\n",
      "\t Params:  tensor([0.8466, 0.5578])\n",
      "\t Grad:  tensor([ 0.0037, -0.0279])\n",
      "Epoch 404, Loss 0.263574\n",
      "\t Params:  tensor([0.8466, 0.5581])\n",
      "\t Grad:  tensor([ 0.0037, -0.0277])\n",
      "Epoch 405, Loss 0.263566\n",
      "\t Params:  tensor([0.8466, 0.5584])\n",
      "\t Grad:  tensor([ 0.0037, -0.0276])\n",
      "Epoch 406, Loss 0.263558\n",
      "\t Params:  tensor([0.8465, 0.5587])\n",
      "\t Grad:  tensor([ 0.0037, -0.0274])\n",
      "Epoch 407, Loss 0.263551\n",
      "\t Params:  tensor([0.8465, 0.5589])\n",
      "\t Grad:  tensor([ 0.0036, -0.0272])\n",
      "Epoch 408, Loss 0.263543\n",
      "\t Params:  tensor([0.8465, 0.5592])\n",
      "\t Grad:  tensor([ 0.0036, -0.0270])\n",
      "Epoch 409, Loss 0.263536\n",
      "\t Params:  tensor([0.8464, 0.5595])\n",
      "\t Grad:  tensor([ 0.0036, -0.0268])\n",
      "Epoch 410, Loss 0.263528\n",
      "\t Params:  tensor([0.8464, 0.5597])\n",
      "\t Grad:  tensor([ 0.0036, -0.0267])\n",
      "Epoch 411, Loss 0.263521\n",
      "\t Params:  tensor([0.8464, 0.5600])\n",
      "\t Grad:  tensor([ 0.0035, -0.0265])\n",
      "Epoch 412, Loss 0.263514\n",
      "\t Params:  tensor([0.8463, 0.5603])\n",
      "\t Grad:  tensor([ 0.0035, -0.0263])\n",
      "Epoch 413, Loss 0.263507\n",
      "\t Params:  tensor([0.8463, 0.5605])\n",
      "\t Grad:  tensor([ 0.0035, -0.0261])\n",
      "Epoch 414, Loss 0.263500\n",
      "\t Params:  tensor([0.8462, 0.5608])\n",
      "\t Grad:  tensor([ 0.0035, -0.0260])\n",
      "Epoch 415, Loss 0.263493\n",
      "\t Params:  tensor([0.8462, 0.5610])\n",
      "\t Grad:  tensor([ 0.0034, -0.0258])\n",
      "Epoch 416, Loss 0.263486\n",
      "\t Params:  tensor([0.8462, 0.5613])\n",
      "\t Grad:  tensor([ 0.0034, -0.0256])\n",
      "Epoch 417, Loss 0.263480\n",
      "\t Params:  tensor([0.8461, 0.5616])\n",
      "\t Grad:  tensor([ 0.0034, -0.0254])\n",
      "Epoch 418, Loss 0.263473\n",
      "\t Params:  tensor([0.8461, 0.5618])\n",
      "\t Grad:  tensor([ 0.0034, -0.0253])\n",
      "Epoch 419, Loss 0.263467\n",
      "\t Params:  tensor([0.8461, 0.5621])\n",
      "\t Grad:  tensor([ 0.0034, -0.0251])\n",
      "Epoch 420, Loss 0.263460\n",
      "\t Params:  tensor([0.8460, 0.5623])\n",
      "\t Grad:  tensor([ 0.0033, -0.0249])\n",
      "Epoch 421, Loss 0.263454\n",
      "\t Params:  tensor([0.8460, 0.5626])\n",
      "\t Grad:  tensor([ 0.0033, -0.0248])\n",
      "Epoch 422, Loss 0.263448\n",
      "\t Params:  tensor([0.8460, 0.5628])\n",
      "\t Grad:  tensor([ 0.0033, -0.0246])\n",
      "Epoch 423, Loss 0.263442\n",
      "\t Params:  tensor([0.8459, 0.5631])\n",
      "\t Grad:  tensor([ 0.0033, -0.0245])\n",
      "Epoch 424, Loss 0.263436\n",
      "\t Params:  tensor([0.8459, 0.5633])\n",
      "\t Grad:  tensor([ 0.0032, -0.0243])\n",
      "Epoch 425, Loss 0.263430\n",
      "\t Params:  tensor([0.8459, 0.5635])\n",
      "\t Grad:  tensor([ 0.0032, -0.0241])\n",
      "Epoch 426, Loss 0.263424\n",
      "\t Params:  tensor([0.8459, 0.5638])\n",
      "\t Grad:  tensor([ 0.0032, -0.0240])\n",
      "Epoch 427, Loss 0.263418\n",
      "\t Params:  tensor([0.8458, 0.5640])\n",
      "\t Grad:  tensor([ 0.0032, -0.0238])\n",
      "Epoch 428, Loss 0.263412\n",
      "\t Params:  tensor([0.8458, 0.5642])\n",
      "\t Grad:  tensor([ 0.0032, -0.0237])\n",
      "Epoch 429, Loss 0.263406\n",
      "\t Params:  tensor([0.8458, 0.5645])\n",
      "\t Grad:  tensor([ 0.0031, -0.0235])\n",
      "Epoch 430, Loss 0.263401\n",
      "\t Params:  tensor([0.8457, 0.5647])\n",
      "\t Grad:  tensor([ 0.0031, -0.0233])\n",
      "Epoch 431, Loss 0.263395\n",
      "\t Params:  tensor([0.8457, 0.5650])\n",
      "\t Grad:  tensor([ 0.0031, -0.0232])\n",
      "Epoch 432, Loss 0.263390\n",
      "\t Params:  tensor([0.8457, 0.5652])\n",
      "\t Grad:  tensor([ 0.0031, -0.0230])\n",
      "Epoch 433, Loss 0.263384\n",
      "\t Params:  tensor([0.8456, 0.5654])\n",
      "\t Grad:  tensor([ 0.0031, -0.0229])\n",
      "Epoch 434, Loss 0.263379\n",
      "\t Params:  tensor([0.8456, 0.5656])\n",
      "\t Grad:  tensor([ 0.0030, -0.0227])\n",
      "Epoch 435, Loss 0.263374\n",
      "\t Params:  tensor([0.8456, 0.5659])\n",
      "\t Grad:  tensor([ 0.0030, -0.0226])\n",
      "Epoch 436, Loss 0.263369\n",
      "\t Params:  tensor([0.8455, 0.5661])\n",
      "\t Grad:  tensor([ 0.0030, -0.0224])\n",
      "Epoch 437, Loss 0.263364\n",
      "\t Params:  tensor([0.8455, 0.5663])\n",
      "\t Grad:  tensor([ 0.0030, -0.0223])\n",
      "Epoch 438, Loss 0.263359\n",
      "\t Params:  tensor([0.8455, 0.5665])\n",
      "\t Grad:  tensor([ 0.0030, -0.0221])\n",
      "Epoch 439, Loss 0.263354\n",
      "\t Params:  tensor([0.8455, 0.5668])\n",
      "\t Grad:  tensor([ 0.0029, -0.0220])\n",
      "Epoch 440, Loss 0.263349\n",
      "\t Params:  tensor([0.8454, 0.5670])\n",
      "\t Grad:  tensor([ 0.0029, -0.0218])\n",
      "Epoch 441, Loss 0.263344\n",
      "\t Params:  tensor([0.8454, 0.5672])\n",
      "\t Grad:  tensor([ 0.0029, -0.0217])\n",
      "Epoch 442, Loss 0.263339\n",
      "\t Params:  tensor([0.8454, 0.5674])\n",
      "\t Grad:  tensor([ 0.0029, -0.0215])\n",
      "Epoch 443, Loss 0.263334\n",
      "\t Params:  tensor([0.8453, 0.5676])\n",
      "\t Grad:  tensor([ 0.0029, -0.0214])\n",
      "Epoch 444, Loss 0.263330\n",
      "\t Params:  tensor([0.8453, 0.5678])\n",
      "\t Grad:  tensor([ 0.0028, -0.0213])\n",
      "Epoch 445, Loss 0.263325\n",
      "\t Params:  tensor([0.8453, 0.5680])\n",
      "\t Grad:  tensor([ 0.0028, -0.0211])\n",
      "Epoch 446, Loss 0.263321\n",
      "\t Params:  tensor([0.8453, 0.5682])\n",
      "\t Grad:  tensor([ 0.0028, -0.0210])\n",
      "Epoch 447, Loss 0.263316\n",
      "\t Params:  tensor([0.8452, 0.5685])\n",
      "\t Grad:  tensor([ 0.0028, -0.0208])\n",
      "Epoch 448, Loss 0.263312\n",
      "\t Params:  tensor([0.8452, 0.5687])\n",
      "\t Grad:  tensor([ 0.0028, -0.0207])\n",
      "Epoch 449, Loss 0.263308\n",
      "\t Params:  tensor([0.8452, 0.5689])\n",
      "\t Grad:  tensor([ 0.0027, -0.0206])\n",
      "Epoch 450, Loss 0.263303\n",
      "\t Params:  tensor([0.8451, 0.5691])\n",
      "\t Grad:  tensor([ 0.0027, -0.0204])\n",
      "Epoch 451, Loss 0.263299\n",
      "\t Params:  tensor([0.8451, 0.5693])\n",
      "\t Grad:  tensor([ 0.0027, -0.0203])\n",
      "Epoch 452, Loss 0.263295\n",
      "\t Params:  tensor([0.8451, 0.5695])\n",
      "\t Grad:  tensor([ 0.0027, -0.0202])\n",
      "Epoch 453, Loss 0.263291\n",
      "\t Params:  tensor([0.8451, 0.5697])\n",
      "\t Grad:  tensor([ 0.0027, -0.0200])\n",
      "Epoch 454, Loss 0.263287\n",
      "\t Params:  tensor([0.8450, 0.5699])\n",
      "\t Grad:  tensor([ 0.0027, -0.0199])\n",
      "Epoch 455, Loss 0.263282\n",
      "\t Params:  tensor([0.8450, 0.5701])\n",
      "\t Grad:  tensor([ 0.0026, -0.0198])\n",
      "Epoch 456, Loss 0.263279\n",
      "\t Params:  tensor([0.8450, 0.5703])\n",
      "\t Grad:  tensor([ 0.0026, -0.0196])\n",
      "Epoch 457, Loss 0.263275\n",
      "\t Params:  tensor([0.8450, 0.5705])\n",
      "\t Grad:  tensor([ 0.0026, -0.0195])\n",
      "Epoch 458, Loss 0.263271\n",
      "\t Params:  tensor([0.8449, 0.5707])\n",
      "\t Grad:  tensor([ 0.0026, -0.0194])\n",
      "Epoch 459, Loss 0.263267\n",
      "\t Params:  tensor([0.8449, 0.5709])\n",
      "\t Grad:  tensor([ 0.0026, -0.0192])\n",
      "Epoch 460, Loss 0.263263\n",
      "\t Params:  tensor([0.8449, 0.5710])\n",
      "\t Grad:  tensor([ 0.0026, -0.0191])\n",
      "Epoch 461, Loss 0.263260\n",
      "\t Params:  tensor([0.8449, 0.5712])\n",
      "\t Grad:  tensor([ 0.0025, -0.0190])\n",
      "Epoch 462, Loss 0.263256\n",
      "\t Params:  tensor([0.8448, 0.5714])\n",
      "\t Grad:  tensor([ 0.0025, -0.0189])\n",
      "Epoch 463, Loss 0.263252\n",
      "\t Params:  tensor([0.8448, 0.5716])\n",
      "\t Grad:  tensor([ 0.0025, -0.0187])\n",
      "Epoch 464, Loss 0.263249\n",
      "\t Params:  tensor([0.8448, 0.5718])\n",
      "\t Grad:  tensor([ 0.0025, -0.0186])\n",
      "Epoch 465, Loss 0.263245\n",
      "\t Params:  tensor([0.8448, 0.5720])\n",
      "\t Grad:  tensor([ 0.0025, -0.0185])\n",
      "Epoch 466, Loss 0.263242\n",
      "\t Params:  tensor([0.8447, 0.5722])\n",
      "\t Grad:  tensor([ 0.0025, -0.0184])\n",
      "Epoch 467, Loss 0.263238\n",
      "\t Params:  tensor([0.8447, 0.5723])\n",
      "\t Grad:  tensor([ 0.0024, -0.0183])\n",
      "Epoch 468, Loss 0.263235\n",
      "\t Params:  tensor([0.8447, 0.5725])\n",
      "\t Grad:  tensor([ 0.0024, -0.0181])\n",
      "Epoch 469, Loss 0.263232\n",
      "\t Params:  tensor([0.8447, 0.5727])\n",
      "\t Grad:  tensor([ 0.0024, -0.0180])\n",
      "Epoch 470, Loss 0.263228\n",
      "\t Params:  tensor([0.8446, 0.5729])\n",
      "\t Grad:  tensor([ 0.0024, -0.0179])\n",
      "Epoch 471, Loss 0.263225\n",
      "\t Params:  tensor([0.8446, 0.5731])\n",
      "\t Grad:  tensor([ 0.0024, -0.0178])\n",
      "Epoch 472, Loss 0.263222\n",
      "\t Params:  tensor([0.8446, 0.5732])\n",
      "\t Grad:  tensor([ 0.0024, -0.0177])\n",
      "Epoch 473, Loss 0.263219\n",
      "\t Params:  tensor([0.8446, 0.5734])\n",
      "\t Grad:  tensor([ 0.0023, -0.0175])\n",
      "Epoch 474, Loss 0.263216\n",
      "\t Params:  tensor([0.8445, 0.5736])\n",
      "\t Grad:  tensor([ 0.0023, -0.0174])\n",
      "Epoch 475, Loss 0.263213\n",
      "\t Params:  tensor([0.8445, 0.5738])\n",
      "\t Grad:  tensor([ 0.0023, -0.0173])\n",
      "Epoch 476, Loss 0.263209\n",
      "\t Params:  tensor([0.8445, 0.5739])\n",
      "\t Grad:  tensor([ 0.0023, -0.0172])\n",
      "Epoch 477, Loss 0.263207\n",
      "\t Params:  tensor([0.8445, 0.5741])\n",
      "\t Grad:  tensor([ 0.0023, -0.0171])\n",
      "Epoch 478, Loss 0.263203\n",
      "\t Params:  tensor([0.8444, 0.5743])\n",
      "\t Grad:  tensor([ 0.0023, -0.0170])\n",
      "Epoch 479, Loss 0.263201\n",
      "\t Params:  tensor([0.8444, 0.5744])\n",
      "\t Grad:  tensor([ 0.0023, -0.0169])\n",
      "Epoch 480, Loss 0.263198\n",
      "\t Params:  tensor([0.8444, 0.5746])\n",
      "\t Grad:  tensor([ 0.0022, -0.0167])\n",
      "Epoch 481, Loss 0.263195\n",
      "\t Params:  tensor([0.8444, 0.5748])\n",
      "\t Grad:  tensor([ 0.0022, -0.0166])\n",
      "Epoch 482, Loss 0.263192\n",
      "\t Params:  tensor([0.8444, 0.5749])\n",
      "\t Grad:  tensor([ 0.0022, -0.0165])\n",
      "Epoch 483, Loss 0.263189\n",
      "\t Params:  tensor([0.8443, 0.5751])\n",
      "\t Grad:  tensor([ 0.0022, -0.0164])\n",
      "Epoch 484, Loss 0.263187\n",
      "\t Params:  tensor([0.8443, 0.5753])\n",
      "\t Grad:  tensor([ 0.0022, -0.0163])\n",
      "Epoch 485, Loss 0.263184\n",
      "\t Params:  tensor([0.8443, 0.5754])\n",
      "\t Grad:  tensor([ 0.0022, -0.0162])\n",
      "Epoch 486, Loss 0.263181\n",
      "\t Params:  tensor([0.8443, 0.5756])\n",
      "\t Grad:  tensor([ 0.0021, -0.0161])\n",
      "Epoch 487, Loss 0.263179\n",
      "\t Params:  tensor([0.8442, 0.5758])\n",
      "\t Grad:  tensor([ 0.0021, -0.0160])\n",
      "Epoch 488, Loss 0.263176\n",
      "\t Params:  tensor([0.8442, 0.5759])\n",
      "\t Grad:  tensor([ 0.0021, -0.0159])\n",
      "Epoch 489, Loss 0.263173\n",
      "\t Params:  tensor([0.8442, 0.5761])\n",
      "\t Grad:  tensor([ 0.0021, -0.0158])\n",
      "Epoch 490, Loss 0.263171\n",
      "\t Params:  tensor([0.8442, 0.5762])\n",
      "\t Grad:  tensor([ 0.0021, -0.0157])\n",
      "Epoch 491, Loss 0.263168\n",
      "\t Params:  tensor([0.8442, 0.5764])\n",
      "\t Grad:  tensor([ 0.0021, -0.0156])\n",
      "Epoch 492, Loss 0.263166\n",
      "\t Params:  tensor([0.8441, 0.5765])\n",
      "\t Grad:  tensor([ 0.0021, -0.0155])\n",
      "Epoch 493, Loss 0.263164\n",
      "\t Params:  tensor([0.8441, 0.5767])\n",
      "\t Grad:  tensor([ 0.0021, -0.0154])\n",
      "Epoch 494, Loss 0.263161\n",
      "\t Params:  tensor([0.8441, 0.5768])\n",
      "\t Grad:  tensor([ 0.0020, -0.0153])\n",
      "Epoch 495, Loss 0.263159\n",
      "\t Params:  tensor([0.8441, 0.5770])\n",
      "\t Grad:  tensor([ 0.0020, -0.0152])\n",
      "Epoch 496, Loss 0.263156\n",
      "\t Params:  tensor([0.8441, 0.5771])\n",
      "\t Grad:  tensor([ 0.0020, -0.0151])\n",
      "Epoch 497, Loss 0.263154\n",
      "\t Params:  tensor([0.8440, 0.5773])\n",
      "\t Grad:  tensor([ 0.0020, -0.0150])\n",
      "Epoch 498, Loss 0.263152\n",
      "\t Params:  tensor([0.8440, 0.5774])\n",
      "\t Grad:  tensor([ 0.0020, -0.0149])\n",
      "Epoch 499, Loss 0.263150\n",
      "\t Params:  tensor([0.8440, 0.5776])\n",
      "\t Grad:  tensor([ 0.0020, -0.0148])\n",
      "Epoch 500, Loss 0.263147\n",
      "\t Params:  tensor([0.8440, 0.5777])\n",
      "\t Grad:  tensor([ 0.0020, -0.0147])\n",
      "Epoch 501, Loss 0.263145\n",
      "\t Params:  tensor([0.8440, 0.5779])\n",
      "\t Grad:  tensor([ 0.0019, -0.0146])\n",
      "Epoch 502, Loss 0.263143\n",
      "\t Params:  tensor([0.8439, 0.5780])\n",
      "\t Grad:  tensor([ 0.0019, -0.0145])\n",
      "Epoch 503, Loss 0.263141\n",
      "\t Params:  tensor([0.8439, 0.5782])\n",
      "\t Grad:  tensor([ 0.0019, -0.0144])\n",
      "Epoch 504, Loss 0.263139\n",
      "\t Params:  tensor([0.8439, 0.5783])\n",
      "\t Grad:  tensor([ 0.0019, -0.0143])\n",
      "Epoch 505, Loss 0.263137\n",
      "\t Params:  tensor([0.8439, 0.5785])\n",
      "\t Grad:  tensor([ 0.0019, -0.0142])\n",
      "Epoch 506, Loss 0.263135\n",
      "\t Params:  tensor([0.8439, 0.5786])\n",
      "\t Grad:  tensor([ 0.0019, -0.0141])\n",
      "Epoch 507, Loss 0.263133\n",
      "\t Params:  tensor([0.8439, 0.5787])\n",
      "\t Grad:  tensor([ 0.0019, -0.0140])\n",
      "Epoch 508, Loss 0.263131\n",
      "\t Params:  tensor([0.8438, 0.5789])\n",
      "\t Grad:  tensor([ 0.0019, -0.0139])\n",
      "Epoch 509, Loss 0.263129\n",
      "\t Params:  tensor([0.8438, 0.5790])\n",
      "\t Grad:  tensor([ 0.0018, -0.0138])\n",
      "Epoch 510, Loss 0.263127\n",
      "\t Params:  tensor([0.8438, 0.5792])\n",
      "\t Grad:  tensor([ 0.0018, -0.0137])\n",
      "Epoch 511, Loss 0.263125\n",
      "\t Params:  tensor([0.8438, 0.5793])\n",
      "\t Grad:  tensor([ 0.0018, -0.0136])\n",
      "Epoch 512, Loss 0.263123\n",
      "\t Params:  tensor([0.8438, 0.5794])\n",
      "\t Grad:  tensor([ 0.0018, -0.0135])\n",
      "Epoch 513, Loss 0.263121\n",
      "\t Params:  tensor([0.8437, 0.5796])\n",
      "\t Grad:  tensor([ 0.0018, -0.0134])\n",
      "Epoch 514, Loss 0.263119\n",
      "\t Params:  tensor([0.8437, 0.5797])\n",
      "\t Grad:  tensor([ 0.0018, -0.0134])\n",
      "Epoch 515, Loss 0.263118\n",
      "\t Params:  tensor([0.8437, 0.5798])\n",
      "\t Grad:  tensor([ 0.0018, -0.0133])\n",
      "Epoch 516, Loss 0.263116\n",
      "\t Params:  tensor([0.8437, 0.5800])\n",
      "\t Grad:  tensor([ 0.0018, -0.0132])\n",
      "Epoch 517, Loss 0.263114\n",
      "\t Params:  tensor([0.8437, 0.5801])\n",
      "\t Grad:  tensor([ 0.0017, -0.0131])\n",
      "Epoch 518, Loss 0.263112\n",
      "\t Params:  tensor([0.8437, 0.5802])\n",
      "\t Grad:  tensor([ 0.0017, -0.0130])\n",
      "Epoch 519, Loss 0.263111\n",
      "\t Params:  tensor([0.8436, 0.5803])\n",
      "\t Grad:  tensor([ 0.0017, -0.0129])\n",
      "Epoch 520, Loss 0.263109\n",
      "\t Params:  tensor([0.8436, 0.5805])\n",
      "\t Grad:  tensor([ 0.0017, -0.0128])\n",
      "Epoch 521, Loss 0.263107\n",
      "\t Params:  tensor([0.8436, 0.5806])\n",
      "\t Grad:  tensor([ 0.0017, -0.0127])\n",
      "Epoch 522, Loss 0.263106\n",
      "\t Params:  tensor([0.8436, 0.5807])\n",
      "\t Grad:  tensor([ 0.0017, -0.0127])\n",
      "Epoch 523, Loss 0.263104\n",
      "\t Params:  tensor([0.8436, 0.5809])\n",
      "\t Grad:  tensor([ 0.0017, -0.0126])\n",
      "Epoch 524, Loss 0.263102\n",
      "\t Params:  tensor([0.8436, 0.5810])\n",
      "\t Grad:  tensor([ 0.0017, -0.0125])\n",
      "Epoch 525, Loss 0.263101\n",
      "\t Params:  tensor([0.8435, 0.5811])\n",
      "\t Grad:  tensor([ 0.0017, -0.0124])\n",
      "Epoch 526, Loss 0.263099\n",
      "\t Params:  tensor([0.8435, 0.5812])\n",
      "\t Grad:  tensor([ 0.0016, -0.0123])\n",
      "Epoch 527, Loss 0.263098\n",
      "\t Params:  tensor([0.8435, 0.5813])\n",
      "\t Grad:  tensor([ 0.0016, -0.0122])\n",
      "Epoch 528, Loss 0.263096\n",
      "\t Params:  tensor([0.8435, 0.5815])\n",
      "\t Grad:  tensor([ 0.0016, -0.0122])\n",
      "Epoch 529, Loss 0.263095\n",
      "\t Params:  tensor([0.8435, 0.5816])\n",
      "\t Grad:  tensor([ 0.0016, -0.0121])\n",
      "Epoch 530, Loss 0.263093\n",
      "\t Params:  tensor([0.8435, 0.5817])\n",
      "\t Grad:  tensor([ 0.0016, -0.0120])\n",
      "Epoch 531, Loss 0.263092\n",
      "\t Params:  tensor([0.8434, 0.5818])\n",
      "\t Grad:  tensor([ 0.0016, -0.0119])\n",
      "Epoch 532, Loss 0.263090\n",
      "\t Params:  tensor([0.8434, 0.5819])\n",
      "\t Grad:  tensor([ 0.0016, -0.0118])\n",
      "Epoch 533, Loss 0.263089\n",
      "\t Params:  tensor([0.8434, 0.5821])\n",
      "\t Grad:  tensor([ 0.0016, -0.0118])\n",
      "Epoch 534, Loss 0.263087\n",
      "\t Params:  tensor([0.8434, 0.5822])\n",
      "\t Grad:  tensor([ 0.0016, -0.0117])\n",
      "Epoch 535, Loss 0.263086\n",
      "\t Params:  tensor([0.8434, 0.5823])\n",
      "\t Grad:  tensor([ 0.0015, -0.0116])\n",
      "Epoch 536, Loss 0.263085\n",
      "\t Params:  tensor([0.8434, 0.5824])\n",
      "\t Grad:  tensor([ 0.0015, -0.0115])\n",
      "Epoch 537, Loss 0.263083\n",
      "\t Params:  tensor([0.8433, 0.5825])\n",
      "\t Grad:  tensor([ 0.0015, -0.0115])\n",
      "Epoch 538, Loss 0.263082\n",
      "\t Params:  tensor([0.8433, 0.5826])\n",
      "\t Grad:  tensor([ 0.0015, -0.0114])\n",
      "Epoch 539, Loss 0.263081\n",
      "\t Params:  tensor([0.8433, 0.5828])\n",
      "\t Grad:  tensor([ 0.0015, -0.0113])\n",
      "Epoch 540, Loss 0.263079\n",
      "\t Params:  tensor([0.8433, 0.5829])\n",
      "\t Grad:  tensor([ 0.0015, -0.0112])\n",
      "Epoch 541, Loss 0.263078\n",
      "\t Params:  tensor([0.8433, 0.5830])\n",
      "\t Grad:  tensor([ 0.0015, -0.0112])\n",
      "Epoch 542, Loss 0.263077\n",
      "\t Params:  tensor([0.8433, 0.5831])\n",
      "\t Grad:  tensor([ 0.0015, -0.0111])\n",
      "Epoch 543, Loss 0.263076\n",
      "\t Params:  tensor([0.8433, 0.5832])\n",
      "\t Grad:  tensor([ 0.0015, -0.0110])\n",
      "Epoch 544, Loss 0.263074\n",
      "\t Params:  tensor([0.8432, 0.5833])\n",
      "\t Grad:  tensor([ 0.0015, -0.0109])\n",
      "Epoch 545, Loss 0.263073\n",
      "\t Params:  tensor([0.8432, 0.5834])\n",
      "\t Grad:  tensor([ 0.0015, -0.0109])\n",
      "Epoch 546, Loss 0.263072\n",
      "\t Params:  tensor([0.8432, 0.5835])\n",
      "\t Grad:  tensor([ 0.0014, -0.0108])\n",
      "Epoch 547, Loss 0.263071\n",
      "\t Params:  tensor([0.8432, 0.5836])\n",
      "\t Grad:  tensor([ 0.0014, -0.0107])\n",
      "Epoch 548, Loss 0.263070\n",
      "\t Params:  tensor([0.8432, 0.5837])\n",
      "\t Grad:  tensor([ 0.0014, -0.0107])\n",
      "Epoch 549, Loss 0.263068\n",
      "\t Params:  tensor([0.8432, 0.5838])\n",
      "\t Grad:  tensor([ 0.0014, -0.0106])\n",
      "Epoch 550, Loss 0.263067\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Params:  tensor([0.8432, 0.5840])\n",
      "\t Grad:  tensor([ 0.0014, -0.0105])\n",
      "Epoch 551, Loss 0.263066\n",
      "\t Params:  tensor([0.8431, 0.5841])\n",
      "\t Grad:  tensor([ 0.0014, -0.0104])\n",
      "Epoch 552, Loss 0.263065\n",
      "\t Params:  tensor([0.8431, 0.5842])\n",
      "\t Grad:  tensor([ 0.0014, -0.0104])\n",
      "Epoch 553, Loss 0.263064\n",
      "\t Params:  tensor([0.8431, 0.5843])\n",
      "\t Grad:  tensor([ 0.0014, -0.0103])\n",
      "Epoch 554, Loss 0.263063\n",
      "\t Params:  tensor([0.8431, 0.5844])\n",
      "\t Grad:  tensor([ 0.0014, -0.0102])\n",
      "Epoch 555, Loss 0.263062\n",
      "\t Params:  tensor([0.8431, 0.5845])\n",
      "\t Grad:  tensor([ 0.0014, -0.0102])\n",
      "Epoch 556, Loss 0.263061\n",
      "\t Params:  tensor([0.8431, 0.5846])\n",
      "\t Grad:  tensor([ 0.0014, -0.0101])\n",
      "Epoch 557, Loss 0.263060\n",
      "\t Params:  tensor([0.8431, 0.5847])\n",
      "\t Grad:  tensor([ 0.0013, -0.0100])\n",
      "Epoch 558, Loss 0.263059\n",
      "\t Params:  tensor([0.8430, 0.5848])\n",
      "\t Grad:  tensor([ 0.0013, -0.0100])\n",
      "Epoch 559, Loss 0.263058\n",
      "\t Params:  tensor([0.8430, 0.5849])\n",
      "\t Grad:  tensor([ 0.0013, -0.0099])\n",
      "Epoch 560, Loss 0.263057\n",
      "\t Params:  tensor([0.8430, 0.5850])\n",
      "\t Grad:  tensor([ 0.0013, -0.0098])\n",
      "Epoch 561, Loss 0.263056\n",
      "\t Params:  tensor([0.8430, 0.5851])\n",
      "\t Grad:  tensor([ 0.0013, -0.0098])\n",
      "Epoch 562, Loss 0.263055\n",
      "\t Params:  tensor([0.8430, 0.5852])\n",
      "\t Grad:  tensor([ 0.0013, -0.0097])\n",
      "Epoch 563, Loss 0.263054\n",
      "\t Params:  tensor([0.8430, 0.5853])\n",
      "\t Grad:  tensor([ 0.0013, -0.0096])\n",
      "Epoch 564, Loss 0.263053\n",
      "\t Params:  tensor([0.8430, 0.5854])\n",
      "\t Grad:  tensor([ 0.0013, -0.0096])\n",
      "Epoch 565, Loss 0.263052\n",
      "\t Params:  tensor([0.8430, 0.5854])\n",
      "\t Grad:  tensor([ 0.0013, -0.0095])\n",
      "Epoch 566, Loss 0.263051\n",
      "\t Params:  tensor([0.8429, 0.5855])\n",
      "\t Grad:  tensor([ 0.0013, -0.0095])\n",
      "Epoch 567, Loss 0.263050\n",
      "\t Params:  tensor([0.8429, 0.5856])\n",
      "\t Grad:  tensor([ 0.0013, -0.0094])\n",
      "Epoch 568, Loss 0.263049\n",
      "\t Params:  tensor([0.8429, 0.5857])\n",
      "\t Grad:  tensor([ 0.0012, -0.0093])\n",
      "Epoch 569, Loss 0.263048\n",
      "\t Params:  tensor([0.8429, 0.5858])\n",
      "\t Grad:  tensor([ 0.0012, -0.0093])\n",
      "Epoch 570, Loss 0.263047\n",
      "\t Params:  tensor([0.8429, 0.5859])\n",
      "\t Grad:  tensor([ 0.0012, -0.0092])\n",
      "Epoch 571, Loss 0.263047\n",
      "\t Params:  tensor([0.8429, 0.5860])\n",
      "\t Grad:  tensor([ 0.0012, -0.0091])\n",
      "Epoch 572, Loss 0.263046\n",
      "\t Params:  tensor([0.8429, 0.5861])\n",
      "\t Grad:  tensor([ 0.0012, -0.0091])\n",
      "Epoch 573, Loss 0.263045\n",
      "\t Params:  tensor([0.8429, 0.5862])\n",
      "\t Grad:  tensor([ 0.0012, -0.0090])\n",
      "Epoch 574, Loss 0.263044\n",
      "\t Params:  tensor([0.8428, 0.5863])\n",
      "\t Grad:  tensor([ 0.0012, -0.0090])\n",
      "Epoch 575, Loss 0.263043\n",
      "\t Params:  tensor([0.8428, 0.5864])\n",
      "\t Grad:  tensor([ 0.0012, -0.0089])\n",
      "Epoch 576, Loss 0.263042\n",
      "\t Params:  tensor([0.8428, 0.5865])\n",
      "\t Grad:  tensor([ 0.0012, -0.0088])\n",
      "Epoch 577, Loss 0.263042\n",
      "\t Params:  tensor([0.8428, 0.5865])\n",
      "\t Grad:  tensor([ 0.0012, -0.0088])\n",
      "Epoch 578, Loss 0.263041\n",
      "\t Params:  tensor([0.8428, 0.5866])\n",
      "\t Grad:  tensor([ 0.0012, -0.0087])\n",
      "Epoch 579, Loss 0.263040\n",
      "\t Params:  tensor([0.8428, 0.5867])\n",
      "\t Grad:  tensor([ 0.0012, -0.0087])\n",
      "Epoch 580, Loss 0.263039\n",
      "\t Params:  tensor([0.8428, 0.5868])\n",
      "\t Grad:  tensor([ 0.0011, -0.0086])\n",
      "Epoch 581, Loss 0.263039\n",
      "\t Params:  tensor([0.8428, 0.5869])\n",
      "\t Grad:  tensor([ 0.0011, -0.0086])\n",
      "Epoch 582, Loss 0.263038\n",
      "\t Params:  tensor([0.8427, 0.5870])\n",
      "\t Grad:  tensor([ 0.0011, -0.0085])\n",
      "Epoch 583, Loss 0.263037\n",
      "\t Params:  tensor([0.8427, 0.5871])\n",
      "\t Grad:  tensor([ 0.0011, -0.0084])\n",
      "Epoch 584, Loss 0.263036\n",
      "\t Params:  tensor([0.8427, 0.5871])\n",
      "\t Grad:  tensor([ 0.0011, -0.0084])\n",
      "Epoch 585, Loss 0.263036\n",
      "\t Params:  tensor([0.8427, 0.5872])\n",
      "\t Grad:  tensor([ 0.0011, -0.0083])\n",
      "Epoch 586, Loss 0.263035\n",
      "\t Params:  tensor([0.8427, 0.5873])\n",
      "\t Grad:  tensor([ 0.0011, -0.0083])\n",
      "Epoch 587, Loss 0.263034\n",
      "\t Params:  tensor([0.8427, 0.5874])\n",
      "\t Grad:  tensor([ 0.0011, -0.0082])\n",
      "Epoch 588, Loss 0.263034\n",
      "\t Params:  tensor([0.8427, 0.5875])\n",
      "\t Grad:  tensor([ 0.0011, -0.0082])\n",
      "Epoch 589, Loss 0.263033\n",
      "\t Params:  tensor([0.8427, 0.5876])\n",
      "\t Grad:  tensor([ 0.0011, -0.0081])\n",
      "Epoch 590, Loss 0.263032\n",
      "\t Params:  tensor([0.8427, 0.5876])\n",
      "\t Grad:  tensor([ 0.0011, -0.0081])\n",
      "Epoch 591, Loss 0.263031\n",
      "\t Params:  tensor([0.8427, 0.5877])\n",
      "\t Grad:  tensor([ 0.0011, -0.0080])\n",
      "Epoch 592, Loss 0.263031\n",
      "\t Params:  tensor([0.8426, 0.5878])\n",
      "\t Grad:  tensor([ 0.0011, -0.0080])\n",
      "Epoch 593, Loss 0.263030\n",
      "\t Params:  tensor([0.8426, 0.5879])\n",
      "\t Grad:  tensor([ 0.0011, -0.0079])\n",
      "Epoch 594, Loss 0.263030\n",
      "\t Params:  tensor([0.8426, 0.5879])\n",
      "\t Grad:  tensor([ 0.0011, -0.0078])\n",
      "Epoch 595, Loss 0.263029\n",
      "\t Params:  tensor([0.8426, 0.5880])\n",
      "\t Grad:  tensor([ 0.0010, -0.0078])\n",
      "Epoch 596, Loss 0.263028\n",
      "\t Params:  tensor([0.8426, 0.5881])\n",
      "\t Grad:  tensor([ 0.0010, -0.0077])\n",
      "Epoch 597, Loss 0.263028\n",
      "\t Params:  tensor([0.8426, 0.5882])\n",
      "\t Grad:  tensor([ 0.0010, -0.0077])\n",
      "Epoch 598, Loss 0.263027\n",
      "\t Params:  tensor([0.8426, 0.5883])\n",
      "\t Grad:  tensor([ 0.0010, -0.0076])\n",
      "Epoch 599, Loss 0.263027\n",
      "\t Params:  tensor([0.8426, 0.5883])\n",
      "\t Grad:  tensor([ 0.0010, -0.0076])\n",
      "Epoch 600, Loss 0.263026\n",
      "\t Params:  tensor([0.8426, 0.5884])\n",
      "\t Grad:  tensor([ 0.0010, -0.0075])\n",
      "Epoch 601, Loss 0.263026\n",
      "\t Params:  tensor([0.8425, 0.5885])\n",
      "\t Grad:  tensor([ 0.0010, -0.0075])\n",
      "Epoch 602, Loss 0.263025\n",
      "\t Params:  tensor([0.8425, 0.5886])\n",
      "\t Grad:  tensor([ 0.0010, -0.0074])\n",
      "Epoch 603, Loss 0.263024\n",
      "\t Params:  tensor([0.8425, 0.5886])\n",
      "\t Grad:  tensor([ 0.0010, -0.0074])\n",
      "Epoch 604, Loss 0.263024\n",
      "\t Params:  tensor([0.8425, 0.5887])\n",
      "\t Grad:  tensor([ 0.0010, -0.0073])\n",
      "Epoch 605, Loss 0.263023\n",
      "\t Params:  tensor([0.8425, 0.5888])\n",
      "\t Grad:  tensor([ 0.0010, -0.0073])\n",
      "Epoch 606, Loss 0.263023\n",
      "\t Params:  tensor([0.8425, 0.5889])\n",
      "\t Grad:  tensor([ 0.0010, -0.0072])\n",
      "Epoch 607, Loss 0.263022\n",
      "\t Params:  tensor([0.8425, 0.5889])\n",
      "\t Grad:  tensor([ 0.0010, -0.0072])\n",
      "Epoch 608, Loss 0.263022\n",
      "\t Params:  tensor([0.8425, 0.5890])\n",
      "\t Grad:  tensor([ 0.0010, -0.0071])\n",
      "Epoch 609, Loss 0.263021\n",
      "\t Params:  tensor([0.8425, 0.5891])\n",
      "\t Grad:  tensor([ 0.0009, -0.0071])\n",
      "Epoch 610, Loss 0.263021\n",
      "\t Params:  tensor([0.8425, 0.5891])\n",
      "\t Grad:  tensor([ 0.0009, -0.0071])\n",
      "Epoch 611, Loss 0.263020\n",
      "\t Params:  tensor([0.8425, 0.5892])\n",
      "\t Grad:  tensor([ 0.0009, -0.0070])\n",
      "Epoch 612, Loss 0.263020\n",
      "\t Params:  tensor([0.8424, 0.5893])\n",
      "\t Grad:  tensor([ 0.0009, -0.0070])\n",
      "Epoch 613, Loss 0.263019\n",
      "\t Params:  tensor([0.8424, 0.5893])\n",
      "\t Grad:  tensor([ 0.0009, -0.0069])\n",
      "Epoch 614, Loss 0.263019\n",
      "\t Params:  tensor([0.8424, 0.5894])\n",
      "\t Grad:  tensor([ 0.0009, -0.0069])\n",
      "Epoch 615, Loss 0.263018\n",
      "\t Params:  tensor([0.8424, 0.5895])\n",
      "\t Grad:  tensor([ 0.0009, -0.0068])\n",
      "Epoch 616, Loss 0.263018\n",
      "\t Params:  tensor([0.8424, 0.5895])\n",
      "\t Grad:  tensor([ 0.0009, -0.0068])\n",
      "Epoch 617, Loss 0.263017\n",
      "\t Params:  tensor([0.8424, 0.5896])\n",
      "\t Grad:  tensor([ 0.0009, -0.0067])\n",
      "Epoch 618, Loss 0.263017\n",
      "\t Params:  tensor([0.8424, 0.5897])\n",
      "\t Grad:  tensor([ 0.0009, -0.0067])\n",
      "Epoch 619, Loss 0.263016\n",
      "\t Params:  tensor([0.8424, 0.5898])\n",
      "\t Grad:  tensor([ 0.0009, -0.0066])\n",
      "Epoch 620, Loss 0.263016\n",
      "\t Params:  tensor([0.8424, 0.5898])\n",
      "\t Grad:  tensor([ 0.0009, -0.0066])\n",
      "Epoch 621, Loss 0.263015\n",
      "\t Params:  tensor([0.8424, 0.5899])\n",
      "\t Grad:  tensor([ 0.0009, -0.0066])\n",
      "Epoch 622, Loss 0.263015\n",
      "\t Params:  tensor([0.8424, 0.5899])\n",
      "\t Grad:  tensor([ 0.0009, -0.0065])\n",
      "Epoch 623, Loss 0.263015\n",
      "\t Params:  tensor([0.8423, 0.5900])\n",
      "\t Grad:  tensor([ 0.0009, -0.0065])\n",
      "Epoch 624, Loss 0.263014\n",
      "\t Params:  tensor([0.8423, 0.5901])\n",
      "\t Grad:  tensor([ 0.0009, -0.0064])\n",
      "Epoch 625, Loss 0.263014\n",
      "\t Params:  tensor([0.8423, 0.5901])\n",
      "\t Grad:  tensor([ 0.0009, -0.0064])\n",
      "Epoch 626, Loss 0.263013\n",
      "\t Params:  tensor([0.8423, 0.5902])\n",
      "\t Grad:  tensor([ 0.0008, -0.0063])\n",
      "Epoch 627, Loss 0.263013\n",
      "\t Params:  tensor([0.8423, 0.5903])\n",
      "\t Grad:  tensor([ 0.0008, -0.0063])\n",
      "Epoch 628, Loss 0.263012\n",
      "\t Params:  tensor([0.8423, 0.5903])\n",
      "\t Grad:  tensor([ 0.0008, -0.0063])\n",
      "Epoch 629, Loss 0.263012\n",
      "\t Params:  tensor([0.8423, 0.5904])\n",
      "\t Grad:  tensor([ 0.0008, -0.0062])\n",
      "Epoch 630, Loss 0.263012\n",
      "\t Params:  tensor([0.8423, 0.5905])\n",
      "\t Grad:  tensor([ 0.0008, -0.0062])\n",
      "Epoch 631, Loss 0.263011\n",
      "\t Params:  tensor([0.8423, 0.5905])\n",
      "\t Grad:  tensor([ 0.0008, -0.0061])\n",
      "Epoch 632, Loss 0.263011\n",
      "\t Params:  tensor([0.8423, 0.5906])\n",
      "\t Grad:  tensor([ 0.0008, -0.0061])\n",
      "Epoch 633, Loss 0.263011\n",
      "\t Params:  tensor([0.8423, 0.5906])\n",
      "\t Grad:  tensor([ 0.0008, -0.0061])\n",
      "Epoch 634, Loss 0.263010\n",
      "\t Params:  tensor([0.8423, 0.5907])\n",
      "\t Grad:  tensor([ 0.0008, -0.0060])\n",
      "Epoch 635, Loss 0.263010\n",
      "\t Params:  tensor([0.8422, 0.5908])\n",
      "\t Grad:  tensor([ 0.0008, -0.0060])\n",
      "Epoch 636, Loss 0.263009\n",
      "\t Params:  tensor([0.8422, 0.5908])\n",
      "\t Grad:  tensor([ 0.0008, -0.0059])\n",
      "Epoch 637, Loss 0.263009\n",
      "\t Params:  tensor([0.8422, 0.5909])\n",
      "\t Grad:  tensor([ 0.0008, -0.0059])\n",
      "Epoch 638, Loss 0.263009\n",
      "\t Params:  tensor([0.8422, 0.5909])\n",
      "\t Grad:  tensor([ 0.0008, -0.0059])\n",
      "Epoch 639, Loss 0.263008\n",
      "\t Params:  tensor([0.8422, 0.5910])\n",
      "\t Grad:  tensor([ 0.0008, -0.0058])\n",
      "Epoch 640, Loss 0.263008\n",
      "\t Params:  tensor([0.8422, 0.5910])\n",
      "\t Grad:  tensor([ 0.0008, -0.0058])\n",
      "Epoch 641, Loss 0.263008\n",
      "\t Params:  tensor([0.8422, 0.5911])\n",
      "\t Grad:  tensor([ 0.0008, -0.0057])\n",
      "Epoch 642, Loss 0.263007\n",
      "\t Params:  tensor([0.8422, 0.5912])\n",
      "\t Grad:  tensor([ 0.0008, -0.0057])\n",
      "Epoch 643, Loss 0.263007\n",
      "\t Params:  tensor([0.8422, 0.5912])\n",
      "\t Grad:  tensor([ 0.0008, -0.0057])\n",
      "Epoch 644, Loss 0.263007\n",
      "\t Params:  tensor([0.8422, 0.5913])\n",
      "\t Grad:  tensor([ 0.0007, -0.0056])\n",
      "Epoch 645, Loss 0.263006\n",
      "\t Params:  tensor([0.8422, 0.5913])\n",
      "\t Grad:  tensor([ 0.0007, -0.0056])\n",
      "Epoch 646, Loss 0.263006\n",
      "\t Params:  tensor([0.8422, 0.5914])\n",
      "\t Grad:  tensor([ 0.0007, -0.0056])\n",
      "Epoch 647, Loss 0.263006\n",
      "\t Params:  tensor([0.8422, 0.5914])\n",
      "\t Grad:  tensor([ 0.0007, -0.0055])\n",
      "Epoch 648, Loss 0.263006\n",
      "\t Params:  tensor([0.8421, 0.5915])\n",
      "\t Grad:  tensor([ 0.0007, -0.0055])\n",
      "Epoch 649, Loss 0.263005\n",
      "\t Params:  tensor([0.8421, 0.5916])\n",
      "\t Grad:  tensor([ 0.0007, -0.0054])\n",
      "Epoch 650, Loss 0.263005\n",
      "\t Params:  tensor([0.8421, 0.5916])\n",
      "\t Grad:  tensor([ 0.0007, -0.0054])\n",
      "Epoch 651, Loss 0.263005\n",
      "\t Params:  tensor([0.8421, 0.5917])\n",
      "\t Grad:  tensor([ 0.0007, -0.0054])\n",
      "Epoch 652, Loss 0.263004\n",
      "\t Params:  tensor([0.8421, 0.5917])\n",
      "\t Grad:  tensor([ 0.0007, -0.0053])\n",
      "Epoch 653, Loss 0.263004\n",
      "\t Params:  tensor([0.8421, 0.5918])\n",
      "\t Grad:  tensor([ 0.0007, -0.0053])\n",
      "Epoch 654, Loss 0.263004\n",
      "\t Params:  tensor([0.8421, 0.5918])\n",
      "\t Grad:  tensor([ 0.0007, -0.0053])\n",
      "Epoch 655, Loss 0.263003\n",
      "\t Params:  tensor([0.8421, 0.5919])\n",
      "\t Grad:  tensor([ 0.0007, -0.0052])\n",
      "Epoch 656, Loss 0.263003\n",
      "\t Params:  tensor([0.8421, 0.5919])\n",
      "\t Grad:  tensor([ 0.0007, -0.0052])\n",
      "Epoch 657, Loss 0.263003\n",
      "\t Params:  tensor([0.8421, 0.5920])\n",
      "\t Grad:  tensor([ 0.0007, -0.0052])\n",
      "Epoch 658, Loss 0.263003\n",
      "\t Params:  tensor([0.8421, 0.5920])\n",
      "\t Grad:  tensor([ 0.0007, -0.0051])\n",
      "Epoch 659, Loss 0.263002\n",
      "\t Params:  tensor([0.8421, 0.5921])\n",
      "\t Grad:  tensor([ 0.0007, -0.0051])\n",
      "Epoch 660, Loss 0.263002\n",
      "\t Params:  tensor([0.8421, 0.5921])\n",
      "\t Grad:  tensor([ 0.0007, -0.0051])\n",
      "Epoch 661, Loss 0.263002\n",
      "\t Params:  tensor([0.8421, 0.5922])\n",
      "\t Grad:  tensor([ 0.0007, -0.0050])\n",
      "Epoch 662, Loss 0.263002\n",
      "\t Params:  tensor([0.8420, 0.5922])\n",
      "\t Grad:  tensor([ 0.0007, -0.0050])\n",
      "Epoch 663, Loss 0.263001\n",
      "\t Params:  tensor([0.8420, 0.5923])\n",
      "\t Grad:  tensor([ 0.0007, -0.0050])\n",
      "Epoch 664, Loss 0.263001\n",
      "\t Params:  tensor([0.8420, 0.5923])\n",
      "\t Grad:  tensor([ 0.0007, -0.0049])\n",
      "Epoch 665, Loss 0.263001\n",
      "\t Params:  tensor([0.8420, 0.5924])\n",
      "\t Grad:  tensor([ 0.0007, -0.0049])\n",
      "Epoch 666, Loss 0.263001\n",
      "\t Params:  tensor([0.8420, 0.5924])\n",
      "\t Grad:  tensor([ 0.0007, -0.0049])\n",
      "Epoch 667, Loss 0.263000\n",
      "\t Params:  tensor([0.8420, 0.5925])\n",
      "\t Grad:  tensor([ 0.0006, -0.0048])\n",
      "Epoch 668, Loss 0.263000\n",
      "\t Params:  tensor([0.8420, 0.5925])\n",
      "\t Grad:  tensor([ 0.0006, -0.0048])\n",
      "Epoch 669, Loss 0.263000\n",
      "\t Params:  tensor([0.8420, 0.5926])\n",
      "\t Grad:  tensor([ 0.0006, -0.0048])\n",
      "Epoch 670, Loss 0.263000\n",
      "\t Params:  tensor([0.8420, 0.5926])\n",
      "\t Grad:  tensor([ 0.0006, -0.0047])\n",
      "Epoch 671, Loss 0.262999\n",
      "\t Params:  tensor([0.8420, 0.5927])\n",
      "\t Grad:  tensor([ 0.0006, -0.0047])\n",
      "Epoch 672, Loss 0.262999\n",
      "\t Params:  tensor([0.8420, 0.5927])\n",
      "\t Grad:  tensor([ 0.0006, -0.0047])\n",
      "Epoch 673, Loss 0.262999\n",
      "\t Params:  tensor([0.8420, 0.5928])\n",
      "\t Grad:  tensor([ 0.0006, -0.0046])\n",
      "Epoch 674, Loss 0.262999\n",
      "\t Params:  tensor([0.8420, 0.5928])\n",
      "\t Grad:  tensor([ 0.0006, -0.0046])\n",
      "Epoch 675, Loss 0.262998\n",
      "\t Params:  tensor([0.8420, 0.5928])\n",
      "\t Grad:  tensor([ 0.0006, -0.0046])\n",
      "Epoch 676, Loss 0.262998\n",
      "\t Params:  tensor([0.8420, 0.5929])\n",
      "\t Grad:  tensor([ 0.0006, -0.0045])\n",
      "Epoch 677, Loss 0.262998\n",
      "\t Params:  tensor([0.8420, 0.5929])\n",
      "\t Grad:  tensor([ 0.0006, -0.0045])\n",
      "Epoch 678, Loss 0.262998\n",
      "\t Params:  tensor([0.8419, 0.5930])\n",
      "\t Grad:  tensor([ 0.0006, -0.0045])\n",
      "Epoch 679, Loss 0.262998\n",
      "\t Params:  tensor([0.8419, 0.5930])\n",
      "\t Grad:  tensor([ 0.0006, -0.0045])\n",
      "Epoch 680, Loss 0.262998\n",
      "\t Params:  tensor([0.8419, 0.5931])\n",
      "\t Grad:  tensor([ 0.0006, -0.0044])\n",
      "Epoch 681, Loss 0.262997\n",
      "\t Params:  tensor([0.8419, 0.5931])\n",
      "\t Grad:  tensor([ 0.0006, -0.0044])\n",
      "Epoch 682, Loss 0.262997\n",
      "\t Params:  tensor([0.8419, 0.5932])\n",
      "\t Grad:  tensor([ 0.0006, -0.0044])\n",
      "Epoch 683, Loss 0.262997\n",
      "\t Params:  tensor([0.8419, 0.5932])\n",
      "\t Grad:  tensor([ 0.0006, -0.0043])\n",
      "Epoch 684, Loss 0.262997\n",
      "\t Params:  tensor([0.8419, 0.5932])\n",
      "\t Grad:  tensor([ 0.0006, -0.0043])\n",
      "Epoch 685, Loss 0.262996\n",
      "\t Params:  tensor([0.8419, 0.5933])\n",
      "\t Grad:  tensor([ 0.0006, -0.0043])\n",
      "Epoch 686, Loss 0.262996\n",
      "\t Params:  tensor([0.8419, 0.5933])\n",
      "\t Grad:  tensor([ 0.0006, -0.0043])\n",
      "Epoch 687, Loss 0.262996\n",
      "\t Params:  tensor([0.8419, 0.5934])\n",
      "\t Grad:  tensor([ 0.0006, -0.0042])\n",
      "Epoch 688, Loss 0.262996\n",
      "\t Params:  tensor([0.8419, 0.5934])\n",
      "\t Grad:  tensor([ 0.0006, -0.0042])\n",
      "Epoch 689, Loss 0.262996\n",
      "\t Params:  tensor([0.8419, 0.5935])\n",
      "\t Grad:  tensor([ 0.0006, -0.0042])\n",
      "Epoch 690, Loss 0.262996\n",
      "\t Params:  tensor([0.8419, 0.5935])\n",
      "\t Grad:  tensor([ 0.0006, -0.0041])\n",
      "Epoch 691, Loss 0.262995\n",
      "\t Params:  tensor([0.8419, 0.5935])\n",
      "\t Grad:  tensor([ 0.0006, -0.0041])\n",
      "Epoch 692, Loss 0.262995\n",
      "\t Params:  tensor([0.8419, 0.5936])\n",
      "\t Grad:  tensor([ 0.0005, -0.0041])\n",
      "Epoch 693, Loss 0.262995\n",
      "\t Params:  tensor([0.8419, 0.5936])\n",
      "\t Grad:  tensor([ 0.0005, -0.0041])\n",
      "Epoch 694, Loss 0.262995\n",
      "\t Params:  tensor([0.8419, 0.5937])\n",
      "\t Grad:  tensor([ 0.0005, -0.0040])\n",
      "Epoch 695, Loss 0.262995\n",
      "\t Params:  tensor([0.8418, 0.5937])\n",
      "\t Grad:  tensor([ 0.0005, -0.0040])\n",
      "Epoch 696, Loss 0.262995\n",
      "\t Params:  tensor([0.8418, 0.5937])\n",
      "\t Grad:  tensor([ 0.0005, -0.0040])\n",
      "Epoch 697, Loss 0.262994\n",
      "\t Params:  tensor([0.8418, 0.5938])\n",
      "\t Grad:  tensor([ 0.0005, -0.0040])\n",
      "Epoch 698, Loss 0.262994\n",
      "\t Params:  tensor([0.8418, 0.5938])\n",
      "\t Grad:  tensor([ 0.0005, -0.0039])\n",
      "Epoch 699, Loss 0.262994\n",
      "\t Params:  tensor([0.8418, 0.5939])\n",
      "\t Grad:  tensor([ 0.0005, -0.0039])\n",
      "Epoch 700, Loss 0.262994\n",
      "\t Params:  tensor([0.8418, 0.5939])\n",
      "\t Grad:  tensor([ 0.0005, -0.0039])\n",
      "Epoch 701, Loss 0.262994\n",
      "\t Params:  tensor([0.8418, 0.5939])\n",
      "\t Grad:  tensor([ 0.0005, -0.0039])\n",
      "Epoch 702, Loss 0.262994\n",
      "\t Params:  tensor([0.8418, 0.5940])\n",
      "\t Grad:  tensor([ 0.0005, -0.0038])\n",
      "Epoch 703, Loss 0.262994\n",
      "\t Params:  tensor([0.8418, 0.5940])\n",
      "\t Grad:  tensor([ 0.0005, -0.0038])\n",
      "Epoch 704, Loss 0.262993\n",
      "\t Params:  tensor([0.8418, 0.5941])\n",
      "\t Grad:  tensor([ 0.0005, -0.0038])\n",
      "Epoch 705, Loss 0.262993\n",
      "\t Params:  tensor([0.8418, 0.5941])\n",
      "\t Grad:  tensor([ 0.0005, -0.0038])\n",
      "Epoch 706, Loss 0.262993\n",
      "\t Params:  tensor([0.8418, 0.5941])\n",
      "\t Grad:  tensor([ 0.0005, -0.0037])\n",
      "Epoch 707, Loss 0.262993\n",
      "\t Params:  tensor([0.8418, 0.5942])\n",
      "\t Grad:  tensor([ 0.0005, -0.0037])\n",
      "Epoch 708, Loss 0.262993\n",
      "\t Params:  tensor([0.8418, 0.5942])\n",
      "\t Grad:  tensor([ 0.0005, -0.0037])\n",
      "Epoch 709, Loss 0.262993\n",
      "\t Params:  tensor([0.8418, 0.5942])\n",
      "\t Grad:  tensor([ 0.0005, -0.0037])\n",
      "Epoch 710, Loss 0.262993\n",
      "\t Params:  tensor([0.8418, 0.5943])\n",
      "\t Grad:  tensor([ 0.0005, -0.0036])\n",
      "Epoch 711, Loss 0.262992\n",
      "\t Params:  tensor([0.8418, 0.5943])\n",
      "\t Grad:  tensor([ 0.0005, -0.0036])\n",
      "Epoch 712, Loss 0.262992\n",
      "\t Params:  tensor([0.8418, 0.5943])\n",
      "\t Grad:  tensor([ 0.0005, -0.0036])\n",
      "Epoch 713, Loss 0.262992\n",
      "\t Params:  tensor([0.8418, 0.5944])\n",
      "\t Grad:  tensor([ 0.0005, -0.0036])\n",
      "Epoch 714, Loss 0.262992\n",
      "\t Params:  tensor([0.8418, 0.5944])\n",
      "\t Grad:  tensor([ 0.0005, -0.0035])\n",
      "Epoch 715, Loss 0.262992\n",
      "\t Params:  tensor([0.8417, 0.5944])\n",
      "\t Grad:  tensor([ 0.0005, -0.0035])\n",
      "Epoch 716, Loss 0.262992\n",
      "\t Params:  tensor([0.8417, 0.5945])\n",
      "\t Grad:  tensor([ 0.0005, -0.0035])\n",
      "Epoch 717, Loss 0.262992\n",
      "\t Params:  tensor([0.8417, 0.5945])\n",
      "\t Grad:  tensor([ 0.0005, -0.0035])\n",
      "Epoch 718, Loss 0.262992\n",
      "\t Params:  tensor([0.8417, 0.5946])\n",
      "\t Grad:  tensor([ 0.0005, -0.0034])\n",
      "Epoch 719, Loss 0.262991\n",
      "\t Params:  tensor([0.8417, 0.5946])\n",
      "\t Grad:  tensor([ 0.0005, -0.0034])\n",
      "Epoch 720, Loss 0.262991\n",
      "\t Params:  tensor([0.8417, 0.5946])\n",
      "\t Grad:  tensor([ 0.0005, -0.0034])\n",
      "Epoch 721, Loss 0.262991\n",
      "\t Params:  tensor([0.8417, 0.5947])\n",
      "\t Grad:  tensor([ 0.0005, -0.0034])\n",
      "Epoch 722, Loss 0.262991\n",
      "\t Params:  tensor([0.8417, 0.5947])\n",
      "\t Grad:  tensor([ 0.0004, -0.0034])\n",
      "Epoch 723, Loss 0.262991\n",
      "\t Params:  tensor([0.8417, 0.5947])\n",
      "\t Grad:  tensor([ 0.0004, -0.0033])\n",
      "Epoch 724, Loss 0.262991\n",
      "\t Params:  tensor([0.8417, 0.5948])\n",
      "\t Grad:  tensor([ 0.0004, -0.0033])\n",
      "Epoch 725, Loss 0.262991\n",
      "\t Params:  tensor([0.8417, 0.5948])\n",
      "\t Grad:  tensor([ 0.0004, -0.0033])\n",
      "Epoch 726, Loss 0.262991\n",
      "\t Params:  tensor([0.8417, 0.5948])\n",
      "\t Grad:  tensor([ 0.0004, -0.0033])\n",
      "Epoch 727, Loss 0.262990\n",
      "\t Params:  tensor([0.8417, 0.5949])\n",
      "\t Grad:  tensor([ 0.0004, -0.0032])\n",
      "Epoch 728, Loss 0.262990\n",
      "\t Params:  tensor([0.8417, 0.5949])\n",
      "\t Grad:  tensor([ 0.0004, -0.0032])\n",
      "Epoch 729, Loss 0.262990\n",
      "\t Params:  tensor([0.8417, 0.5949])\n",
      "\t Grad:  tensor([ 0.0004, -0.0032])\n",
      "Epoch 730, Loss 0.262990\n",
      "\t Params:  tensor([0.8417, 0.5949])\n",
      "\t Grad:  tensor([ 0.0004, -0.0032])\n",
      "Epoch 731, Loss 0.262990\n",
      "\t Params:  tensor([0.8417, 0.5950])\n",
      "\t Grad:  tensor([ 0.0004, -0.0032])\n",
      "Epoch 732, Loss 0.262990\n",
      "\t Params:  tensor([0.8417, 0.5950])\n",
      "\t Grad:  tensor([ 0.0004, -0.0031])\n",
      "Epoch 733, Loss 0.262990\n",
      "\t Params:  tensor([0.8417, 0.5950])\n",
      "\t Grad:  tensor([ 0.0004, -0.0031])\n",
      "Epoch 734, Loss 0.262990\n",
      "\t Params:  tensor([0.8417, 0.5951])\n",
      "\t Grad:  tensor([ 0.0004, -0.0031])\n",
      "Epoch 735, Loss 0.262990\n",
      "\t Params:  tensor([0.8417, 0.5951])\n",
      "\t Grad:  tensor([ 0.0004, -0.0031])\n",
      "Epoch 736, Loss 0.262989\n",
      "\t Params:  tensor([0.8417, 0.5951])\n",
      "\t Grad:  tensor([ 0.0004, -0.0031])\n",
      "Epoch 737, Loss 0.262989\n",
      "\t Params:  tensor([0.8417, 0.5952])\n",
      "\t Grad:  tensor([ 0.0004, -0.0030])\n",
      "Epoch 738, Loss 0.262989\n",
      "\t Params:  tensor([0.8416, 0.5952])\n",
      "\t Grad:  tensor([ 0.0004, -0.0030])\n",
      "Epoch 739, Loss 0.262989\n",
      "\t Params:  tensor([0.8416, 0.5952])\n",
      "\t Grad:  tensor([ 0.0004, -0.0030])\n",
      "Epoch 740, Loss 0.262989\n",
      "\t Params:  tensor([0.8416, 0.5953])\n",
      "\t Grad:  tensor([ 0.0004, -0.0030])\n",
      "Epoch 741, Loss 0.262989\n",
      "\t Params:  tensor([0.8416, 0.5953])\n",
      "\t Grad:  tensor([ 0.0004, -0.0030])\n",
      "Epoch 742, Loss 0.262989\n",
      "\t Params:  tensor([0.8416, 0.5953])\n",
      "\t Grad:  tensor([ 0.0004, -0.0029])\n",
      "Epoch 743, Loss 0.262989\n",
      "\t Params:  tensor([0.8416, 0.5953])\n",
      "\t Grad:  tensor([ 0.0004, -0.0029])\n",
      "Epoch 744, Loss 0.262989\n",
      "\t Params:  tensor([0.8416, 0.5954])\n",
      "\t Grad:  tensor([ 0.0004, -0.0029])\n",
      "Epoch 745, Loss 0.262989\n",
      "\t Params:  tensor([0.8416, 0.5954])\n",
      "\t Grad:  tensor([ 0.0004, -0.0029])\n",
      "Epoch 746, Loss 0.262989\n",
      "\t Params:  tensor([0.8416, 0.5954])\n",
      "\t Grad:  tensor([ 0.0004, -0.0029])\n",
      "Epoch 747, Loss 0.262989\n",
      "\t Params:  tensor([0.8416, 0.5955])\n",
      "\t Grad:  tensor([ 0.0004, -0.0028])\n",
      "Epoch 748, Loss 0.262989\n",
      "\t Params:  tensor([0.8416, 0.5955])\n",
      "\t Grad:  tensor([ 0.0004, -0.0028])\n",
      "Epoch 749, Loss 0.262988\n",
      "\t Params:  tensor([0.8416, 0.5955])\n",
      "\t Grad:  tensor([ 0.0004, -0.0028])\n",
      "Epoch 750, Loss 0.262988\n",
      "\t Params:  tensor([0.8416, 0.5955])\n",
      "\t Grad:  tensor([ 0.0004, -0.0028])\n",
      "Epoch 751, Loss 0.262988\n",
      "\t Params:  tensor([0.8416, 0.5956])\n",
      "\t Grad:  tensor([ 0.0004, -0.0028])\n",
      "Epoch 752, Loss 0.262988\n",
      "\t Params:  tensor([0.8416, 0.5956])\n",
      "\t Grad:  tensor([ 0.0004, -0.0027])\n",
      "Epoch 753, Loss 0.262988\n",
      "\t Params:  tensor([0.8416, 0.5956])\n",
      "\t Grad:  tensor([ 0.0004, -0.0027])\n",
      "Epoch 754, Loss 0.262988\n",
      "\t Params:  tensor([0.8416, 0.5957])\n",
      "\t Grad:  tensor([ 0.0004, -0.0027])\n",
      "Epoch 755, Loss 0.262988\n",
      "\t Params:  tensor([0.8416, 0.5957])\n",
      "\t Grad:  tensor([ 0.0004, -0.0027])\n",
      "Epoch 756, Loss 0.262988\n",
      "\t Params:  tensor([0.8416, 0.5957])\n",
      "\t Grad:  tensor([ 0.0004, -0.0027])\n",
      "Epoch 757, Loss 0.262988\n",
      "\t Params:  tensor([0.8416, 0.5957])\n",
      "\t Grad:  tensor([ 0.0004, -0.0027])\n",
      "Epoch 758, Loss 0.262988\n",
      "\t Params:  tensor([0.8416, 0.5958])\n",
      "\t Grad:  tensor([ 0.0004, -0.0026])\n",
      "Epoch 759, Loss 0.262988\n",
      "\t Params:  tensor([0.8416, 0.5958])\n",
      "\t Grad:  tensor([ 0.0003, -0.0026])\n",
      "Epoch 760, Loss 0.262988\n",
      "\t Params:  tensor([0.8416, 0.5958])\n",
      "\t Grad:  tensor([ 0.0003, -0.0026])\n",
      "Epoch 761, Loss 0.262988\n",
      "\t Params:  tensor([0.8416, 0.5958])\n",
      "\t Grad:  tensor([ 0.0003, -0.0026])\n",
      "Epoch 762, Loss 0.262987\n",
      "\t Params:  tensor([0.8416, 0.5959])\n",
      "\t Grad:  tensor([ 0.0003, -0.0026])\n",
      "Epoch 763, Loss 0.262987\n",
      "\t Params:  tensor([0.8416, 0.5959])\n",
      "\t Grad:  tensor([ 0.0003, -0.0026])\n",
      "Epoch 764, Loss 0.262987\n",
      "\t Params:  tensor([0.8416, 0.5959])\n",
      "\t Grad:  tensor([ 0.0003, -0.0025])\n",
      "Epoch 765, Loss 0.262987\n",
      "\t Params:  tensor([0.8416, 0.5959])\n",
      "\t Grad:  tensor([ 0.0003, -0.0025])\n",
      "Epoch 766, Loss 0.262987\n",
      "\t Params:  tensor([0.8415, 0.5960])\n",
      "\t Grad:  tensor([ 0.0003, -0.0025])\n",
      "Epoch 767, Loss 0.262987\n",
      "\t Params:  tensor([0.8415, 0.5960])\n",
      "\t Grad:  tensor([ 0.0003, -0.0025])\n",
      "Epoch 768, Loss 0.262987\n",
      "\t Params:  tensor([0.8415, 0.5960])\n",
      "\t Grad:  tensor([ 0.0003, -0.0025])\n",
      "Epoch 769, Loss 0.262987\n",
      "\t Params:  tensor([0.8415, 0.5960])\n",
      "\t Grad:  tensor([ 0.0003, -0.0025])\n",
      "Epoch 770, Loss 0.262987\n",
      "\t Params:  tensor([0.8415, 0.5961])\n",
      "\t Grad:  tensor([ 0.0003, -0.0024])\n",
      "Epoch 771, Loss 0.262987\n",
      "\t Params:  tensor([0.8415, 0.5961])\n",
      "\t Grad:  tensor([ 0.0003, -0.0024])\n",
      "Epoch 772, Loss 0.262987\n",
      "\t Params:  tensor([0.8415, 0.5961])\n",
      "\t Grad:  tensor([ 0.0003, -0.0024])\n",
      "Epoch 773, Loss 0.262987\n",
      "\t Params:  tensor([0.8415, 0.5961])\n",
      "\t Grad:  tensor([ 0.0003, -0.0024])\n",
      "Epoch 774, Loss 0.262987\n",
      "\t Params:  tensor([0.8415, 0.5962])\n",
      "\t Grad:  tensor([ 0.0003, -0.0024])\n",
      "Epoch 775, Loss 0.262987\n",
      "\t Params:  tensor([0.8415, 0.5962])\n",
      "\t Grad:  tensor([ 0.0003, -0.0024])\n",
      "Epoch 776, Loss 0.262987\n",
      "\t Params:  tensor([0.8415, 0.5962])\n",
      "\t Grad:  tensor([ 0.0003, -0.0023])\n",
      "Epoch 777, Loss 0.262987\n",
      "\t Params:  tensor([0.8415, 0.5962])\n",
      "\t Grad:  tensor([ 0.0003, -0.0023])\n",
      "Epoch 778, Loss 0.262986\n",
      "\t Params:  tensor([0.8415, 0.5963])\n",
      "\t Grad:  tensor([ 0.0003, -0.0023])\n",
      "Epoch 779, Loss 0.262987\n",
      "\t Params:  tensor([0.8415, 0.5963])\n",
      "\t Grad:  tensor([ 0.0003, -0.0023])\n",
      "Epoch 780, Loss 0.262986\n",
      "\t Params:  tensor([0.8415, 0.5963])\n",
      "\t Grad:  tensor([ 0.0003, -0.0023])\n",
      "Epoch 781, Loss 0.262986\n",
      "\t Params:  tensor([0.8415, 0.5963])\n",
      "\t Grad:  tensor([ 0.0003, -0.0023])\n",
      "Epoch 782, Loss 0.262986\n",
      "\t Params:  tensor([0.8415, 0.5963])\n",
      "\t Grad:  tensor([ 0.0003, -0.0022])\n",
      "Epoch 783, Loss 0.262986\n",
      "\t Params:  tensor([0.8415, 0.5964])\n",
      "\t Grad:  tensor([ 0.0003, -0.0022])\n",
      "Epoch 784, Loss 0.262986\n",
      "\t Params:  tensor([0.8415, 0.5964])\n",
      "\t Grad:  tensor([ 0.0003, -0.0022])\n",
      "Epoch 785, Loss 0.262986\n",
      "\t Params:  tensor([0.8415, 0.5964])\n",
      "\t Grad:  tensor([ 0.0003, -0.0022])\n",
      "Epoch 786, Loss 0.262986\n",
      "\t Params:  tensor([0.8415, 0.5964])\n",
      "\t Grad:  tensor([ 0.0003, -0.0022])\n",
      "Epoch 787, Loss 0.262986\n",
      "\t Params:  tensor([0.8415, 0.5965])\n",
      "\t Grad:  tensor([ 0.0003, -0.0022])\n",
      "Epoch 788, Loss 0.262986\n",
      "\t Params:  tensor([0.8415, 0.5965])\n",
      "\t Grad:  tensor([ 0.0003, -0.0022])\n",
      "Epoch 789, Loss 0.262986\n",
      "\t Params:  tensor([0.8415, 0.5965])\n",
      "\t Grad:  tensor([ 0.0003, -0.0021])\n",
      "Epoch 790, Loss 0.262986\n",
      "\t Params:  tensor([0.8415, 0.5965])\n",
      "\t Grad:  tensor([ 0.0003, -0.0021])\n",
      "Epoch 791, Loss 0.262986\n",
      "\t Params:  tensor([0.8415, 0.5965])\n",
      "\t Grad:  tensor([ 0.0003, -0.0021])\n",
      "Epoch 792, Loss 0.262986\n",
      "\t Params:  tensor([0.8415, 0.5966])\n",
      "\t Grad:  tensor([ 0.0003, -0.0021])\n",
      "Epoch 793, Loss 0.262986\n",
      "\t Params:  tensor([0.8415, 0.5966])\n",
      "\t Grad:  tensor([ 0.0003, -0.0021])\n",
      "Epoch 794, Loss 0.262986\n",
      "\t Params:  tensor([0.8415, 0.5966])\n",
      "\t Grad:  tensor([ 0.0003, -0.0021])\n",
      "Epoch 795, Loss 0.262986\n",
      "\t Params:  tensor([0.8415, 0.5966])\n",
      "\t Grad:  tensor([ 0.0003, -0.0021])\n",
      "Epoch 796, Loss 0.262986\n",
      "\t Params:  tensor([0.8415, 0.5966])\n",
      "\t Grad:  tensor([ 0.0003, -0.0020])\n",
      "Epoch 797, Loss 0.262986\n",
      "\t Params:  tensor([0.8415, 0.5967])\n",
      "\t Grad:  tensor([ 0.0003, -0.0020])\n",
      "Epoch 798, Loss 0.262985\n",
      "\t Params:  tensor([0.8415, 0.5967])\n",
      "\t Grad:  tensor([ 0.0003, -0.0020])\n",
      "Epoch 799, Loss 0.262985\n",
      "\t Params:  tensor([0.8414, 0.5967])\n",
      "\t Grad:  tensor([ 0.0003, -0.0020])\n",
      "Epoch 800, Loss 0.262986\n",
      "\t Params:  tensor([0.8414, 0.5967])\n",
      "\t Grad:  tensor([ 0.0003, -0.0020])\n",
      "Epoch 801, Loss 0.262985\n",
      "\t Params:  tensor([0.8414, 0.5967])\n",
      "\t Grad:  tensor([ 0.0003, -0.0020])\n",
      "Epoch 802, Loss 0.262985\n",
      "\t Params:  tensor([0.8414, 0.5968])\n",
      "\t Grad:  tensor([ 0.0003, -0.0020])\n",
      "Epoch 803, Loss 0.262985\n",
      "\t Params:  tensor([0.8414, 0.5968])\n",
      "\t Grad:  tensor([ 0.0003, -0.0020])\n",
      "Epoch 804, Loss 0.262985\n",
      "\t Params:  tensor([0.8414, 0.5968])\n",
      "\t Grad:  tensor([ 0.0003, -0.0019])\n",
      "Epoch 805, Loss 0.262985\n",
      "\t Params:  tensor([0.8414, 0.5968])\n",
      "\t Grad:  tensor([ 0.0003, -0.0019])\n",
      "Epoch 806, Loss 0.262985\n",
      "\t Params:  tensor([0.8414, 0.5968])\n",
      "\t Grad:  tensor([ 0.0003, -0.0019])\n",
      "Epoch 807, Loss 0.262985\n",
      "\t Params:  tensor([0.8414, 0.5969])\n",
      "\t Grad:  tensor([ 0.0003, -0.0019])\n",
      "Epoch 808, Loss 0.262985\n",
      "\t Params:  tensor([0.8414, 0.5969])\n",
      "\t Grad:  tensor([ 0.0003, -0.0019])\n",
      "Epoch 809, Loss 0.262985\n",
      "\t Params:  tensor([0.8414, 0.5969])\n",
      "\t Grad:  tensor([ 0.0002, -0.0019])\n",
      "Epoch 810, Loss 0.262985\n",
      "\t Params:  tensor([0.8414, 0.5969])\n",
      "\t Grad:  tensor([ 0.0002, -0.0019])\n",
      "Epoch 811, Loss 0.262985\n",
      "\t Params:  tensor([0.8414, 0.5969])\n",
      "\t Grad:  tensor([ 0.0002, -0.0019])\n",
      "Epoch 812, Loss 0.262985\n",
      "\t Params:  tensor([0.8414, 0.5970])\n",
      "\t Grad:  tensor([ 0.0002, -0.0018])\n",
      "Epoch 813, Loss 0.262985\n",
      "\t Params:  tensor([0.8414, 0.5970])\n",
      "\t Grad:  tensor([ 0.0002, -0.0018])\n",
      "Epoch 814, Loss 0.262985\n",
      "\t Params:  tensor([0.8414, 0.5970])\n",
      "\t Grad:  tensor([ 0.0002, -0.0018])\n",
      "Epoch 815, Loss 0.262985\n",
      "\t Params:  tensor([0.8414, 0.5970])\n",
      "\t Grad:  tensor([ 0.0002, -0.0018])\n",
      "Epoch 816, Loss 0.262985\n",
      "\t Params:  tensor([0.8414, 0.5970])\n",
      "\t Grad:  tensor([ 0.0002, -0.0018])\n",
      "Epoch 817, Loss 0.262985\n",
      "\t Params:  tensor([0.8414, 0.5970])\n",
      "\t Grad:  tensor([ 0.0002, -0.0018])\n",
      "Epoch 818, Loss 0.262985\n",
      "\t Params:  tensor([0.8414, 0.5971])\n",
      "\t Grad:  tensor([ 0.0002, -0.0018])\n",
      "Epoch 819, Loss 0.262985\n",
      "\t Params:  tensor([0.8414, 0.5971])\n",
      "\t Grad:  tensor([ 0.0002, -0.0018])\n",
      "Epoch 820, Loss 0.262985\n",
      "\t Params:  tensor([0.8414, 0.5971])\n",
      "\t Grad:  tensor([ 0.0002, -0.0017])\n",
      "Epoch 821, Loss 0.262985\n",
      "\t Params:  tensor([0.8414, 0.5971])\n",
      "\t Grad:  tensor([ 0.0002, -0.0017])\n",
      "Epoch 822, Loss 0.262985\n",
      "\t Params:  tensor([0.8414, 0.5971])\n",
      "\t Grad:  tensor([ 0.0002, -0.0017])\n",
      "Epoch 823, Loss 0.262985\n",
      "\t Params:  tensor([0.8414, 0.5971])\n",
      "\t Grad:  tensor([ 0.0002, -0.0017])\n",
      "Epoch 824, Loss 0.262985\n",
      "\t Params:  tensor([0.8414, 0.5972])\n",
      "\t Grad:  tensor([ 0.0002, -0.0017])\n",
      "Epoch 825, Loss 0.262985\n",
      "\t Params:  tensor([0.8414, 0.5972])\n",
      "\t Grad:  tensor([ 0.0002, -0.0017])\n",
      "Epoch 826, Loss 0.262985\n",
      "\t Params:  tensor([0.8414, 0.5972])\n",
      "\t Grad:  tensor([ 0.0002, -0.0017])\n",
      "Epoch 827, Loss 0.262985\n",
      "\t Params:  tensor([0.8414, 0.5972])\n",
      "\t Grad:  tensor([ 0.0002, -0.0017])\n",
      "Epoch 828, Loss 0.262985\n",
      "\t Params:  tensor([0.8414, 0.5972])\n",
      "\t Grad:  tensor([ 0.0002, -0.0017])\n",
      "Epoch 829, Loss 0.262985\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Params:  tensor([0.8414, 0.5972])\n",
      "\t Grad:  tensor([ 0.0002, -0.0016])\n",
      "Epoch 830, Loss 0.262985\n",
      "\t Params:  tensor([0.8414, 0.5973])\n",
      "\t Grad:  tensor([ 0.0002, -0.0016])\n",
      "Epoch 831, Loss 0.262984\n",
      "\t Params:  tensor([0.8414, 0.5973])\n",
      "\t Grad:  tensor([ 0.0002, -0.0016])\n",
      "Epoch 832, Loss 0.262984\n",
      "\t Params:  tensor([0.8414, 0.5973])\n",
      "\t Grad:  tensor([ 0.0002, -0.0016])\n",
      "Epoch 833, Loss 0.262984\n",
      "\t Params:  tensor([0.8414, 0.5973])\n",
      "\t Grad:  tensor([ 0.0002, -0.0016])\n",
      "Epoch 834, Loss 0.262984\n",
      "\t Params:  tensor([0.8414, 0.5973])\n",
      "\t Grad:  tensor([ 0.0002, -0.0016])\n",
      "Epoch 835, Loss 0.262984\n",
      "\t Params:  tensor([0.8414, 0.5973])\n",
      "\t Grad:  tensor([ 0.0002, -0.0016])\n",
      "Epoch 836, Loss 0.262984\n",
      "\t Params:  tensor([0.8414, 0.5974])\n",
      "\t Grad:  tensor([ 0.0002, -0.0016])\n",
      "Epoch 837, Loss 0.262984\n",
      "\t Params:  tensor([0.8414, 0.5974])\n",
      "\t Grad:  tensor([ 0.0002, -0.0016])\n",
      "Epoch 838, Loss 0.262984\n",
      "\t Params:  tensor([0.8414, 0.5974])\n",
      "\t Grad:  tensor([ 0.0002, -0.0015])\n",
      "Epoch 839, Loss 0.262984\n",
      "\t Params:  tensor([0.8414, 0.5974])\n",
      "\t Grad:  tensor([ 0.0002, -0.0015])\n",
      "Epoch 840, Loss 0.262984\n",
      "\t Params:  tensor([0.8414, 0.5974])\n",
      "\t Grad:  tensor([ 0.0002, -0.0015])\n",
      "Epoch 841, Loss 0.262984\n",
      "\t Params:  tensor([0.8414, 0.5974])\n",
      "\t Grad:  tensor([ 0.0002, -0.0015])\n",
      "Epoch 842, Loss 0.262984\n",
      "\t Params:  tensor([0.8413, 0.5974])\n",
      "\t Grad:  tensor([ 0.0002, -0.0015])\n",
      "Epoch 843, Loss 0.262984\n",
      "\t Params:  tensor([0.8413, 0.5975])\n",
      "\t Grad:  tensor([ 0.0002, -0.0015])\n",
      "Epoch 844, Loss 0.262984\n",
      "\t Params:  tensor([0.8413, 0.5975])\n",
      "\t Grad:  tensor([ 0.0002, -0.0015])\n",
      "Epoch 845, Loss 0.262984\n",
      "\t Params:  tensor([0.8413, 0.5975])\n",
      "\t Grad:  tensor([ 0.0002, -0.0015])\n",
      "Epoch 846, Loss 0.262984\n",
      "\t Params:  tensor([0.8413, 0.5975])\n",
      "\t Grad:  tensor([ 0.0002, -0.0015])\n",
      "Epoch 847, Loss 0.262984\n",
      "\t Params:  tensor([0.8413, 0.5975])\n",
      "\t Grad:  tensor([ 0.0002, -0.0015])\n",
      "Epoch 848, Loss 0.262984\n",
      "\t Params:  tensor([0.8413, 0.5975])\n",
      "\t Grad:  tensor([ 0.0002, -0.0015])\n",
      "Epoch 849, Loss 0.262984\n",
      "\t Params:  tensor([0.8413, 0.5976])\n",
      "\t Grad:  tensor([ 0.0002, -0.0014])\n",
      "Epoch 850, Loss 0.262984\n",
      "\t Params:  tensor([0.8413, 0.5976])\n",
      "\t Grad:  tensor([ 0.0002, -0.0014])\n",
      "Epoch 851, Loss 0.262984\n",
      "\t Params:  tensor([0.8413, 0.5976])\n",
      "\t Grad:  tensor([ 0.0002, -0.0014])\n",
      "Epoch 852, Loss 0.262984\n",
      "\t Params:  tensor([0.8413, 0.5976])\n",
      "\t Grad:  tensor([ 0.0002, -0.0014])\n",
      "Epoch 853, Loss 0.262984\n",
      "\t Params:  tensor([0.8413, 0.5976])\n",
      "\t Grad:  tensor([ 0.0002, -0.0014])\n",
      "Epoch 854, Loss 0.262984\n",
      "\t Params:  tensor([0.8413, 0.5976])\n",
      "\t Grad:  tensor([ 0.0002, -0.0014])\n",
      "Epoch 855, Loss 0.262984\n",
      "\t Params:  tensor([0.8413, 0.5976])\n",
      "\t Grad:  tensor([ 0.0002, -0.0014])\n",
      "Epoch 856, Loss 0.262984\n",
      "\t Params:  tensor([0.8413, 0.5977])\n",
      "\t Grad:  tensor([ 0.0002, -0.0014])\n",
      "Epoch 857, Loss 0.262984\n",
      "\t Params:  tensor([0.8413, 0.5977])\n",
      "\t Grad:  tensor([ 0.0002, -0.0014])\n",
      "Epoch 858, Loss 0.262984\n",
      "\t Params:  tensor([0.8413, 0.5977])\n",
      "\t Grad:  tensor([ 0.0002, -0.0014])\n",
      "Epoch 859, Loss 0.262984\n",
      "\t Params:  tensor([0.8413, 0.5977])\n",
      "\t Grad:  tensor([ 0.0002, -0.0013])\n",
      "Epoch 860, Loss 0.262984\n",
      "\t Params:  tensor([0.8413, 0.5977])\n",
      "\t Grad:  tensor([ 0.0002, -0.0013])\n",
      "Epoch 861, Loss 0.262984\n",
      "\t Params:  tensor([0.8413, 0.5977])\n",
      "\t Grad:  tensor([ 0.0002, -0.0013])\n",
      "Epoch 862, Loss 0.262984\n",
      "\t Params:  tensor([0.8413, 0.5977])\n",
      "\t Grad:  tensor([ 0.0002, -0.0013])\n",
      "Epoch 863, Loss 0.262984\n",
      "\t Params:  tensor([0.8413, 0.5977])\n",
      "\t Grad:  tensor([ 0.0002, -0.0013])\n",
      "Epoch 864, Loss 0.262984\n",
      "\t Params:  tensor([0.8413, 0.5978])\n",
      "\t Grad:  tensor([ 0.0002, -0.0013])\n",
      "Epoch 865, Loss 0.262984\n",
      "\t Params:  tensor([0.8413, 0.5978])\n",
      "\t Grad:  tensor([ 0.0002, -0.0013])\n",
      "Epoch 866, Loss 0.262984\n",
      "\t Params:  tensor([0.8413, 0.5978])\n",
      "\t Grad:  tensor([ 0.0002, -0.0013])\n",
      "Epoch 867, Loss 0.262984\n",
      "\t Params:  tensor([0.8413, 0.5978])\n",
      "\t Grad:  tensor([ 0.0002, -0.0013])\n",
      "Epoch 868, Loss 0.262984\n",
      "\t Params:  tensor([0.8413, 0.5978])\n",
      "\t Grad:  tensor([ 0.0002, -0.0013])\n",
      "Epoch 869, Loss 0.262984\n",
      "\t Params:  tensor([0.8413, 0.5978])\n",
      "\t Grad:  tensor([ 0.0002, -0.0013])\n",
      "Epoch 870, Loss 0.262984\n",
      "\t Params:  tensor([0.8413, 0.5978])\n",
      "\t Grad:  tensor([ 0.0002, -0.0013])\n",
      "Epoch 871, Loss 0.262984\n",
      "\t Params:  tensor([0.8413, 0.5978])\n",
      "\t Grad:  tensor([ 0.0002, -0.0012])\n",
      "Epoch 872, Loss 0.262983\n",
      "\t Params:  tensor([0.8413, 0.5979])\n",
      "\t Grad:  tensor([ 0.0002, -0.0012])\n",
      "Epoch 873, Loss 0.262984\n",
      "\t Params:  tensor([0.8413, 0.5979])\n",
      "\t Grad:  tensor([ 0.0002, -0.0012])\n",
      "Epoch 874, Loss 0.262984\n",
      "\t Params:  tensor([0.8413, 0.5979])\n",
      "\t Grad:  tensor([ 0.0002, -0.0012])\n",
      "Epoch 875, Loss 0.262984\n",
      "\t Params:  tensor([0.8413, 0.5979])\n",
      "\t Grad:  tensor([ 0.0002, -0.0012])\n",
      "Epoch 876, Loss 0.262984\n",
      "\t Params:  tensor([0.8413, 0.5979])\n",
      "\t Grad:  tensor([ 0.0002, -0.0012])\n",
      "Epoch 877, Loss 0.262983\n",
      "\t Params:  tensor([0.8413, 0.5979])\n",
      "\t Grad:  tensor([ 0.0002, -0.0012])\n",
      "Epoch 878, Loss 0.262983\n",
      "\t Params:  tensor([0.8413, 0.5979])\n",
      "\t Grad:  tensor([ 0.0002, -0.0012])\n",
      "Epoch 879, Loss 0.262983\n",
      "\t Params:  tensor([0.8413, 0.5979])\n",
      "\t Grad:  tensor([ 0.0002, -0.0012])\n",
      "Epoch 880, Loss 0.262983\n",
      "\t Params:  tensor([0.8413, 0.5980])\n",
      "\t Grad:  tensor([ 0.0002, -0.0012])\n",
      "Epoch 881, Loss 0.262984\n",
      "\t Params:  tensor([0.8413, 0.5980])\n",
      "\t Grad:  tensor([ 0.0002, -0.0012])\n",
      "Epoch 882, Loss 0.262983\n",
      "\t Params:  tensor([0.8413, 0.5980])\n",
      "\t Grad:  tensor([ 0.0002, -0.0012])\n",
      "Epoch 883, Loss 0.262983\n",
      "\t Params:  tensor([0.8413, 0.5980])\n",
      "\t Grad:  tensor([ 0.0002, -0.0011])\n",
      "Epoch 884, Loss 0.262983\n",
      "\t Params:  tensor([0.8413, 0.5980])\n",
      "\t Grad:  tensor([ 0.0002, -0.0011])\n",
      "Epoch 885, Loss 0.262983\n",
      "\t Params:  tensor([0.8413, 0.5980])\n",
      "\t Grad:  tensor([ 0.0001, -0.0011])\n",
      "Epoch 886, Loss 0.262983\n",
      "\t Params:  tensor([0.8413, 0.5980])\n",
      "\t Grad:  tensor([ 0.0002, -0.0011])\n",
      "Epoch 887, Loss 0.262983\n",
      "\t Params:  tensor([0.8413, 0.5980])\n",
      "\t Grad:  tensor([ 0.0001, -0.0011])\n",
      "Epoch 888, Loss 0.262983\n",
      "\t Params:  tensor([0.8413, 0.5980])\n",
      "\t Grad:  tensor([ 0.0001, -0.0011])\n",
      "Epoch 889, Loss 0.262983\n",
      "\t Params:  tensor([0.8413, 0.5981])\n",
      "\t Grad:  tensor([ 0.0002, -0.0011])\n",
      "Epoch 890, Loss 0.262984\n",
      "\t Params:  tensor([0.8413, 0.5981])\n",
      "\t Grad:  tensor([ 0.0001, -0.0011])\n",
      "Epoch 891, Loss 0.262983\n",
      "\t Params:  tensor([0.8413, 0.5981])\n",
      "\t Grad:  tensor([ 0.0001, -0.0011])\n",
      "Epoch 892, Loss 0.262983\n",
      "\t Params:  tensor([0.8413, 0.5981])\n",
      "\t Grad:  tensor([ 0.0001, -0.0011])\n",
      "Epoch 893, Loss 0.262983\n",
      "\t Params:  tensor([0.8413, 0.5981])\n",
      "\t Grad:  tensor([ 0.0001, -0.0011])\n",
      "Epoch 894, Loss 0.262983\n",
      "\t Params:  tensor([0.8413, 0.5981])\n",
      "\t Grad:  tensor([ 0.0001, -0.0011])\n",
      "Epoch 895, Loss 0.262983\n",
      "\t Params:  tensor([0.8413, 0.5981])\n",
      "\t Grad:  tensor([ 0.0001, -0.0011])\n",
      "Epoch 896, Loss 0.262983\n",
      "\t Params:  tensor([0.8413, 0.5981])\n",
      "\t Grad:  tensor([ 0.0001, -0.0011])\n",
      "Epoch 897, Loss 0.262983\n",
      "\t Params:  tensor([0.8413, 0.5981])\n",
      "\t Grad:  tensor([ 0.0001, -0.0010])\n",
      "Epoch 898, Loss 0.262983\n",
      "\t Params:  tensor([0.8413, 0.5982])\n",
      "\t Grad:  tensor([ 0.0001, -0.0010])\n",
      "Epoch 899, Loss 0.262983\n",
      "\t Params:  tensor([0.8413, 0.5982])\n",
      "\t Grad:  tensor([ 0.0001, -0.0010])\n",
      "Epoch 900, Loss 0.262983\n",
      "\t Params:  tensor([0.8413, 0.5982])\n",
      "\t Grad:  tensor([ 0.0001, -0.0010])\n",
      "Epoch 901, Loss 0.262983\n",
      "\t Params:  tensor([0.8413, 0.5982])\n",
      "\t Grad:  tensor([ 0.0001, -0.0010])\n",
      "Epoch 902, Loss 0.262983\n",
      "\t Params:  tensor([0.8412, 0.5982])\n",
      "\t Grad:  tensor([ 0.0001, -0.0010])\n",
      "Epoch 903, Loss 0.262983\n",
      "\t Params:  tensor([0.8412, 0.5982])\n",
      "\t Grad:  tensor([ 0.0001, -0.0010])\n",
      "Epoch 904, Loss 0.262983\n",
      "\t Params:  tensor([0.8412, 0.5982])\n",
      "\t Grad:  tensor([ 0.0001, -0.0010])\n",
      "Epoch 905, Loss 0.262983\n",
      "\t Params:  tensor([0.8412, 0.5982])\n",
      "\t Grad:  tensor([ 0.0001, -0.0010])\n",
      "Epoch 906, Loss 0.262983\n",
      "\t Params:  tensor([0.8412, 0.5982])\n",
      "\t Grad:  tensor([ 0.0001, -0.0010])\n",
      "Epoch 907, Loss 0.262983\n",
      "\t Params:  tensor([0.8412, 0.5982])\n",
      "\t Grad:  tensor([ 0.0001, -0.0010])\n",
      "Epoch 908, Loss 0.262983\n",
      "\t Params:  tensor([0.8412, 0.5983])\n",
      "\t Grad:  tensor([ 0.0001, -0.0010])\n",
      "Epoch 909, Loss 0.262983\n",
      "\t Params:  tensor([0.8412, 0.5983])\n",
      "\t Grad:  tensor([ 0.0001, -0.0010])\n",
      "Epoch 910, Loss 0.262983\n",
      "\t Params:  tensor([0.8412, 0.5983])\n",
      "\t Grad:  tensor([ 0.0001, -0.0010])\n",
      "Epoch 911, Loss 0.262983\n",
      "\t Params:  tensor([0.8412, 0.5983])\n",
      "\t Grad:  tensor([ 0.0001, -0.0010])\n",
      "Epoch 912, Loss 0.262983\n",
      "\t Params:  tensor([0.8412, 0.5983])\n",
      "\t Grad:  tensor([ 0.0001, -0.0009])\n",
      "Epoch 913, Loss 0.262983\n",
      "\t Params:  tensor([0.8412, 0.5983])\n",
      "\t Grad:  tensor([ 0.0001, -0.0009])\n",
      "Epoch 914, Loss 0.262983\n",
      "\t Params:  tensor([0.8412, 0.5983])\n",
      "\t Grad:  tensor([ 0.0001, -0.0009])\n",
      "Epoch 915, Loss 0.262983\n",
      "\t Params:  tensor([0.8412, 0.5983])\n",
      "\t Grad:  tensor([ 0.0001, -0.0009])\n",
      "Epoch 916, Loss 0.262983\n",
      "\t Params:  tensor([0.8412, 0.5983])\n",
      "\t Grad:  tensor([ 0.0001, -0.0009])\n",
      "Epoch 917, Loss 0.262983\n",
      "\t Params:  tensor([0.8412, 0.5983])\n",
      "\t Grad:  tensor([ 0.0001, -0.0009])\n",
      "Epoch 918, Loss 0.262983\n",
      "\t Params:  tensor([0.8412, 0.5983])\n",
      "\t Grad:  tensor([ 0.0001, -0.0009])\n",
      "Epoch 919, Loss 0.262983\n",
      "\t Params:  tensor([0.8412, 0.5984])\n",
      "\t Grad:  tensor([ 0.0001, -0.0009])\n",
      "Epoch 920, Loss 0.262983\n",
      "\t Params:  tensor([0.8412, 0.5984])\n",
      "\t Grad:  tensor([ 0.0001, -0.0009])\n",
      "Epoch 921, Loss 0.262983\n",
      "\t Params:  tensor([0.8412, 0.5984])\n",
      "\t Grad:  tensor([ 0.0001, -0.0009])\n",
      "Epoch 922, Loss 0.262983\n",
      "\t Params:  tensor([0.8412, 0.5984])\n",
      "\t Grad:  tensor([ 0.0001, -0.0009])\n",
      "Epoch 923, Loss 0.262983\n",
      "\t Params:  tensor([0.8412, 0.5984])\n",
      "\t Grad:  tensor([ 0.0001, -0.0009])\n",
      "Epoch 924, Loss 0.262983\n",
      "\t Params:  tensor([0.8412, 0.5984])\n",
      "\t Grad:  tensor([ 0.0001, -0.0009])\n",
      "Epoch 925, Loss 0.262983\n",
      "\t Params:  tensor([0.8412, 0.5984])\n",
      "\t Grad:  tensor([ 0.0001, -0.0009])\n",
      "Epoch 926, Loss 0.262983\n",
      "\t Params:  tensor([0.8412, 0.5984])\n",
      "\t Grad:  tensor([ 0.0001, -0.0009])\n",
      "Epoch 927, Loss 0.262983\n",
      "\t Params:  tensor([0.8412, 0.5984])\n",
      "\t Grad:  tensor([ 0.0001, -0.0009])\n",
      "Epoch 928, Loss 0.262983\n",
      "\t Params:  tensor([0.8412, 0.5984])\n",
      "\t Grad:  tensor([ 0.0001, -0.0009])\n",
      "Epoch 929, Loss 0.262983\n",
      "\t Params:  tensor([0.8412, 0.5984])\n",
      "\t Grad:  tensor([ 0.0001, -0.0008])\n",
      "Epoch 930, Loss 0.262983\n",
      "\t Params:  tensor([0.8412, 0.5985])\n",
      "\t Grad:  tensor([ 0.0001, -0.0008])\n",
      "Epoch 931, Loss 0.262983\n",
      "\t Params:  tensor([0.8412, 0.5985])\n",
      "\t Grad:  tensor([ 0.0001, -0.0008])\n",
      "Epoch 932, Loss 0.262983\n",
      "\t Params:  tensor([0.8412, 0.5985])\n",
      "\t Grad:  tensor([ 0.0001, -0.0008])\n",
      "Epoch 933, Loss 0.262983\n",
      "\t Params:  tensor([0.8412, 0.5985])\n",
      "\t Grad:  tensor([ 0.0001, -0.0008])\n",
      "Epoch 934, Loss 0.262983\n",
      "\t Params:  tensor([0.8412, 0.5985])\n",
      "\t Grad:  tensor([ 0.0001, -0.0008])\n",
      "Epoch 935, Loss 0.262983\n",
      "\t Params:  tensor([0.8412, 0.5985])\n",
      "\t Grad:  tensor([ 0.0001, -0.0008])\n",
      "Epoch 936, Loss 0.262983\n",
      "\t Params:  tensor([0.8412, 0.5985])\n",
      "\t Grad:  tensor([ 0.0001, -0.0008])\n",
      "Epoch 937, Loss 0.262983\n",
      "\t Params:  tensor([0.8412, 0.5985])\n",
      "\t Grad:  tensor([ 0.0001, -0.0008])\n",
      "Epoch 938, Loss 0.262983\n",
      "\t Params:  tensor([0.8412, 0.5985])\n",
      "\t Grad:  tensor([ 0.0001, -0.0008])\n",
      "Epoch 939, Loss 0.262983\n",
      "\t Params:  tensor([0.8412, 0.5985])\n",
      "\t Grad:  tensor([ 0.0001, -0.0008])\n",
      "Epoch 940, Loss 0.262983\n",
      "\t Params:  tensor([0.8412, 0.5985])\n",
      "\t Grad:  tensor([ 0.0001, -0.0008])\n",
      "Epoch 941, Loss 0.262983\n",
      "\t Params:  tensor([0.8412, 0.5985])\n",
      "\t Grad:  tensor([ 0.0001, -0.0008])\n",
      "Epoch 942, Loss 0.262983\n",
      "\t Params:  tensor([0.8412, 0.5985])\n",
      "\t Grad:  tensor([ 0.0001, -0.0008])\n",
      "Epoch 943, Loss 0.262983\n",
      "\t Params:  tensor([0.8412, 0.5986])\n",
      "\t Grad:  tensor([ 0.0001, -0.0008])\n",
      "Epoch 944, Loss 0.262983\n",
      "\t Params:  tensor([0.8412, 0.5986])\n",
      "\t Grad:  tensor([ 0.0001, -0.0008])\n",
      "Epoch 945, Loss 0.262983\n",
      "\t Params:  tensor([0.8412, 0.5986])\n",
      "\t Grad:  tensor([ 0.0001, -0.0008])\n",
      "Epoch 946, Loss 0.262983\n",
      "\t Params:  tensor([0.8412, 0.5986])\n",
      "\t Grad:  tensor([ 0.0001, -0.0008])\n",
      "Epoch 947, Loss 0.262983\n",
      "\t Params:  tensor([0.8412, 0.5986])\n",
      "\t Grad:  tensor([ 0.0001, -0.0008])\n",
      "Epoch 948, Loss 0.262983\n",
      "\t Params:  tensor([0.8412, 0.5986])\n",
      "\t Grad:  tensor([ 0.0001, -0.0007])\n",
      "Epoch 949, Loss 0.262983\n",
      "\t Params:  tensor([0.8412, 0.5986])\n",
      "\t Grad:  tensor([ 9.8348e-05, -7.4108e-04])\n",
      "Epoch 950, Loss 0.262983\n",
      "\t Params:  tensor([0.8412, 0.5986])\n",
      "\t Grad:  tensor([ 9.5844e-05, -7.3629e-04])\n",
      "Epoch 951, Loss 0.262983\n",
      "\t Params:  tensor([0.8412, 0.5986])\n",
      "\t Grad:  tensor([ 9.5069e-05, -7.3139e-04])\n",
      "Epoch 952, Loss 0.262983\n",
      "\t Params:  tensor([0.8412, 0.5986])\n",
      "\t Grad:  tensor([ 0.0001, -0.0007])\n",
      "Epoch 953, Loss 0.262983\n",
      "\t Params:  tensor([0.8412, 0.5986])\n",
      "\t Grad:  tensor([ 9.2506e-05, -7.2181e-04])\n",
      "Epoch 954, Loss 0.262983\n",
      "\t Params:  tensor([0.8412, 0.5986])\n",
      "\t Grad:  tensor([ 9.6619e-05, -7.1650e-04])\n",
      "Epoch 955, Loss 0.262983\n",
      "\t Params:  tensor([0.8412, 0.5986])\n",
      "\t Grad:  tensor([ 9.3102e-05, -7.1204e-04])\n",
      "Epoch 956, Loss 0.262983\n",
      "\t Params:  tensor([0.8412, 0.5987])\n",
      "\t Grad:  tensor([ 9.5487e-05, -7.0708e-04])\n",
      "Epoch 957, Loss 0.262983\n",
      "\t Params:  tensor([0.8412, 0.5987])\n",
      "\t Grad:  tensor([ 9.1553e-05, -7.0278e-04])\n",
      "Epoch 958, Loss 0.262983\n",
      "\t Params:  tensor([0.8412, 0.5987])\n",
      "\t Grad:  tensor([ 9.5665e-05, -6.9757e-04])\n",
      "Epoch 959, Loss 0.262983\n",
      "\t Params:  tensor([0.8412, 0.5987])\n",
      "\t Grad:  tensor([ 9.2626e-05, -6.9327e-04])\n",
      "Epoch 960, Loss 0.262983\n",
      "\t Params:  tensor([0.8412, 0.5987])\n",
      "\t Grad:  tensor([ 9.0599e-05, -6.8879e-04])\n",
      "Epoch 961, Loss 0.262983\n",
      "\t Params:  tensor([0.8412, 0.5987])\n",
      "\t Grad:  tensor([ 9.1970e-05, -6.8394e-04])\n",
      "Epoch 962, Loss 0.262983\n",
      "\t Params:  tensor([0.8412, 0.5987])\n",
      "\t Grad:  tensor([ 9.2924e-05, -6.7926e-04])\n",
      "Epoch 963, Loss 0.262983\n",
      "\t Params:  tensor([0.8412, 0.5987])\n",
      "\t Grad:  tensor([ 8.9884e-05, -6.7502e-04])\n",
      "Epoch 964, Loss 0.262983\n",
      "\t Params:  tensor([0.8412, 0.5987])\n",
      "\t Grad:  tensor([ 8.9347e-05, -6.7058e-04])\n",
      "Epoch 965, Loss 0.262983\n",
      "\t Params:  tensor([0.8412, 0.5987])\n",
      "\t Grad:  tensor([ 8.8751e-05, -6.6607e-04])\n",
      "Epoch 966, Loss 0.262983\n",
      "\t Params:  tensor([0.8412, 0.5987])\n",
      "\t Grad:  tensor([ 8.7261e-05, -6.6175e-04])\n",
      "Epoch 967, Loss 0.262983\n",
      "\t Params:  tensor([0.8412, 0.5987])\n",
      "\t Grad:  tensor([ 8.7917e-05, -6.5726e-04])\n",
      "Epoch 968, Loss 0.262983\n",
      "\t Params:  tensor([0.8412, 0.5987])\n",
      "\t Grad:  tensor([ 8.6784e-05, -6.5302e-04])\n",
      "Epoch 969, Loss 0.262983\n",
      "\t Params:  tensor([0.8412, 0.5987])\n",
      "\t Grad:  tensor([ 8.3506e-05, -6.4907e-04])\n",
      "Epoch 970, Loss 0.262983\n",
      "\t Params:  tensor([0.8412, 0.5987])\n",
      "\t Grad:  tensor([ 8.3148e-05, -6.4461e-04])\n",
      "Epoch 971, Loss 0.262983\n",
      "\t Params:  tensor([0.8412, 0.5988])\n",
      "\t Grad:  tensor([ 8.5950e-05, -6.3997e-04])\n",
      "Epoch 972, Loss 0.262983\n",
      "\t Params:  tensor([0.8412, 0.5988])\n",
      "\t Grad:  tensor([ 8.8453e-05, -6.3530e-04])\n",
      "Epoch 973, Loss 0.262983\n",
      "\t Params:  tensor([0.8412, 0.5988])\n",
      "\t Grad:  tensor([ 8.5056e-05, -6.3155e-04])\n",
      "Epoch 974, Loss 0.262983\n",
      "\t Params:  tensor([0.8412, 0.5988])\n",
      "\t Grad:  tensor([ 8.1778e-05, -6.2758e-04])\n",
      "Epoch 975, Loss 0.262983\n",
      "\t Params:  tensor([0.8412, 0.5988])\n",
      "\t Grad:  tensor([ 8.4221e-05, -6.2314e-04])\n",
      "Epoch 976, Loss 0.262983\n",
      "\t Params:  tensor([0.8412, 0.5988])\n",
      "\t Grad:  tensor([ 8.2850e-05, -6.1906e-04])\n",
      "Epoch 977, Loss 0.262983\n",
      "\t Params:  tensor([0.8412, 0.5988])\n",
      "\t Grad:  tensor([ 8.3148e-05, -6.1484e-04])\n",
      "Epoch 978, Loss 0.262983\n",
      "\t Params:  tensor([0.8412, 0.5988])\n",
      "\t Grad:  tensor([ 7.9036e-05, -6.1122e-04])\n",
      "Epoch 979, Loss 0.262983\n",
      "\t Params:  tensor([0.8412, 0.5988])\n",
      "\t Grad:  tensor([ 8.2493e-05, -6.0672e-04])\n",
      "Epoch 980, Loss 0.262983\n",
      "\t Params:  tensor([0.8412, 0.5988])\n",
      "\t Grad:  tensor([ 7.8857e-05, -6.0302e-04])\n",
      "Epoch 981, Loss 0.262983\n",
      "\t Params:  tensor([0.8412, 0.5988])\n",
      "\t Grad:  tensor([ 8.3148e-05, -5.9841e-04])\n",
      "Epoch 982, Loss 0.262983\n",
      "\t Params:  tensor([0.8412, 0.5988])\n",
      "\t Grad:  tensor([ 7.9393e-05, -5.9495e-04])\n",
      "Epoch 983, Loss 0.262983\n",
      "\t Params:  tensor([0.8412, 0.5988])\n",
      "\t Grad:  tensor([ 8.1897e-05, -5.9059e-04])\n",
      "Epoch 984, Loss 0.262983\n",
      "\t Params:  tensor([0.8412, 0.5988])\n",
      "\t Grad:  tensor([ 7.4744e-05, -5.8749e-04])\n",
      "Epoch 985, Loss 0.262983\n",
      "\t Params:  tensor([0.8412, 0.5988])\n",
      "\t Grad:  tensor([ 7.6354e-05, -5.8331e-04])\n",
      "Epoch 986, Loss 0.262983\n",
      "\t Params:  tensor([0.8412, 0.5988])\n",
      "\t Grad:  tensor([ 7.5579e-05, -5.7938e-04])\n",
      "Epoch 987, Loss 0.262983\n",
      "\t Params:  tensor([0.8412, 0.5988])\n",
      "\t Grad:  tensor([ 7.4983e-05, -5.7555e-04])\n",
      "Epoch 988, Loss 0.262983\n",
      "\t Params:  tensor([0.8412, 0.5989])\n",
      "\t Grad:  tensor([ 7.8440e-05, -5.7139e-04])\n",
      "Epoch 989, Loss 0.262983\n",
      "\t Params:  tensor([0.8412, 0.5989])\n",
      "\t Grad:  tensor([ 7.4208e-05, -5.6801e-04])\n",
      "Epoch 990, Loss 0.262983\n",
      "\t Params:  tensor([0.8412, 0.5989])\n",
      "\t Grad:  tensor([ 7.5877e-05, -5.6387e-04])\n",
      "Epoch 991, Loss 0.262983\n",
      "\t Params:  tensor([0.8412, 0.5989])\n",
      "\t Grad:  tensor([ 7.4685e-05, -5.6029e-04])\n",
      "Epoch 992, Loss 0.262983\n",
      "\t Params:  tensor([0.8412, 0.5989])\n",
      "\t Grad:  tensor([ 7.0751e-05, -5.5705e-04])\n",
      "Epoch 993, Loss 0.262983\n",
      "\t Params:  tensor([0.8412, 0.5989])\n",
      "\t Grad:  tensor([ 7.3373e-05, -5.5298e-04])\n",
      "Epoch 994, Loss 0.262983\n",
      "\t Params:  tensor([0.8412, 0.5989])\n",
      "\t Grad:  tensor([ 7.7307e-05, -5.4870e-04])\n",
      "Epoch 995, Loss 0.262983\n",
      "\t Params:  tensor([0.8412, 0.5989])\n",
      "\t Grad:  tensor([ 7.2122e-05, -5.4572e-04])\n",
      "Epoch 996, Loss 0.262983\n",
      "\t Params:  tensor([0.8412, 0.5989])\n",
      "\t Grad:  tensor([ 7.1526e-05, -5.4202e-04])\n",
      "Epoch 997, Loss 0.262983\n",
      "\t Params:  tensor([0.8412, 0.5989])\n",
      "\t Grad:  tensor([ 7.2956e-05, -5.3823e-04])\n",
      "Epoch 998, Loss 0.262983\n",
      "\t Params:  tensor([0.8412, 0.5989])\n",
      "\t Grad:  tensor([ 7.3016e-05, -5.3463e-04])\n",
      "Epoch 999, Loss 0.262983\n",
      "\t Params:  tensor([0.8412, 0.5989])\n",
      "\t Grad:  tensor([ 7.2479e-05, -5.3101e-04])\n",
      "Epoch 1000, Loss 0.262983\n",
      "\t Params:  tensor([0.8412, 0.5989])\n",
      "\t Grad:  tensor([ 7.3373e-05, -5.2740e-04])\n",
      "Epoch 1001, Loss 0.262983\n",
      "\t Params:  tensor([0.8412, 0.5989])\n",
      "\t Grad:  tensor([ 7.0810e-05, -5.2416e-04])\n",
      "Epoch 1002, Loss 0.262983\n",
      "\t Params:  tensor([0.8412, 0.5989])\n",
      "\t Grad:  tensor([ 7.0989e-05, -5.2063e-04])\n",
      "Epoch 1003, Loss 0.262983\n",
      "\t Params:  tensor([0.8411, 0.5989])\n",
      "\t Grad:  tensor([ 6.7949e-05, -5.1751e-04])\n",
      "Epoch 1004, Loss 0.262983\n",
      "\t Params:  tensor([0.8411, 0.5989])\n",
      "\t Grad:  tensor([ 6.9320e-05, -5.1384e-04])\n",
      "Epoch 1005, Loss 0.262983\n",
      "\t Params:  tensor([0.8411, 0.5989])\n",
      "\t Grad:  tensor([ 6.4433e-05, -5.1095e-04])\n",
      "Epoch 1006, Loss 0.262983\n",
      "\t Params:  tensor([0.8411, 0.5990])\n",
      "\t Grad:  tensor([ 6.6042e-05, -5.0728e-04])\n",
      "Epoch 1007, Loss 0.262983\n",
      "\t Params:  tensor([0.8411, 0.5990])\n",
      "\t Grad:  tensor([ 7.2122e-05, -5.0320e-04])\n",
      "Epoch 1008, Loss 0.262983\n",
      "\t Params:  tensor([0.8411, 0.5990])\n",
      "\t Grad:  tensor([ 6.6400e-05, -5.0062e-04])\n",
      "Epoch 1009, Loss 0.262983\n",
      "\t Params:  tensor([0.8411, 0.5990])\n",
      "\t Grad:  tensor([ 6.7294e-05, -4.9700e-04])\n",
      "Epoch 1010, Loss 0.262983\n",
      "\t Params:  tensor([0.8411, 0.5990])\n",
      "\t Grad:  tensor([ 6.4671e-05, -4.9403e-04])\n",
      "Epoch 1011, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5990])\n",
      "\t Grad:  tensor([ 6.6757e-05, -4.9046e-04])\n",
      "Epoch 1012, Loss 0.262983\n",
      "\t Params:  tensor([0.8411, 0.5990])\n",
      "\t Grad:  tensor([ 6.7174e-05, -4.8719e-04])\n",
      "Epoch 1013, Loss 0.262983\n",
      "\t Params:  tensor([0.8411, 0.5990])\n",
      "\t Grad:  tensor([ 6.5565e-05, -4.8397e-04])\n",
      "Epoch 1014, Loss 0.262983\n",
      "\t Params:  tensor([0.8411, 0.5990])\n",
      "\t Grad:  tensor([ 6.3837e-05, -4.8098e-04])\n",
      "Epoch 1015, Loss 0.262983\n",
      "\t Params:  tensor([0.8411, 0.5990])\n",
      "\t Grad:  tensor([ 6.1870e-05, -4.7799e-04])\n",
      "Epoch 1016, Loss 0.262983\n",
      "\t Params:  tensor([0.8411, 0.5990])\n",
      "\t Grad:  tensor([ 6.4552e-05, -4.7440e-04])\n",
      "Epoch 1017, Loss 0.262983\n",
      "\t Params:  tensor([0.8411, 0.5990])\n",
      "\t Grad:  tensor([ 6.3598e-05, -4.7140e-04])\n",
      "Epoch 1018, Loss 0.262983\n",
      "\t Params:  tensor([0.8411, 0.5990])\n",
      "\t Grad:  tensor([ 5.9664e-05, -4.6859e-04])\n",
      "Epoch 1019, Loss 0.262983\n",
      "\t Params:  tensor([0.8411, 0.5990])\n",
      "\t Grad:  tensor([ 6.3419e-05, -4.6500e-04])\n",
      "Epoch 1020, Loss 0.262983\n",
      "\t Params:  tensor([0.8411, 0.5990])\n",
      "\t Grad:  tensor([ 6.3717e-05, -4.6188e-04])\n",
      "Epoch 1021, Loss 0.262983\n",
      "\t Params:  tensor([0.8411, 0.5990])\n",
      "\t Grad:  tensor([ 5.6505e-05, -4.5969e-04])\n",
      "Epoch 1022, Loss 0.262983\n",
      "\t Params:  tensor([0.8411, 0.5990])\n",
      "\t Grad:  tensor([ 6.0976e-05, -4.5587e-04])\n",
      "Epoch 1023, Loss 0.262983\n",
      "\t Params:  tensor([0.8411, 0.5990])\n",
      "\t Grad:  tensor([ 6.3002e-05, -4.5268e-04])\n",
      "Epoch 1024, Loss 0.262983\n",
      "\t Params:  tensor([0.8411, 0.5990])\n",
      "\t Grad:  tensor([ 5.8055e-05, -4.5023e-04])\n",
      "Epoch 1025, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5990])\n",
      "\t Grad:  tensor([ 5.9783e-05, -4.4702e-04])\n",
      "Epoch 1026, Loss 0.262983\n",
      "\t Params:  tensor([0.8411, 0.5990])\n",
      "\t Grad:  tensor([ 5.7280e-05, -4.4425e-04])\n",
      "Epoch 1027, Loss 0.262983\n",
      "\t Params:  tensor([0.8411, 0.5991])\n",
      "\t Grad:  tensor([ 5.6028e-05, -4.4142e-04])\n",
      "Epoch 1028, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5991])\n",
      "\t Grad:  tensor([ 6.1691e-05, -4.3778e-04])\n",
      "Epoch 1029, Loss 0.262983\n",
      "\t Params:  tensor([0.8411, 0.5991])\n",
      "\t Grad:  tensor([ 6.0380e-05, -4.3499e-04])\n",
      "Epoch 1030, Loss 0.262983\n",
      "\t Params:  tensor([0.8411, 0.5991])\n",
      "\t Grad:  tensor([ 5.9903e-05, -4.3211e-04])\n",
      "Epoch 1031, Loss 0.262983\n",
      "\t Params:  tensor([0.8411, 0.5991])\n",
      "\t Grad:  tensor([ 5.6207e-05, -4.2967e-04])\n",
      "Epoch 1032, Loss 0.262983\n",
      "\t Params:  tensor([0.8411, 0.5991])\n",
      "\t Grad:  tensor([ 6.1333e-05, -4.2616e-04])\n",
      "Epoch 1033, Loss 0.262983\n",
      "\t Params:  tensor([0.8411, 0.5991])\n",
      "\t Grad:  tensor([ 5.6505e-05, -4.2392e-04])\n",
      "Epoch 1034, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5991])\n",
      "\t Grad:  tensor([ 5.8293e-05, -4.2088e-04])\n",
      "Epoch 1035, Loss 0.262983\n",
      "\t Params:  tensor([0.8411, 0.5991])\n",
      "\t Grad:  tensor([ 5.4002e-05, -4.1849e-04])\n",
      "Epoch 1036, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5991])\n",
      "\t Grad:  tensor([ 5.6624e-05, -4.1538e-04])\n",
      "Epoch 1037, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5991])\n",
      "\t Grad:  tensor([ 5.5850e-05, -4.1265e-04])\n",
      "Epoch 1038, Loss 0.262983\n",
      "\t Params:  tensor([0.8411, 0.5991])\n",
      "\t Grad:  tensor([ 5.2869e-05, -4.1027e-04])\n",
      "Epoch 1039, Loss 0.262983\n",
      "\t Params:  tensor([0.8411, 0.5991])\n",
      "\t Grad:  tensor([ 5.3585e-05, -4.0741e-04])\n",
      "Epoch 1040, Loss 0.262983\n",
      "\t Params:  tensor([0.8411, 0.5991])\n",
      "\t Grad:  tensor([ 5.7757e-05, -4.0422e-04])\n",
      "Epoch 1041, Loss 0.262983\n",
      "\t Params:  tensor([0.8411, 0.5991])\n",
      "\t Grad:  tensor([ 5.2094e-05, -4.0214e-04])\n",
      "Epoch 1042, Loss 0.262983\n",
      "\t Params:  tensor([0.8411, 0.5991])\n",
      "\t Grad:  tensor([ 4.8161e-05, -3.9992e-04])\n",
      "Epoch 1043, Loss 0.262983\n",
      "\t Params:  tensor([0.8411, 0.5991])\n",
      "\t Grad:  tensor([ 5.3108e-05, -3.9655e-04])\n",
      "Epoch 1044, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5991])\n",
      "\t Grad:  tensor([ 5.4419e-05, -3.9387e-04])\n",
      "Epoch 1045, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5991])\n",
      "\t Grad:  tensor([ 5.3644e-05, -3.9127e-04])\n",
      "Epoch 1046, Loss 0.262983\n",
      "\t Params:  tensor([0.8411, 0.5991])\n",
      "\t Grad:  tensor([ 5.2929e-05, -3.8866e-04])\n",
      "Epoch 1047, Loss 0.262983\n",
      "\t Params:  tensor([0.8411, 0.5991])\n",
      "\t Grad:  tensor([ 5.0366e-05, -3.8642e-04])\n",
      "Epoch 1048, Loss 0.262983\n",
      "\t Params:  tensor([0.8411, 0.5991])\n",
      "\t Grad:  tensor([ 5.3883e-05, -3.8331e-04])\n",
      "Epoch 1049, Loss 0.262983\n",
      "\t Params:  tensor([0.8411, 0.5991])\n",
      "\t Grad:  tensor([ 5.2094e-05, -3.8105e-04])\n",
      "Epoch 1050, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5991])\n",
      "\t Grad:  tensor([ 4.8518e-05, -3.7888e-04])\n",
      "Epoch 1051, Loss 0.262983\n",
      "\t Params:  tensor([0.8411, 0.5991])\n",
      "\t Grad:  tensor([ 5.0962e-05, -3.7596e-04])\n",
      "Epoch 1052, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5992])\n",
      "\t Grad:  tensor([ 4.6670e-05, -3.7396e-04])\n",
      "Epoch 1053, Loss 0.262983\n",
      "\t Params:  tensor([0.8411, 0.5992])\n",
      "\t Grad:  tensor([ 4.8995e-05, -3.7117e-04])\n",
      "Epoch 1054, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5992])\n",
      "\t Grad:  tensor([ 5.0902e-05, -3.6850e-04])\n",
      "Epoch 1055, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5992])\n",
      "\t Grad:  tensor([ 4.6849e-05, -3.6659e-04])\n",
      "Epoch 1056, Loss 0.262983\n",
      "\t Params:  tensor([0.8411, 0.5992])\n",
      "\t Grad:  tensor([ 4.4525e-05, -3.6414e-04])\n",
      "Epoch 1057, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5992])\n",
      "\t Grad:  tensor([ 5.0366e-05, -3.6102e-04])\n",
      "Epoch 1058, Loss 0.262983\n",
      "\t Params:  tensor([0.8411, 0.5992])\n",
      "\t Grad:  tensor([ 5.0426e-05, -3.5854e-04])\n",
      "Epoch 1059, Loss 0.262983\n",
      "\t Params:  tensor([0.8411, 0.5992])\n",
      "\t Grad:  tensor([ 5.3287e-05, -3.5588e-04])\n",
      "Epoch 1060, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5992])\n",
      "\t Grad:  tensor([ 4.8339e-05, -3.5410e-04])\n",
      "Epoch 1061, Loss 0.262983\n",
      "\t Params:  tensor([0.8411, 0.5992])\n",
      "\t Grad:  tensor([ 4.5419e-05, -3.5203e-04])\n",
      "Epoch 1062, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5992])\n",
      "\t Grad:  tensor([ 4.7088e-05, -3.4950e-04])\n",
      "Epoch 1063, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5992])\n",
      "\t Grad:  tensor([ 4.4525e-05, -3.4749e-04])\n",
      "Epoch 1064, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5992])\n",
      "\t Grad:  tensor([ 4.7624e-05, -3.4475e-04])\n",
      "Epoch 1065, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5992])\n",
      "\t Grad:  tensor([ 4.7445e-05, -3.4236e-04])\n",
      "Epoch 1066, Loss 0.262983\n",
      "\t Params:  tensor([0.8411, 0.5992])\n",
      "\t Grad:  tensor([ 4.3273e-05, -3.4058e-04])\n",
      "Epoch 1067, Loss 0.262983\n",
      "\t Params:  tensor([0.8411, 0.5992])\n",
      "\t Grad:  tensor([ 4.6670e-05, -3.3788e-04])\n",
      "Epoch 1068, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5992])\n",
      "\t Grad:  tensor([ 4.5657e-05, -3.3587e-04])\n",
      "Epoch 1069, Loss 0.262983\n",
      "\t Params:  tensor([0.8411, 0.5992])\n",
      "\t Grad:  tensor([ 4.2439e-05, -3.3396e-04])\n",
      "Epoch 1070, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5992])\n",
      "\t Grad:  tensor([ 4.5657e-05, -3.3134e-04])\n",
      "Epoch 1071, Loss 0.262983\n",
      "\t Params:  tensor([0.8411, 0.5992])\n",
      "\t Grad:  tensor([ 4.1723e-05, -3.2958e-04])\n",
      "Epoch 1072, Loss 0.262983\n",
      "\t Params:  tensor([0.8411, 0.5992])\n",
      "\t Grad:  tensor([ 3.9935e-05, -3.2748e-04])\n",
      "Epoch 1073, Loss 0.262983\n",
      "\t Params:  tensor([0.8411, 0.5992])\n",
      "\t Grad:  tensor([ 4.3869e-05, -3.2480e-04])\n",
      "Epoch 1074, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5992])\n",
      "\t Grad:  tensor([ 4.5061e-05, -3.2249e-04])\n",
      "Epoch 1075, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5992])\n",
      "\t Grad:  tensor([ 4.0710e-05, -3.2094e-04])\n",
      "Epoch 1076, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5992])\n",
      "\t Grad:  tensor([ 4.1962e-05, -3.1862e-04])\n",
      "Epoch 1077, Loss 0.262983\n",
      "\t Params:  tensor([0.8411, 0.5992])\n",
      "\t Grad:  tensor([ 4.2081e-05, -3.1637e-04])\n",
      "Epoch 1078, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5992])\n",
      "\t Grad:  tensor([ 4.2200e-05, -3.1429e-04])\n",
      "Epoch 1079, Loss 0.262983\n",
      "\t Params:  tensor([0.8411, 0.5992])\n",
      "\t Grad:  tensor([ 4.2200e-05, -3.1213e-04])\n",
      "Epoch 1080, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5992])\n",
      "\t Grad:  tensor([ 3.9995e-05, -3.1028e-04])\n",
      "Epoch 1081, Loss 0.262983\n",
      "\t Params:  tensor([0.8411, 0.5993])\n",
      "\t Grad:  tensor([ 4.0829e-05, -3.0813e-04])\n",
      "Epoch 1082, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5993])\n",
      "\t Grad:  tensor([ 4.1902e-05, -3.0598e-04])\n",
      "Epoch 1083, Loss 0.262983\n",
      "\t Params:  tensor([0.8411, 0.5993])\n",
      "\t Grad:  tensor([ 3.9697e-05, -3.0410e-04])\n",
      "Epoch 1084, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5993])\n",
      "\t Grad:  tensor([ 3.7372e-05, -3.0234e-04])\n",
      "Epoch 1085, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5993])\n",
      "\t Grad:  tensor([ 4.3035e-05, -2.9960e-04])\n",
      "Epoch 1086, Loss 0.262983\n",
      "\t Params:  tensor([0.8411, 0.5993])\n",
      "\t Grad:  tensor([ 3.8147e-05, -2.9817e-04])\n",
      "Epoch 1087, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5993])\n",
      "\t Grad:  tensor([ 4.4346e-05, -2.9551e-04])\n",
      "Epoch 1088, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5993])\n",
      "\t Grad:  tensor([ 4.1306e-05, -2.9387e-04])\n",
      "Epoch 1089, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5993])\n",
      "\t Grad:  tensor([ 3.9518e-05, -2.9211e-04])\n",
      "Epoch 1090, Loss 0.262983\n",
      "\t Params:  tensor([0.8411, 0.5993])\n",
      "\t Grad:  tensor([ 3.7909e-05, -2.9033e-04])\n",
      "Epoch 1091, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5993])\n",
      "\t Grad:  tensor([ 4.0412e-05, -2.8802e-04])\n",
      "Epoch 1092, Loss 0.262983\n",
      "\t Params:  tensor([0.8411, 0.5993])\n",
      "\t Grad:  tensor([ 3.7611e-05, -2.8646e-04])\n",
      "Epoch 1093, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5993])\n",
      "\t Grad:  tensor([ 3.7968e-05, -2.8452e-04])\n",
      "Epoch 1094, Loss 0.262983\n",
      "\t Params:  tensor([0.8411, 0.5993])\n",
      "\t Grad:  tensor([ 4.1306e-05, -2.8215e-04])\n",
      "Epoch 1095, Loss 0.262983\n",
      "\t Params:  tensor([0.8411, 0.5993])\n",
      "\t Grad:  tensor([ 3.7551e-05, -2.8076e-04])\n",
      "Epoch 1096, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5993])\n",
      "\t Grad:  tensor([ 3.8385e-05, -2.7880e-04])\n",
      "Epoch 1097, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5993])\n",
      "\t Grad:  tensor([ 3.8445e-05, -2.7688e-04])\n",
      "Epoch 1098, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5993])\n",
      "\t Grad:  tensor([ 3.8803e-05, -2.7496e-04])\n",
      "Epoch 1099, Loss 0.262983\n",
      "\t Params:  tensor([0.8411, 0.5993])\n",
      "\t Grad:  tensor([ 3.2604e-05, -2.7384e-04])\n",
      "Epoch 1100, Loss 0.262983\n",
      "\t Params:  tensor([0.8411, 0.5993])\n",
      "\t Grad:  tensor([ 3.8564e-05, -2.7121e-04])\n",
      "Epoch 1101, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5993])\n",
      "\t Grad:  tensor([ 3.9577e-05, -2.6929e-04])\n",
      "Epoch 1102, Loss 0.262983\n",
      "\t Params:  tensor([0.8411, 0.5993])\n",
      "\t Grad:  tensor([ 3.3617e-05, -2.6826e-04])\n",
      "Epoch 1103, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5993])\n",
      "\t Grad:  tensor([ 3.6180e-05, -2.6612e-04])\n",
      "Epoch 1104, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5993])\n",
      "\t Grad:  tensor([ 3.4630e-05, -2.6456e-04])\n",
      "Epoch 1105, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5993])\n",
      "\t Grad:  tensor([ 3.3438e-05, -2.6286e-04])\n",
      "Epoch 1106, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5993])\n",
      "\t Grad:  tensor([ 3.2008e-05, -2.6125e-04])\n",
      "Epoch 1107, Loss 0.262982\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Params:  tensor([0.8411, 0.5993])\n",
      "\t Grad:  tensor([ 3.8087e-05, -2.5875e-04])\n",
      "Epoch 1108, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5993])\n",
      "\t Grad:  tensor([ 3.6001e-05, -2.5737e-04])\n",
      "Epoch 1109, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5993])\n",
      "\t Grad:  tensor([ 3.4332e-05, -2.5576e-04])\n",
      "Epoch 1110, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5993])\n",
      "\t Grad:  tensor([ 3.2365e-05, -2.5430e-04])\n",
      "Epoch 1111, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5993])\n",
      "\t Grad:  tensor([ 3.6895e-05, -2.5203e-04])\n",
      "Epoch 1112, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5993])\n",
      "\t Grad:  tensor([ 3.3796e-05, -2.5072e-04])\n",
      "Epoch 1113, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5993])\n",
      "\t Grad:  tensor([ 3.3140e-05, -2.4912e-04])\n",
      "Epoch 1114, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5993])\n",
      "\t Grad:  tensor([ 2.9683e-05, -2.4790e-04])\n",
      "Epoch 1115, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5993])\n",
      "\t Grad:  tensor([ 3.1710e-05, -2.4589e-04])\n",
      "Epoch 1116, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5993])\n",
      "\t Grad:  tensor([ 3.5107e-05, -2.4389e-04])\n",
      "Epoch 1117, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5993])\n",
      "\t Grad:  tensor([ 3.1471e-05, -2.4259e-04])\n",
      "Epoch 1118, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5994])\n",
      "\t Grad:  tensor([ 3.6120e-05, -2.4040e-04])\n",
      "Epoch 1119, Loss 0.262983\n",
      "\t Params:  tensor([0.8411, 0.5994])\n",
      "\t Grad:  tensor([ 3.0339e-05, -2.3950e-04])\n",
      "Epoch 1120, Loss 0.262983\n",
      "\t Params:  tensor([0.8411, 0.5994])\n",
      "\t Grad:  tensor([ 3.4034e-05, -2.3755e-04])\n",
      "Epoch 1121, Loss 0.262983\n",
      "\t Params:  tensor([0.8411, 0.5994])\n",
      "\t Grad:  tensor([ 3.0994e-05, -2.3628e-04])\n",
      "Epoch 1122, Loss 0.262983\n",
      "\t Params:  tensor([0.8411, 0.5994])\n",
      "\t Grad:  tensor([ 3.2187e-05, -2.3449e-04])\n",
      "Epoch 1123, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5994])\n",
      "\t Grad:  tensor([ 3.1173e-05, -2.3304e-04])\n",
      "Epoch 1124, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5994])\n",
      "\t Grad:  tensor([ 3.3796e-05, -2.3122e-04])\n",
      "Epoch 1125, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5994])\n",
      "\t Grad:  tensor([ 2.7537e-05, -2.3048e-04])\n",
      "Epoch 1126, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5994])\n",
      "\t Grad:  tensor([ 2.8908e-05, -2.2869e-04])\n",
      "Epoch 1127, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5994])\n",
      "\t Grad:  tensor([ 2.9504e-05, -2.2706e-04])\n",
      "Epoch 1128, Loss 0.262983\n",
      "\t Params:  tensor([0.8411, 0.5994])\n",
      "\t Grad:  tensor([ 2.8968e-05, -2.2560e-04])\n",
      "Epoch 1129, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5994])\n",
      "\t Grad:  tensor([ 2.8610e-05, -2.2411e-04])\n",
      "Epoch 1130, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5994])\n",
      "\t Grad:  tensor([ 2.8849e-05, -2.2258e-04])\n",
      "Epoch 1131, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5994])\n",
      "\t Grad:  tensor([ 2.9147e-05, -2.2095e-04])\n",
      "Epoch 1132, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5994])\n",
      "\t Grad:  tensor([ 2.7716e-05, -2.1977e-04])\n",
      "Epoch 1133, Loss 0.262983\n",
      "\t Params:  tensor([0.8411, 0.5994])\n",
      "\t Grad:  tensor([ 2.8074e-05, -2.1816e-04])\n",
      "Epoch 1134, Loss 0.262983\n",
      "\t Params:  tensor([0.8411, 0.5994])\n",
      "\t Grad:  tensor([ 2.7537e-05, -2.1676e-04])\n",
      "Epoch 1135, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5994])\n",
      "\t Grad:  tensor([ 2.6822e-05, -2.1545e-04])\n",
      "Epoch 1136, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5994])\n",
      "\t Grad:  tensor([ 2.7418e-05, -2.1391e-04])\n",
      "Epoch 1137, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5994])\n",
      "\t Grad:  tensor([ 2.4378e-05, -2.1289e-04])\n",
      "Epoch 1138, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5994])\n",
      "\t Grad:  tensor([ 2.8670e-05, -2.1080e-04])\n",
      "Epoch 1139, Loss 0.262983\n",
      "\t Params:  tensor([0.8411, 0.5994])\n",
      "\t Grad:  tensor([ 2.5988e-05, -2.0976e-04])\n",
      "Epoch 1140, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5994])\n",
      "\t Grad:  tensor([ 2.9087e-05, -2.0798e-04])\n",
      "Epoch 1141, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5994])\n",
      "\t Grad:  tensor([ 2.7716e-05, -2.0675e-04])\n",
      "Epoch 1142, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5994])\n",
      "\t Grad:  tensor([ 2.5332e-05, -2.0571e-04])\n",
      "Epoch 1143, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5994])\n",
      "\t Grad:  tensor([ 2.7657e-05, -2.0391e-04])\n",
      "Epoch 1144, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5994])\n",
      "\t Grad:  tensor([ 2.6345e-05, -2.0271e-04])\n",
      "Epoch 1145, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5994])\n",
      "\t Grad:  tensor([ 3.1710e-05, -2.0079e-04])\n",
      "Epoch 1146, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5994])\n",
      "\t Grad:  tensor([ 2.8968e-05, -1.9976e-04])\n",
      "Epoch 1147, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5994])\n",
      "\t Grad:  tensor([ 2.5630e-05, -1.9879e-04])\n",
      "Epoch 1148, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5994])\n",
      "\t Grad:  tensor([ 2.8551e-05, -1.9708e-04])\n",
      "Epoch 1149, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5994])\n",
      "\t Grad:  tensor([ 2.3782e-05, -1.9628e-04])\n",
      "Epoch 1150, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5994])\n",
      "\t Grad:  tensor([ 2.6762e-05, -1.9462e-04])\n",
      "Epoch 1151, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5994])\n",
      "\t Grad:  tensor([ 2.8491e-05, -1.9310e-04])\n",
      "Epoch 1152, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5994])\n",
      "\t Grad:  tensor([ 2.5153e-05, -1.9228e-04])\n",
      "Epoch 1153, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5994])\n",
      "\t Grad:  tensor([ 2.4974e-05, -1.9094e-04])\n",
      "Epoch 1154, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5994])\n",
      "\t Grad:  tensor([ 2.5868e-05, -1.8955e-04])\n",
      "Epoch 1155, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5994])\n",
      "\t Grad:  tensor([ 2.7657e-05, -1.8805e-04])\n",
      "Epoch 1156, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5994])\n",
      "\t Grad:  tensor([ 2.2352e-05, -1.8743e-04])\n",
      "Epoch 1157, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5994])\n",
      "\t Grad:  tensor([ 2.5690e-05, -1.8578e-04])\n",
      "Epoch 1158, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5994])\n",
      "\t Grad:  tensor([ 2.7120e-05, -1.8435e-04])\n",
      "Epoch 1159, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5994])\n",
      "\t Grad:  tensor([ 2.2948e-05, -1.8367e-04])\n",
      "Epoch 1160, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5994])\n",
      "\t Grad:  tensor([ 2.1935e-05, -1.8253e-04])\n",
      "Epoch 1161, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5994])\n",
      "\t Grad:  tensor([ 2.1458e-05, -1.8132e-04])\n",
      "Epoch 1162, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5994])\n",
      "\t Grad:  tensor([ 2.2173e-05, -1.8009e-04])\n",
      "Epoch 1163, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5994])\n",
      "\t Grad:  tensor([ 1.9729e-05, -1.7903e-04])\n",
      "Epoch 1164, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5994])\n",
      "\t Grad:  tensor([ 2.4378e-05, -1.7730e-04])\n",
      "Epoch 1165, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5994])\n",
      "\t Grad:  tensor([ 2.3663e-05, -1.7621e-04])\n",
      "Epoch 1166, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5994])\n",
      "\t Grad:  tensor([ 2.4319e-05, -1.7497e-04])\n",
      "Epoch 1167, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5995])\n",
      "\t Grad:  tensor([ 2.5153e-05, -1.7369e-04])\n",
      "Epoch 1168, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5995])\n",
      "\t Grad:  tensor([ 2.5868e-05, -1.7251e-04])\n",
      "Epoch 1169, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5995])\n",
      "\t Grad:  tensor([ 2.5392e-05, -1.7134e-04])\n",
      "Epoch 1170, Loss 0.262983\n",
      "\t Params:  tensor([0.8411, 0.5995])\n",
      "\t Grad:  tensor([ 2.2352e-05, -1.7052e-04])\n",
      "Epoch 1171, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5995])\n",
      "\t Grad:  tensor([ 2.0981e-05, -1.6954e-04])\n",
      "Epoch 1172, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5995])\n",
      "\t Grad:  tensor([ 2.1219e-05, -1.6845e-04])\n",
      "Epoch 1173, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5995])\n",
      "\t Grad:  tensor([ 2.0146e-05, -1.6750e-04])\n",
      "Epoch 1174, Loss 0.262983\n",
      "\t Params:  tensor([0.8411, 0.5995])\n",
      "\t Grad:  tensor([ 2.1875e-05, -1.6602e-04])\n",
      "Epoch 1175, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5995])\n",
      "\t Grad:  tensor([ 1.9670e-05, -1.6525e-04])\n",
      "Epoch 1176, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5995])\n",
      "\t Grad:  tensor([ 2.6524e-05, -1.6328e-04])\n",
      "Epoch 1177, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5995])\n",
      "\t Grad:  tensor([ 2.4080e-05, -1.6249e-04])\n",
      "Epoch 1178, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5995])\n",
      "\t Grad:  tensor([ 1.9908e-05, -1.6186e-04])\n",
      "Epoch 1179, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5995])\n",
      "\t Grad:  tensor([ 2.3723e-05, -1.6025e-04])\n",
      "Epoch 1180, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5995])\n",
      "\t Grad:  tensor([ 2.3603e-05, -1.5938e-04])\n",
      "Epoch 1181, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5995])\n",
      "\t Grad:  tensor([ 1.8775e-05, -1.5871e-04])\n",
      "Epoch 1182, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5995])\n",
      "\t Grad:  tensor([ 2.0802e-05, -1.5742e-04])\n",
      "Epoch 1183, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5995])\n",
      "\t Grad:  tensor([ 2.4974e-05, -1.5588e-04])\n",
      "Epoch 1184, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5995])\n",
      "\t Grad:  tensor([ 1.9550e-05, -1.5543e-04])\n",
      "Epoch 1185, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5995])\n",
      "\t Grad:  tensor([ 2.3603e-05, -1.5385e-04])\n",
      "Epoch 1186, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5995])\n",
      "\t Grad:  tensor([ 2.3127e-05, -1.5302e-04])\n",
      "Epoch 1187, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5995])\n",
      "\t Grad:  tensor([ 1.7941e-05, -1.5255e-04])\n",
      "Epoch 1188, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5995])\n",
      "\t Grad:  tensor([ 2.0325e-05, -1.5125e-04])\n",
      "Epoch 1189, Loss 0.262983\n",
      "\t Params:  tensor([0.8411, 0.5995])\n",
      "\t Grad:  tensor([ 2.1756e-05, -1.4997e-04])\n",
      "Epoch 1190, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5995])\n",
      "\t Grad:  tensor([ 1.9789e-05, -1.4926e-04])\n",
      "Epoch 1191, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5995])\n",
      "\t Grad:  tensor([ 1.8597e-05, -1.4833e-04])\n",
      "Epoch 1192, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5995])\n",
      "\t Grad:  tensor([ 2.0385e-05, -1.4716e-04])\n",
      "Epoch 1193, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5995])\n",
      "\t Grad:  tensor([ 2.3484e-05, -1.4583e-04])\n",
      "Epoch 1194, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5995])\n",
      "\t Grad:  tensor([ 1.9550e-05, -1.4536e-04])\n",
      "Epoch 1195, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5995])\n",
      "\t Grad:  tensor([ 2.1815e-05, -1.4417e-04])\n",
      "Epoch 1196, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5995])\n",
      "\t Grad:  tensor([ 1.6272e-05, -1.4387e-04])\n",
      "Epoch 1197, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5995])\n",
      "\t Grad:  tensor([ 1.8537e-05, -1.4265e-04])\n",
      "Epoch 1198, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5995])\n",
      "\t Grad:  tensor([ 1.7583e-05, -1.4175e-04])\n",
      "Epoch 1199, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5995])\n",
      "\t Grad:  tensor([ 1.9729e-05, -1.4053e-04])\n",
      "Epoch 1200, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5995])\n",
      "\t Grad:  tensor([ 1.9908e-05, -1.3953e-04])\n",
      "Epoch 1201, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5995])\n",
      "\t Grad:  tensor([ 2.1458e-05, -1.3844e-04])\n",
      "Epoch 1202, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5995])\n",
      "\t Grad:  tensor([ 1.7405e-05, -1.3798e-04])\n",
      "Epoch 1203, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5995])\n",
      "\t Grad:  tensor([ 1.7464e-05, -1.3704e-04])\n",
      "Epoch 1204, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5995])\n",
      "\t Grad:  tensor([ 1.9252e-05, -1.3594e-04])\n",
      "Epoch 1205, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5995])\n",
      "\t Grad:  tensor([ 1.5140e-05, -1.3552e-04])\n",
      "Epoch 1206, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5995])\n",
      "\t Grad:  tensor([ 1.6928e-05, -1.3440e-04])\n",
      "Epoch 1207, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5995])\n",
      "\t Grad:  tensor([ 1.6212e-05, -1.3352e-04])\n",
      "Epoch 1208, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5995])\n",
      "\t Grad:  tensor([ 1.7762e-05, -1.3251e-04])\n",
      "Epoch 1209, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5995])\n",
      "\t Grad:  tensor([ 1.8179e-05, -1.3158e-04])\n",
      "Epoch 1210, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5995])\n",
      "\t Grad:  tensor([ 1.4663e-05, -1.3108e-04])\n",
      "Epoch 1211, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5995])\n",
      "\t Grad:  tensor([ 1.9491e-05, -1.2963e-04])\n",
      "Epoch 1212, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5995])\n",
      "\t Grad:  tensor([ 1.9193e-05, -1.2872e-04])\n",
      "Epoch 1213, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5995])\n",
      "\t Grad:  tensor([ 1.7762e-05, -1.2793e-04])\n",
      "Epoch 1214, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5995])\n",
      "\t Grad:  tensor([ 1.6928e-05, -1.2729e-04])\n",
      "Epoch 1215, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5995])\n",
      "\t Grad:  tensor([ 1.7285e-05, -1.2641e-04])\n",
      "Epoch 1216, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5995])\n",
      "\t Grad:  tensor([ 1.5676e-05, -1.2569e-04])\n",
      "Epoch 1217, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5995])\n",
      "\t Grad:  tensor([ 1.5795e-05, -1.2495e-04])\n",
      "Epoch 1218, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5995])\n",
      "\t Grad:  tensor([ 1.3828e-05, -1.2430e-04])\n",
      "Epoch 1219, Loss 0.262983\n",
      "\t Params:  tensor([0.8411, 0.5995])\n",
      "\t Grad:  tensor([ 1.7405e-05, -1.2297e-04])\n",
      "Epoch 1220, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5995])\n",
      "\t Grad:  tensor([ 1.7524e-05, -1.2221e-04])\n",
      "Epoch 1221, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5995])\n",
      "\t Grad:  tensor([ 1.5974e-05, -1.2150e-04])\n",
      "Epoch 1222, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5995])\n",
      "\t Grad:  tensor([ 1.4246e-05, -1.2098e-04])\n",
      "Epoch 1223, Loss 0.262983\n",
      "\t Params:  tensor([0.8411, 0.5995])\n",
      "\t Grad:  tensor([ 1.6391e-05, -1.1984e-04])\n",
      "Epoch 1224, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5995])\n",
      "\t Grad:  tensor([ 1.7583e-05, -1.1899e-04])\n",
      "Epoch 1225, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5995])\n",
      "\t Grad:  tensor([ 1.6153e-05, -1.1829e-04])\n",
      "Epoch 1226, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5995])\n",
      "\t Grad:  tensor([ 1.1623e-05, -1.1803e-04])\n",
      "Epoch 1227, Loss 0.262983\n",
      "\t Params:  tensor([0.8411, 0.5995])\n",
      "\t Grad:  tensor([ 1.4663e-05, -1.1682e-04])\n",
      "Epoch 1228, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5995])\n",
      "\t Grad:  tensor([ 1.9729e-05, -1.1550e-04])\n",
      "Epoch 1229, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5995])\n",
      "\t Grad:  tensor([ 1.8120e-05, -1.1486e-04])\n",
      "Epoch 1230, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5995])\n",
      "\t Grad:  tensor([ 1.4067e-05, -1.1460e-04])\n",
      "Epoch 1231, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5995])\n",
      "\t Grad:  tensor([ 1.5378e-05, -1.1368e-04])\n",
      "Epoch 1232, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5995])\n",
      "\t Grad:  tensor([ 1.3709e-05, -1.1310e-04])\n",
      "Epoch 1233, Loss 0.262983\n",
      "\t Params:  tensor([0.8411, 0.5995])\n",
      "\t Grad:  tensor([ 1.7524e-05, -1.1189e-04])\n",
      "Epoch 1234, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5995])\n",
      "\t Grad:  tensor([ 1.4961e-05, -1.1139e-04])\n",
      "Epoch 1235, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5995])\n",
      "\t Grad:  tensor([ 1.1623e-05, -1.1104e-04])\n",
      "Epoch 1236, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5995])\n",
      "\t Grad:  tensor([ 1.2457e-05, -1.1025e-04])\n",
      "Epoch 1237, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5995])\n",
      "\t Grad:  tensor([ 1.6510e-05, -1.0897e-04])\n",
      "Epoch 1238, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5995])\n",
      "\t Grad:  tensor([ 1.2219e-05, -1.0882e-04])\n",
      "Epoch 1239, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5996])\n",
      "\t Grad:  tensor([ 1.4305e-05, -1.0782e-04])\n",
      "Epoch 1240, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5996])\n",
      "\t Grad:  tensor([ 1.7703e-05, -1.0672e-04])\n",
      "Epoch 1241, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5996])\n",
      "\t Grad:  tensor([ 1.5616e-05, -1.0625e-04])\n",
      "Epoch 1242, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5996])\n",
      "\t Grad:  tensor([ 1.2338e-05, -1.0591e-04])\n",
      "Epoch 1243, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5996])\n",
      "\t Grad:  tensor([ 1.3292e-05, -1.0508e-04])\n",
      "Epoch 1244, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5996])\n",
      "\t Grad:  tensor([ 1.3590e-05, -1.0426e-04])\n",
      "Epoch 1245, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5996])\n",
      "\t Grad:  tensor([ 1.5020e-05, -1.0344e-04])\n",
      "Epoch 1246, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5996])\n",
      "\t Grad:  tensor([ 1.3173e-05, -1.0297e-04])\n",
      "Epoch 1247, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5996])\n",
      "\t Grad:  tensor([ 1.4484e-05, -1.0211e-04])\n",
      "Epoch 1248, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5996])\n",
      "\t Grad:  tensor([ 1.4305e-05, -1.0151e-04])\n",
      "Epoch 1249, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5996])\n",
      "\t Grad:  tensor([ 1.4067e-05, -1.0073e-04])\n",
      "Epoch 1250, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5996])\n",
      "\t Grad:  tensor([ 1.6093e-05, -9.9823e-05])\n",
      "Epoch 1251, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5996])\n",
      "\t Grad:  tensor([ 1.5378e-05, -9.9346e-05])\n",
      "Epoch 1252, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5996])\n",
      "\t Grad:  tensor([ 1.0669e-05, -9.9242e-05])\n",
      "Epoch 1253, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5996])\n",
      "\t Grad:  tensor([ 1.2696e-05, -9.8318e-05])\n",
      "Epoch 1254, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5996])\n",
      "\t Grad:  tensor([ 1.2398e-05, -9.7692e-05])\n",
      "Epoch 1255, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5996])\n",
      "\t Grad:  tensor([ 1.3590e-05, -9.6925e-05])\n",
      "Epoch 1256, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5996])\n",
      "\t Grad:  tensor([ 1.5020e-05, -9.6105e-05])\n",
      "Epoch 1257, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5996])\n",
      "\t Grad:  tensor([ 1.1384e-05, -9.5919e-05])\n",
      "Epoch 1258, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5996])\n",
      "\t Grad:  tensor([ 1.0371e-05, -9.5405e-05])\n",
      "Epoch 1259, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5996])\n",
      "\t Grad:  tensor([ 1.0669e-05, -9.4675e-05])\n",
      "Epoch 1260, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5996])\n",
      "\t Grad:  tensor([ 1.1504e-05, -9.4004e-05])\n",
      "Epoch 1261, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5996])\n",
      "\t Grad:  tensor([ 1.1742e-05, -9.3251e-05])\n",
      "Epoch 1262, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5996])\n",
      "\t Grad:  tensor([ 1.2934e-05, -9.2521e-05])\n",
      "Epoch 1263, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5996])\n",
      "\t Grad:  tensor([ 1.4603e-05, -9.1672e-05])\n",
      "Epoch 1264, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5996])\n",
      "\t Grad:  tensor([ 1.2636e-05, -9.1240e-05])\n",
      "Epoch 1265, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5996])\n",
      "\t Grad:  tensor([ 1.3828e-05, -9.0554e-05])\n",
      "Epoch 1266, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5996])\n",
      "\t Grad:  tensor([ 1.5140e-05, -8.9787e-05])\n",
      "Epoch 1267, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5996])\n",
      "\t Grad:  tensor([ 7.6294e-06, -9.0018e-05])\n",
      "Epoch 1268, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5996])\n",
      "\t Grad:  tensor([ 1.2636e-05, -8.8811e-05])\n",
      "Epoch 1269, Loss 0.262983\n",
      "\t Params:  tensor([0.8411, 0.5996])\n",
      "\t Grad:  tensor([ 1.2040e-05, -8.8304e-05])\n",
      "Epoch 1270, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5996])\n",
      "\t Grad:  tensor([ 1.0550e-05, -8.7805e-05])\n",
      "Epoch 1271, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5996])\n",
      "\t Grad:  tensor([ 9.8348e-06, -8.7388e-05])\n",
      "Epoch 1272, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5996])\n",
      "\t Grad:  tensor([ 1.1981e-05, -8.6509e-05])\n",
      "Epoch 1273, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5996])\n",
      "\t Grad:  tensor([ 1.1206e-05, -8.6002e-05])\n",
      "Epoch 1274, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5996])\n",
      "\t Grad:  tensor([ 1.1563e-05, -8.5361e-05])\n",
      "Epoch 1275, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5996])\n",
      "\t Grad:  tensor([ 1.2159e-05, -8.4810e-05])\n",
      "Epoch 1276, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5996])\n",
      "\t Grad:  tensor([ 9.7156e-06, -8.4490e-05])\n",
      "Epoch 1277, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5996])\n",
      "\t Grad:  tensor([ 1.0073e-05, -8.4013e-05])\n",
      "Epoch 1278, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5996])\n",
      "\t Grad:  tensor([ 8.5831e-06, -8.3499e-05])\n",
      "Epoch 1279, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5996])\n",
      "\t Grad:  tensor([ 1.2279e-05, -8.2448e-05])\n",
      "Epoch 1280, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5996])\n",
      "\t Grad:  tensor([ 1.1325e-05, -8.2053e-05])\n",
      "Epoch 1281, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5996])\n",
      "\t Grad:  tensor([ 1.0431e-05, -8.1636e-05])\n",
      "Epoch 1282, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5996])\n",
      "\t Grad:  tensor([ 1.0312e-05, -8.1055e-05])\n",
      "Epoch 1283, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5996])\n",
      "\t Grad:  tensor([ 1.1981e-05, -8.0317e-05])\n",
      "Epoch 1284, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5996])\n",
      "\t Grad:  tensor([ 8.2254e-06, -8.0280e-05])\n",
      "Epoch 1285, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5996])\n",
      "\t Grad:  tensor([ 1.0490e-05, -7.9378e-05])\n",
      "Epoch 1286, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5996])\n",
      "\t Grad:  tensor([ 9.2983e-06, -7.9043e-05])\n",
      "Epoch 1287, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5996])\n",
      "\t Grad:  tensor([ 8.9407e-06, -7.8581e-05])\n",
      "Epoch 1288, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5996])\n",
      "\t Grad:  tensor([ 1.0133e-05, -7.7970e-05])\n",
      "Epoch 1289, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5996])\n",
      "\t Grad:  tensor([ 6.9737e-06, -7.7844e-05])\n",
      "Epoch 1290, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5996])\n",
      "\t Grad:  tensor([ 1.0908e-05, -7.6756e-05])\n",
      "Epoch 1291, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5996])\n",
      "\t Grad:  tensor([ 8.2850e-06, -7.6547e-05])\n",
      "Epoch 1292, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5996])\n",
      "\t Grad:  tensor([ 1.1146e-05, -7.5668e-05])\n",
      "Epoch 1293, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5996])\n",
      "\t Grad:  tensor([ 1.0908e-05, -7.5176e-05])\n",
      "Epoch 1294, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5996])\n",
      "\t Grad:  tensor([ 1.0788e-05, -7.4670e-05])\n",
      "Epoch 1295, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5996])\n",
      "\t Grad:  tensor([ 1.1265e-05, -7.4148e-05])\n",
      "Epoch 1296, Loss 0.262983\n",
      "\t Params:  tensor([0.8411, 0.5996])\n",
      "\t Grad:  tensor([ 4.7684e-06, -7.4431e-05])\n",
      "Epoch 1297, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5996])\n",
      "\t Grad:  tensor([ 9.7156e-06, -7.3396e-05])\n",
      "Epoch 1298, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5996])\n",
      "\t Grad:  tensor([ 8.7023e-06, -7.3016e-05])\n",
      "Epoch 1299, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5996])\n",
      "\t Grad:  tensor([ 1.1981e-05, -7.2137e-05])\n",
      "Epoch 1300, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5996])\n",
      "\t Grad:  tensor([ 7.5698e-06, -7.2099e-05])\n",
      "Epoch 1301, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5996])\n",
      "\t Grad:  tensor([ 1.0252e-05, -7.1332e-05])\n",
      "Epoch 1302, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5996])\n",
      "\t Grad:  tensor([ 9.3579e-06, -7.1011e-05])\n",
      "Epoch 1303, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5996])\n",
      "\t Grad:  tensor([ 8.5235e-06, -7.0639e-05])\n",
      "Epoch 1304, Loss 0.262983\n",
      "\t Params:  tensor([0.8411, 0.5996])\n",
      "\t Grad:  tensor([ 9.0003e-06, -6.9968e-05])\n",
      "Epoch 1305, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5996])\n",
      "\t Grad:  tensor([ 7.5698e-06, -6.9790e-05])\n",
      "Epoch 1306, Loss 0.262983\n",
      "\t Params:  tensor([0.8411, 0.5996])\n",
      "\t Grad:  tensor([ 1.1265e-05, -6.8761e-05])\n",
      "Epoch 1307, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5996])\n",
      "\t Grad:  tensor([ 9.5963e-06, -6.8612e-05])\n",
      "Epoch 1308, Loss 0.262983\n",
      "\t Params:  tensor([0.8411, 0.5996])\n",
      "\t Grad:  tensor([ 7.5698e-06, -6.8329e-05])\n",
      "Epoch 1309, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5996])\n",
      "\t Grad:  tensor([ 8.1658e-06, -6.7748e-05])\n",
      "Epoch 1310, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5996])\n",
      "\t Grad:  tensor([ 1.1206e-05, -6.7011e-05])\n",
      "Epoch 1311, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5996])\n",
      "\t Grad:  tensor([ 8.5235e-06, -6.6824e-05])\n",
      "Epoch 1312, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5996])\n",
      "\t Grad:  tensor([ 1.0908e-05, -6.6176e-05])\n",
      "Epoch 1313, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5996])\n",
      "\t Grad:  tensor([ 8.7619e-06, -6.6005e-05])\n",
      "Epoch 1314, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5996])\n",
      "\t Grad:  tensor([ 1.0371e-05, -6.5222e-05])\n",
      "Epoch 1315, Loss 0.262983\n",
      "\t Params:  tensor([0.8411, 0.5996])\n",
      "\t Grad:  tensor([ 7.0333e-06, -6.5297e-05])\n",
      "Epoch 1316, Loss 0.262983\n",
      "\t Params:  tensor([0.8411, 0.5996])\n",
      "\t Grad:  tensor([ 1.1921e-05, -6.4254e-05])\n",
      "Epoch 1317, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5996])\n",
      "\t Grad:  tensor([ 9.2983e-06, -6.4157e-05])\n",
      "Epoch 1318, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5996])\n",
      "\t Grad:  tensor([ 3.0398e-06, -6.4492e-05])\n",
      "Epoch 1319, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5996])\n",
      "\t Grad:  tensor([ 6.7353e-06, -6.3494e-05])\n",
      "Epoch 1320, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5996])\n",
      "\t Grad:  tensor([ 1.1325e-05, -6.2622e-05])\n",
      "Epoch 1321, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5996])\n",
      "\t Grad:  tensor([ 5.7817e-06, -6.2771e-05])\n",
      "Epoch 1322, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5996])\n",
      "\t Grad:  tensor([ 9.5963e-06, -6.1914e-05])\n",
      "Epoch 1323, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5996])\n",
      "\t Grad:  tensor([ 6.8545e-06, -6.1892e-05])\n",
      "Epoch 1324, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5996])\n",
      "\t Grad:  tensor([ 6.3181e-06, -6.1475e-05])\n",
      "Epoch 1325, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5996])\n",
      "\t Grad:  tensor([ 1.0669e-05, -6.0551e-05])\n",
      "Epoch 1326, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5996])\n",
      "\t Grad:  tensor([ 6.6161e-06, -6.0648e-05])\n",
      "Epoch 1327, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5996])\n",
      "\t Grad:  tensor([ 6.8545e-06, -6.0260e-05])\n",
      "Epoch 1328, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5996])\n",
      "\t Grad:  tensor([ 7.5698e-06, -5.9754e-05])\n",
      "Epoch 1329, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5996])\n",
      "\t Grad:  tensor([ 1.1265e-05, -5.8845e-05])\n",
      "Epoch 1330, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5996])\n",
      "\t Grad:  tensor([ 7.9274e-06, -5.8845e-05])\n",
      "Epoch 1331, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5996])\n",
      "\t Grad:  tensor([ 1.0312e-05, -5.8278e-05])\n",
      "Epoch 1332, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5996])\n",
      "\t Grad:  tensor([ 3.8743e-06, -5.8614e-05])\n",
      "Epoch 1333, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5996])\n",
      "\t Grad:  tensor([ 8.6427e-06, -5.7645e-05])\n",
      "Epoch 1334, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5996])\n",
      "\t Grad:  tensor([ 8.1658e-06, -5.7213e-05])\n",
      "Epoch 1335, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5996])\n",
      "\t Grad:  tensor([ 9.2387e-06, -5.6706e-05])\n",
      "Epoch 1336, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5996])\n",
      "\t Grad:  tensor([ 8.0466e-06, -5.6535e-05])\n",
      "Epoch 1337, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5996])\n",
      "\t Grad:  tensor([ 5.9605e-06, -5.6356e-05])\n",
      "Epoch 1338, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5996])\n",
      "\t Grad:  tensor([ 1.1623e-05, -5.5343e-05])\n",
      "Epoch 1339, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5996])\n",
      "\t Grad:  tensor([ 3.8147e-06, -5.5827e-05])\n",
      "Epoch 1340, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5996])\n",
      "\t Grad:  tensor([ 7.8678e-06, -5.5045e-05])\n",
      "Epoch 1341, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5996])\n",
      "\t Grad:  tensor([ 8.2254e-06, -5.4680e-05])\n",
      "Epoch 1342, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5996])\n",
      "\t Grad:  tensor([ 7.2718e-06, -5.4426e-05])\n",
      "Epoch 1343, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5996])\n",
      "\t Grad:  tensor([ 1.0252e-05, -5.3704e-05])\n",
      "Epoch 1344, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5996])\n",
      "\t Grad:  tensor([ 2.7418e-06, -5.4263e-05])\n",
      "Epoch 1345, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5996])\n",
      "\t Grad:  tensor([ 1.0133e-05, -5.3026e-05])\n",
      "Epoch 1346, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5996])\n",
      "\t Grad:  tensor([ 7.7486e-06, -5.2832e-05])\n",
      "Epoch 1347, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5996])\n",
      "\t Grad:  tensor([ 8.3447e-06, -5.2415e-05])\n",
      "Epoch 1348, Loss 0.262983\n",
      "\t Params:  tensor([0.8411, 0.5996])\n",
      "\t Grad:  tensor([ 7.7486e-06, -5.2057e-05])\n",
      "Epoch 1349, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5996])\n",
      "\t Grad:  tensor([ 9.6560e-06, -5.1551e-05])\n",
      "Epoch 1350, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5996])\n",
      "\t Grad:  tensor([ 8.2254e-06, -5.1506e-05])\n",
      "Epoch 1351, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5996])\n",
      "\t Grad:  tensor([ 5.6624e-06, -5.1349e-05])\n",
      "Epoch 1352, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5996])\n",
      "\t Grad:  tensor([ 6.2585e-06, -5.0917e-05])\n",
      "Epoch 1353, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5996])\n",
      "\t Grad:  tensor([ 7.3314e-06, -5.0366e-05])\n",
      "Epoch 1354, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5996])\n",
      "\t Grad:  tensor([ 8.7619e-06, -4.9874e-05])\n",
      "Epoch 1355, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5996])\n",
      "\t Grad:  tensor([ 7.2122e-06, -4.9733e-05])\n",
      "Epoch 1356, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5996])\n",
      "\t Grad:  tensor([ 9.4771e-06, -4.9174e-05])\n",
      "Epoch 1357, Loss 0.262983\n",
      "\t Params:  tensor([0.8411, 0.5996])\n",
      "\t Grad:  tensor([ 4.2319e-06, -4.9487e-05])\n",
      "Epoch 1358, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5996])\n",
      "\t Grad:  tensor([ 6.8545e-06, -4.8898e-05])\n",
      "Epoch 1359, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5996])\n",
      "\t Grad:  tensor([ 5.3048e-06, -4.8742e-05])\n",
      "Epoch 1360, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5996])\n",
      "\t Grad:  tensor([ 5.6624e-06, -4.8406e-05])\n",
      "Epoch 1361, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5996])\n",
      "\t Grad:  tensor([ 5.5432e-06, -4.8019e-05])\n",
      "Epoch 1362, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5996])\n",
      "\t Grad:  tensor([ 7.6890e-06, -4.7512e-05])\n",
      "Epoch 1363, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5996])\n",
      "\t Grad:  tensor([ 5.3644e-06, -4.7341e-05])\n",
      "Epoch 1364, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5996])\n",
      "\t Grad:  tensor([ 9.4771e-06, -4.6633e-05])\n",
      "Epoch 1365, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5996])\n",
      "\t Grad:  tensor([ 3.5763e-07, -4.7393e-05])\n",
      "Epoch 1366, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5996])\n",
      "\t Grad:  tensor([ 6.7949e-06, -4.6283e-05])\n",
      "Epoch 1367, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5996])\n",
      "\t Grad:  tensor([ 7.3910e-06, -4.5896e-05])\n",
      "Epoch 1368, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5996])\n",
      "\t Grad:  tensor([ 7.7486e-06, -4.5568e-05])\n",
      "Epoch 1369, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5996])\n",
      "\t Grad:  tensor([ 7.2718e-06, -4.5232e-05])\n",
      "Epoch 1370, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5996])\n",
      "\t Grad:  tensor([ 1.0490e-05, -4.4569e-05])\n",
      "Epoch 1371, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5996])\n",
      "\t Grad:  tensor([ 1.2517e-06, -4.5404e-05])\n",
      "Epoch 1372, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5996])\n",
      "\t Grad:  tensor([ 7.9870e-06, -4.4279e-05])\n",
      "Epoch 1373, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5996])\n",
      "\t Grad:  tensor([ 7.6294e-06, -4.4025e-05])\n",
      "Epoch 1374, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5996])\n",
      "\t Grad:  tensor([ 1.0252e-05, -4.3459e-05])\n",
      "Epoch 1375, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5996])\n",
      "\t Grad:  tensor([ 2.9206e-06, -4.4122e-05])\n",
      "Epoch 1376, Loss 0.262983\n",
      "\t Params:  tensor([0.8411, 0.5996])\n",
      "\t Grad:  tensor([ 5.7817e-06, -4.3362e-05])\n",
      "Epoch 1377, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5996])\n",
      "\t Grad:  tensor([ 8.2850e-06, -4.2751e-05])\n",
      "Epoch 1378, Loss 0.262983\n",
      "\t Params:  tensor([0.8411, 0.5996])\n",
      "\t Grad:  tensor([ 5.4240e-06, -4.2811e-05])\n",
      "Epoch 1379, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5996])\n",
      "\t Grad:  tensor([ 6.9737e-06, -4.2513e-05])\n",
      "Epoch 1380, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5996])\n",
      "\t Grad:  tensor([ 4.5896e-06, -4.2409e-05])\n",
      "Epoch 1381, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5996])\n",
      "\t Grad:  tensor([ 4.9472e-06, -4.2066e-05])\n",
      "Epoch 1382, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5996])\n",
      "\t Grad:  tensor([ 5.5432e-06, -4.1798e-05])\n",
      "Epoch 1383, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5997])\n",
      "\t Grad:  tensor([ 4.2319e-06, -4.1611e-05])\n",
      "Epoch 1384, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5997])\n",
      "\t Grad:  tensor([ 4.9472e-06, -4.1239e-05])\n",
      "Epoch 1385, Loss 0.262983\n",
      "\t Params:  tensor([0.8411, 0.5997])\n",
      "\t Grad:  tensor([ 3.7551e-06, -4.1135e-05])\n",
      "Epoch 1386, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5997])\n",
      "\t Grad:  tensor([ 4.5896e-06, -4.0777e-05])\n",
      "Epoch 1387, Loss 0.262983\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Params:  tensor([0.8411, 0.5997])\n",
      "\t Grad:  tensor([ 3.0994e-06, -4.0621e-05])\n",
      "Epoch 1388, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5997])\n",
      "\t Grad:  tensor([ 3.3975e-06, -4.0397e-05])\n",
      "Epoch 1389, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5997])\n",
      "\t Grad:  tensor([ 1.7881e-06, -4.0285e-05])\n",
      "Epoch 1390, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5997])\n",
      "\t Grad:  tensor([ 8.4043e-06, -3.9190e-05])\n",
      "Epoch 1391, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5997])\n",
      "\t Grad:  tensor([ 5.6028e-06, -3.9138e-05])\n",
      "Epoch 1392, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5997])\n",
      "\t Grad:  tensor([ 4.7684e-06, -3.8922e-05])\n",
      "Epoch 1393, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5997])\n",
      "\t Grad:  tensor([ 6.6757e-06, -3.8482e-05])\n",
      "Epoch 1394, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5997])\n",
      "\t Grad:  tensor([ 1.6689e-06, -3.8855e-05])\n",
      "Epoch 1395, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5997])\n",
      "\t Grad:  tensor([ 7.6294e-06, -3.7856e-05])\n",
      "Epoch 1396, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5997])\n",
      "\t Grad:  tensor([ 8.2254e-06, -3.7573e-05])\n",
      "Epoch 1397, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5997])\n",
      "\t Grad:  tensor([ 5.8413e-06, -3.7469e-05])\n",
      "Epoch 1398, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5997])\n",
      "\t Grad:  tensor([ 5.9605e-06, -3.7290e-05])\n",
      "Epoch 1399, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5997])\n",
      "\t Grad:  tensor([ 4.4107e-06, -3.7305e-05])\n",
      "Epoch 1400, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5997])\n",
      "\t Grad:  tensor([ 4.6492e-06, -3.7082e-05])\n",
      "Epoch 1401, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5997])\n",
      "\t Grad:  tensor([ 3.8147e-06, -3.6888e-05])\n",
      "Epoch 1402, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5997])\n",
      "\t Grad:  tensor([ 2.2054e-06, -3.6873e-05])\n",
      "Epoch 1403, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5997])\n",
      "\t Grad:  tensor([ 6.3181e-06, -3.6143e-05])\n",
      "Epoch 1404, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5997])\n",
      "\t Grad:  tensor([ 3.3975e-06, -3.6187e-05])\n",
      "Epoch 1405, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5997])\n",
      "\t Grad:  tensor([ 2.6226e-06, -3.6001e-05])\n",
      "Epoch 1406, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5997])\n",
      "\t Grad:  tensor([ 8.9407e-06, -3.4995e-05])\n",
      "Epoch 1407, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5997])\n",
      "\t Grad:  tensor([ 1.9073e-06, -3.5673e-05])\n",
      "Epoch 1408, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5997])\n",
      "\t Grad:  tensor([ 7.8678e-06, -3.4712e-05])\n",
      "Epoch 1409, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5997])\n",
      "\t Grad:  tensor([ 4.8876e-06, -3.4794e-05])\n",
      "Epoch 1410, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5997])\n",
      "\t Grad:  tensor([ 3.2187e-06, -3.4772e-05])\n",
      "Epoch 1411, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5997])\n",
      "\t Grad:  tensor([ 3.6955e-06, -3.4541e-05])\n",
      "Epoch 1412, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5997])\n",
      "\t Grad:  tensor([ 7.1526e-07, -3.4645e-05])\n",
      "Epoch 1413, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5997])\n",
      "\t Grad:  tensor([ 3.8147e-06, -3.3900e-05])\n",
      "Epoch 1414, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5997])\n",
      "\t Grad:  tensor([ 7.2718e-06, -3.3394e-05])\n",
      "Epoch 1415, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5997])\n",
      "\t Grad:  tensor([ 2.0266e-06, -3.3706e-05])\n",
      "Epoch 1416, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5997])\n",
      "\t Grad:  tensor([ 8.2254e-06, -3.2723e-05])\n",
      "Epoch 1417, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5997])\n",
      "\t Grad:  tensor([ 5.3644e-06, -3.2842e-05])\n",
      "Epoch 1418, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5997])\n",
      "\t Grad:  tensor([ 4.7684e-06, -3.2790e-05])\n",
      "Epoch 1419, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5997])\n",
      "\t Grad:  tensor([ 4.2915e-06, -3.2596e-05])\n",
      "Epoch 1420, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5997])\n",
      "\t Grad:  tensor([ 3.0994e-06, -3.2499e-05])\n",
      "Epoch 1421, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-07, -3.2693e-05])\n",
      "Epoch 1422, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5997])\n",
      "\t Grad:  tensor([ 5.2452e-06, -3.1918e-05])\n",
      "Epoch 1423, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5997])\n",
      "\t Grad:  tensor([ 3.2187e-06, -3.1896e-05])\n",
      "Epoch 1424, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5997])\n",
      "\t Grad:  tensor([-5.3644e-07, -3.2142e-05])\n",
      "Epoch 1425, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5997])\n",
      "\t Grad:  tensor([ 5.6028e-06, -3.1196e-05])\n",
      "Epoch 1426, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5997])\n",
      "\t Grad:  tensor([ 4.4107e-06, -3.1121e-05])\n",
      "Epoch 1427, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5997])\n",
      "\t Grad:  tensor([ 4.1723e-07, -3.1479e-05])\n",
      "Epoch 1428, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5997])\n",
      "\t Grad:  tensor([ 3.2783e-06, -3.0808e-05])\n",
      "Epoch 1429, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5997])\n",
      "\t Grad:  tensor([ 4.5300e-06, -3.0540e-05])\n",
      "Epoch 1430, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5997])\n",
      "\t Grad:  tensor([ 6.5565e-07, -3.0845e-05])\n",
      "Epoch 1431, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5997])\n",
      "\t Grad:  tensor([ 3.5167e-06, -3.0130e-05])\n",
      "Epoch 1432, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5997])\n",
      "\t Grad:  tensor([ 2.9206e-06, -3.0160e-05])\n",
      "Epoch 1433, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5997])\n",
      "\t Grad:  tensor([ 5.5432e-06, -2.9571e-05])\n",
      "Epoch 1434, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5997])\n",
      "\t Grad:  tensor([ 4.1127e-06, -2.9549e-05])\n",
      "Epoch 1435, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5997])\n",
      "\t Grad:  tensor([ 4.1127e-06, -2.9445e-05])\n",
      "Epoch 1436, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5997])\n",
      "\t Grad:  tensor([ 1.4901e-06, -2.9482e-05])\n",
      "Epoch 1437, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5997])\n",
      "\t Grad:  tensor([ 2.3246e-06, -2.9184e-05])\n",
      "Epoch 1438, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5997])\n",
      "\t Grad:  tensor([ 8.0466e-06, -2.8282e-05])\n",
      "Epoch 1439, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5997])\n",
      "\t Grad:  tensor([ 6.6161e-06, -2.8223e-05])\n",
      "Epoch 1440, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5997])\n",
      "\t Grad:  tensor([ 2.6822e-06, -2.8558e-05])\n",
      "Epoch 1441, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5997])\n",
      "\t Grad:  tensor([ 5.3048e-06, -2.7947e-05])\n",
      "Epoch 1442, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5997])\n",
      "\t Grad:  tensor([ 5.9009e-06, -2.7709e-05])\n",
      "Epoch 1443, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5997])\n",
      "\t Grad:  tensor([ 4.5896e-06, -2.7694e-05])\n",
      "Epoch 1444, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5997])\n",
      "\t Grad:  tensor([ 1.2517e-06, -2.7895e-05])\n",
      "Epoch 1445, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5997])\n",
      "\t Grad:  tensor([ 5.4240e-06, -2.7232e-05])\n",
      "Epoch 1446, Loss 0.262983\n",
      "\t Params:  tensor([0.8411, 0.5997])\n",
      "\t Grad:  tensor([ 2.5630e-06, -2.7344e-05])\n",
      "Epoch 1447, Loss 0.262983\n",
      "\t Params:  tensor([0.8411, 0.5997])\n",
      "\t Grad:  tensor([ 5.4240e-06, -2.6718e-05])\n",
      "Epoch 1448, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5997])\n",
      "\t Grad:  tensor([ 6.8545e-06, -2.6457e-05])\n",
      "Epoch 1449, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5997])\n",
      "\t Grad:  tensor([ 4.1127e-06, -2.6569e-05])\n",
      "Epoch 1450, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5997])\n",
      "\t Grad:  tensor([ 5.9605e-08, -2.6971e-05])\n",
      "Epoch 1451, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5997])\n",
      "\t Grad:  tensor([ 2.5630e-06, -2.6390e-05])\n",
      "Epoch 1452, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5997])\n",
      "\t Grad:  tensor([ 6.4373e-06, -2.5839e-05])\n",
      "Epoch 1453, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5997])\n",
      "\t Grad:  tensor([ 5.4240e-06, -2.5734e-05])\n",
      "Epoch 1454, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5997])\n",
      "\t Grad:  tensor([-2.9802e-07, -2.6338e-05])\n",
      "Epoch 1455, Loss 0.262983\n",
      "\t Params:  tensor([0.8411, 0.5997])\n",
      "\t Grad:  tensor([ 2.2054e-06, -2.5772e-05])\n",
      "Epoch 1456, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5997])\n",
      "\t Grad:  tensor([ 6.0201e-06, -2.5198e-05])\n",
      "Epoch 1457, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5997])\n",
      "\t Grad:  tensor([ 4.7088e-06, -2.5213e-05])\n",
      "Epoch 1458, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5997])\n",
      "\t Grad:  tensor([ 1.4901e-06, -2.5451e-05])\n",
      "Epoch 1459, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5997])\n",
      "\t Grad:  tensor([ 4.1127e-06, -2.4915e-05])\n",
      "Epoch 1460, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5997])\n",
      "\t Grad:  tensor([ 3.7551e-06, -2.4900e-05])\n",
      "Epoch 1461, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5997])\n",
      "\t Grad:  tensor([ 4.1723e-07, -2.5116e-05])\n",
      "Epoch 1462, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5997])\n",
      "\t Grad:  tensor([ 7.7486e-07, -2.4900e-05])\n",
      "Epoch 1463, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5997])\n",
      "\t Grad:  tensor([ 3.2783e-06, -2.4356e-05])\n",
      "Epoch 1464, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5997])\n",
      "\t Grad:  tensor([ 3.5167e-06, -2.4281e-05])\n",
      "Epoch 1465, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5997])\n",
      "\t Grad:  tensor([ 2.2054e-06, -2.4281e-05])\n",
      "Epoch 1466, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5997])\n",
      "\t Grad:  tensor([ 2.4438e-06, -2.4058e-05])\n",
      "Epoch 1467, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5997])\n",
      "\t Grad:  tensor([ 5.1856e-06, -2.3499e-05])\n",
      "Epoch 1468, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5997])\n",
      "\t Grad:  tensor([ 2.9206e-06, -2.3708e-05])\n",
      "Epoch 1469, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5997])\n",
      "\t Grad:  tensor([ 5.4240e-06, -2.3142e-05])\n",
      "Epoch 1470, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5997])\n",
      "\t Grad:  tensor([ 3.3975e-06, -2.3335e-05])\n",
      "Epoch 1471, Loss 0.262983\n",
      "\t Params:  tensor([0.8411, 0.5997])\n",
      "\t Grad:  tensor([ 9.5367e-07, -2.3380e-05])\n",
      "Epoch 1472, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5997])\n",
      "\t Grad:  tensor([ 4.7088e-06, -2.2829e-05])\n",
      "Epoch 1473, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5997])\n",
      "\t Grad:  tensor([ 1.9073e-06, -2.2963e-05])\n",
      "Epoch 1474, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5997])\n",
      "\t Grad:  tensor([ 2.1458e-06, -2.2754e-05])\n",
      "Epoch 1475, Loss 0.262983\n",
      "\t Params:  tensor([0.8411, 0.5997])\n",
      "\t Grad:  tensor([ 5.0068e-06, -2.2203e-05])\n",
      "Epoch 1476, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5997])\n",
      "\t Grad:  tensor([ 6.3777e-06, -2.1987e-05])\n",
      "Epoch 1477, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5997])\n",
      "\t Grad:  tensor([ 3.4571e-06, -2.2128e-05])\n",
      "Epoch 1478, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5997])\n",
      "\t Grad:  tensor([-7.1526e-07, -2.2538e-05])\n",
      "Epoch 1479, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5997])\n",
      "\t Grad:  tensor([ 2.1458e-06, -2.1957e-05])\n",
      "Epoch 1480, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5997])\n",
      "\t Grad:  tensor([ 5.8413e-06, -2.1391e-05])\n",
      "Epoch 1481, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 4.8876e-06, -2.1338e-05])\n",
      "Epoch 1482, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([-1.0729e-06, -2.1912e-05])\n",
      "Epoch 1483, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 1.9073e-06, -2.1361e-05])\n",
      "Epoch 1484, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 5.6028e-06, -2.0802e-05])\n",
      "Epoch 1485, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 4.1723e-06, -2.0884e-05])\n",
      "Epoch 1486, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 3.5763e-07, -2.1197e-05])\n",
      "Epoch 1487, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 3.0994e-06, -2.0631e-05])\n",
      "Epoch 1488, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.9802e-06, -2.0631e-05])\n",
      "Epoch 1489, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 4.4107e-06, -2.0273e-05])\n",
      "Epoch 1490, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-07, -2.0646e-05])\n",
      "Epoch 1491, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 4.7684e-07, -2.0437e-05])\n",
      "Epoch 1492, Loss 0.262983\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.9802e-06, -1.9990e-05])\n",
      "Epoch 1493, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 5.9605e-07, -2.0258e-05])\n",
      "Epoch 1494, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.8610e-06, -1.9901e-05])\n",
      "Epoch 1495, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 5.6028e-06, -1.9349e-05])\n",
      "Epoch 1496, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 1.1921e-06, -1.9856e-05])\n",
      "Epoch 1497, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.0266e-06, -1.9602e-05])\n",
      "Epoch 1498, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 4.0531e-06, -1.9141e-05])\n",
      "Epoch 1499, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.1458e-06, -1.9379e-05])\n",
      "Epoch 1500, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 4.7684e-06, -1.8887e-05])\n",
      "Epoch 1501, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 3.4571e-06, -1.8962e-05])\n",
      "Epoch 1502, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([-2.3246e-06, -1.9558e-05])\n",
      "Epoch 1503, Loss 0.262983\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 4.1723e-07, -1.9006e-05])\n",
      "Epoch 1504, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 3.9339e-06, -1.8559e-05])\n",
      "Epoch 1505, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([-2.9802e-07, -1.8954e-05])\n",
      "Epoch 1506, Loss 0.262983\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 1.0133e-06, -1.8597e-05])\n",
      "Epoch 1507, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 1.2517e-06, -1.8448e-05])\n",
      "Epoch 1508, Loss 0.262983\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 3.8743e-06, -1.7963e-05])\n",
      "Epoch 1509, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 5.3644e-06, -1.7799e-05])\n",
      "Epoch 1510, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.9802e-07, -1.8284e-05])\n",
      "Epoch 1511, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.8014e-06, -1.7755e-05])\n",
      "Epoch 1512, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.9206e-06, -1.7650e-05])\n",
      "Epoch 1513, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 4.3511e-06, -1.7293e-05])\n",
      "Epoch 1514, Loss 0.262983\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 1.3709e-06, -1.7546e-05])\n",
      "Epoch 1515, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 5.1856e-06, -1.7032e-05])\n",
      "Epoch 1516, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 3.8743e-06, -1.7099e-05])\n",
      "Epoch 1517, Loss 0.262983\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([-1.7285e-06, -1.7680e-05])\n",
      "Epoch 1518, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([-1.4901e-06, -1.7494e-05])\n",
      "Epoch 1519, Loss 0.262983\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 1.0133e-06, -1.6987e-05])\n",
      "Epoch 1520, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 4.5896e-06, -1.6525e-05])\n",
      "Epoch 1521, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3246e-06, -1.6697e-05])\n",
      "Epoch 1522, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 3.6359e-06, -1.6332e-05])\n",
      "Epoch 1523, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([-2.9802e-07, -1.6823e-05])\n",
      "Epoch 1524, Loss 0.262983\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.0862e-06, -1.6332e-05])\n",
      "Epoch 1525, Loss 0.262983\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3246e-06, -1.6153e-05])\n",
      "Epoch 1526, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 6.0201e-06, -1.5631e-05])\n",
      "Epoch 1527, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 4.2319e-06, -1.5683e-05])\n",
      "Epoch 1528, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([-1.6093e-06, -1.6324e-05])\n",
      "Epoch 1529, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([-4.1723e-07, -1.5989e-05])\n",
      "Epoch 1530, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 1.2517e-06, -1.5609e-05])\n",
      "Epoch 1531, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 4.9472e-06, -1.5125e-05])\n",
      "Epoch 1532, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 3.7551e-06, -1.5184e-05])\n",
      "Epoch 1533, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 1.7881e-07, -1.5475e-05])\n",
      "Epoch 1534, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 4.1723e-07, -1.5296e-05])\n",
      "Epoch 1535, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 3.0398e-06, -1.4782e-05])\n",
      "Epoch 1536, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([-1.4305e-06, -1.5326e-05])\n",
      "Epoch 1537, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 6.5565e-07, -1.4983e-05])\n",
      "Epoch 1538, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 3.2783e-06, -1.4514e-05])\n",
      "Epoch 1539, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 1.6093e-06, -1.4730e-05])\n",
      "Epoch 1540, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 1.6093e-06, -1.4655e-05])\n",
      "Epoch 1541, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.5630e-06, -1.4439e-05])\n",
      "Epoch 1542, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 4.7088e-06, -1.4007e-05])\n",
      "Epoch 1543, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 7.1526e-07, -1.4447e-05])\n",
      "Epoch 1544, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.8014e-06, -1.4171e-05])\n",
      "Epoch 1545, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 4.2319e-06, -1.3888e-05])\n",
      "Epoch 1546, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 1.4305e-06, -1.4164e-05])\n",
      "Epoch 1547, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 1.7881e-06, -1.3985e-05])\n",
      "Epoch 1548, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.0266e-06, -1.3858e-05])\n",
      "Epoch 1549, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 3.8147e-06, -1.3493e-05])\n",
      "Epoch 1550, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 3.5763e-07, -1.3903e-05])\n",
      "Epoch 1551, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.1458e-06, -1.3635e-05])\n",
      "Epoch 1552, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 3.9935e-06, -1.3418e-05])\n",
      "Epoch 1553, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([-1.0729e-06, -1.3918e-05])\n",
      "Epoch 1554, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 5.9605e-07, -1.3545e-05])\n",
      "Epoch 1555, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 1.0729e-06, -1.3441e-05])\n",
      "Epoch 1556, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 1.0729e-06, -1.3337e-05])\n",
      "Epoch 1557, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.8610e-06, -1.2979e-05])\n",
      "Epoch 1558, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 3.8147e-06, -1.2785e-05])\n",
      "Epoch 1559, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 3.3379e-06, -1.2867e-05])\n",
      "Epoch 1560, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([-1.1921e-07, -1.3292e-05])\n",
      "Epoch 1561, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 1.6689e-06, -1.2919e-05])\n",
      "Epoch 1562, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -1.2711e-05])\n",
      "Epoch 1563, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.5034e-06, -1.2606e-05])\n",
      "Epoch 1564, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.7418e-06, -1.2502e-05])\n",
      "Epoch 1565, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 3.6955e-06, -1.2293e-05])\n",
      "Epoch 1566, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 1.3113e-06, -1.2480e-05])\n",
      "Epoch 1567, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 4.7684e-06, -1.2048e-05])\n",
      "Epoch 1568, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([-5.9605e-07, -1.2629e-05])\n",
      "Epoch 1569, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 7.1526e-07, -1.2375e-05])\n",
      "Epoch 1570, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 1.7881e-06, -1.2152e-05])\n",
      "Epoch 1571, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.0266e-06, -1.1988e-05])\n",
      "Epoch 1572, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.5034e-06, -1.1839e-05])\n",
      "Epoch 1573, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 3.6955e-06, -1.1601e-05])\n",
      "Epoch 1574, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 8.3447e-07, -1.1884e-05])\n",
      "Epoch 1575, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.7418e-06, -1.1608e-05])\n",
      "Epoch 1576, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 4.6492e-06, -1.1310e-05])\n",
      "Epoch 1577, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 1.7881e-06, -1.1601e-05])\n",
      "Epoch 1578, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 3.6955e-06, -1.1198e-05])\n",
      "Epoch 1579, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([-2.3246e-06, -1.1921e-05])\n",
      "Epoch 1580, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([-2.0862e-06, -1.1802e-05])\n",
      "Epoch 1581, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([-1.3709e-06, -1.1638e-05])\n",
      "Epoch 1582, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 1.7881e-07, -1.1310e-05])\n",
      "Epoch 1583, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 5.3644e-07, -1.1146e-05])\n",
      "Epoch 1584, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 4.2915e-06, -1.0662e-05])\n",
      "Epoch 1585, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 3.3379e-06, -1.0796e-05])\n",
      "Epoch 1586, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([-1.3709e-06, -1.1265e-05])\n",
      "Epoch 1587, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([-1.1325e-06, -1.1146e-05])\n",
      "Epoch 1588, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([-1.0133e-06, -1.1042e-05])\n",
      "Epoch 1589, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 4.1723e-07, -1.0759e-05])\n",
      "Epoch 1590, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 1.8477e-06, -1.0476e-05])\n",
      "Epoch 1591, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 1.9670e-06, -1.0356e-05])\n",
      "Epoch 1592, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 5.6028e-06, -9.9018e-06])\n",
      "Epoch 1593, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 3.2783e-06, -1.0073e-05])\n",
      "Epoch 1594, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 1.1325e-06, -1.0312e-05])\n",
      "Epoch 1595, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 1.3709e-06, -1.0222e-05])\n",
      "Epoch 1596, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 1.4901e-06, -1.0103e-05])\n",
      "Epoch 1597, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 3.6359e-06, -9.7305e-06])\n",
      "Epoch 1598, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([-1.8477e-06, -1.0267e-05])\n",
      "Epoch 1599, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 1.7285e-06, -9.8199e-06])\n",
      "Epoch 1600, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 1.7285e-06, -9.7752e-06])\n",
      "Epoch 1601, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 3.9935e-06, -9.3430e-06])\n",
      "Epoch 1602, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 1.7881e-07, -9.7454e-06])\n",
      "Epoch 1603, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.9802e-07, -9.6411e-06])\n",
      "Epoch 1604, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 5.3644e-07, -9.5367e-06])\n",
      "Epoch 1605, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.6822e-06, -9.1493e-06])\n",
      "Epoch 1606, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 3.5167e-06, -8.9556e-06])\n",
      "Epoch 1607, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 3.1590e-06, -9.0599e-06])\n",
      "Epoch 1608, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([-2.2054e-06, -9.6709e-06])\n",
      "Epoch 1609, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([-1.3709e-06, -9.4324e-06])\n",
      "Epoch 1610, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 6.5565e-07, -9.0227e-06])\n",
      "Epoch 1611, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 6.5565e-07, -8.9630e-06])\n",
      "Epoch 1612, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 6.5565e-07, -8.8885e-06])\n",
      "Epoch 1613, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 1.0133e-06, -8.7395e-06])\n",
      "Epoch 1614, Loss 0.262983\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.9206e-06, -8.4713e-06])\n",
      "Epoch 1615, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 3.3975e-06, -8.3372e-06])\n",
      "Epoch 1616, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([-1.1325e-06, -8.9556e-06])\n",
      "Epoch 1617, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 8.9407e-07, -8.6054e-06])\n",
      "Epoch 1618, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.8014e-06, -8.4043e-06])\n",
      "Epoch 1619, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 3.0398e-06, -8.2999e-06])\n",
      "Epoch 1620, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([-1.2517e-06, -8.8885e-06])\n",
      "Epoch 1621, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([-6.5565e-07, -8.7544e-06])\n",
      "Epoch 1622, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 1.2517e-06, -8.4415e-06])\n",
      "Epoch 1623, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 1.4901e-06, -8.3447e-06])\n",
      "Epoch 1624, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 1.4901e-06, -8.3223e-06])\n",
      "Epoch 1625, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 1.4901e-06, -8.2776e-06])\n",
      "Epoch 1626, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 1.4901e-06, -8.2329e-06])\n",
      "Epoch 1627, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 1.4901e-06, -8.2180e-06])\n",
      "Epoch 1628, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 1.7285e-06, -8.1137e-06])\n",
      "Epoch 1629, Loss 0.262983\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 3.6359e-06, -7.7859e-06])\n",
      "Epoch 1630, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.9802e-07, -8.1956e-06])\n",
      "Epoch 1631, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.9802e-07, -8.1658e-06])\n",
      "Epoch 1632, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 5.3644e-07, -8.0764e-06])\n",
      "Epoch 1633, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.2054e-06, -7.8306e-06])\n",
      "Epoch 1634, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 4.1127e-06, -7.6294e-06])\n",
      "Epoch 1635, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([-1.0133e-06, -8.2180e-06])\n",
      "Epoch 1636, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([-8.9407e-07, -8.1509e-06])\n",
      "Epoch 1637, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([-4.1723e-07, -8.0317e-06])\n",
      "Epoch 1638, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 1.3709e-06, -7.7337e-06])\n",
      "Epoch 1639, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 1.4901e-06, -7.6592e-06])\n",
      "Epoch 1640, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 1.6093e-06, -7.5847e-06])\n",
      "Epoch 1641, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 1.6093e-06, -7.5549e-06])\n",
      "Epoch 1642, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 1.6093e-06, -7.5251e-06])\n",
      "Epoch 1643, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 1.7285e-06, -7.4431e-06])\n",
      "Epoch 1644, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 1.8477e-06, -7.3686e-06])\n",
      "Epoch 1645, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 3.7551e-06, -7.0855e-06])\n",
      "Epoch 1646, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 5.9605e-08, -7.5474e-06])\n",
      "Epoch 1647, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 4.1723e-07, -7.4208e-06])\n",
      "Epoch 1648, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 4.1723e-07, -7.3761e-06])\n",
      "Epoch 1649, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 4.1723e-07, -7.3314e-06])\n",
      "Epoch 1650, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 3.8743e-06, -6.9886e-06])\n",
      "Epoch 1651, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 4.1723e-07, -7.3984e-06])\n",
      "Epoch 1652, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 4.1723e-07, -7.3388e-06])\n",
      "Epoch 1653, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 4.1723e-07, -7.3090e-06])\n",
      "Epoch 1654, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.5630e-06, -6.9439e-06])\n",
      "Epoch 1655, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.5630e-06, -6.9290e-06])\n",
      "Epoch 1656, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.9206e-06, -6.7800e-06])\n",
      "Epoch 1657, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.9206e-06, -6.7502e-06])\n",
      "Epoch 1658, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.9206e-06, -6.7204e-06])\n",
      "Epoch 1659, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 3.1590e-06, -6.6459e-06])\n",
      "Epoch 1660, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([-2.6226e-06, -7.3239e-06])\n",
      "Epoch 1661, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([-2.6226e-06, -7.2792e-06])\n",
      "Epoch 1662, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([-4.7684e-07, -6.8992e-06])\n",
      "Epoch 1663, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([-2.3842e-07, -6.8545e-06])\n",
      "Epoch 1664, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([-1.1921e-07, -6.7800e-06])\n",
      "Epoch 1665, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 1.8477e-06, -6.5714e-06])\n",
      "Epoch 1666, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 3.5167e-06, -6.3479e-06])\n",
      "Epoch 1667, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 1.7285e-06, -6.6236e-06])\n",
      "Epoch 1668, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Grad:  tensor([ 1.8477e-06, -6.5491e-06])\n",
      "Epoch 1669, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3246e-06, -6.4522e-06])\n",
      "Epoch 1670, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 3.9935e-06, -6.1691e-06])\n",
      "Epoch 1671, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([-1.7881e-06, -6.8471e-06])\n",
      "Epoch 1672, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([-1.6689e-06, -6.7726e-06])\n",
      "Epoch 1673, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([-1.6689e-06, -6.7577e-06])\n",
      "Epoch 1674, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([-1.6689e-06, -6.7428e-06])\n",
      "Epoch 1675, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([-1.5497e-06, -6.6236e-06])\n",
      "Epoch 1676, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([-1.3113e-06, -6.5640e-06])\n",
      "Epoch 1677, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([-8.3447e-07, -6.4597e-06])\n",
      "Epoch 1678, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 8.3447e-07, -6.1989e-06])\n",
      "Epoch 1679, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 1.0729e-06, -6.0946e-06])\n",
      "Epoch 1680, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 1.1921e-06, -6.0201e-06])\n",
      "Epoch 1681, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 3.0398e-06, -5.8189e-06])\n",
      "Epoch 1682, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([-1.1921e-07, -6.1840e-06])\n",
      "Epoch 1683, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-07, -6.0722e-06])\n",
      "Epoch 1684, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-07, -6.0275e-06])\n",
      "Epoch 1685, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 1.9073e-06, -5.7295e-06])\n",
      "Epoch 1686, Loss 0.262983\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.5034e-06, -5.6401e-06])\n",
      "Epoch 1687, Loss 0.262983\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.5034e-06, -5.6103e-06])\n",
      "Epoch 1688, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.8610e-06, -5.4836e-06])\n",
      "Epoch 1689, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.8610e-06, -5.4315e-06])\n",
      "Epoch 1690, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.8610e-06, -5.4166e-06])\n",
      "Epoch 1691, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 3.0994e-06, -5.2974e-06])\n",
      "Epoch 1692, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 5.9605e-07, -5.7071e-06])\n",
      "Epoch 1693, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.2650e-06, -5.4166e-06])\n",
      "Epoch 1694, Loss 0.262983\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.8610e-06, -5.2825e-06])\n",
      "Epoch 1695, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 3.0994e-06, -5.1782e-06])\n",
      "Epoch 1696, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([-2.1458e-06, -5.8040e-06])\n",
      "Epoch 1697, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([-4.7684e-07, -5.6326e-06])\n",
      "Epoch 1698, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 1.3113e-06, -5.3868e-06])\n",
      "Epoch 1699, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 1.5497e-06, -5.3123e-06])\n",
      "Epoch 1700, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 1.6689e-06, -5.2378e-06])\n",
      "Epoch 1701, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.8610e-06, -5.0440e-06])\n",
      "Epoch 1702, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 4.0531e-06, -4.8354e-06])\n",
      "Epoch 1703, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-07, -5.2601e-06])\n",
      "Epoch 1704, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 4.7684e-07, -5.1633e-06])\n",
      "Epoch 1705, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 4.7684e-07, -5.1409e-06])\n",
      "Epoch 1706, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 5.9605e-07, -5.0962e-06])\n",
      "Epoch 1707, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 8.3447e-07, -4.9546e-06])\n",
      "Epoch 1708, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 8.3447e-07, -4.9546e-06])\n",
      "Epoch 1709, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 1.5497e-06, -4.8354e-06])\n",
      "Epoch 1710, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 3.0994e-06, -4.5374e-06])\n",
      "Epoch 1711, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([-8.3447e-07, -5.0291e-06])\n",
      "Epoch 1712, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([-8.3447e-07, -4.9993e-06])\n",
      "Epoch 1713, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 8.3447e-07, -4.7907e-06])\n",
      "Epoch 1714, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.6226e-06, -4.6045e-06])\n",
      "Epoch 1715, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.6226e-06, -4.5449e-06])\n",
      "Epoch 1716, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.8610e-06, -4.4331e-06])\n",
      "Epoch 1717, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 3.5763e-06, -4.3064e-06])\n",
      "Epoch 1718, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([-8.3447e-07, -4.8205e-06])\n",
      "Epoch 1719, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([-8.3447e-07, -4.8056e-06])\n",
      "Epoch 1720, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([-3.5763e-07, -4.6864e-06])\n",
      "Epoch 1721, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([-3.5763e-07, -4.6268e-06])\n",
      "Epoch 1722, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([-3.5763e-07, -4.5970e-06])\n",
      "Epoch 1723, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([-1.1921e-07, -4.4927e-06])\n",
      "Epoch 1724, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([-1.1921e-07, -4.4629e-06])\n",
      "Epoch 1725, Loss 0.262983\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 7.1526e-07, -4.3139e-06])\n",
      "Epoch 1726, Loss 0.262983\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.2650e-06, -4.0457e-06])\n",
      "Epoch 1727, Loss 0.262983\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.2650e-06, -4.0159e-06])\n",
      "Epoch 1728, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.5034e-06, -3.8967e-06])\n",
      "Epoch 1729, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 4.2915e-06, -3.6657e-06])\n",
      "Epoch 1730, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -3.9414e-06])\n",
      "Epoch 1731, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.5034e-06, -3.8818e-06])\n",
      "Epoch 1732, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.6226e-06, -3.8221e-06])\n",
      "Epoch 1733, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 3.9339e-06, -3.5837e-06])\n",
      "Epoch 1734, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 1.1921e-06, -3.9414e-06])\n",
      "Epoch 1735, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 1.3113e-06, -3.8669e-06])\n",
      "Epoch 1736, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 1.5497e-06, -3.7774e-06])\n",
      "Epoch 1737, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 1.5497e-06, -3.7625e-06])\n",
      "Epoch 1738, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 1.5497e-06, -3.7178e-06])\n",
      "Epoch 1739, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 1.7881e-06, -3.6433e-06])\n",
      "Epoch 1740, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 1.9073e-06, -3.5539e-06])\n",
      "Epoch 1741, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 3.5763e-06, -3.2708e-06])\n",
      "Epoch 1742, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([-3.5763e-07, -3.7774e-06])\n",
      "Epoch 1743, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([-1.1921e-07, -3.7029e-06])\n",
      "Epoch 1744, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 0.0000e+00, -3.5986e-06])\n",
      "Epoch 1745, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 3.5763e-06, -3.2336e-06])\n",
      "Epoch 1746, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([-2.5034e-06, -3.9712e-06])\n",
      "Epoch 1747, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([-2.2650e-06, -3.8818e-06])\n",
      "Epoch 1748, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([-2.1458e-06, -3.8221e-06])\n",
      "Epoch 1749, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([-1.4305e-06, -3.7029e-06])\n",
      "Epoch 1750, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 1.1921e-07, -3.4049e-06])\n",
      "Epoch 1751, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 1.1921e-07, -3.4049e-06])\n",
      "Epoch 1752, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 3.5763e-07, -3.2708e-06])\n",
      "Epoch 1753, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 3.5763e-07, -3.2410e-06])\n",
      "Epoch 1754, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 3.5763e-07, -3.2112e-06])\n",
      "Epoch 1755, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 7.1526e-07, -3.1069e-06])\n",
      "Epoch 1756, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 7.1526e-07, -3.0473e-06])\n",
      "Epoch 1757, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1758, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1759, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1760, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1761, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1762, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1763, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1764, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1765, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1766, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1767, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1768, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1769, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1770, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1771, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1772, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1773, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1774, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1775, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1776, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1777, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1778, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1779, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1780, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1781, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1782, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1783, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1784, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1785, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1786, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1787, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1788, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1789, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1790, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1791, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1792, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1793, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1794, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1795, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1796, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1797, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1798, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1799, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1800, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1801, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1802, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1803, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1804, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1805, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1806, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1807, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1808, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1809, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1810, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1811, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1812, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1813, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1814, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1815, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1816, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1817, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1818, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1819, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1820, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1821, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1822, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1823, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1824, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1825, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1826, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1827, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1828, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1829, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1830, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1831, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1832, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1833, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1834, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1835, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1836, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1837, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1838, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1839, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1840, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1841, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1842, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1843, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1844, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1845, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1846, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1847, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1848, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1849, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1850, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1851, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1852, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1853, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1854, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1855, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1856, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1857, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1858, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1859, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1860, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1861, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1862, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1863, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1864, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1865, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1866, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1867, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1868, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1869, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1870, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1871, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1872, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1873, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1874, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1875, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1876, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1877, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1878, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1879, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1880, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1881, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1882, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1883, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1884, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1885, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1886, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1887, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1888, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1889, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1890, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1891, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1892, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1893, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1894, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1895, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1896, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1897, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1898, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1899, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1900, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1901, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1902, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1903, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1904, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1905, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1906, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1907, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1908, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1909, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1910, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1911, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1912, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1913, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1914, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1915, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1916, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1917, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1918, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1919, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1920, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1921, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1922, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1923, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1924, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1925, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1926, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1927, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1928, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1929, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1930, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1931, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1932, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1933, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1934, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1935, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1936, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1937, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1938, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1939, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1940, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1941, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1942, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1943, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1944, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1945, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1946, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1947, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1948, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1949, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1950, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1951, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1952, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1953, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1954, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1955, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1956, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1957, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1958, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1959, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1960, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1961, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1962, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1963, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1964, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1965, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1966, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1967, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1968, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1969, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1970, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1971, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1972, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1973, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1974, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1975, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1976, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1977, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1978, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1979, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1980, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1981, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1982, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1983, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1984, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1985, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1986, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1987, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1988, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1989, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1990, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1991, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1992, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1993, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1994, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1995, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1996, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1997, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1998, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1999, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2000, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2001, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2002, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2003, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2004, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2005, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2006, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2007, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2008, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2009, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2010, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2011, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2012, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2013, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2014, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2015, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2016, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2017, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2018, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2019, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2020, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2021, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2022, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2023, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2024, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2025, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2026, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2027, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2028, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2029, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2030, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2031, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2032, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2033, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2034, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2035, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2036, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2037, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2038, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2039, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2040, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2041, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2042, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2043, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2044, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2045, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2046, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2047, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2048, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2049, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2050, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2051, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2052, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2053, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2054, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2055, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2056, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2057, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2058, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2059, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2060, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2061, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2062, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2063, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2064, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2065, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2066, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2067, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2068, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2069, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2070, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2071, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2072, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2073, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2074, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2075, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2076, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2077, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2078, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2079, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2080, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2081, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2082, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2083, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2084, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2085, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2086, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2087, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2088, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2089, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2090, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2091, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2092, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2093, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2094, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2095, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2096, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2097, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2098, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2099, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2100, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2101, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2102, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2103, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2104, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2105, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2106, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2107, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2108, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2109, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2110, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2111, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2112, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2113, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2114, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2115, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2116, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2117, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2118, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2119, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2120, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2121, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2122, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2123, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2124, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2125, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2126, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2127, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2128, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2129, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2130, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2131, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2132, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2133, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2134, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2135, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2136, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2137, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2138, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2139, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2140, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2141, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2142, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2143, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2144, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2145, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2146, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2147, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2148, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2149, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2150, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2151, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2152, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2153, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2154, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2155, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2156, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2157, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2158, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2159, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2160, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2161, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2162, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2163, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2164, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2165, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2166, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2167, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2168, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2169, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2170, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2171, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2172, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2173, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2174, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2175, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2176, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2177, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2178, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2179, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2180, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2181, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2182, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2183, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2184, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2185, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2186, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2187, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2188, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2189, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2190, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2191, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2192, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2193, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2194, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2195, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2196, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2197, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2198, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2199, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2200, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2201, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2202, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2203, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2204, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2205, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2206, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2207, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2208, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2209, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2210, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2211, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2212, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2213, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2214, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2215, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2216, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2217, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2218, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2219, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2220, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2221, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2222, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2223, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2224, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2225, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2226, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2227, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2228, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2229, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2230, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2231, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2232, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2233, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2234, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2235, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2236, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2237, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2238, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2239, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2240, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2241, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2242, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2243, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2244, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2245, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2246, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2247, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2248, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2249, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2250, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2251, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2252, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2253, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2254, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2255, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2256, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2257, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2258, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2259, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2260, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2261, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2262, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2263, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2264, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2265, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2266, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2267, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2268, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2269, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2270, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2271, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2272, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2273, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2274, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2275, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2276, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2277, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2278, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2279, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2280, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2281, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2282, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2283, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2284, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2285, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2286, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2287, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2288, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2289, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2290, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2291, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2292, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2293, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2294, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2295, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2296, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2297, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2298, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2299, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2300, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2301, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2302, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2303, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2304, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2305, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2306, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2307, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2308, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2309, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2310, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2311, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2312, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2313, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2314, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2315, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2316, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2317, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2318, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2319, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2320, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2321, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2322, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2323, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2324, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2325, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2326, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2327, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2328, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2329, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2330, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2331, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2332, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2333, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2334, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2335, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2336, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2337, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2338, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2339, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2340, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2341, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2342, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2343, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2344, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2345, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2346, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2347, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2348, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2349, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2350, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2351, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2352, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2353, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2354, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2355, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2356, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2357, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2358, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2359, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2360, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2361, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2362, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2363, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2364, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2365, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2366, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2367, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2368, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2369, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2370, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2371, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2372, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2373, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2374, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2375, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2376, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2377, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2378, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2379, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2380, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2381, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2382, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2383, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2384, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2385, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2386, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2387, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2388, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2389, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2390, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2391, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2392, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2393, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2394, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2395, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2396, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2397, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2398, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2399, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2400, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2401, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2402, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2403, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2404, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2405, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2406, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2407, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2408, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2409, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2410, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2411, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2412, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2413, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2414, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2415, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2416, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2417, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2418, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2419, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2420, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2421, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2422, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2423, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2424, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2425, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2426, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2427, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2428, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2429, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2430, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2431, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2432, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2433, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2434, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2435, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2436, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2437, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2438, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2439, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2440, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2441, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2442, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2443, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2444, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2445, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2446, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2447, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2448, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2449, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2450, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2451, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2452, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2453, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2454, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2455, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2456, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2457, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2458, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2459, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2460, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2461, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2462, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2463, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2464, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2465, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2466, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2467, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2468, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2469, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2470, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2471, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2472, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2473, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2474, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2475, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2476, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2477, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2478, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2479, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2480, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2481, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2482, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2483, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2484, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2485, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2486, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2487, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2488, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2489, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2490, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2491, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2492, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2493, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2494, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2495, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2496, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2497, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2498, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2499, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2500, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2501, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2502, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2503, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2504, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2505, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2506, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2507, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2508, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2509, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2510, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2511, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2512, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2513, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2514, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2515, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2516, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2517, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2518, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2519, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2520, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2521, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2522, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2523, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2524, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2525, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2526, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2527, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2528, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2529, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2530, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2531, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2532, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2533, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2534, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2535, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2536, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2537, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2538, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2539, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2540, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2541, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2542, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2543, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2544, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2545, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2546, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2547, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2548, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2549, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2550, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2551, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2552, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2553, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2554, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2555, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2556, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2557, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2558, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2559, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2560, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2561, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2562, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2563, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2564, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2565, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2566, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2567, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2568, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2569, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2570, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2571, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2572, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2573, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2574, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2575, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2576, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2577, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2578, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2579, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2580, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2581, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2582, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2583, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2584, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2585, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2586, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2587, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2588, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2589, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2590, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2591, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2592, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2593, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2594, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2595, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2596, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2597, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2598, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2599, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2600, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2601, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2602, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2603, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2604, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2605, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2606, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2607, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2608, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2609, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2610, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2611, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2612, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2613, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2614, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2615, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2616, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2617, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2618, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2619, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2620, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2621, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2622, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2623, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2624, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2625, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2626, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2627, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2628, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2629, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2630, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2631, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2632, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2633, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2634, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2635, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2636, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2637, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2638, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2639, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2640, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2641, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2642, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2643, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2644, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2645, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2646, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2647, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2648, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2649, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2650, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2651, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2652, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2653, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2654, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2655, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2656, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2657, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2658, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2659, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2660, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2661, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2662, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2663, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2664, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2665, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2666, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2667, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2668, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2669, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2670, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2671, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2672, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2673, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2674, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2675, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2676, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2677, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2678, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2679, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2680, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2681, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2682, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2683, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2684, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2685, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2686, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2687, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2688, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2689, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2690, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2691, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2692, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2693, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2694, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2695, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2696, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2697, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2698, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2699, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2700, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2701, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2702, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2703, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2704, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2705, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2706, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2707, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2708, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2709, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2710, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2711, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2712, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2713, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2714, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2715, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2716, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2717, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2718, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2719, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2720, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2721, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2722, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2723, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2724, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2725, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2726, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2727, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2728, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2729, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2730, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2731, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2732, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2733, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2734, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2735, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2736, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2737, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2738, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2739, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2740, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2741, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2742, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2743, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2744, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2745, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2746, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2747, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2748, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2749, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2750, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2751, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2752, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2753, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2754, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2755, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2756, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2757, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2758, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2759, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2760, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2761, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2762, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2763, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2764, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2765, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2766, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2767, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2768, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2769, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2770, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2771, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2772, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2773, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2774, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2775, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2776, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2777, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2778, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2779, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2780, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2781, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2782, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2783, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2784, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2785, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2786, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2787, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2788, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2789, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2790, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2791, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2792, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2793, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2794, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2795, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2796, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2797, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2798, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2799, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2800, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2801, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2802, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2803, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2804, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2805, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2806, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2807, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2808, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2809, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2810, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2811, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2812, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2813, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2814, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2815, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2816, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2817, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2818, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2819, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2820, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2821, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2822, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2823, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2824, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2825, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2826, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2827, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2828, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2829, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2830, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2831, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2832, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2833, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2834, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2835, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2836, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2837, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2838, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2839, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2840, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2841, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2842, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2843, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2844, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2845, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2846, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2847, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2848, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2849, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2850, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2851, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2852, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2853, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2854, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2855, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2856, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2857, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2858, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2859, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2860, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2861, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2862, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2863, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2864, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2865, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2866, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2867, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2868, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2869, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2870, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2871, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2872, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2873, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2874, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2875, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2876, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2877, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2878, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2879, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2880, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2881, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2882, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2883, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2884, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2885, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2886, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2887, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2888, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2889, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2890, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2891, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2892, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2893, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2894, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2895, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2896, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2897, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2898, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2899, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2900, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2901, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2902, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2903, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2904, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2905, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2906, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2907, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2908, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2909, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2910, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2911, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2912, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2913, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2914, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2915, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2916, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2917, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2918, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2919, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2920, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2921, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2922, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2923, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2924, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2925, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2926, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2927, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2928, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2929, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2930, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2931, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2932, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2933, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2934, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2935, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2936, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2937, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2938, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2939, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2940, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2941, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2942, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2943, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2944, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2945, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2946, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2947, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2948, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2949, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2950, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2951, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2952, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2953, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2954, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2955, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2956, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2957, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2958, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2959, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2960, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2961, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2962, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2963, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2964, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2965, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2966, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2967, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2968, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2969, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2970, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2971, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2972, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2973, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2974, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2975, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2976, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2977, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2978, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2979, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2980, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2981, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2982, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2983, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2984, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2985, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2986, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2987, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2988, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2989, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2990, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2991, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2992, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2993, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2994, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2995, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2996, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2997, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2998, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2999, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3000, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3001, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3002, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3003, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3004, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3005, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3006, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3007, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3008, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3009, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3010, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3011, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3012, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3013, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3014, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3015, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3016, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3017, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3018, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3019, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3020, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3021, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3022, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3023, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3024, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3025, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3026, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3027, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3028, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3029, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3030, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3031, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3032, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3033, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3034, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3035, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3036, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3037, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3038, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3039, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3040, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3041, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3042, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3043, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3044, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3045, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3046, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3047, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3048, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3049, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3050, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3051, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3052, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3053, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3054, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3055, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3056, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3057, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3058, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3059, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3060, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3061, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3062, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3063, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3064, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3065, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3066, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3067, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3068, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3069, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3070, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3071, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3072, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3073, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3074, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3075, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3076, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3077, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3078, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3079, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3080, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3081, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3082, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3083, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3084, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3085, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3086, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3087, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3088, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3089, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3090, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3091, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3092, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3093, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3094, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3095, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3096, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3097, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3098, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3099, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3100, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3101, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3102, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3103, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3104, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3105, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3106, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3107, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3108, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3109, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3110, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3111, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3112, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3113, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3114, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3115, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3116, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3117, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3118, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3119, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3120, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3121, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3122, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3123, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3124, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3125, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3126, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3127, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3128, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3129, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3130, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3131, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3132, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3133, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3134, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3135, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3136, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3137, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3138, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3139, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3140, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3141, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3142, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3143, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3144, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3145, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3146, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3147, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3148, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3149, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3150, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3151, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3152, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3153, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3154, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3155, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3156, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3157, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3158, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3159, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3160, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3161, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3162, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3163, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3164, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3165, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3166, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3167, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3168, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3169, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3170, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3171, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3172, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3173, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3174, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3175, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3176, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3177, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3178, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3179, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3180, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3181, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3182, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3183, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3184, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3185, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3186, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3187, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3188, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3189, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3190, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3191, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3192, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3193, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3194, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3195, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3196, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3197, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3198, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3199, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3200, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3201, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3202, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3203, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3204, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3205, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3206, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3207, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3208, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3209, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3210, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3211, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3212, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3213, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3214, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3215, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3216, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3217, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3218, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3219, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3220, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3221, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3222, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3223, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3224, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3225, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3226, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3227, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3228, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3229, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3230, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3231, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3232, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3233, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3234, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3235, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3236, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3237, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3238, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3239, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3240, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3241, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3242, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3243, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3244, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3245, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3246, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3247, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3248, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3249, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3250, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3251, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3252, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3253, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3254, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3255, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3256, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3257, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3258, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3259, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3260, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3261, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3262, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3263, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3264, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3265, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3266, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3267, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3268, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3269, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3270, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3271, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3272, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3273, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3274, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3275, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3276, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3277, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3278, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3279, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3280, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3281, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3282, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3283, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3284, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3285, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3286, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3287, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3288, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3289, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3290, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3291, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3292, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3293, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3294, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3295, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3296, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3297, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3298, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3299, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3300, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3301, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3302, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3303, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3304, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3305, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3306, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3307, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3308, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3309, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3310, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3311, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3312, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3313, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3314, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3315, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3316, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3317, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3318, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3319, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3320, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3321, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3322, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3323, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3324, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3325, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3326, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3327, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3328, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3329, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3330, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3331, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3332, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3333, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3334, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3335, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3336, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3337, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3338, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3339, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3340, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3341, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3342, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3343, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3344, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3345, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3346, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3347, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3348, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3349, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3350, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3351, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3352, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3353, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3354, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3355, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3356, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3357, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3358, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3359, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3360, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3361, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3362, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3363, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3364, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3365, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3366, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3367, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3368, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3369, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3370, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3371, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3372, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3373, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3374, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3375, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3376, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3377, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3378, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3379, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3380, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3381, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3382, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3383, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3384, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3385, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3386, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3387, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3388, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3389, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3390, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3391, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3392, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3393, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3394, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3395, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3396, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3397, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3398, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3399, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3400, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3401, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3402, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3403, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3404, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3405, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3406, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3407, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3408, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3409, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3410, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3411, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3412, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3413, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3414, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3415, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3416, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3417, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3418, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3419, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3420, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3421, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3422, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3423, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3424, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3425, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3426, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3427, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3428, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3429, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3430, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3431, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3432, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3433, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3434, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3435, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3436, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3437, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3438, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3439, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3440, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3441, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3442, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3443, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3444, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3445, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3446, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3447, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3448, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3449, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3450, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3451, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3452, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3453, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3454, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3455, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3456, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3457, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3458, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3459, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3460, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3461, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3462, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3463, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3464, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3465, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3466, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3467, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3468, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3469, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3470, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3471, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3472, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3473, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3474, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3475, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3476, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3477, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3478, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3479, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3480, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3481, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3482, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3483, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3484, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3485, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3486, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3487, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3488, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3489, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3490, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3491, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3492, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3493, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3494, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3495, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3496, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3497, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3498, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3499, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3500, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3501, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3502, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3503, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3504, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3505, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3506, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3507, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3508, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3509, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3510, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3511, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3512, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3513, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3514, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3515, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3516, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3517, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3518, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3519, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3520, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3521, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3522, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3523, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3524, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3525, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3526, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3527, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3528, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3529, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3530, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3531, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3532, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3533, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3534, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3535, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3536, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3537, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3538, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3539, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3540, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3541, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3542, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3543, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3544, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3545, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3546, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3547, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3548, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3549, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3550, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3551, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3552, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3553, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3554, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3555, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3556, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3557, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3558, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3559, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3560, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3561, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3562, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3563, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3564, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3565, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3566, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3567, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3568, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3569, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3570, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3571, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3572, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3573, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3574, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3575, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3576, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3577, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3578, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3579, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3580, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3581, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3582, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3583, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3584, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3585, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3586, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3587, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3588, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3589, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3590, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3591, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3592, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3593, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3594, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3595, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3596, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3597, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3598, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3599, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3600, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3601, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3602, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3603, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3604, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3605, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3606, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3607, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3608, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3609, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3610, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3611, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3612, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3613, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3614, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3615, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3616, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3617, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3618, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3619, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3620, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3621, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3622, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3623, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3624, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3625, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3626, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3627, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3628, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3629, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3630, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3631, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3632, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3633, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3634, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3635, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3636, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3637, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3638, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3639, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3640, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3641, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3642, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3643, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3644, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3645, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3646, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3647, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3648, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3649, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3650, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3651, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3652, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3653, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3654, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3655, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3656, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3657, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3658, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3659, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3660, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3661, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3662, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3663, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3664, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3665, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3666, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3667, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3668, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3669, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3670, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3671, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3672, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3673, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3674, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3675, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3676, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3677, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3678, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3679, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3680, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3681, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3682, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3683, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3684, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3685, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3686, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3687, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3688, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3689, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3690, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3691, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3692, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3693, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3694, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3695, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3696, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3697, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3698, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3699, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3700, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3701, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3702, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3703, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3704, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3705, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3706, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3707, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3708, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3709, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3710, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3711, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3712, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3713, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3714, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3715, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3716, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3717, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3718, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3719, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3720, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3721, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3722, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3723, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3724, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3725, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3726, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3727, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3728, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3729, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3730, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3731, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3732, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3733, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3734, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3735, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3736, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3737, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3738, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3739, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3740, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3741, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3742, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3743, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3744, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3745, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3746, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3747, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3748, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3749, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3750, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3751, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3752, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3753, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3754, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3755, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3756, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3757, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3758, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3759, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3760, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3761, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3762, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3763, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3764, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3765, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3766, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3767, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3768, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3769, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3770, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3771, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3772, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3773, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3774, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3775, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3776, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3777, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3778, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3779, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3780, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3781, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3782, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3783, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3784, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3785, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3786, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3787, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3788, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3789, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3790, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3791, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3792, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3793, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3794, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3795, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3796, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3797, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3798, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3799, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3800, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3801, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3802, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3803, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3804, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3805, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3806, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3807, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3808, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3809, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3810, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3811, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3812, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3813, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3814, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3815, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3816, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3817, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3818, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3819, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3820, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3821, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3822, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3823, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3824, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3825, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3826, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3827, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3828, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3829, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3830, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3831, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3832, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3833, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3834, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3835, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3836, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3837, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3838, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3839, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3840, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3841, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3842, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3843, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3844, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3845, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3846, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3847, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3848, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3849, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3850, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3851, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3852, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3853, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3854, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3855, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3856, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3857, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3858, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3859, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3860, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3861, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3862, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3863, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3864, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3865, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3866, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3867, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3868, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3869, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3870, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3871, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3872, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3873, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3874, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3875, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3876, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3877, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3878, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3879, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3880, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3881, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3882, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3883, Loss 0.262982\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3884, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3885, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3886, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3887, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3888, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3889, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3890, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3891, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3892, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3893, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3894, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3895, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3896, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3897, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3898, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3899, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3900, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3901, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3902, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3903, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3904, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3905, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3906, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3907, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3908, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3909, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3910, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3911, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3912, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3913, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3914, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3915, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3916, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3917, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3918, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3919, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3920, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3921, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3922, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3923, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3924, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3925, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3926, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3927, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3928, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3929, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3930, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3931, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3932, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3933, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3934, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3935, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3936, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3937, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3938, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3939, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3940, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3941, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3942, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3943, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3944, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3945, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3946, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3947, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3948, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3949, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3950, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3951, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3952, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3953, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3954, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3955, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3956, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3957, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3958, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3959, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3960, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3961, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3962, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3963, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3964, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3965, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3966, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3967, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3968, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3969, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3970, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3971, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3972, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3973, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3974, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3975, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3976, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3977, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3978, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3979, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3980, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3981, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3982, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3983, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3984, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3985, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3986, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3987, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3988, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3989, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3990, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3991, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3992, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3993, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3994, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3995, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3996, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3997, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3998, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3999, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4000, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4001, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4002, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4003, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4004, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4005, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4006, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4007, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4008, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4009, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4010, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4011, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4012, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4013, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4014, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4015, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4016, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4017, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4018, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4019, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4020, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4021, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4022, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4023, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4024, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4025, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4026, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4027, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4028, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4029, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4030, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4031, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4032, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4033, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4034, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4035, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4036, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4037, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4038, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4039, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4040, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4041, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4042, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4043, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4044, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4045, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4046, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4047, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4048, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4049, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4050, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4051, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4052, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4053, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4054, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4055, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4056, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4057, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4058, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4059, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4060, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4061, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4062, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4063, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4064, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4065, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4066, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4067, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4068, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4069, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4070, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4071, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4072, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4073, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4074, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4075, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4076, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4077, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4078, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4079, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4080, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4081, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4082, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4083, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4084, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4085, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4086, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4087, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4088, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4089, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4090, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4091, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4092, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4093, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4094, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4095, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4096, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4097, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4098, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4099, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4100, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4101, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4102, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4103, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4104, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4105, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4106, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4107, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4108, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4109, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4110, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4111, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4112, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4113, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4114, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4115, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4116, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4117, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4118, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4119, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4120, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4121, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4122, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4123, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4124, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4125, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4126, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4127, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4128, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4129, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4130, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4131, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4132, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4133, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4134, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4135, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4136, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4137, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4138, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4139, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4140, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4141, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4142, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4143, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4144, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4145, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4146, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4147, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4148, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4149, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4150, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4151, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4152, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4153, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4154, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4155, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4156, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4157, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4158, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4159, Loss 0.262982\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4160, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4161, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4162, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4163, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4164, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4165, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4166, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4167, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4168, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4169, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4170, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4171, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4172, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4173, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4174, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4175, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4176, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4177, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4178, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4179, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4180, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4181, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4182, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4183, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4184, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4185, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4186, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4187, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4188, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4189, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4190, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4191, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4192, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4193, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4194, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4195, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4196, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4197, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4198, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4199, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4200, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4201, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4202, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4203, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4204, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4205, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4206, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4207, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4208, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4209, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4210, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4211, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4212, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4213, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4214, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4215, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4216, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4217, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4218, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4219, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4220, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4221, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4222, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4223, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4224, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4225, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4226, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4227, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4228, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4229, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4230, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4231, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4232, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4233, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4234, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4235, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4236, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4237, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4238, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4239, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4240, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4241, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4242, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4243, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4244, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4245, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4246, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4247, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4248, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4249, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4250, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4251, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4252, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4253, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4254, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4255, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4256, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4257, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4258, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4259, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4260, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4261, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4262, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4263, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4264, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4265, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4266, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4267, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4268, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4269, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4270, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4271, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4272, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4273, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4274, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4275, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4276, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4277, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4278, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4279, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4280, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4281, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4282, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4283, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4284, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4285, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4286, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4287, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4288, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4289, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4290, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4291, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4292, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4293, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4294, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4295, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4296, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4297, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4298, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4299, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4300, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4301, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4302, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4303, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4304, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4305, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4306, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4307, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4308, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4309, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4310, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4311, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4312, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4313, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4314, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4315, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4316, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4317, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4318, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4319, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4320, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4321, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4322, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4323, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4324, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4325, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4326, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4327, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4328, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4329, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4330, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4331, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4332, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4333, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4334, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4335, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4336, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4337, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4338, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4339, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4340, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4341, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4342, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4343, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4344, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4345, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4346, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4347, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4348, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4349, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4350, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4351, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4352, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4353, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4354, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4355, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4356, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4357, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4358, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4359, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4360, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4361, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4362, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4363, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4364, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4365, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4366, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4367, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4368, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4369, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4370, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4371, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4372, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4373, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4374, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4375, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4376, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4377, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4378, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4379, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4380, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4381, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4382, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4383, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4384, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4385, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4386, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4387, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4388, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4389, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4390, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4391, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4392, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4393, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4394, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4395, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4396, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4397, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4398, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4399, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4400, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4401, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4402, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4403, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4404, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4405, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4406, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4407, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4408, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4409, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4410, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4411, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4412, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4413, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4414, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4415, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4416, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4417, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4418, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4419, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4420, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4421, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4422, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4423, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4424, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4425, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4426, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4427, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4428, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4429, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4430, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4431, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4432, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4433, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4434, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4435, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4436, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4437, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4438, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4439, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4440, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4441, Loss 0.262982\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4442, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4443, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4444, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4445, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4446, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4447, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4448, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4449, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4450, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4451, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4452, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4453, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4454, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4455, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4456, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4457, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4458, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4459, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4460, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4461, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4462, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4463, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4464, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4465, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4466, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4467, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4468, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4469, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4470, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4471, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4472, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4473, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4474, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4475, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4476, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4477, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4478, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4479, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4480, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4481, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4482, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4483, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4484, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4485, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4486, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4487, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4488, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4489, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4490, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4491, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4492, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4493, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4494, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4495, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4496, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4497, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4498, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4499, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4500, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4501, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4502, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4503, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4504, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4505, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4506, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4507, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4508, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4509, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4510, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4511, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4512, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4513, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4514, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4515, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4516, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4517, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4518, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4519, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4520, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4521, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4522, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4523, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4524, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4525, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4526, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4527, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4528, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4529, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4530, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4531, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4532, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4533, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4534, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4535, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4536, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4537, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4538, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4539, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4540, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4541, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4542, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4543, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4544, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4545, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4546, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4547, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4548, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4549, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4550, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4551, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4552, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4553, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4554, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4555, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4556, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4557, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4558, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4559, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4560, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4561, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4562, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4563, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4564, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4565, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4566, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4567, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4568, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4569, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4570, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4571, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4572, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4573, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4574, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4575, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4576, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4577, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4578, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4579, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4580, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4581, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4582, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4583, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4584, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4585, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4586, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4587, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4588, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4589, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4590, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4591, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4592, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4593, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4594, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4595, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4596, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4597, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4598, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4599, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4600, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4601, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4602, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4603, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4604, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4605, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4606, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4607, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4608, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4609, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4610, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4611, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4612, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4613, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4614, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4615, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4616, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4617, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4618, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4619, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4620, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4621, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4622, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4623, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4624, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4625, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4626, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4627, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4628, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4629, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4630, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4631, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4632, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4633, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4634, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4635, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4636, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4637, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4638, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4639, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4640, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4641, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4642, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4643, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4644, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4645, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4646, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4647, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4648, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4649, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4650, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4651, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4652, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4653, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4654, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4655, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4656, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4657, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4658, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4659, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4660, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4661, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4662, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4663, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4664, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4665, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4666, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4667, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4668, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4669, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4670, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4671, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4672, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4673, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4674, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4675, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4676, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4677, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4678, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4679, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4680, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4681, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4682, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4683, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4684, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4685, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4686, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4687, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4688, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4689, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4690, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4691, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4692, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4693, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4694, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4695, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4696, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4697, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4698, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4699, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4700, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4701, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4702, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4703, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4704, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4705, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4706, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4707, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4708, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4709, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4710, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4711, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4712, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4713, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4714, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4715, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4716, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4717, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4718, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4719, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4720, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4721, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4722, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4723, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4724, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4725, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4726, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4727, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4728, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4729, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4730, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4731, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4732, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4733, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4734, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4735, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4736, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4737, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4738, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4739, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4740, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4741, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4742, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4743, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4744, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4745, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4746, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4747, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4748, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4749, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4750, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4751, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4752, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4753, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4754, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4755, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4756, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4757, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4758, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4759, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4760, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4761, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4762, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4763, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4764, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4765, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4766, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4767, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4768, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4769, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4770, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4771, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4772, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4773, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4774, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4775, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4776, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4777, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4778, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4779, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4780, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4781, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4782, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4783, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4784, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4785, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4786, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4787, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4788, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4789, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4790, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4791, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4792, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4793, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4794, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4795, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4796, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4797, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4798, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4799, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4800, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4801, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4802, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4803, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4804, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4805, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4806, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4807, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4808, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4809, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4810, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4811, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4812, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4813, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4814, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4815, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4816, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4817, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4818, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4819, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4820, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4821, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4822, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4823, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4824, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4825, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4826, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4827, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4828, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4829, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4830, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4831, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4832, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4833, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4834, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4835, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4836, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4837, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4838, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4839, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4840, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4841, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4842, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4843, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4844, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4845, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4846, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4847, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4848, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4849, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4850, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4851, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4852, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4853, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4854, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4855, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4856, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4857, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4858, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4859, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4860, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4861, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4862, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4863, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4864, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4865, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4866, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4867, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4868, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4869, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4870, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4871, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4872, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4873, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4874, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4875, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4876, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4877, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4878, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4879, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4880, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4881, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4882, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4883, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4884, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4885, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4886, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4887, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4888, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4889, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4890, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4891, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4892, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4893, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4894, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4895, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4896, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4897, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4898, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4899, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4900, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4901, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4902, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4903, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4904, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4905, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4906, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4907, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4908, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4909, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4910, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4911, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4912, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4913, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4914, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4915, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4916, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4917, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4918, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4919, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4920, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4921, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4922, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4923, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4924, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4925, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4926, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4927, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4928, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4929, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4930, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4931, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4932, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4933, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4934, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4935, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4936, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4937, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4938, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4939, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4940, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4941, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4942, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4943, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4944, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4945, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4946, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4947, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4948, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4949, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4950, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4951, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4952, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4953, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4954, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4955, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4956, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4957, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4958, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4959, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4960, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4961, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4962, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4963, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4964, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4965, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4966, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4967, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4968, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4969, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4970, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4971, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4972, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4973, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4974, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4975, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4976, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4977, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4978, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4979, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4980, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4981, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4982, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4983, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4984, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4985, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4986, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4987, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4988, Loss 0.262982\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4989, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4990, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4991, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4992, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4993, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4994, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4995, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4996, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4997, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4998, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4999, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 5000, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([0.8410, 0.5997])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params = training_loop(\n",
    "    n_epochs = 5000,\n",
    "    learning_rate = 1e-2,\n",
    "    params = torch.tensor([1.0, 0.0]),\n",
    "    x = x,\n",
    "    y = y)\n",
    "params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "executed-liverpool",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([ 3.4552,  7.4454,  8.0987,  0.3458,  5.6478,  2.6982,  9.5794, 10.6228,\n",
       "          6.1169,  0.1090,  0.8963]),\n",
       " tensor(0.8410),\n",
       " tensor(0.5997),\n",
       " tensor([3.5057, 6.8616, 7.4111, 0.8906, 5.3498, 2.8690, 8.6564, 9.5340, 5.7443,\n",
       "         0.6913, 1.3535]))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yy = model(x, *params)\n",
    "w, b = params\n",
    "x, w, b, yy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "opening-validation",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 3600x2400 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "fig = plt.figure(dpi=600)\n",
    "plt.xlabel(\"x label\")\n",
    "plt.ylabel(\"y label\")\n",
    "plt.plot(x.numpy(), yy.detach().numpy(), '*')\n",
    "plt.plot(x.numpy(), yy.detach().numpy())\n",
    "plt.plot(x.numpy(), y.numpy(), 'o')\n",
    "plt.savefig(\"dl_ch1t2_plot01.png\", format=\"png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "challenging-dublin",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
