{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cutting-rwanda",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "exotic-exposure",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = [4.209015  , 6.0251656 , 6.586659  , 1.0785204 , 5.323591,   2.9644287, 8.885769  , 9.895647  ,  6.464806  , 0.18034637, 1.2534696]\n",
    "x = [34.552039 , 74.45411  , 80.987488 ,  3.458197 , 56.4778655, 26.98163  , 95.79415  , 106.228316 , 61.169422 , 1.089516 , 8.962632]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "blank-armstrong",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 34.5520,  74.4541,  80.9875,   3.4582,  56.4779,  26.9816,  95.7942,\n",
       "        106.2283,  61.1694,   1.0895,   8.9626])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = torch.tensor(y)\n",
    "x = torch.tensor(x)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "rural-reggae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(x, w, b):\n",
    "    return w * x + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "patient-brush",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn(yy, y):\n",
    "    squared_diffs = (yy - y)**2\n",
    "    return squared_diffs.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "precious-fifth",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dloss_fn(yy, y):\n",
    "    dsq_diffs = 2 * (yy - y) / yy.size(0)\n",
    "    return dsq_diffs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "available-convert",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dmodel_dw(x, w, b):\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "labeled-conducting",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dmodel_db(x, w, b):\n",
    "    return 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "plastic-enclosure",
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad_fn(x, y, yy, w, b):\n",
    "    dloss_dyy = dloss_fn(yy,y)\n",
    "    dloss_dw = dloss_dyy * dmodel_dw(x, w, b)\n",
    "    dloss_db = dloss_dyy * dmodel_db(x, w, b)\n",
    "    return torch.stack([dloss_dw.sum(), dloss_db.sum()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fabulous-producer",
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_loop(n_epochs, learning_rate, params, x, y):\n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        w, b = params\n",
    "        yy = model(x, w, b)\n",
    "        loss = loss_fn(yy, y)                \n",
    "        grad = grad_fn(x, y, yy, w, b)        \n",
    "        params = params - learning_rate * grad\n",
    "        print('Epoch %d, Loss %f' % (epoch, float(loss)))\n",
    "        print('\\t Params: ', params)\n",
    "        print('\\t Grad: ', grad)\n",
    "        \n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "objective-metro",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 3.4552,  7.4454,  8.0987,  0.3458,  5.6478,  2.6982,  9.5794, 10.6228,\n",
       "         6.1169,  0.1090,  0.8963])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = x * 0.1\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "initial-kuwait",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss 0.622568\n",
      "\t Params:  tensor([ 0.9400, -0.0039])\n",
      "\t Grad:  tensor([5.9979, 0.3906])\n",
      "Epoch 2, Loss 0.399406\n",
      "\t Params:  tensor([ 0.9257, -0.0017])\n",
      "\t Grad:  tensor([ 1.4320, -0.2172])\n",
      "Epoch 3, Loss 0.385859\n",
      "\t Params:  tensor([0.9220, 0.0018])\n",
      "\t Grad:  tensor([ 0.3730, -0.3561])\n",
      "Epoch 4, Loss 0.383605\n",
      "\t Params:  tensor([0.9207, 0.0057])\n",
      "\t Grad:  tensor([ 0.1271, -0.3863])\n",
      "Epoch 5, Loss 0.381978\n",
      "\t Params:  tensor([0.9200, 0.0096])\n",
      "\t Grad:  tensor([ 0.0698, -0.3913])\n",
      "Epoch 6, Loss 0.380404\n",
      "\t Params:  tensor([0.9194, 0.0135])\n",
      "\t Grad:  tensor([ 0.0563, -0.3904])\n",
      "Epoch 7, Loss 0.378853\n",
      "\t Params:  tensor([0.9189, 0.0174])\n",
      "\t Grad:  tensor([ 0.0529, -0.3883])\n",
      "Epoch 8, Loss 0.377323\n",
      "\t Params:  tensor([0.9184, 0.0212])\n",
      "\t Grad:  tensor([ 0.0518, -0.3858])\n",
      "Epoch 9, Loss 0.375813\n",
      "\t Params:  tensor([0.9179, 0.0251])\n",
      "\t Grad:  tensor([ 0.0513, -0.3832])\n",
      "Epoch 10, Loss 0.374323\n",
      "\t Params:  tensor([0.9174, 0.0289])\n",
      "\t Grad:  tensor([ 0.0509, -0.3807])\n",
      "Epoch 11, Loss 0.372852\n",
      "\t Params:  tensor([0.9169, 0.0327])\n",
      "\t Grad:  tensor([ 0.0506, -0.3782])\n",
      "Epoch 12, Loss 0.371401\n",
      "\t Params:  tensor([0.9164, 0.0364])\n",
      "\t Grad:  tensor([ 0.0502, -0.3757])\n",
      "Epoch 13, Loss 0.369969\n",
      "\t Params:  tensor([0.9159, 0.0402])\n",
      "\t Grad:  tensor([ 0.0499, -0.3732])\n",
      "Epoch 14, Loss 0.368557\n",
      "\t Params:  tensor([0.9154, 0.0439])\n",
      "\t Grad:  tensor([ 0.0496, -0.3707])\n",
      "Epoch 15, Loss 0.367162\n",
      "\t Params:  tensor([0.9149, 0.0475])\n",
      "\t Grad:  tensor([ 0.0492, -0.3683])\n",
      "Epoch 16, Loss 0.365786\n",
      "\t Params:  tensor([0.9144, 0.0512])\n",
      "\t Grad:  tensor([ 0.0489, -0.3658])\n",
      "Epoch 17, Loss 0.364429\n",
      "\t Params:  tensor([0.9139, 0.0548])\n",
      "\t Grad:  tensor([ 0.0486, -0.3634])\n",
      "Epoch 18, Loss 0.363089\n",
      "\t Params:  tensor([0.9134, 0.0585])\n",
      "\t Grad:  tensor([ 0.0483, -0.3610])\n",
      "Epoch 19, Loss 0.361767\n",
      "\t Params:  tensor([0.9129, 0.0620])\n",
      "\t Grad:  tensor([ 0.0480, -0.3586])\n",
      "Epoch 20, Loss 0.360462\n",
      "\t Params:  tensor([0.9125, 0.0656])\n",
      "\t Grad:  tensor([ 0.0476, -0.3562])\n",
      "Epoch 21, Loss 0.359175\n",
      "\t Params:  tensor([0.9120, 0.0691])\n",
      "\t Grad:  tensor([ 0.0473, -0.3539])\n",
      "Epoch 22, Loss 0.357905\n",
      "\t Params:  tensor([0.9115, 0.0727])\n",
      "\t Grad:  tensor([ 0.0470, -0.3515])\n",
      "Epoch 23, Loss 0.356651\n",
      "\t Params:  tensor([0.9111, 0.0761])\n",
      "\t Grad:  tensor([ 0.0467, -0.3492])\n",
      "Epoch 24, Loss 0.355414\n",
      "\t Params:  tensor([0.9106, 0.0796])\n",
      "\t Grad:  tensor([ 0.0464, -0.3469])\n",
      "Epoch 25, Loss 0.354193\n",
      "\t Params:  tensor([0.9101, 0.0831])\n",
      "\t Grad:  tensor([ 0.0461, -0.3446])\n",
      "Epoch 26, Loss 0.352988\n",
      "\t Params:  tensor([0.9097, 0.0865])\n",
      "\t Grad:  tensor([ 0.0458, -0.3423])\n",
      "Epoch 27, Loss 0.351800\n",
      "\t Params:  tensor([0.9092, 0.0899])\n",
      "\t Grad:  tensor([ 0.0455, -0.3400])\n",
      "Epoch 28, Loss 0.350627\n",
      "\t Params:  tensor([0.9088, 0.0933])\n",
      "\t Grad:  tensor([ 0.0452, -0.3378])\n",
      "Epoch 29, Loss 0.349469\n",
      "\t Params:  tensor([0.9083, 0.0966])\n",
      "\t Grad:  tensor([ 0.0449, -0.3355])\n",
      "Epoch 30, Loss 0.348327\n",
      "\t Params:  tensor([0.9079, 0.0999])\n",
      "\t Grad:  tensor([ 0.0446, -0.3333])\n",
      "Epoch 31, Loss 0.347200\n",
      "\t Params:  tensor([0.9074, 0.1033])\n",
      "\t Grad:  tensor([ 0.0443, -0.3311])\n",
      "Epoch 32, Loss 0.346088\n",
      "\t Params:  tensor([0.9070, 0.1065])\n",
      "\t Grad:  tensor([ 0.0440, -0.3289])\n",
      "Epoch 33, Loss 0.344990\n",
      "\t Params:  tensor([0.9065, 0.1098])\n",
      "\t Grad:  tensor([ 0.0437, -0.3267])\n",
      "Epoch 34, Loss 0.343907\n",
      "\t Params:  tensor([0.9061, 0.1131])\n",
      "\t Grad:  tensor([ 0.0434, -0.3246])\n",
      "Epoch 35, Loss 0.342838\n",
      "\t Params:  tensor([0.9057, 0.1163])\n",
      "\t Grad:  tensor([ 0.0431, -0.3224])\n",
      "Epoch 36, Loss 0.341784\n",
      "\t Params:  tensor([0.9053, 0.1195])\n",
      "\t Grad:  tensor([ 0.0428, -0.3203])\n",
      "Epoch 37, Loss 0.340743\n",
      "\t Params:  tensor([0.9048, 0.1227])\n",
      "\t Grad:  tensor([ 0.0425, -0.3182])\n",
      "Epoch 38, Loss 0.339716\n",
      "\t Params:  tensor([0.9044, 0.1258])\n",
      "\t Grad:  tensor([ 0.0423, -0.3161])\n",
      "Epoch 39, Loss 0.338703\n",
      "\t Params:  tensor([0.9040, 0.1290])\n",
      "\t Grad:  tensor([ 0.0420, -0.3140])\n",
      "Epoch 40, Loss 0.337703\n",
      "\t Params:  tensor([0.9036, 0.1321])\n",
      "\t Grad:  tensor([ 0.0417, -0.3119])\n",
      "Epoch 41, Loss 0.336716\n",
      "\t Params:  tensor([0.9032, 0.1352])\n",
      "\t Grad:  tensor([ 0.0414, -0.3098])\n",
      "Epoch 42, Loss 0.335742\n",
      "\t Params:  tensor([0.9027, 0.1383])\n",
      "\t Grad:  tensor([ 0.0412, -0.3078])\n",
      "Epoch 43, Loss 0.334781\n",
      "\t Params:  tensor([0.9023, 0.1413])\n",
      "\t Grad:  tensor([ 0.0409, -0.3057])\n",
      "Epoch 44, Loss 0.333833\n",
      "\t Params:  tensor([0.9019, 0.1444])\n",
      "\t Grad:  tensor([ 0.0406, -0.3037])\n",
      "Epoch 45, Loss 0.332897\n",
      "\t Params:  tensor([0.9015, 0.1474])\n",
      "\t Grad:  tensor([ 0.0403, -0.3017])\n",
      "Epoch 46, Loss 0.331974\n",
      "\t Params:  tensor([0.9011, 0.1504])\n",
      "\t Grad:  tensor([ 0.0401, -0.2997])\n",
      "Epoch 47, Loss 0.331063\n",
      "\t Params:  tensor([0.9007, 0.1534])\n",
      "\t Grad:  tensor([ 0.0398, -0.2977])\n",
      "Epoch 48, Loss 0.330164\n",
      "\t Params:  tensor([0.9003, 0.1563])\n",
      "\t Grad:  tensor([ 0.0395, -0.2957])\n",
      "Epoch 49, Loss 0.329276\n",
      "\t Params:  tensor([0.8999, 0.1592])\n",
      "\t Grad:  tensor([ 0.0393, -0.2938])\n",
      "Epoch 50, Loss 0.328401\n",
      "\t Params:  tensor([0.8995, 0.1622])\n",
      "\t Grad:  tensor([ 0.0390, -0.2918])\n",
      "Epoch 51, Loss 0.327537\n",
      "\t Params:  tensor([0.8992, 0.1651])\n",
      "\t Grad:  tensor([ 0.0388, -0.2899])\n",
      "Epoch 52, Loss 0.326684\n",
      "\t Params:  tensor([0.8988, 0.1679])\n",
      "\t Grad:  tensor([ 0.0385, -0.2880])\n",
      "Epoch 53, Loss 0.325843\n",
      "\t Params:  tensor([0.8984, 0.1708])\n",
      "\t Grad:  tensor([ 0.0383, -0.2861])\n",
      "Epoch 54, Loss 0.325013\n",
      "\t Params:  tensor([0.8980, 0.1736])\n",
      "\t Grad:  tensor([ 0.0380, -0.2842])\n",
      "Epoch 55, Loss 0.324194\n",
      "\t Params:  tensor([0.8976, 0.1765])\n",
      "\t Grad:  tensor([ 0.0377, -0.2823])\n",
      "Epoch 56, Loss 0.323385\n",
      "\t Params:  tensor([0.8973, 0.1793])\n",
      "\t Grad:  tensor([ 0.0375, -0.2804])\n",
      "Epoch 57, Loss 0.322587\n",
      "\t Params:  tensor([0.8969, 0.1821])\n",
      "\t Grad:  tensor([ 0.0372, -0.2786])\n",
      "Epoch 58, Loss 0.321800\n",
      "\t Params:  tensor([0.8965, 0.1848])\n",
      "\t Grad:  tensor([ 0.0370, -0.2767])\n",
      "Epoch 59, Loss 0.321023\n",
      "\t Params:  tensor([0.8962, 0.1876])\n",
      "\t Grad:  tensor([ 0.0368, -0.2749])\n",
      "Epoch 60, Loss 0.320257\n",
      "\t Params:  tensor([0.8958, 0.1903])\n",
      "\t Grad:  tensor([ 0.0365, -0.2731])\n",
      "Epoch 61, Loss 0.319500\n",
      "\t Params:  tensor([0.8954, 0.1930])\n",
      "\t Grad:  tensor([ 0.0363, -0.2712])\n",
      "Epoch 62, Loss 0.318754\n",
      "\t Params:  tensor([0.8951, 0.1957])\n",
      "\t Grad:  tensor([ 0.0360, -0.2694])\n",
      "Epoch 63, Loss 0.318018\n",
      "\t Params:  tensor([0.8947, 0.1984])\n",
      "\t Grad:  tensor([ 0.0358, -0.2677])\n",
      "Epoch 64, Loss 0.317291\n",
      "\t Params:  tensor([0.8944, 0.2010])\n",
      "\t Grad:  tensor([ 0.0356, -0.2659])\n",
      "Epoch 65, Loss 0.316574\n",
      "\t Params:  tensor([0.8940, 0.2037])\n",
      "\t Grad:  tensor([ 0.0353, -0.2641])\n",
      "Epoch 66, Loss 0.315866\n",
      "\t Params:  tensor([0.8936, 0.2063])\n",
      "\t Grad:  tensor([ 0.0351, -0.2624])\n",
      "Epoch 67, Loss 0.315167\n",
      "\t Params:  tensor([0.8933, 0.2089])\n",
      "\t Grad:  tensor([ 0.0348, -0.2606])\n",
      "Epoch 68, Loss 0.314478\n",
      "\t Params:  tensor([0.8930, 0.2115])\n",
      "\t Grad:  tensor([ 0.0346, -0.2589])\n",
      "Epoch 69, Loss 0.313798\n",
      "\t Params:  tensor([0.8926, 0.2141])\n",
      "\t Grad:  tensor([ 0.0344, -0.2572])\n",
      "Epoch 70, Loss 0.313127\n",
      "\t Params:  tensor([0.8923, 0.2166])\n",
      "\t Grad:  tensor([ 0.0342, -0.2555])\n",
      "Epoch 71, Loss 0.312465\n",
      "\t Params:  tensor([0.8919, 0.2192])\n",
      "\t Grad:  tensor([ 0.0339, -0.2538])\n",
      "Epoch 72, Loss 0.311811\n",
      "\t Params:  tensor([0.8916, 0.2217])\n",
      "\t Grad:  tensor([ 0.0337, -0.2521])\n",
      "Epoch 73, Loss 0.311166\n",
      "\t Params:  tensor([0.8913, 0.2242])\n",
      "\t Grad:  tensor([ 0.0335, -0.2505])\n",
      "Epoch 74, Loss 0.310530\n",
      "\t Params:  tensor([0.8909, 0.2267])\n",
      "\t Grad:  tensor([ 0.0333, -0.2488])\n",
      "Epoch 75, Loss 0.309902\n",
      "\t Params:  tensor([0.8906, 0.2292])\n",
      "\t Grad:  tensor([ 0.0330, -0.2471])\n",
      "Epoch 76, Loss 0.309282\n",
      "\t Params:  tensor([0.8903, 0.2316])\n",
      "\t Grad:  tensor([ 0.0328, -0.2455])\n",
      "Epoch 77, Loss 0.308671\n",
      "\t Params:  tensor([0.8899, 0.2341])\n",
      "\t Grad:  tensor([ 0.0326, -0.2439])\n",
      "Epoch 78, Loss 0.308067\n",
      "\t Params:  tensor([0.8896, 0.2365])\n",
      "\t Grad:  tensor([ 0.0324, -0.2423])\n",
      "Epoch 79, Loss 0.307472\n",
      "\t Params:  tensor([0.8893, 0.2389])\n",
      "\t Grad:  tensor([ 0.0322, -0.2407])\n",
      "Epoch 80, Loss 0.306884\n",
      "\t Params:  tensor([0.8890, 0.2413])\n",
      "\t Grad:  tensor([ 0.0320, -0.2391])\n",
      "Epoch 81, Loss 0.306305\n",
      "\t Params:  tensor([0.8887, 0.2436])\n",
      "\t Grad:  tensor([ 0.0318, -0.2375])\n",
      "Epoch 82, Loss 0.305732\n",
      "\t Params:  tensor([0.8883, 0.2460])\n",
      "\t Grad:  tensor([ 0.0315, -0.2359])\n",
      "Epoch 83, Loss 0.305168\n",
      "\t Params:  tensor([0.8880, 0.2483])\n",
      "\t Grad:  tensor([ 0.0313, -0.2343])\n",
      "Epoch 84, Loss 0.304611\n",
      "\t Params:  tensor([0.8877, 0.2507])\n",
      "\t Grad:  tensor([ 0.0311, -0.2328])\n",
      "Epoch 85, Loss 0.304061\n",
      "\t Params:  tensor([0.8874, 0.2530])\n",
      "\t Grad:  tensor([ 0.0309, -0.2312])\n",
      "Epoch 86, Loss 0.303519\n",
      "\t Params:  tensor([0.8871, 0.2553])\n",
      "\t Grad:  tensor([ 0.0307, -0.2297])\n",
      "Epoch 87, Loss 0.302983\n",
      "\t Params:  tensor([0.8868, 0.2576])\n",
      "\t Grad:  tensor([ 0.0305, -0.2282])\n",
      "Epoch 88, Loss 0.302455\n",
      "\t Params:  tensor([0.8865, 0.2598])\n",
      "\t Grad:  tensor([ 0.0303, -0.2267])\n",
      "Epoch 89, Loss 0.301933\n",
      "\t Params:  tensor([0.8862, 0.2621])\n",
      "\t Grad:  tensor([ 0.0301, -0.2252])\n",
      "Epoch 90, Loss 0.301419\n",
      "\t Params:  tensor([0.8859, 0.2643])\n",
      "\t Grad:  tensor([ 0.0299, -0.2237])\n",
      "Epoch 91, Loss 0.300911\n",
      "\t Params:  tensor([0.8856, 0.2665])\n",
      "\t Grad:  tensor([ 0.0297, -0.2222])\n",
      "Epoch 92, Loss 0.300411\n",
      "\t Params:  tensor([0.8853, 0.2688])\n",
      "\t Grad:  tensor([ 0.0295, -0.2207])\n",
      "Epoch 93, Loss 0.299916\n",
      "\t Params:  tensor([0.8850, 0.2709])\n",
      "\t Grad:  tensor([ 0.0293, -0.2193])\n",
      "Epoch 94, Loss 0.299428\n",
      "\t Params:  tensor([0.8847, 0.2731])\n",
      "\t Grad:  tensor([ 0.0291, -0.2178])\n",
      "Epoch 95, Loss 0.298947\n",
      "\t Params:  tensor([0.8844, 0.2753])\n",
      "\t Grad:  tensor([ 0.0289, -0.2164])\n",
      "Epoch 96, Loss 0.298472\n",
      "\t Params:  tensor([0.8841, 0.2774])\n",
      "\t Grad:  tensor([ 0.0287, -0.2149])\n",
      "Epoch 97, Loss 0.298003\n",
      "\t Params:  tensor([0.8839, 0.2796])\n",
      "\t Grad:  tensor([ 0.0285, -0.2135])\n",
      "Epoch 98, Loss 0.297541\n",
      "\t Params:  tensor([0.8836, 0.2817])\n",
      "\t Grad:  tensor([ 0.0284, -0.2121])\n",
      "Epoch 99, Loss 0.297085\n",
      "\t Params:  tensor([0.8833, 0.2838])\n",
      "\t Grad:  tensor([ 0.0282, -0.2107])\n",
      "Epoch 100, Loss 0.296634\n",
      "\t Params:  tensor([0.8830, 0.2859])\n",
      "\t Grad:  tensor([ 0.0280, -0.2093])\n",
      "Epoch 101, Loss 0.296190\n",
      "\t Params:  tensor([0.8827, 0.2880])\n",
      "\t Grad:  tensor([ 0.0278, -0.2079])\n",
      "Epoch 102, Loss 0.295751\n",
      "\t Params:  tensor([0.8825, 0.2900])\n",
      "\t Grad:  tensor([ 0.0276, -0.2065])\n",
      "Epoch 103, Loss 0.295318\n",
      "\t Params:  tensor([0.8822, 0.2921])\n",
      "\t Grad:  tensor([ 0.0274, -0.2052])\n",
      "Epoch 104, Loss 0.294891\n",
      "\t Params:  tensor([0.8819, 0.2941])\n",
      "\t Grad:  tensor([ 0.0273, -0.2038])\n",
      "Epoch 105, Loss 0.294470\n",
      "\t Params:  tensor([0.8816, 0.2962])\n",
      "\t Grad:  tensor([ 0.0271, -0.2025])\n",
      "Epoch 106, Loss 0.294054\n",
      "\t Params:  tensor([0.8814, 0.2982])\n",
      "\t Grad:  tensor([ 0.0269, -0.2011])\n",
      "Epoch 107, Loss 0.293644\n",
      "\t Params:  tensor([0.8811, 0.3002])\n",
      "\t Grad:  tensor([ 0.0267, -0.1998])\n",
      "Epoch 108, Loss 0.293239\n",
      "\t Params:  tensor([0.8808, 0.3021])\n",
      "\t Grad:  tensor([ 0.0265, -0.1985])\n",
      "Epoch 109, Loss 0.292839\n",
      "\t Params:  tensor([0.8806, 0.3041])\n",
      "\t Grad:  tensor([ 0.0264, -0.1971])\n",
      "Epoch 110, Loss 0.292445\n",
      "\t Params:  tensor([0.8803, 0.3061])\n",
      "\t Grad:  tensor([ 0.0262, -0.1958])\n",
      "Epoch 111, Loss 0.292056\n",
      "\t Params:  tensor([0.8800, 0.3080])\n",
      "\t Grad:  tensor([ 0.0260, -0.1945])\n",
      "Epoch 112, Loss 0.291672\n",
      "\t Params:  tensor([0.8798, 0.3100])\n",
      "\t Grad:  tensor([ 0.0258, -0.1933])\n",
      "Epoch 113, Loss 0.291293\n",
      "\t Params:  tensor([0.8795, 0.3119])\n",
      "\t Grad:  tensor([ 0.0257, -0.1920])\n",
      "Epoch 114, Loss 0.290919\n",
      "\t Params:  tensor([0.8793, 0.3138])\n",
      "\t Grad:  tensor([ 0.0255, -0.1907])\n",
      "Epoch 115, Loss 0.290550\n",
      "\t Params:  tensor([0.8790, 0.3157])\n",
      "\t Grad:  tensor([ 0.0253, -0.1894])\n",
      "Epoch 116, Loss 0.290186\n",
      "\t Params:  tensor([0.8788, 0.3176])\n",
      "\t Grad:  tensor([ 0.0252, -0.1882])\n",
      "Epoch 117, Loss 0.289827\n",
      "\t Params:  tensor([0.8785, 0.3194])\n",
      "\t Grad:  tensor([ 0.0250, -0.1869])\n",
      "Epoch 118, Loss 0.289472\n",
      "\t Params:  tensor([0.8783, 0.3213])\n",
      "\t Grad:  tensor([ 0.0248, -0.1857])\n",
      "Epoch 119, Loss 0.289122\n",
      "\t Params:  tensor([0.8780, 0.3231])\n",
      "\t Grad:  tensor([ 0.0247, -0.1845])\n",
      "Epoch 120, Loss 0.288777\n",
      "\t Params:  tensor([0.8778, 0.3250])\n",
      "\t Grad:  tensor([ 0.0245, -0.1832])\n",
      "Epoch 121, Loss 0.288436\n",
      "\t Params:  tensor([0.8775, 0.3268])\n",
      "\t Grad:  tensor([ 0.0243, -0.1820])\n",
      "Epoch 122, Loss 0.288100\n",
      "\t Params:  tensor([0.8773, 0.3286])\n",
      "\t Grad:  tensor([ 0.0242, -0.1808])\n",
      "Epoch 123, Loss 0.287769\n",
      "\t Params:  tensor([0.8771, 0.3304])\n",
      "\t Grad:  tensor([ 0.0240, -0.1796])\n",
      "Epoch 124, Loss 0.287441\n",
      "\t Params:  tensor([0.8768, 0.3322])\n",
      "\t Grad:  tensor([ 0.0239, -0.1784])\n",
      "Epoch 125, Loss 0.287118\n",
      "\t Params:  tensor([0.8766, 0.3339])\n",
      "\t Grad:  tensor([ 0.0237, -0.1773])\n",
      "Epoch 126, Loss 0.286800\n",
      "\t Params:  tensor([0.8763, 0.3357])\n",
      "\t Grad:  tensor([ 0.0235, -0.1761])\n",
      "Epoch 127, Loss 0.286485\n",
      "\t Params:  tensor([0.8761, 0.3375])\n",
      "\t Grad:  tensor([ 0.0234, -0.1749])\n",
      "Epoch 128, Loss 0.286174\n",
      "\t Params:  tensor([0.8759, 0.3392])\n",
      "\t Grad:  tensor([ 0.0232, -0.1738])\n",
      "Epoch 129, Loss 0.285868\n",
      "\t Params:  tensor([0.8756, 0.3409])\n",
      "\t Grad:  tensor([ 0.0231, -0.1726])\n",
      "Epoch 130, Loss 0.285566\n",
      "\t Params:  tensor([0.8754, 0.3426])\n",
      "\t Grad:  tensor([ 0.0229, -0.1715])\n",
      "Epoch 131, Loss 0.285268\n",
      "\t Params:  tensor([0.8752, 0.3443])\n",
      "\t Grad:  tensor([ 0.0228, -0.1703])\n",
      "Epoch 132, Loss 0.284974\n",
      "\t Params:  tensor([0.8750, 0.3460])\n",
      "\t Grad:  tensor([ 0.0226, -0.1692])\n",
      "Epoch 133, Loss 0.284683\n",
      "\t Params:  tensor([0.8747, 0.3477])\n",
      "\t Grad:  tensor([ 0.0225, -0.1681])\n",
      "Epoch 134, Loss 0.284396\n",
      "\t Params:  tensor([0.8745, 0.3494])\n",
      "\t Grad:  tensor([ 0.0223, -0.1670])\n",
      "Epoch 135, Loss 0.284114\n",
      "\t Params:  tensor([0.8743, 0.3510])\n",
      "\t Grad:  tensor([ 0.0222, -0.1659])\n",
      "Epoch 136, Loss 0.283835\n",
      "\t Params:  tensor([0.8741, 0.3527])\n",
      "\t Grad:  tensor([ 0.0220, -0.1648])\n",
      "Epoch 137, Loss 0.283559\n",
      "\t Params:  tensor([0.8739, 0.3543])\n",
      "\t Grad:  tensor([ 0.0219, -0.1637])\n",
      "Epoch 138, Loss 0.283287\n",
      "\t Params:  tensor([0.8736, 0.3559])\n",
      "\t Grad:  tensor([ 0.0217, -0.1626])\n",
      "Epoch 139, Loss 0.283019\n",
      "\t Params:  tensor([0.8734, 0.3576])\n",
      "\t Grad:  tensor([ 0.0216, -0.1615])\n",
      "Epoch 140, Loss 0.282755\n",
      "\t Params:  tensor([0.8732, 0.3592])\n",
      "\t Grad:  tensor([ 0.0215, -0.1604])\n",
      "Epoch 141, Loss 0.282494\n",
      "\t Params:  tensor([0.8730, 0.3608])\n",
      "\t Grad:  tensor([ 0.0213, -0.1594])\n",
      "Epoch 142, Loss 0.282236\n",
      "\t Params:  tensor([0.8728, 0.3623])\n",
      "\t Grad:  tensor([ 0.0212, -0.1583])\n",
      "Epoch 143, Loss 0.281981\n",
      "\t Params:  tensor([0.8726, 0.3639])\n",
      "\t Grad:  tensor([ 0.0210, -0.1573])\n",
      "Epoch 144, Loss 0.281731\n",
      "\t Params:  tensor([0.8724, 0.3655])\n",
      "\t Grad:  tensor([ 0.0209, -0.1562])\n",
      "Epoch 145, Loss 0.281483\n",
      "\t Params:  tensor([0.8722, 0.3670])\n",
      "\t Grad:  tensor([ 0.0207, -0.1552])\n",
      "Epoch 146, Loss 0.281239\n",
      "\t Params:  tensor([0.8720, 0.3686])\n",
      "\t Grad:  tensor([ 0.0206, -0.1542])\n",
      "Epoch 147, Loss 0.280998\n",
      "\t Params:  tensor([0.8717, 0.3701])\n",
      "\t Grad:  tensor([ 0.0205, -0.1531])\n",
      "Epoch 148, Loss 0.280760\n",
      "\t Params:  tensor([0.8715, 0.3716])\n",
      "\t Grad:  tensor([ 0.0203, -0.1521])\n",
      "Epoch 149, Loss 0.280525\n",
      "\t Params:  tensor([0.8713, 0.3731])\n",
      "\t Grad:  tensor([ 0.0202, -0.1511])\n",
      "Epoch 150, Loss 0.280293\n",
      "\t Params:  tensor([0.8711, 0.3746])\n",
      "\t Grad:  tensor([ 0.0201, -0.1501])\n",
      "Epoch 151, Loss 0.280065\n",
      "\t Params:  tensor([0.8709, 0.3761])\n",
      "\t Grad:  tensor([ 0.0199, -0.1491])\n",
      "Epoch 152, Loss 0.279839\n",
      "\t Params:  tensor([0.8707, 0.3776])\n",
      "\t Grad:  tensor([ 0.0198, -0.1481])\n",
      "Epoch 153, Loss 0.279616\n",
      "\t Params:  tensor([0.8705, 0.3791])\n",
      "\t Grad:  tensor([ 0.0197, -0.1472])\n",
      "Epoch 154, Loss 0.279397\n",
      "\t Params:  tensor([0.8704, 0.3805])\n",
      "\t Grad:  tensor([ 0.0195, -0.1462])\n",
      "Epoch 155, Loss 0.279180\n",
      "\t Params:  tensor([0.8702, 0.3820])\n",
      "\t Grad:  tensor([ 0.0194, -0.1452])\n",
      "Epoch 156, Loss 0.278966\n",
      "\t Params:  tensor([0.8700, 0.3834])\n",
      "\t Grad:  tensor([ 0.0193, -0.1442])\n",
      "Epoch 157, Loss 0.278755\n",
      "\t Params:  tensor([0.8698, 0.3849])\n",
      "\t Grad:  tensor([ 0.0192, -0.1433])\n",
      "Epoch 158, Loss 0.278547\n",
      "\t Params:  tensor([0.8696, 0.3863])\n",
      "\t Grad:  tensor([ 0.0190, -0.1423])\n",
      "Epoch 159, Loss 0.278341\n",
      "\t Params:  tensor([0.8694, 0.3877])\n",
      "\t Grad:  tensor([ 0.0189, -0.1414])\n",
      "Epoch 160, Loss 0.278138\n",
      "\t Params:  tensor([0.8692, 0.3891])\n",
      "\t Grad:  tensor([ 0.0188, -0.1405])\n",
      "Epoch 161, Loss 0.277938\n",
      "\t Params:  tensor([0.8690, 0.3905])\n",
      "\t Grad:  tensor([ 0.0187, -0.1395])\n",
      "Epoch 162, Loss 0.277741\n",
      "\t Params:  tensor([0.8688, 0.3919])\n",
      "\t Grad:  tensor([ 0.0185, -0.1386])\n",
      "Epoch 163, Loss 0.277546\n",
      "\t Params:  tensor([0.8686, 0.3933])\n",
      "\t Grad:  tensor([ 0.0184, -0.1377])\n",
      "Epoch 164, Loss 0.277353\n",
      "\t Params:  tensor([0.8685, 0.3946])\n",
      "\t Grad:  tensor([ 0.0183, -0.1368])\n",
      "Epoch 165, Loss 0.277164\n",
      "\t Params:  tensor([0.8683, 0.3960])\n",
      "\t Grad:  tensor([ 0.0182, -0.1359])\n",
      "Epoch 166, Loss 0.276976\n",
      "\t Params:  tensor([0.8681, 0.3973])\n",
      "\t Grad:  tensor([ 0.0180, -0.1350])\n",
      "Epoch 167, Loss 0.276791\n",
      "\t Params:  tensor([0.8679, 0.3987])\n",
      "\t Grad:  tensor([ 0.0179, -0.1341])\n",
      "Epoch 168, Loss 0.276609\n",
      "\t Params:  tensor([0.8677, 0.4000])\n",
      "\t Grad:  tensor([ 0.0178, -0.1332])\n",
      "Epoch 169, Loss 0.276429\n",
      "\t Params:  tensor([0.8676, 0.4013])\n",
      "\t Grad:  tensor([ 0.0177, -0.1323])\n",
      "Epoch 170, Loss 0.276251\n",
      "\t Params:  tensor([0.8674, 0.4027])\n",
      "\t Grad:  tensor([ 0.0176, -0.1314])\n",
      "Epoch 171, Loss 0.276076\n",
      "\t Params:  tensor([0.8672, 0.4040])\n",
      "\t Grad:  tensor([ 0.0175, -0.1306])\n",
      "Epoch 172, Loss 0.275903\n",
      "\t Params:  tensor([0.8670, 0.4053])\n",
      "\t Grad:  tensor([ 0.0173, -0.1297])\n",
      "Epoch 173, Loss 0.275733\n",
      "\t Params:  tensor([0.8669, 0.4065])\n",
      "\t Grad:  tensor([ 0.0172, -0.1288])\n",
      "Epoch 174, Loss 0.275564\n",
      "\t Params:  tensor([0.8667, 0.4078])\n",
      "\t Grad:  tensor([ 0.0171, -0.1280])\n",
      "Epoch 175, Loss 0.275398\n",
      "\t Params:  tensor([0.8665, 0.4091])\n",
      "\t Grad:  tensor([ 0.0170, -0.1271])\n",
      "Epoch 176, Loss 0.275234\n",
      "\t Params:  tensor([0.8664, 0.4104])\n",
      "\t Grad:  tensor([ 0.0169, -0.1263])\n",
      "Epoch 177, Loss 0.275072\n",
      "\t Params:  tensor([0.8662, 0.4116])\n",
      "\t Grad:  tensor([ 0.0168, -0.1255])\n",
      "Epoch 178, Loss 0.274913\n",
      "\t Params:  tensor([0.8660, 0.4129])\n",
      "\t Grad:  tensor([ 0.0167, -0.1246])\n",
      "Epoch 179, Loss 0.274755\n",
      "\t Params:  tensor([0.8659, 0.4141])\n",
      "\t Grad:  tensor([ 0.0166, -0.1238])\n",
      "Epoch 180, Loss 0.274600\n",
      "\t Params:  tensor([0.8657, 0.4153])\n",
      "\t Grad:  tensor([ 0.0164, -0.1230])\n",
      "Epoch 181, Loss 0.274446\n",
      "\t Params:  tensor([0.8655, 0.4165])\n",
      "\t Grad:  tensor([ 0.0163, -0.1222])\n",
      "Epoch 182, Loss 0.274295\n",
      "\t Params:  tensor([0.8654, 0.4178])\n",
      "\t Grad:  tensor([ 0.0162, -0.1214])\n",
      "Epoch 183, Loss 0.274145\n",
      "\t Params:  tensor([0.8652, 0.4190])\n",
      "\t Grad:  tensor([ 0.0161, -0.1205])\n",
      "Epoch 184, Loss 0.273998\n",
      "\t Params:  tensor([0.8651, 0.4202])\n",
      "\t Grad:  tensor([ 0.0160, -0.1197])\n",
      "Epoch 185, Loss 0.273853\n",
      "\t Params:  tensor([0.8649, 0.4214])\n",
      "\t Grad:  tensor([ 0.0159, -0.1190])\n",
      "Epoch 186, Loss 0.273709\n",
      "\t Params:  tensor([0.8647, 0.4225])\n",
      "\t Grad:  tensor([ 0.0158, -0.1182])\n",
      "Epoch 187, Loss 0.273567\n",
      "\t Params:  tensor([0.8646, 0.4237])\n",
      "\t Grad:  tensor([ 0.0157, -0.1174])\n",
      "Epoch 188, Loss 0.273427\n",
      "\t Params:  tensor([0.8644, 0.4249])\n",
      "\t Grad:  tensor([ 0.0156, -0.1166])\n",
      "Epoch 189, Loss 0.273290\n",
      "\t Params:  tensor([0.8643, 0.4260])\n",
      "\t Grad:  tensor([ 0.0155, -0.1158])\n",
      "Epoch 190, Loss 0.273153\n",
      "\t Params:  tensor([0.8641, 0.4272])\n",
      "\t Grad:  tensor([ 0.0154, -0.1151])\n",
      "Epoch 191, Loss 0.273019\n",
      "\t Params:  tensor([0.8640, 0.4283])\n",
      "\t Grad:  tensor([ 0.0153, -0.1143])\n",
      "Epoch 192, Loss 0.272886\n",
      "\t Params:  tensor([0.8638, 0.4295])\n",
      "\t Grad:  tensor([ 0.0152, -0.1135])\n",
      "Epoch 193, Loss 0.272756\n",
      "\t Params:  tensor([0.8637, 0.4306])\n",
      "\t Grad:  tensor([ 0.0151, -0.1128])\n",
      "Epoch 194, Loss 0.272627\n",
      "\t Params:  tensor([0.8635, 0.4317])\n",
      "\t Grad:  tensor([ 0.0150, -0.1120])\n",
      "Epoch 195, Loss 0.272499\n",
      "\t Params:  tensor([0.8634, 0.4328])\n",
      "\t Grad:  tensor([ 0.0149, -0.1113])\n",
      "Epoch 196, Loss 0.272374\n",
      "\t Params:  tensor([0.8632, 0.4339])\n",
      "\t Grad:  tensor([ 0.0148, -0.1106])\n",
      "Epoch 197, Loss 0.272250\n",
      "\t Params:  tensor([0.8631, 0.4350])\n",
      "\t Grad:  tensor([ 0.0147, -0.1098])\n",
      "Epoch 198, Loss 0.272127\n",
      "\t Params:  tensor([0.8629, 0.4361])\n",
      "\t Grad:  tensor([ 0.0146, -0.1091])\n",
      "Epoch 199, Loss 0.272006\n",
      "\t Params:  tensor([0.8628, 0.4372])\n",
      "\t Grad:  tensor([ 0.0145, -0.1084])\n",
      "Epoch 200, Loss 0.271887\n",
      "\t Params:  tensor([0.8626, 0.4383])\n",
      "\t Grad:  tensor([ 0.0144, -0.1077])\n",
      "Epoch 201, Loss 0.271770\n",
      "\t Params:  tensor([0.8625, 0.4394])\n",
      "\t Grad:  tensor([ 0.0143, -0.1070])\n",
      "Epoch 202, Loss 0.271654\n",
      "\t Params:  tensor([0.8623, 0.4404])\n",
      "\t Grad:  tensor([ 0.0142, -0.1062])\n",
      "Epoch 203, Loss 0.271539\n",
      "\t Params:  tensor([0.8622, 0.4415])\n",
      "\t Grad:  tensor([ 0.0141, -0.1055])\n",
      "Epoch 204, Loss 0.271426\n",
      "\t Params:  tensor([0.8621, 0.4425])\n",
      "\t Grad:  tensor([ 0.0140, -0.1048])\n",
      "Epoch 205, Loss 0.271315\n",
      "\t Params:  tensor([0.8619, 0.4436])\n",
      "\t Grad:  tensor([ 0.0139, -0.1041])\n",
      "Epoch 206, Loss 0.271205\n",
      "\t Params:  tensor([0.8618, 0.4446])\n",
      "\t Grad:  tensor([ 0.0138, -0.1035])\n",
      "Epoch 207, Loss 0.271096\n",
      "\t Params:  tensor([0.8616, 0.4456])\n",
      "\t Grad:  tensor([ 0.0137, -0.1028])\n",
      "Epoch 208, Loss 0.270989\n",
      "\t Params:  tensor([0.8615, 0.4466])\n",
      "\t Grad:  tensor([ 0.0137, -0.1021])\n",
      "Epoch 209, Loss 0.270883\n",
      "\t Params:  tensor([0.8614, 0.4477])\n",
      "\t Grad:  tensor([ 0.0136, -0.1014])\n",
      "Epoch 210, Loss 0.270779\n",
      "\t Params:  tensor([0.8612, 0.4487])\n",
      "\t Grad:  tensor([ 0.0135, -0.1007])\n",
      "Epoch 211, Loss 0.270676\n",
      "\t Params:  tensor([0.8611, 0.4497])\n",
      "\t Grad:  tensor([ 0.0134, -0.1001])\n",
      "Epoch 212, Loss 0.270574\n",
      "\t Params:  tensor([0.8610, 0.4507])\n",
      "\t Grad:  tensor([ 0.0133, -0.0994])\n",
      "Epoch 213, Loss 0.270474\n",
      "\t Params:  tensor([0.8608, 0.4516])\n",
      "\t Grad:  tensor([ 0.0132, -0.0988])\n",
      "Epoch 214, Loss 0.270375\n",
      "\t Params:  tensor([0.8607, 0.4526])\n",
      "\t Grad:  tensor([ 0.0131, -0.0981])\n",
      "Epoch 215, Loss 0.270277\n",
      "\t Params:  tensor([0.8606, 0.4536])\n",
      "\t Grad:  tensor([ 0.0130, -0.0974])\n",
      "Epoch 216, Loss 0.270181\n",
      "\t Params:  tensor([0.8605, 0.4546])\n",
      "\t Grad:  tensor([ 0.0129, -0.0968])\n",
      "Epoch 217, Loss 0.270086\n",
      "\t Params:  tensor([0.8603, 0.4555])\n",
      "\t Grad:  tensor([ 0.0129, -0.0962])\n",
      "Epoch 218, Loss 0.269992\n",
      "\t Params:  tensor([0.8602, 0.4565])\n",
      "\t Grad:  tensor([ 0.0128, -0.0955])\n",
      "Epoch 219, Loss 0.269899\n",
      "\t Params:  tensor([0.8601, 0.4574])\n",
      "\t Grad:  tensor([ 0.0127, -0.0949])\n",
      "Epoch 220, Loss 0.269808\n",
      "\t Params:  tensor([0.8599, 0.4584])\n",
      "\t Grad:  tensor([ 0.0126, -0.0943])\n",
      "Epoch 221, Loss 0.269718\n",
      "\t Params:  tensor([0.8598, 0.4593])\n",
      "\t Grad:  tensor([ 0.0125, -0.0936])\n",
      "Epoch 222, Loss 0.269629\n",
      "\t Params:  tensor([0.8597, 0.4602])\n",
      "\t Grad:  tensor([ 0.0124, -0.0930])\n",
      "Epoch 223, Loss 0.269541\n",
      "\t Params:  tensor([0.8596, 0.4612])\n",
      "\t Grad:  tensor([ 0.0124, -0.0924])\n",
      "Epoch 224, Loss 0.269455\n",
      "\t Params:  tensor([0.8594, 0.4621])\n",
      "\t Grad:  tensor([ 0.0123, -0.0918])\n",
      "Epoch 225, Loss 0.269369\n",
      "\t Params:  tensor([0.8593, 0.4630])\n",
      "\t Grad:  tensor([ 0.0122, -0.0912])\n",
      "Epoch 226, Loss 0.269285\n",
      "\t Params:  tensor([0.8592, 0.4639])\n",
      "\t Grad:  tensor([ 0.0121, -0.0906])\n",
      "Epoch 227, Loss 0.269202\n",
      "\t Params:  tensor([0.8591, 0.4648])\n",
      "\t Grad:  tensor([ 0.0120, -0.0900])\n",
      "Epoch 228, Loss 0.269119\n",
      "\t Params:  tensor([0.8590, 0.4657])\n",
      "\t Grad:  tensor([ 0.0119, -0.0894])\n",
      "Epoch 229, Loss 0.269038\n",
      "\t Params:  tensor([0.8588, 0.4666])\n",
      "\t Grad:  tensor([ 0.0119, -0.0888])\n",
      "Epoch 230, Loss 0.268958\n",
      "\t Params:  tensor([0.8587, 0.4675])\n",
      "\t Grad:  tensor([ 0.0118, -0.0882])\n",
      "Epoch 231, Loss 0.268879\n",
      "\t Params:  tensor([0.8586, 0.4683])\n",
      "\t Grad:  tensor([ 0.0117, -0.0876])\n",
      "Epoch 232, Loss 0.268802\n",
      "\t Params:  tensor([0.8585, 0.4692])\n",
      "\t Grad:  tensor([ 0.0116, -0.0870])\n",
      "Epoch 233, Loss 0.268725\n",
      "\t Params:  tensor([0.8584, 0.4701])\n",
      "\t Grad:  tensor([ 0.0116, -0.0865])\n",
      "Epoch 234, Loss 0.268649\n",
      "\t Params:  tensor([0.8583, 0.4709])\n",
      "\t Grad:  tensor([ 0.0115, -0.0859])\n",
      "Epoch 235, Loss 0.268574\n",
      "\t Params:  tensor([0.8582, 0.4718])\n",
      "\t Grad:  tensor([ 0.0114, -0.0853])\n",
      "Epoch 236, Loss 0.268500\n",
      "\t Params:  tensor([0.8580, 0.4726])\n",
      "\t Grad:  tensor([ 0.0113, -0.0848])\n",
      "Epoch 237, Loss 0.268427\n",
      "\t Params:  tensor([0.8579, 0.4735])\n",
      "\t Grad:  tensor([ 0.0113, -0.0842])\n",
      "Epoch 238, Loss 0.268355\n",
      "\t Params:  tensor([0.8578, 0.4743])\n",
      "\t Grad:  tensor([ 0.0112, -0.0836])\n",
      "Epoch 239, Loss 0.268285\n",
      "\t Params:  tensor([0.8577, 0.4751])\n",
      "\t Grad:  tensor([ 0.0111, -0.0831])\n",
      "Epoch 240, Loss 0.268215\n",
      "\t Params:  tensor([0.8576, 0.4760])\n",
      "\t Grad:  tensor([ 0.0110, -0.0825])\n",
      "Epoch 241, Loss 0.268145\n",
      "\t Params:  tensor([0.8575, 0.4768])\n",
      "\t Grad:  tensor([ 0.0110, -0.0820])\n",
      "Epoch 242, Loss 0.268077\n",
      "\t Params:  tensor([0.8574, 0.4776])\n",
      "\t Grad:  tensor([ 0.0109, -0.0814])\n",
      "Epoch 243, Loss 0.268010\n",
      "\t Params:  tensor([0.8573, 0.4784])\n",
      "\t Grad:  tensor([ 0.0108, -0.0809])\n",
      "Epoch 244, Loss 0.267943\n",
      "\t Params:  tensor([0.8572, 0.4792])\n",
      "\t Grad:  tensor([ 0.0107, -0.0804])\n",
      "Epoch 245, Loss 0.267878\n",
      "\t Params:  tensor([0.8571, 0.4800])\n",
      "\t Grad:  tensor([ 0.0107, -0.0798])\n",
      "Epoch 246, Loss 0.267813\n",
      "\t Params:  tensor([0.8569, 0.4808])\n",
      "\t Grad:  tensor([ 0.0106, -0.0793])\n",
      "Epoch 247, Loss 0.267749\n",
      "\t Params:  tensor([0.8568, 0.4816])\n",
      "\t Grad:  tensor([ 0.0105, -0.0788])\n",
      "Epoch 248, Loss 0.267687\n",
      "\t Params:  tensor([0.8567, 0.4824])\n",
      "\t Grad:  tensor([ 0.0105, -0.0783])\n",
      "Epoch 249, Loss 0.267624\n",
      "\t Params:  tensor([0.8566, 0.4832])\n",
      "\t Grad:  tensor([ 0.0104, -0.0777])\n",
      "Epoch 250, Loss 0.267563\n",
      "\t Params:  tensor([0.8565, 0.4839])\n",
      "\t Grad:  tensor([ 0.0103, -0.0772])\n",
      "Epoch 251, Loss 0.267503\n",
      "\t Params:  tensor([0.8564, 0.4847])\n",
      "\t Grad:  tensor([ 0.0103, -0.0767])\n",
      "Epoch 252, Loss 0.267443\n",
      "\t Params:  tensor([0.8563, 0.4855])\n",
      "\t Grad:  tensor([ 0.0102, -0.0762])\n",
      "Epoch 253, Loss 0.267384\n",
      "\t Params:  tensor([0.8562, 0.4862])\n",
      "\t Grad:  tensor([ 0.0101, -0.0757])\n",
      "Epoch 254, Loss 0.267326\n",
      "\t Params:  tensor([0.8561, 0.4870])\n",
      "\t Grad:  tensor([ 0.0101, -0.0752])\n",
      "Epoch 255, Loss 0.267269\n",
      "\t Params:  tensor([0.8560, 0.4877])\n",
      "\t Grad:  tensor([ 0.0100, -0.0747])\n",
      "Epoch 256, Loss 0.267212\n",
      "\t Params:  tensor([0.8559, 0.4885])\n",
      "\t Grad:  tensor([ 0.0099, -0.0742])\n",
      "Epoch 257, Loss 0.267156\n",
      "\t Params:  tensor([0.8558, 0.4892])\n",
      "\t Grad:  tensor([ 0.0099, -0.0737])\n",
      "Epoch 258, Loss 0.267101\n",
      "\t Params:  tensor([0.8557, 0.4899])\n",
      "\t Grad:  tensor([ 0.0098, -0.0732])\n",
      "Epoch 259, Loss 0.267047\n",
      "\t Params:  tensor([0.8556, 0.4907])\n",
      "\t Grad:  tensor([ 0.0097, -0.0727])\n",
      "Epoch 260, Loss 0.266993\n",
      "\t Params:  tensor([0.8555, 0.4914])\n",
      "\t Grad:  tensor([ 0.0097, -0.0723])\n",
      "Epoch 261, Loss 0.266940\n",
      "\t Params:  tensor([0.8554, 0.4921])\n",
      "\t Grad:  tensor([ 0.0096, -0.0718])\n",
      "Epoch 262, Loss 0.266888\n",
      "\t Params:  tensor([0.8553, 0.4928])\n",
      "\t Grad:  tensor([ 0.0095, -0.0713])\n",
      "Epoch 263, Loss 0.266836\n",
      "\t Params:  tensor([0.8552, 0.4935])\n",
      "\t Grad:  tensor([ 0.0095, -0.0708])\n",
      "Epoch 264, Loss 0.266785\n",
      "\t Params:  tensor([0.8552, 0.4942])\n",
      "\t Grad:  tensor([ 0.0094, -0.0704])\n",
      "Epoch 265, Loss 0.266735\n",
      "\t Params:  tensor([0.8551, 0.4949])\n",
      "\t Grad:  tensor([ 0.0093, -0.0699])\n",
      "Epoch 266, Loss 0.266685\n",
      "\t Params:  tensor([0.8550, 0.4956])\n",
      "\t Grad:  tensor([ 0.0093, -0.0694])\n",
      "Epoch 267, Loss 0.266636\n",
      "\t Params:  tensor([0.8549, 0.4963])\n",
      "\t Grad:  tensor([ 0.0092, -0.0690])\n",
      "Epoch 268, Loss 0.266588\n",
      "\t Params:  tensor([0.8548, 0.4970])\n",
      "\t Grad:  tensor([ 0.0092, -0.0685])\n",
      "Epoch 269, Loss 0.266541\n",
      "\t Params:  tensor([0.8547, 0.4977])\n",
      "\t Grad:  tensor([ 0.0091, -0.0681])\n",
      "Epoch 270, Loss 0.266494\n",
      "\t Params:  tensor([0.8546, 0.4983])\n",
      "\t Grad:  tensor([ 0.0090, -0.0676])\n",
      "Epoch 271, Loss 0.266447\n",
      "\t Params:  tensor([0.8545, 0.4990])\n",
      "\t Grad:  tensor([ 0.0090, -0.0672])\n",
      "Epoch 272, Loss 0.266401\n",
      "\t Params:  tensor([0.8544, 0.4997])\n",
      "\t Grad:  tensor([ 0.0089, -0.0667])\n",
      "Epoch 273, Loss 0.266356\n",
      "\t Params:  tensor([0.8543, 0.5003])\n",
      "\t Grad:  tensor([ 0.0089, -0.0663])\n",
      "Epoch 274, Loss 0.266312\n",
      "\t Params:  tensor([0.8542, 0.5010])\n",
      "\t Grad:  tensor([ 0.0088, -0.0658])\n",
      "Epoch 275, Loss 0.266268\n",
      "\t Params:  tensor([0.8542, 0.5017])\n",
      "\t Grad:  tensor([ 0.0087, -0.0654])\n",
      "Epoch 276, Loss 0.266224\n",
      "\t Params:  tensor([0.8541, 0.5023])\n",
      "\t Grad:  tensor([ 0.0087, -0.0650])\n",
      "Epoch 277, Loss 0.266182\n",
      "\t Params:  tensor([0.8540, 0.5030])\n",
      "\t Grad:  tensor([ 0.0086, -0.0645])\n",
      "Epoch 278, Loss 0.266139\n",
      "\t Params:  tensor([0.8539, 0.5036])\n",
      "\t Grad:  tensor([ 0.0086, -0.0641])\n",
      "Epoch 279, Loss 0.266098\n",
      "\t Params:  tensor([0.8538, 0.5042])\n",
      "\t Grad:  tensor([ 0.0085, -0.0637])\n",
      "Epoch 280, Loss 0.266056\n",
      "\t Params:  tensor([0.8537, 0.5049])\n",
      "\t Grad:  tensor([ 0.0085, -0.0633])\n",
      "Epoch 281, Loss 0.266016\n",
      "\t Params:  tensor([0.8536, 0.5055])\n",
      "\t Grad:  tensor([ 0.0084, -0.0628])\n",
      "Epoch 282, Loss 0.265976\n",
      "\t Params:  tensor([0.8536, 0.5061])\n",
      "\t Grad:  tensor([ 0.0083, -0.0624])\n",
      "Epoch 283, Loss 0.265936\n",
      "\t Params:  tensor([0.8535, 0.5067])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Grad:  tensor([ 0.0083, -0.0620])\n",
      "Epoch 284, Loss 0.265897\n",
      "\t Params:  tensor([0.8534, 0.5074])\n",
      "\t Grad:  tensor([ 0.0082, -0.0616])\n",
      "Epoch 285, Loss 0.265859\n",
      "\t Params:  tensor([0.8533, 0.5080])\n",
      "\t Grad:  tensor([ 0.0082, -0.0612])\n",
      "Epoch 286, Loss 0.265821\n",
      "\t Params:  tensor([0.8532, 0.5086])\n",
      "\t Grad:  tensor([ 0.0081, -0.0608])\n",
      "Epoch 287, Loss 0.265783\n",
      "\t Params:  tensor([0.8532, 0.5092])\n",
      "\t Grad:  tensor([ 0.0081, -0.0604])\n",
      "Epoch 288, Loss 0.265746\n",
      "\t Params:  tensor([0.8531, 0.5098])\n",
      "\t Grad:  tensor([ 0.0080, -0.0600])\n",
      "Epoch 289, Loss 0.265710\n",
      "\t Params:  tensor([0.8530, 0.5104])\n",
      "\t Grad:  tensor([ 0.0080, -0.0596])\n",
      "Epoch 290, Loss 0.265674\n",
      "\t Params:  tensor([0.8529, 0.5110])\n",
      "\t Grad:  tensor([ 0.0079, -0.0592])\n",
      "Epoch 291, Loss 0.265638\n",
      "\t Params:  tensor([0.8528, 0.5116])\n",
      "\t Grad:  tensor([ 0.0079, -0.0588])\n",
      "Epoch 292, Loss 0.265603\n",
      "\t Params:  tensor([0.8528, 0.5121])\n",
      "\t Grad:  tensor([ 0.0078, -0.0584])\n",
      "Epoch 293, Loss 0.265569\n",
      "\t Params:  tensor([0.8527, 0.5127])\n",
      "\t Grad:  tensor([ 0.0078, -0.0580])\n",
      "Epoch 294, Loss 0.265534\n",
      "\t Params:  tensor([0.8526, 0.5133])\n",
      "\t Grad:  tensor([ 0.0077, -0.0576])\n",
      "Epoch 295, Loss 0.265501\n",
      "\t Params:  tensor([0.8525, 0.5139])\n",
      "\t Grad:  tensor([ 0.0077, -0.0573])\n",
      "Epoch 296, Loss 0.265467\n",
      "\t Params:  tensor([0.8524, 0.5144])\n",
      "\t Grad:  tensor([ 0.0076, -0.0569])\n",
      "Epoch 297, Loss 0.265435\n",
      "\t Params:  tensor([0.8524, 0.5150])\n",
      "\t Grad:  tensor([ 0.0076, -0.0565])\n",
      "Epoch 298, Loss 0.265402\n",
      "\t Params:  tensor([0.8523, 0.5156])\n",
      "\t Grad:  tensor([ 0.0075, -0.0561])\n",
      "Epoch 299, Loss 0.265370\n",
      "\t Params:  tensor([0.8522, 0.5161])\n",
      "\t Grad:  tensor([ 0.0075, -0.0558])\n",
      "Epoch 300, Loss 0.265339\n",
      "\t Params:  tensor([0.8521, 0.5167])\n",
      "\t Grad:  tensor([ 0.0074, -0.0554])\n",
      "Epoch 301, Loss 0.265308\n",
      "\t Params:  tensor([0.8521, 0.5172])\n",
      "\t Grad:  tensor([ 0.0074, -0.0550])\n",
      "Epoch 302, Loss 0.265277\n",
      "\t Params:  tensor([0.8520, 0.5178])\n",
      "\t Grad:  tensor([ 0.0073, -0.0547])\n",
      "Epoch 303, Loss 0.265247\n",
      "\t Params:  tensor([0.8519, 0.5183])\n",
      "\t Grad:  tensor([ 0.0073, -0.0543])\n",
      "Epoch 304, Loss 0.265217\n",
      "\t Params:  tensor([0.8519, 0.5188])\n",
      "\t Grad:  tensor([ 0.0072, -0.0539])\n",
      "Epoch 305, Loss 0.265187\n",
      "\t Params:  tensor([0.8518, 0.5194])\n",
      "\t Grad:  tensor([ 0.0072, -0.0536])\n",
      "Epoch 306, Loss 0.265158\n",
      "\t Params:  tensor([0.8517, 0.5199])\n",
      "\t Grad:  tensor([ 0.0071, -0.0532])\n",
      "Epoch 307, Loss 0.265129\n",
      "\t Params:  tensor([0.8516, 0.5204])\n",
      "\t Grad:  tensor([ 0.0071, -0.0529])\n",
      "Epoch 308, Loss 0.265101\n",
      "\t Params:  tensor([0.8516, 0.5210])\n",
      "\t Grad:  tensor([ 0.0070, -0.0525])\n",
      "Epoch 309, Loss 0.265073\n",
      "\t Params:  tensor([0.8515, 0.5215])\n",
      "\t Grad:  tensor([ 0.0070, -0.0522])\n",
      "Epoch 310, Loss 0.265045\n",
      "\t Params:  tensor([0.8514, 0.5220])\n",
      "\t Grad:  tensor([ 0.0069, -0.0518])\n",
      "Epoch 311, Loss 0.265018\n",
      "\t Params:  tensor([0.8514, 0.5225])\n",
      "\t Grad:  tensor([ 0.0069, -0.0515])\n",
      "Epoch 312, Loss 0.264991\n",
      "\t Params:  tensor([0.8513, 0.5230])\n",
      "\t Grad:  tensor([ 0.0068, -0.0511])\n",
      "Epoch 313, Loss 0.264965\n",
      "\t Params:  tensor([0.8512, 0.5235])\n",
      "\t Grad:  tensor([ 0.0068, -0.0508])\n",
      "Epoch 314, Loss 0.264939\n",
      "\t Params:  tensor([0.8512, 0.5240])\n",
      "\t Grad:  tensor([ 0.0067, -0.0505])\n",
      "Epoch 315, Loss 0.264913\n",
      "\t Params:  tensor([0.8511, 0.5246])\n",
      "\t Grad:  tensor([ 0.0067, -0.0501])\n",
      "Epoch 316, Loss 0.264887\n",
      "\t Params:  tensor([0.8510, 0.5250])\n",
      "\t Grad:  tensor([ 0.0067, -0.0498])\n",
      "Epoch 317, Loss 0.264862\n",
      "\t Params:  tensor([0.8510, 0.5255])\n",
      "\t Grad:  tensor([ 0.0066, -0.0495])\n",
      "Epoch 318, Loss 0.264837\n",
      "\t Params:  tensor([0.8509, 0.5260])\n",
      "\t Grad:  tensor([ 0.0066, -0.0491])\n",
      "Epoch 319, Loss 0.264813\n",
      "\t Params:  tensor([0.8508, 0.5265])\n",
      "\t Grad:  tensor([ 0.0065, -0.0488])\n",
      "Epoch 320, Loss 0.264789\n",
      "\t Params:  tensor([0.8508, 0.5270])\n",
      "\t Grad:  tensor([ 0.0065, -0.0485])\n",
      "Epoch 321, Loss 0.264765\n",
      "\t Params:  tensor([0.8507, 0.5275])\n",
      "\t Grad:  tensor([ 0.0064, -0.0482])\n",
      "Epoch 322, Loss 0.264741\n",
      "\t Params:  tensor([0.8506, 0.5280])\n",
      "\t Grad:  tensor([ 0.0064, -0.0478])\n",
      "Epoch 323, Loss 0.264718\n",
      "\t Params:  tensor([0.8506, 0.5284])\n",
      "\t Grad:  tensor([ 0.0064, -0.0475])\n",
      "Epoch 324, Loss 0.264695\n",
      "\t Params:  tensor([0.8505, 0.5289])\n",
      "\t Grad:  tensor([ 0.0063, -0.0472])\n",
      "Epoch 325, Loss 0.264672\n",
      "\t Params:  tensor([0.8504, 0.5294])\n",
      "\t Grad:  tensor([ 0.0063, -0.0469])\n",
      "Epoch 326, Loss 0.264650\n",
      "\t Params:  tensor([0.8504, 0.5299])\n",
      "\t Grad:  tensor([ 0.0062, -0.0466])\n",
      "Epoch 327, Loss 0.264628\n",
      "\t Params:  tensor([0.8503, 0.5303])\n",
      "\t Grad:  tensor([ 0.0062, -0.0463])\n",
      "Epoch 328, Loss 0.264606\n",
      "\t Params:  tensor([0.8503, 0.5308])\n",
      "\t Grad:  tensor([ 0.0061, -0.0460])\n",
      "Epoch 329, Loss 0.264585\n",
      "\t Params:  tensor([0.8502, 0.5312])\n",
      "\t Grad:  tensor([ 0.0061, -0.0457])\n",
      "Epoch 330, Loss 0.264564\n",
      "\t Params:  tensor([0.8501, 0.5317])\n",
      "\t Grad:  tensor([ 0.0061, -0.0454])\n",
      "Epoch 331, Loss 0.264543\n",
      "\t Params:  tensor([0.8501, 0.5321])\n",
      "\t Grad:  tensor([ 0.0060, -0.0451])\n",
      "Epoch 332, Loss 0.264522\n",
      "\t Params:  tensor([0.8500, 0.5326])\n",
      "\t Grad:  tensor([ 0.0060, -0.0448])\n",
      "Epoch 333, Loss 0.264502\n",
      "\t Params:  tensor([0.8500, 0.5330])\n",
      "\t Grad:  tensor([ 0.0059, -0.0445])\n",
      "Epoch 334, Loss 0.264482\n",
      "\t Params:  tensor([0.8499, 0.5335])\n",
      "\t Grad:  tensor([ 0.0059, -0.0442])\n",
      "Epoch 335, Loss 0.264462\n",
      "\t Params:  tensor([0.8498, 0.5339])\n",
      "\t Grad:  tensor([ 0.0059, -0.0439])\n",
      "Epoch 336, Loss 0.264443\n",
      "\t Params:  tensor([0.8498, 0.5343])\n",
      "\t Grad:  tensor([ 0.0058, -0.0436])\n",
      "Epoch 337, Loss 0.264423\n",
      "\t Params:  tensor([0.8497, 0.5348])\n",
      "\t Grad:  tensor([ 0.0058, -0.0433])\n",
      "Epoch 338, Loss 0.264404\n",
      "\t Params:  tensor([0.8497, 0.5352])\n",
      "\t Grad:  tensor([ 0.0058, -0.0430])\n",
      "Epoch 339, Loss 0.264385\n",
      "\t Params:  tensor([0.8496, 0.5356])\n",
      "\t Grad:  tensor([ 0.0057, -0.0427])\n",
      "Epoch 340, Loss 0.264367\n",
      "\t Params:  tensor([0.8496, 0.5361])\n",
      "\t Grad:  tensor([ 0.0057, -0.0425])\n",
      "Epoch 341, Loss 0.264349\n",
      "\t Params:  tensor([0.8495, 0.5365])\n",
      "\t Grad:  tensor([ 0.0056, -0.0422])\n",
      "Epoch 342, Loss 0.264331\n",
      "\t Params:  tensor([0.8494, 0.5369])\n",
      "\t Grad:  tensor([ 0.0056, -0.0419])\n",
      "Epoch 343, Loss 0.264313\n",
      "\t Params:  tensor([0.8494, 0.5373])\n",
      "\t Grad:  tensor([ 0.0056, -0.0416])\n",
      "Epoch 344, Loss 0.264295\n",
      "\t Params:  tensor([0.8493, 0.5377])\n",
      "\t Grad:  tensor([ 0.0055, -0.0413])\n",
      "Epoch 345, Loss 0.264278\n",
      "\t Params:  tensor([0.8493, 0.5381])\n",
      "\t Grad:  tensor([ 0.0055, -0.0411])\n",
      "Epoch 346, Loss 0.264261\n",
      "\t Params:  tensor([0.8492, 0.5385])\n",
      "\t Grad:  tensor([ 0.0055, -0.0408])\n",
      "Epoch 347, Loss 0.264244\n",
      "\t Params:  tensor([0.8492, 0.5390])\n",
      "\t Grad:  tensor([ 0.0054, -0.0405])\n",
      "Epoch 348, Loss 0.264227\n",
      "\t Params:  tensor([0.8491, 0.5394])\n",
      "\t Grad:  tensor([ 0.0054, -0.0403])\n",
      "Epoch 349, Loss 0.264211\n",
      "\t Params:  tensor([0.8491, 0.5398])\n",
      "\t Grad:  tensor([ 0.0053, -0.0400])\n",
      "Epoch 350, Loss 0.264195\n",
      "\t Params:  tensor([0.8490, 0.5402])\n",
      "\t Grad:  tensor([ 0.0053, -0.0397])\n",
      "Epoch 351, Loss 0.264179\n",
      "\t Params:  tensor([0.8490, 0.5405])\n",
      "\t Grad:  tensor([ 0.0053, -0.0395])\n",
      "Epoch 352, Loss 0.264163\n",
      "\t Params:  tensor([0.8489, 0.5409])\n",
      "\t Grad:  tensor([ 0.0052, -0.0392])\n",
      "Epoch 353, Loss 0.264147\n",
      "\t Params:  tensor([0.8489, 0.5413])\n",
      "\t Grad:  tensor([ 0.0052, -0.0389])\n",
      "Epoch 354, Loss 0.264132\n",
      "\t Params:  tensor([0.8488, 0.5417])\n",
      "\t Grad:  tensor([ 0.0052, -0.0387])\n",
      "Epoch 355, Loss 0.264117\n",
      "\t Params:  tensor([0.8487, 0.5421])\n",
      "\t Grad:  tensor([ 0.0051, -0.0384])\n",
      "Epoch 356, Loss 0.264102\n",
      "\t Params:  tensor([0.8487, 0.5425])\n",
      "\t Grad:  tensor([ 0.0051, -0.0382])\n",
      "Epoch 357, Loss 0.264087\n",
      "\t Params:  tensor([0.8486, 0.5429])\n",
      "\t Grad:  tensor([ 0.0051, -0.0379])\n",
      "Epoch 358, Loss 0.264072\n",
      "\t Params:  tensor([0.8486, 0.5432])\n",
      "\t Grad:  tensor([ 0.0050, -0.0377])\n",
      "Epoch 359, Loss 0.264058\n",
      "\t Params:  tensor([0.8485, 0.5436])\n",
      "\t Grad:  tensor([ 0.0050, -0.0374])\n",
      "Epoch 360, Loss 0.264044\n",
      "\t Params:  tensor([0.8485, 0.5440])\n",
      "\t Grad:  tensor([ 0.0050, -0.0372])\n",
      "Epoch 361, Loss 0.264030\n",
      "\t Params:  tensor([0.8484, 0.5444])\n",
      "\t Grad:  tensor([ 0.0049, -0.0369])\n",
      "Epoch 362, Loss 0.264016\n",
      "\t Params:  tensor([0.8484, 0.5447])\n",
      "\t Grad:  tensor([ 0.0049, -0.0367])\n",
      "Epoch 363, Loss 0.264002\n",
      "\t Params:  tensor([0.8483, 0.5451])\n",
      "\t Grad:  tensor([ 0.0049, -0.0364])\n",
      "Epoch 364, Loss 0.263989\n",
      "\t Params:  tensor([0.8483, 0.5454])\n",
      "\t Grad:  tensor([ 0.0048, -0.0362])\n",
      "Epoch 365, Loss 0.263975\n",
      "\t Params:  tensor([0.8483, 0.5458])\n",
      "\t Grad:  tensor([ 0.0048, -0.0360])\n",
      "Epoch 366, Loss 0.263962\n",
      "\t Params:  tensor([0.8482, 0.5462])\n",
      "\t Grad:  tensor([ 0.0048, -0.0357])\n",
      "Epoch 367, Loss 0.263949\n",
      "\t Params:  tensor([0.8482, 0.5465])\n",
      "\t Grad:  tensor([ 0.0047, -0.0355])\n",
      "Epoch 368, Loss 0.263937\n",
      "\t Params:  tensor([0.8481, 0.5469])\n",
      "\t Grad:  tensor([ 0.0047, -0.0352])\n",
      "Epoch 369, Loss 0.263924\n",
      "\t Params:  tensor([0.8481, 0.5472])\n",
      "\t Grad:  tensor([ 0.0047, -0.0350])\n",
      "Epoch 370, Loss 0.263912\n",
      "\t Params:  tensor([0.8480, 0.5476])\n",
      "\t Grad:  tensor([ 0.0046, -0.0348])\n",
      "Epoch 371, Loss 0.263899\n",
      "\t Params:  tensor([0.8480, 0.5479])\n",
      "\t Grad:  tensor([ 0.0046, -0.0345])\n",
      "Epoch 372, Loss 0.263887\n",
      "\t Params:  tensor([0.8479, 0.5483])\n",
      "\t Grad:  tensor([ 0.0046, -0.0343])\n",
      "Epoch 373, Loss 0.263875\n",
      "\t Params:  tensor([0.8479, 0.5486])\n",
      "\t Grad:  tensor([ 0.0046, -0.0341])\n",
      "Epoch 374, Loss 0.263863\n",
      "\t Params:  tensor([0.8478, 0.5489])\n",
      "\t Grad:  tensor([ 0.0045, -0.0339])\n",
      "Epoch 375, Loss 0.263852\n",
      "\t Params:  tensor([0.8478, 0.5493])\n",
      "\t Grad:  tensor([ 0.0045, -0.0336])\n",
      "Epoch 376, Loss 0.263840\n",
      "\t Params:  tensor([0.8477, 0.5496])\n",
      "\t Grad:  tensor([ 0.0045, -0.0334])\n",
      "Epoch 377, Loss 0.263829\n",
      "\t Params:  tensor([0.8477, 0.5499])\n",
      "\t Grad:  tensor([ 0.0044, -0.0332])\n",
      "Epoch 378, Loss 0.263818\n",
      "\t Params:  tensor([0.8477, 0.5503])\n",
      "\t Grad:  tensor([ 0.0044, -0.0330])\n",
      "Epoch 379, Loss 0.263807\n",
      "\t Params:  tensor([0.8476, 0.5506])\n",
      "\t Grad:  tensor([ 0.0044, -0.0328])\n",
      "Epoch 380, Loss 0.263796\n",
      "\t Params:  tensor([0.8476, 0.5509])\n",
      "\t Grad:  tensor([ 0.0044, -0.0325])\n",
      "Epoch 381, Loss 0.263785\n",
      "\t Params:  tensor([0.8475, 0.5512])\n",
      "\t Grad:  tensor([ 0.0043, -0.0323])\n",
      "Epoch 382, Loss 0.263775\n",
      "\t Params:  tensor([0.8475, 0.5516])\n",
      "\t Grad:  tensor([ 0.0043, -0.0321])\n",
      "Epoch 383, Loss 0.263764\n",
      "\t Params:  tensor([0.8474, 0.5519])\n",
      "\t Grad:  tensor([ 0.0043, -0.0319])\n",
      "Epoch 384, Loss 0.263754\n",
      "\t Params:  tensor([0.8474, 0.5522])\n",
      "\t Grad:  tensor([ 0.0042, -0.0317])\n",
      "Epoch 385, Loss 0.263744\n",
      "\t Params:  tensor([0.8474, 0.5525])\n",
      "\t Grad:  tensor([ 0.0042, -0.0315])\n",
      "Epoch 386, Loss 0.263733\n",
      "\t Params:  tensor([0.8473, 0.5528])\n",
      "\t Grad:  tensor([ 0.0042, -0.0313])\n",
      "Epoch 387, Loss 0.263724\n",
      "\t Params:  tensor([0.8473, 0.5531])\n",
      "\t Grad:  tensor([ 0.0042, -0.0311])\n",
      "Epoch 388, Loss 0.263714\n",
      "\t Params:  tensor([0.8472, 0.5534])\n",
      "\t Grad:  tensor([ 0.0041, -0.0309])\n",
      "Epoch 389, Loss 0.263704\n",
      "\t Params:  tensor([0.8472, 0.5538])\n",
      "\t Grad:  tensor([ 0.0041, -0.0307])\n",
      "Epoch 390, Loss 0.263695\n",
      "\t Params:  tensor([0.8471, 0.5541])\n",
      "\t Grad:  tensor([ 0.0041, -0.0304])\n",
      "Epoch 391, Loss 0.263685\n",
      "\t Params:  tensor([0.8471, 0.5544])\n",
      "\t Grad:  tensor([ 0.0040, -0.0302])\n",
      "Epoch 392, Loss 0.263676\n",
      "\t Params:  tensor([0.8471, 0.5547])\n",
      "\t Grad:  tensor([ 0.0040, -0.0300])\n",
      "Epoch 393, Loss 0.263667\n",
      "\t Params:  tensor([0.8470, 0.5550])\n",
      "\t Grad:  tensor([ 0.0040, -0.0298])\n",
      "Epoch 394, Loss 0.263658\n",
      "\t Params:  tensor([0.8470, 0.5553])\n",
      "\t Grad:  tensor([ 0.0040, -0.0296])\n",
      "Epoch 395, Loss 0.263649\n",
      "\t Params:  tensor([0.8470, 0.5556])\n",
      "\t Grad:  tensor([ 0.0039, -0.0295])\n",
      "Epoch 396, Loss 0.263640\n",
      "\t Params:  tensor([0.8469, 0.5558])\n",
      "\t Grad:  tensor([ 0.0039, -0.0293])\n",
      "Epoch 397, Loss 0.263631\n",
      "\t Params:  tensor([0.8469, 0.5561])\n",
      "\t Grad:  tensor([ 0.0039, -0.0291])\n",
      "Epoch 398, Loss 0.263623\n",
      "\t Params:  tensor([0.8468, 0.5564])\n",
      "\t Grad:  tensor([ 0.0039, -0.0289])\n",
      "Epoch 399, Loss 0.263614\n",
      "\t Params:  tensor([0.8468, 0.5567])\n",
      "\t Grad:  tensor([ 0.0038, -0.0287])\n",
      "Epoch 400, Loss 0.263606\n",
      "\t Params:  tensor([0.8468, 0.5570])\n",
      "\t Grad:  tensor([ 0.0038, -0.0285])\n",
      "Epoch 401, Loss 0.263598\n",
      "\t Params:  tensor([0.8467, 0.5573])\n",
      "\t Grad:  tensor([ 0.0038, -0.0283])\n",
      "Epoch 402, Loss 0.263590\n",
      "\t Params:  tensor([0.8467, 0.5576])\n",
      "\t Grad:  tensor([ 0.0038, -0.0281])\n",
      "Epoch 403, Loss 0.263582\n",
      "\t Params:  tensor([0.8466, 0.5578])\n",
      "\t Grad:  tensor([ 0.0037, -0.0279])\n",
      "Epoch 404, Loss 0.263574\n",
      "\t Params:  tensor([0.8466, 0.5581])\n",
      "\t Grad:  tensor([ 0.0037, -0.0277])\n",
      "Epoch 405, Loss 0.263566\n",
      "\t Params:  tensor([0.8466, 0.5584])\n",
      "\t Grad:  tensor([ 0.0037, -0.0276])\n",
      "Epoch 406, Loss 0.263558\n",
      "\t Params:  tensor([0.8465, 0.5587])\n",
      "\t Grad:  tensor([ 0.0037, -0.0274])\n",
      "Epoch 407, Loss 0.263551\n",
      "\t Params:  tensor([0.8465, 0.5589])\n",
      "\t Grad:  tensor([ 0.0036, -0.0272])\n",
      "Epoch 408, Loss 0.263543\n",
      "\t Params:  tensor([0.8465, 0.5592])\n",
      "\t Grad:  tensor([ 0.0036, -0.0270])\n",
      "Epoch 409, Loss 0.263536\n",
      "\t Params:  tensor([0.8464, 0.5595])\n",
      "\t Grad:  tensor([ 0.0036, -0.0268])\n",
      "Epoch 410, Loss 0.263528\n",
      "\t Params:  tensor([0.8464, 0.5597])\n",
      "\t Grad:  tensor([ 0.0036, -0.0267])\n",
      "Epoch 411, Loss 0.263521\n",
      "\t Params:  tensor([0.8464, 0.5600])\n",
      "\t Grad:  tensor([ 0.0035, -0.0265])\n",
      "Epoch 412, Loss 0.263514\n",
      "\t Params:  tensor([0.8463, 0.5603])\n",
      "\t Grad:  tensor([ 0.0035, -0.0263])\n",
      "Epoch 413, Loss 0.263507\n",
      "\t Params:  tensor([0.8463, 0.5605])\n",
      "\t Grad:  tensor([ 0.0035, -0.0261])\n",
      "Epoch 414, Loss 0.263500\n",
      "\t Params:  tensor([0.8462, 0.5608])\n",
      "\t Grad:  tensor([ 0.0035, -0.0260])\n",
      "Epoch 415, Loss 0.263493\n",
      "\t Params:  tensor([0.8462, 0.5610])\n",
      "\t Grad:  tensor([ 0.0034, -0.0258])\n",
      "Epoch 416, Loss 0.263486\n",
      "\t Params:  tensor([0.8462, 0.5613])\n",
      "\t Grad:  tensor([ 0.0034, -0.0256])\n",
      "Epoch 417, Loss 0.263480\n",
      "\t Params:  tensor([0.8461, 0.5616])\n",
      "\t Grad:  tensor([ 0.0034, -0.0254])\n",
      "Epoch 418, Loss 0.263473\n",
      "\t Params:  tensor([0.8461, 0.5618])\n",
      "\t Grad:  tensor([ 0.0034, -0.0253])\n",
      "Epoch 419, Loss 0.263467\n",
      "\t Params:  tensor([0.8461, 0.5621])\n",
      "\t Grad:  tensor([ 0.0034, -0.0251])\n",
      "Epoch 420, Loss 0.263460\n",
      "\t Params:  tensor([0.8460, 0.5623])\n",
      "\t Grad:  tensor([ 0.0033, -0.0249])\n",
      "Epoch 421, Loss 0.263454\n",
      "\t Params:  tensor([0.8460, 0.5626])\n",
      "\t Grad:  tensor([ 0.0033, -0.0248])\n",
      "Epoch 422, Loss 0.263448\n",
      "\t Params:  tensor([0.8460, 0.5628])\n",
      "\t Grad:  tensor([ 0.0033, -0.0246])\n",
      "Epoch 423, Loss 0.263442\n",
      "\t Params:  tensor([0.8459, 0.5631])\n",
      "\t Grad:  tensor([ 0.0033, -0.0245])\n",
      "Epoch 424, Loss 0.263436\n",
      "\t Params:  tensor([0.8459, 0.5633])\n",
      "\t Grad:  tensor([ 0.0032, -0.0243])\n",
      "Epoch 425, Loss 0.263430\n",
      "\t Params:  tensor([0.8459, 0.5635])\n",
      "\t Grad:  tensor([ 0.0032, -0.0241])\n",
      "Epoch 426, Loss 0.263424\n",
      "\t Params:  tensor([0.8459, 0.5638])\n",
      "\t Grad:  tensor([ 0.0032, -0.0240])\n",
      "Epoch 427, Loss 0.263418\n",
      "\t Params:  tensor([0.8458, 0.5640])\n",
      "\t Grad:  tensor([ 0.0032, -0.0238])\n",
      "Epoch 428, Loss 0.263412\n",
      "\t Params:  tensor([0.8458, 0.5642])\n",
      "\t Grad:  tensor([ 0.0032, -0.0237])\n",
      "Epoch 429, Loss 0.263406\n",
      "\t Params:  tensor([0.8458, 0.5645])\n",
      "\t Grad:  tensor([ 0.0031, -0.0235])\n",
      "Epoch 430, Loss 0.263401\n",
      "\t Params:  tensor([0.8457, 0.5647])\n",
      "\t Grad:  tensor([ 0.0031, -0.0233])\n",
      "Epoch 431, Loss 0.263395\n",
      "\t Params:  tensor([0.8457, 0.5650])\n",
      "\t Grad:  tensor([ 0.0031, -0.0232])\n",
      "Epoch 432, Loss 0.263390\n",
      "\t Params:  tensor([0.8457, 0.5652])\n",
      "\t Grad:  tensor([ 0.0031, -0.0230])\n",
      "Epoch 433, Loss 0.263384\n",
      "\t Params:  tensor([0.8456, 0.5654])\n",
      "\t Grad:  tensor([ 0.0031, -0.0229])\n",
      "Epoch 434, Loss 0.263379\n",
      "\t Params:  tensor([0.8456, 0.5656])\n",
      "\t Grad:  tensor([ 0.0030, -0.0227])\n",
      "Epoch 435, Loss 0.263374\n",
      "\t Params:  tensor([0.8456, 0.5659])\n",
      "\t Grad:  tensor([ 0.0030, -0.0226])\n",
      "Epoch 436, Loss 0.263369\n",
      "\t Params:  tensor([0.8455, 0.5661])\n",
      "\t Grad:  tensor([ 0.0030, -0.0224])\n",
      "Epoch 437, Loss 0.263364\n",
      "\t Params:  tensor([0.8455, 0.5663])\n",
      "\t Grad:  tensor([ 0.0030, -0.0223])\n",
      "Epoch 438, Loss 0.263359\n",
      "\t Params:  tensor([0.8455, 0.5665])\n",
      "\t Grad:  tensor([ 0.0030, -0.0221])\n",
      "Epoch 439, Loss 0.263354\n",
      "\t Params:  tensor([0.8455, 0.5668])\n",
      "\t Grad:  tensor([ 0.0029, -0.0220])\n",
      "Epoch 440, Loss 0.263349\n",
      "\t Params:  tensor([0.8454, 0.5670])\n",
      "\t Grad:  tensor([ 0.0029, -0.0218])\n",
      "Epoch 441, Loss 0.263344\n",
      "\t Params:  tensor([0.8454, 0.5672])\n",
      "\t Grad:  tensor([ 0.0029, -0.0217])\n",
      "Epoch 442, Loss 0.263339\n",
      "\t Params:  tensor([0.8454, 0.5674])\n",
      "\t Grad:  tensor([ 0.0029, -0.0215])\n",
      "Epoch 443, Loss 0.263334\n",
      "\t Params:  tensor([0.8453, 0.5676])\n",
      "\t Grad:  tensor([ 0.0029, -0.0214])\n",
      "Epoch 444, Loss 0.263330\n",
      "\t Params:  tensor([0.8453, 0.5678])\n",
      "\t Grad:  tensor([ 0.0028, -0.0213])\n",
      "Epoch 445, Loss 0.263325\n",
      "\t Params:  tensor([0.8453, 0.5680])\n",
      "\t Grad:  tensor([ 0.0028, -0.0211])\n",
      "Epoch 446, Loss 0.263321\n",
      "\t Params:  tensor([0.8453, 0.5682])\n",
      "\t Grad:  tensor([ 0.0028, -0.0210])\n",
      "Epoch 447, Loss 0.263316\n",
      "\t Params:  tensor([0.8452, 0.5685])\n",
      "\t Grad:  tensor([ 0.0028, -0.0208])\n",
      "Epoch 448, Loss 0.263312\n",
      "\t Params:  tensor([0.8452, 0.5687])\n",
      "\t Grad:  tensor([ 0.0028, -0.0207])\n",
      "Epoch 449, Loss 0.263308\n",
      "\t Params:  tensor([0.8452, 0.5689])\n",
      "\t Grad:  tensor([ 0.0027, -0.0206])\n",
      "Epoch 450, Loss 0.263303\n",
      "\t Params:  tensor([0.8451, 0.5691])\n",
      "\t Grad:  tensor([ 0.0027, -0.0204])\n",
      "Epoch 451, Loss 0.263299\n",
      "\t Params:  tensor([0.8451, 0.5693])\n",
      "\t Grad:  tensor([ 0.0027, -0.0203])\n",
      "Epoch 452, Loss 0.263295\n",
      "\t Params:  tensor([0.8451, 0.5695])\n",
      "\t Grad:  tensor([ 0.0027, -0.0202])\n",
      "Epoch 453, Loss 0.263291\n",
      "\t Params:  tensor([0.8451, 0.5697])\n",
      "\t Grad:  tensor([ 0.0027, -0.0200])\n",
      "Epoch 454, Loss 0.263287\n",
      "\t Params:  tensor([0.8450, 0.5699])\n",
      "\t Grad:  tensor([ 0.0027, -0.0199])\n",
      "Epoch 455, Loss 0.263282\n",
      "\t Params:  tensor([0.8450, 0.5701])\n",
      "\t Grad:  tensor([ 0.0026, -0.0198])\n",
      "Epoch 456, Loss 0.263279\n",
      "\t Params:  tensor([0.8450, 0.5703])\n",
      "\t Grad:  tensor([ 0.0026, -0.0196])\n",
      "Epoch 457, Loss 0.263275\n",
      "\t Params:  tensor([0.8450, 0.5705])\n",
      "\t Grad:  tensor([ 0.0026, -0.0195])\n",
      "Epoch 458, Loss 0.263271\n",
      "\t Params:  tensor([0.8449, 0.5707])\n",
      "\t Grad:  tensor([ 0.0026, -0.0194])\n",
      "Epoch 459, Loss 0.263267\n",
      "\t Params:  tensor([0.8449, 0.5709])\n",
      "\t Grad:  tensor([ 0.0026, -0.0192])\n",
      "Epoch 460, Loss 0.263263\n",
      "\t Params:  tensor([0.8449, 0.5710])\n",
      "\t Grad:  tensor([ 0.0026, -0.0191])\n",
      "Epoch 461, Loss 0.263260\n",
      "\t Params:  tensor([0.8449, 0.5712])\n",
      "\t Grad:  tensor([ 0.0025, -0.0190])\n",
      "Epoch 462, Loss 0.263256\n",
      "\t Params:  tensor([0.8448, 0.5714])\n",
      "\t Grad:  tensor([ 0.0025, -0.0189])\n",
      "Epoch 463, Loss 0.263252\n",
      "\t Params:  tensor([0.8448, 0.5716])\n",
      "\t Grad:  tensor([ 0.0025, -0.0187])\n",
      "Epoch 464, Loss 0.263249\n",
      "\t Params:  tensor([0.8448, 0.5718])\n",
      "\t Grad:  tensor([ 0.0025, -0.0186])\n",
      "Epoch 465, Loss 0.263245\n",
      "\t Params:  tensor([0.8448, 0.5720])\n",
      "\t Grad:  tensor([ 0.0025, -0.0185])\n",
      "Epoch 466, Loss 0.263242\n",
      "\t Params:  tensor([0.8447, 0.5722])\n",
      "\t Grad:  tensor([ 0.0025, -0.0184])\n",
      "Epoch 467, Loss 0.263238\n",
      "\t Params:  tensor([0.8447, 0.5723])\n",
      "\t Grad:  tensor([ 0.0024, -0.0183])\n",
      "Epoch 468, Loss 0.263235\n",
      "\t Params:  tensor([0.8447, 0.5725])\n",
      "\t Grad:  tensor([ 0.0024, -0.0181])\n",
      "Epoch 469, Loss 0.263232\n",
      "\t Params:  tensor([0.8447, 0.5727])\n",
      "\t Grad:  tensor([ 0.0024, -0.0180])\n",
      "Epoch 470, Loss 0.263228\n",
      "\t Params:  tensor([0.8446, 0.5729])\n",
      "\t Grad:  tensor([ 0.0024, -0.0179])\n",
      "Epoch 471, Loss 0.263225\n",
      "\t Params:  tensor([0.8446, 0.5731])\n",
      "\t Grad:  tensor([ 0.0024, -0.0178])\n",
      "Epoch 472, Loss 0.263222\n",
      "\t Params:  tensor([0.8446, 0.5732])\n",
      "\t Grad:  tensor([ 0.0024, -0.0177])\n",
      "Epoch 473, Loss 0.263219\n",
      "\t Params:  tensor([0.8446, 0.5734])\n",
      "\t Grad:  tensor([ 0.0023, -0.0175])\n",
      "Epoch 474, Loss 0.263216\n",
      "\t Params:  tensor([0.8445, 0.5736])\n",
      "\t Grad:  tensor([ 0.0023, -0.0174])\n",
      "Epoch 475, Loss 0.263213\n",
      "\t Params:  tensor([0.8445, 0.5738])\n",
      "\t Grad:  tensor([ 0.0023, -0.0173])\n",
      "Epoch 476, Loss 0.263209\n",
      "\t Params:  tensor([0.8445, 0.5739])\n",
      "\t Grad:  tensor([ 0.0023, -0.0172])\n",
      "Epoch 477, Loss 0.263207\n",
      "\t Params:  tensor([0.8445, 0.5741])\n",
      "\t Grad:  tensor([ 0.0023, -0.0171])\n",
      "Epoch 478, Loss 0.263203\n",
      "\t Params:  tensor([0.8444, 0.5743])\n",
      "\t Grad:  tensor([ 0.0023, -0.0170])\n",
      "Epoch 479, Loss 0.263201\n",
      "\t Params:  tensor([0.8444, 0.5744])\n",
      "\t Grad:  tensor([ 0.0023, -0.0169])\n",
      "Epoch 480, Loss 0.263198\n",
      "\t Params:  tensor([0.8444, 0.5746])\n",
      "\t Grad:  tensor([ 0.0022, -0.0167])\n",
      "Epoch 481, Loss 0.263195\n",
      "\t Params:  tensor([0.8444, 0.5748])\n",
      "\t Grad:  tensor([ 0.0022, -0.0166])\n",
      "Epoch 482, Loss 0.263192\n",
      "\t Params:  tensor([0.8444, 0.5749])\n",
      "\t Grad:  tensor([ 0.0022, -0.0165])\n",
      "Epoch 483, Loss 0.263189\n",
      "\t Params:  tensor([0.8443, 0.5751])\n",
      "\t Grad:  tensor([ 0.0022, -0.0164])\n",
      "Epoch 484, Loss 0.263187\n",
      "\t Params:  tensor([0.8443, 0.5753])\n",
      "\t Grad:  tensor([ 0.0022, -0.0163])\n",
      "Epoch 485, Loss 0.263184\n",
      "\t Params:  tensor([0.8443, 0.5754])\n",
      "\t Grad:  tensor([ 0.0022, -0.0162])\n",
      "Epoch 486, Loss 0.263181\n",
      "\t Params:  tensor([0.8443, 0.5756])\n",
      "\t Grad:  tensor([ 0.0021, -0.0161])\n",
      "Epoch 487, Loss 0.263179\n",
      "\t Params:  tensor([0.8442, 0.5758])\n",
      "\t Grad:  tensor([ 0.0021, -0.0160])\n",
      "Epoch 488, Loss 0.263176\n",
      "\t Params:  tensor([0.8442, 0.5759])\n",
      "\t Grad:  tensor([ 0.0021, -0.0159])\n",
      "Epoch 489, Loss 0.263173\n",
      "\t Params:  tensor([0.8442, 0.5761])\n",
      "\t Grad:  tensor([ 0.0021, -0.0158])\n",
      "Epoch 490, Loss 0.263171\n",
      "\t Params:  tensor([0.8442, 0.5762])\n",
      "\t Grad:  tensor([ 0.0021, -0.0157])\n",
      "Epoch 491, Loss 0.263168\n",
      "\t Params:  tensor([0.8442, 0.5764])\n",
      "\t Grad:  tensor([ 0.0021, -0.0156])\n",
      "Epoch 492, Loss 0.263166\n",
      "\t Params:  tensor([0.8441, 0.5765])\n",
      "\t Grad:  tensor([ 0.0021, -0.0155])\n",
      "Epoch 493, Loss 0.263164\n",
      "\t Params:  tensor([0.8441, 0.5767])\n",
      "\t Grad:  tensor([ 0.0021, -0.0154])\n",
      "Epoch 494, Loss 0.263161\n",
      "\t Params:  tensor([0.8441, 0.5768])\n",
      "\t Grad:  tensor([ 0.0020, -0.0153])\n",
      "Epoch 495, Loss 0.263159\n",
      "\t Params:  tensor([0.8441, 0.5770])\n",
      "\t Grad:  tensor([ 0.0020, -0.0152])\n",
      "Epoch 496, Loss 0.263156\n",
      "\t Params:  tensor([0.8441, 0.5771])\n",
      "\t Grad:  tensor([ 0.0020, -0.0151])\n",
      "Epoch 497, Loss 0.263154\n",
      "\t Params:  tensor([0.8440, 0.5773])\n",
      "\t Grad:  tensor([ 0.0020, -0.0150])\n",
      "Epoch 498, Loss 0.263152\n",
      "\t Params:  tensor([0.8440, 0.5774])\n",
      "\t Grad:  tensor([ 0.0020, -0.0149])\n",
      "Epoch 499, Loss 0.263150\n",
      "\t Params:  tensor([0.8440, 0.5776])\n",
      "\t Grad:  tensor([ 0.0020, -0.0148])\n",
      "Epoch 500, Loss 0.263147\n",
      "\t Params:  tensor([0.8440, 0.5777])\n",
      "\t Grad:  tensor([ 0.0020, -0.0147])\n",
      "Epoch 501, Loss 0.263145\n",
      "\t Params:  tensor([0.8440, 0.5779])\n",
      "\t Grad:  tensor([ 0.0019, -0.0146])\n",
      "Epoch 502, Loss 0.263143\n",
      "\t Params:  tensor([0.8439, 0.5780])\n",
      "\t Grad:  tensor([ 0.0019, -0.0145])\n",
      "Epoch 503, Loss 0.263141\n",
      "\t Params:  tensor([0.8439, 0.5782])\n",
      "\t Grad:  tensor([ 0.0019, -0.0144])\n",
      "Epoch 504, Loss 0.263139\n",
      "\t Params:  tensor([0.8439, 0.5783])\n",
      "\t Grad:  tensor([ 0.0019, -0.0143])\n",
      "Epoch 505, Loss 0.263137\n",
      "\t Params:  tensor([0.8439, 0.5785])\n",
      "\t Grad:  tensor([ 0.0019, -0.0142])\n",
      "Epoch 506, Loss 0.263135\n",
      "\t Params:  tensor([0.8439, 0.5786])\n",
      "\t Grad:  tensor([ 0.0019, -0.0141])\n",
      "Epoch 507, Loss 0.263133\n",
      "\t Params:  tensor([0.8439, 0.5787])\n",
      "\t Grad:  tensor([ 0.0019, -0.0140])\n",
      "Epoch 508, Loss 0.263131\n",
      "\t Params:  tensor([0.8438, 0.5789])\n",
      "\t Grad:  tensor([ 0.0019, -0.0139])\n",
      "Epoch 509, Loss 0.263129\n",
      "\t Params:  tensor([0.8438, 0.5790])\n",
      "\t Grad:  tensor([ 0.0018, -0.0138])\n",
      "Epoch 510, Loss 0.263127\n",
      "\t Params:  tensor([0.8438, 0.5792])\n",
      "\t Grad:  tensor([ 0.0018, -0.0137])\n",
      "Epoch 511, Loss 0.263125\n",
      "\t Params:  tensor([0.8438, 0.5793])\n",
      "\t Grad:  tensor([ 0.0018, -0.0136])\n",
      "Epoch 512, Loss 0.263123\n",
      "\t Params:  tensor([0.8438, 0.5794])\n",
      "\t Grad:  tensor([ 0.0018, -0.0135])\n",
      "Epoch 513, Loss 0.263121\n",
      "\t Params:  tensor([0.8437, 0.5796])\n",
      "\t Grad:  tensor([ 0.0018, -0.0134])\n",
      "Epoch 514, Loss 0.263119\n",
      "\t Params:  tensor([0.8437, 0.5797])\n",
      "\t Grad:  tensor([ 0.0018, -0.0134])\n",
      "Epoch 515, Loss 0.263118\n",
      "\t Params:  tensor([0.8437, 0.5798])\n",
      "\t Grad:  tensor([ 0.0018, -0.0133])\n",
      "Epoch 516, Loss 0.263116\n",
      "\t Params:  tensor([0.8437, 0.5800])\n",
      "\t Grad:  tensor([ 0.0018, -0.0132])\n",
      "Epoch 517, Loss 0.263114\n",
      "\t Params:  tensor([0.8437, 0.5801])\n",
      "\t Grad:  tensor([ 0.0017, -0.0131])\n",
      "Epoch 518, Loss 0.263112\n",
      "\t Params:  tensor([0.8437, 0.5802])\n",
      "\t Grad:  tensor([ 0.0017, -0.0130])\n",
      "Epoch 519, Loss 0.263111\n",
      "\t Params:  tensor([0.8436, 0.5803])\n",
      "\t Grad:  tensor([ 0.0017, -0.0129])\n",
      "Epoch 520, Loss 0.263109\n",
      "\t Params:  tensor([0.8436, 0.5805])\n",
      "\t Grad:  tensor([ 0.0017, -0.0128])\n",
      "Epoch 521, Loss 0.263107\n",
      "\t Params:  tensor([0.8436, 0.5806])\n",
      "\t Grad:  tensor([ 0.0017, -0.0127])\n",
      "Epoch 522, Loss 0.263106\n",
      "\t Params:  tensor([0.8436, 0.5807])\n",
      "\t Grad:  tensor([ 0.0017, -0.0127])\n",
      "Epoch 523, Loss 0.263104\n",
      "\t Params:  tensor([0.8436, 0.5809])\n",
      "\t Grad:  tensor([ 0.0017, -0.0126])\n",
      "Epoch 524, Loss 0.263102\n",
      "\t Params:  tensor([0.8436, 0.5810])\n",
      "\t Grad:  tensor([ 0.0017, -0.0125])\n",
      "Epoch 525, Loss 0.263101\n",
      "\t Params:  tensor([0.8435, 0.5811])\n",
      "\t Grad:  tensor([ 0.0017, -0.0124])\n",
      "Epoch 526, Loss 0.263099\n",
      "\t Params:  tensor([0.8435, 0.5812])\n",
      "\t Grad:  tensor([ 0.0016, -0.0123])\n",
      "Epoch 527, Loss 0.263098\n",
      "\t Params:  tensor([0.8435, 0.5813])\n",
      "\t Grad:  tensor([ 0.0016, -0.0122])\n",
      "Epoch 528, Loss 0.263096\n",
      "\t Params:  tensor([0.8435, 0.5815])\n",
      "\t Grad:  tensor([ 0.0016, -0.0122])\n",
      "Epoch 529, Loss 0.263095\n",
      "\t Params:  tensor([0.8435, 0.5816])\n",
      "\t Grad:  tensor([ 0.0016, -0.0121])\n",
      "Epoch 530, Loss 0.263093\n",
      "\t Params:  tensor([0.8435, 0.5817])\n",
      "\t Grad:  tensor([ 0.0016, -0.0120])\n",
      "Epoch 531, Loss 0.263092\n",
      "\t Params:  tensor([0.8434, 0.5818])\n",
      "\t Grad:  tensor([ 0.0016, -0.0119])\n",
      "Epoch 532, Loss 0.263090\n",
      "\t Params:  tensor([0.8434, 0.5819])\n",
      "\t Grad:  tensor([ 0.0016, -0.0118])\n",
      "Epoch 533, Loss 0.263089\n",
      "\t Params:  tensor([0.8434, 0.5821])\n",
      "\t Grad:  tensor([ 0.0016, -0.0118])\n",
      "Epoch 534, Loss 0.263087\n",
      "\t Params:  tensor([0.8434, 0.5822])\n",
      "\t Grad:  tensor([ 0.0016, -0.0117])\n",
      "Epoch 535, Loss 0.263086\n",
      "\t Params:  tensor([0.8434, 0.5823])\n",
      "\t Grad:  tensor([ 0.0015, -0.0116])\n",
      "Epoch 536, Loss 0.263085\n",
      "\t Params:  tensor([0.8434, 0.5824])\n",
      "\t Grad:  tensor([ 0.0015, -0.0115])\n",
      "Epoch 537, Loss 0.263083\n",
      "\t Params:  tensor([0.8433, 0.5825])\n",
      "\t Grad:  tensor([ 0.0015, -0.0115])\n",
      "Epoch 538, Loss 0.263082\n",
      "\t Params:  tensor([0.8433, 0.5826])\n",
      "\t Grad:  tensor([ 0.0015, -0.0114])\n",
      "Epoch 539, Loss 0.263081\n",
      "\t Params:  tensor([0.8433, 0.5828])\n",
      "\t Grad:  tensor([ 0.0015, -0.0113])\n",
      "Epoch 540, Loss 0.263079\n",
      "\t Params:  tensor([0.8433, 0.5829])\n",
      "\t Grad:  tensor([ 0.0015, -0.0112])\n",
      "Epoch 541, Loss 0.263078\n",
      "\t Params:  tensor([0.8433, 0.5830])\n",
      "\t Grad:  tensor([ 0.0015, -0.0112])\n",
      "Epoch 542, Loss 0.263077\n",
      "\t Params:  tensor([0.8433, 0.5831])\n",
      "\t Grad:  tensor([ 0.0015, -0.0111])\n",
      "Epoch 543, Loss 0.263076\n",
      "\t Params:  tensor([0.8433, 0.5832])\n",
      "\t Grad:  tensor([ 0.0015, -0.0110])\n",
      "Epoch 544, Loss 0.263074\n",
      "\t Params:  tensor([0.8432, 0.5833])\n",
      "\t Grad:  tensor([ 0.0015, -0.0109])\n",
      "Epoch 545, Loss 0.263073\n",
      "\t Params:  tensor([0.8432, 0.5834])\n",
      "\t Grad:  tensor([ 0.0015, -0.0109])\n",
      "Epoch 546, Loss 0.263072\n",
      "\t Params:  tensor([0.8432, 0.5835])\n",
      "\t Grad:  tensor([ 0.0014, -0.0108])\n",
      "Epoch 547, Loss 0.263071\n",
      "\t Params:  tensor([0.8432, 0.5836])\n",
      "\t Grad:  tensor([ 0.0014, -0.0107])\n",
      "Epoch 548, Loss 0.263070\n",
      "\t Params:  tensor([0.8432, 0.5837])\n",
      "\t Grad:  tensor([ 0.0014, -0.0107])\n",
      "Epoch 549, Loss 0.263068\n",
      "\t Params:  tensor([0.8432, 0.5838])\n",
      "\t Grad:  tensor([ 0.0014, -0.0106])\n",
      "Epoch 550, Loss 0.263067\n",
      "\t Params:  tensor([0.8432, 0.5840])\n",
      "\t Grad:  tensor([ 0.0014, -0.0105])\n",
      "Epoch 551, Loss 0.263066\n",
      "\t Params:  tensor([0.8431, 0.5841])\n",
      "\t Grad:  tensor([ 0.0014, -0.0104])\n",
      "Epoch 552, Loss 0.263065\n",
      "\t Params:  tensor([0.8431, 0.5842])\n",
      "\t Grad:  tensor([ 0.0014, -0.0104])\n",
      "Epoch 553, Loss 0.263064\n",
      "\t Params:  tensor([0.8431, 0.5843])\n",
      "\t Grad:  tensor([ 0.0014, -0.0103])\n",
      "Epoch 554, Loss 0.263063\n",
      "\t Params:  tensor([0.8431, 0.5844])\n",
      "\t Grad:  tensor([ 0.0014, -0.0102])\n",
      "Epoch 555, Loss 0.263062\n",
      "\t Params:  tensor([0.8431, 0.5845])\n",
      "\t Grad:  tensor([ 0.0014, -0.0102])\n",
      "Epoch 556, Loss 0.263061\n",
      "\t Params:  tensor([0.8431, 0.5846])\n",
      "\t Grad:  tensor([ 0.0014, -0.0101])\n",
      "Epoch 557, Loss 0.263060\n",
      "\t Params:  tensor([0.8431, 0.5847])\n",
      "\t Grad:  tensor([ 0.0013, -0.0100])\n",
      "Epoch 558, Loss 0.263059\n",
      "\t Params:  tensor([0.8430, 0.5848])\n",
      "\t Grad:  tensor([ 0.0013, -0.0100])\n",
      "Epoch 559, Loss 0.263058\n",
      "\t Params:  tensor([0.8430, 0.5849])\n",
      "\t Grad:  tensor([ 0.0013, -0.0099])\n",
      "Epoch 560, Loss 0.263057\n",
      "\t Params:  tensor([0.8430, 0.5850])\n",
      "\t Grad:  tensor([ 0.0013, -0.0098])\n",
      "Epoch 561, Loss 0.263056\n",
      "\t Params:  tensor([0.8430, 0.5851])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Grad:  tensor([ 0.0013, -0.0098])\n",
      "Epoch 562, Loss 0.263055\n",
      "\t Params:  tensor([0.8430, 0.5852])\n",
      "\t Grad:  tensor([ 0.0013, -0.0097])\n",
      "Epoch 563, Loss 0.263054\n",
      "\t Params:  tensor([0.8430, 0.5853])\n",
      "\t Grad:  tensor([ 0.0013, -0.0096])\n",
      "Epoch 564, Loss 0.263053\n",
      "\t Params:  tensor([0.8430, 0.5854])\n",
      "\t Grad:  tensor([ 0.0013, -0.0096])\n",
      "Epoch 565, Loss 0.263052\n",
      "\t Params:  tensor([0.8430, 0.5854])\n",
      "\t Grad:  tensor([ 0.0013, -0.0095])\n",
      "Epoch 566, Loss 0.263051\n",
      "\t Params:  tensor([0.8429, 0.5855])\n",
      "\t Grad:  tensor([ 0.0013, -0.0095])\n",
      "Epoch 567, Loss 0.263050\n",
      "\t Params:  tensor([0.8429, 0.5856])\n",
      "\t Grad:  tensor([ 0.0013, -0.0094])\n",
      "Epoch 568, Loss 0.263049\n",
      "\t Params:  tensor([0.8429, 0.5857])\n",
      "\t Grad:  tensor([ 0.0012, -0.0093])\n",
      "Epoch 569, Loss 0.263048\n",
      "\t Params:  tensor([0.8429, 0.5858])\n",
      "\t Grad:  tensor([ 0.0012, -0.0093])\n",
      "Epoch 570, Loss 0.263047\n",
      "\t Params:  tensor([0.8429, 0.5859])\n",
      "\t Grad:  tensor([ 0.0012, -0.0092])\n",
      "Epoch 571, Loss 0.263047\n",
      "\t Params:  tensor([0.8429, 0.5860])\n",
      "\t Grad:  tensor([ 0.0012, -0.0091])\n",
      "Epoch 572, Loss 0.263046\n",
      "\t Params:  tensor([0.8429, 0.5861])\n",
      "\t Grad:  tensor([ 0.0012, -0.0091])\n",
      "Epoch 573, Loss 0.263045\n",
      "\t Params:  tensor([0.8429, 0.5862])\n",
      "\t Grad:  tensor([ 0.0012, -0.0090])\n",
      "Epoch 574, Loss 0.263044\n",
      "\t Params:  tensor([0.8428, 0.5863])\n",
      "\t Grad:  tensor([ 0.0012, -0.0090])\n",
      "Epoch 575, Loss 0.263043\n",
      "\t Params:  tensor([0.8428, 0.5864])\n",
      "\t Grad:  tensor([ 0.0012, -0.0089])\n",
      "Epoch 576, Loss 0.263042\n",
      "\t Params:  tensor([0.8428, 0.5865])\n",
      "\t Grad:  tensor([ 0.0012, -0.0088])\n",
      "Epoch 577, Loss 0.263042\n",
      "\t Params:  tensor([0.8428, 0.5865])\n",
      "\t Grad:  tensor([ 0.0012, -0.0088])\n",
      "Epoch 578, Loss 0.263041\n",
      "\t Params:  tensor([0.8428, 0.5866])\n",
      "\t Grad:  tensor([ 0.0012, -0.0087])\n",
      "Epoch 579, Loss 0.263040\n",
      "\t Params:  tensor([0.8428, 0.5867])\n",
      "\t Grad:  tensor([ 0.0012, -0.0087])\n",
      "Epoch 580, Loss 0.263039\n",
      "\t Params:  tensor([0.8428, 0.5868])\n",
      "\t Grad:  tensor([ 0.0011, -0.0086])\n",
      "Epoch 581, Loss 0.263039\n",
      "\t Params:  tensor([0.8428, 0.5869])\n",
      "\t Grad:  tensor([ 0.0011, -0.0086])\n",
      "Epoch 582, Loss 0.263038\n",
      "\t Params:  tensor([0.8427, 0.5870])\n",
      "\t Grad:  tensor([ 0.0011, -0.0085])\n",
      "Epoch 583, Loss 0.263037\n",
      "\t Params:  tensor([0.8427, 0.5871])\n",
      "\t Grad:  tensor([ 0.0011, -0.0084])\n",
      "Epoch 584, Loss 0.263036\n",
      "\t Params:  tensor([0.8427, 0.5871])\n",
      "\t Grad:  tensor([ 0.0011, -0.0084])\n",
      "Epoch 585, Loss 0.263036\n",
      "\t Params:  tensor([0.8427, 0.5872])\n",
      "\t Grad:  tensor([ 0.0011, -0.0083])\n",
      "Epoch 586, Loss 0.263035\n",
      "\t Params:  tensor([0.8427, 0.5873])\n",
      "\t Grad:  tensor([ 0.0011, -0.0083])\n",
      "Epoch 587, Loss 0.263034\n",
      "\t Params:  tensor([0.8427, 0.5874])\n",
      "\t Grad:  tensor([ 0.0011, -0.0082])\n",
      "Epoch 588, Loss 0.263034\n",
      "\t Params:  tensor([0.8427, 0.5875])\n",
      "\t Grad:  tensor([ 0.0011, -0.0082])\n",
      "Epoch 589, Loss 0.263033\n",
      "\t Params:  tensor([0.8427, 0.5876])\n",
      "\t Grad:  tensor([ 0.0011, -0.0081])\n",
      "Epoch 590, Loss 0.263032\n",
      "\t Params:  tensor([0.8427, 0.5876])\n",
      "\t Grad:  tensor([ 0.0011, -0.0081])\n",
      "Epoch 591, Loss 0.263031\n",
      "\t Params:  tensor([0.8427, 0.5877])\n",
      "\t Grad:  tensor([ 0.0011, -0.0080])\n",
      "Epoch 592, Loss 0.263031\n",
      "\t Params:  tensor([0.8426, 0.5878])\n",
      "\t Grad:  tensor([ 0.0011, -0.0080])\n",
      "Epoch 593, Loss 0.263030\n",
      "\t Params:  tensor([0.8426, 0.5879])\n",
      "\t Grad:  tensor([ 0.0011, -0.0079])\n",
      "Epoch 594, Loss 0.263030\n",
      "\t Params:  tensor([0.8426, 0.5879])\n",
      "\t Grad:  tensor([ 0.0011, -0.0078])\n",
      "Epoch 595, Loss 0.263029\n",
      "\t Params:  tensor([0.8426, 0.5880])\n",
      "\t Grad:  tensor([ 0.0010, -0.0078])\n",
      "Epoch 596, Loss 0.263028\n",
      "\t Params:  tensor([0.8426, 0.5881])\n",
      "\t Grad:  tensor([ 0.0010, -0.0077])\n",
      "Epoch 597, Loss 0.263028\n",
      "\t Params:  tensor([0.8426, 0.5882])\n",
      "\t Grad:  tensor([ 0.0010, -0.0077])\n",
      "Epoch 598, Loss 0.263027\n",
      "\t Params:  tensor([0.8426, 0.5883])\n",
      "\t Grad:  tensor([ 0.0010, -0.0076])\n",
      "Epoch 599, Loss 0.263027\n",
      "\t Params:  tensor([0.8426, 0.5883])\n",
      "\t Grad:  tensor([ 0.0010, -0.0076])\n",
      "Epoch 600, Loss 0.263026\n",
      "\t Params:  tensor([0.8426, 0.5884])\n",
      "\t Grad:  tensor([ 0.0010, -0.0075])\n",
      "Epoch 601, Loss 0.263026\n",
      "\t Params:  tensor([0.8425, 0.5885])\n",
      "\t Grad:  tensor([ 0.0010, -0.0075])\n",
      "Epoch 602, Loss 0.263025\n",
      "\t Params:  tensor([0.8425, 0.5886])\n",
      "\t Grad:  tensor([ 0.0010, -0.0074])\n",
      "Epoch 603, Loss 0.263024\n",
      "\t Params:  tensor([0.8425, 0.5886])\n",
      "\t Grad:  tensor([ 0.0010, -0.0074])\n",
      "Epoch 604, Loss 0.263024\n",
      "\t Params:  tensor([0.8425, 0.5887])\n",
      "\t Grad:  tensor([ 0.0010, -0.0073])\n",
      "Epoch 605, Loss 0.263023\n",
      "\t Params:  tensor([0.8425, 0.5888])\n",
      "\t Grad:  tensor([ 0.0010, -0.0073])\n",
      "Epoch 606, Loss 0.263023\n",
      "\t Params:  tensor([0.8425, 0.5889])\n",
      "\t Grad:  tensor([ 0.0010, -0.0072])\n",
      "Epoch 607, Loss 0.263022\n",
      "\t Params:  tensor([0.8425, 0.5889])\n",
      "\t Grad:  tensor([ 0.0010, -0.0072])\n",
      "Epoch 608, Loss 0.263022\n",
      "\t Params:  tensor([0.8425, 0.5890])\n",
      "\t Grad:  tensor([ 0.0010, -0.0071])\n",
      "Epoch 609, Loss 0.263021\n",
      "\t Params:  tensor([0.8425, 0.5891])\n",
      "\t Grad:  tensor([ 0.0009, -0.0071])\n",
      "Epoch 610, Loss 0.263021\n",
      "\t Params:  tensor([0.8425, 0.5891])\n",
      "\t Grad:  tensor([ 0.0009, -0.0071])\n",
      "Epoch 611, Loss 0.263020\n",
      "\t Params:  tensor([0.8425, 0.5892])\n",
      "\t Grad:  tensor([ 0.0009, -0.0070])\n",
      "Epoch 612, Loss 0.263020\n",
      "\t Params:  tensor([0.8424, 0.5893])\n",
      "\t Grad:  tensor([ 0.0009, -0.0070])\n",
      "Epoch 613, Loss 0.263019\n",
      "\t Params:  tensor([0.8424, 0.5893])\n",
      "\t Grad:  tensor([ 0.0009, -0.0069])\n",
      "Epoch 614, Loss 0.263019\n",
      "\t Params:  tensor([0.8424, 0.5894])\n",
      "\t Grad:  tensor([ 0.0009, -0.0069])\n",
      "Epoch 615, Loss 0.263018\n",
      "\t Params:  tensor([0.8424, 0.5895])\n",
      "\t Grad:  tensor([ 0.0009, -0.0068])\n",
      "Epoch 616, Loss 0.263018\n",
      "\t Params:  tensor([0.8424, 0.5895])\n",
      "\t Grad:  tensor([ 0.0009, -0.0068])\n",
      "Epoch 617, Loss 0.263017\n",
      "\t Params:  tensor([0.8424, 0.5896])\n",
      "\t Grad:  tensor([ 0.0009, -0.0067])\n",
      "Epoch 618, Loss 0.263017\n",
      "\t Params:  tensor([0.8424, 0.5897])\n",
      "\t Grad:  tensor([ 0.0009, -0.0067])\n",
      "Epoch 619, Loss 0.263016\n",
      "\t Params:  tensor([0.8424, 0.5898])\n",
      "\t Grad:  tensor([ 0.0009, -0.0066])\n",
      "Epoch 620, Loss 0.263016\n",
      "\t Params:  tensor([0.8424, 0.5898])\n",
      "\t Grad:  tensor([ 0.0009, -0.0066])\n",
      "Epoch 621, Loss 0.263015\n",
      "\t Params:  tensor([0.8424, 0.5899])\n",
      "\t Grad:  tensor([ 0.0009, -0.0066])\n",
      "Epoch 622, Loss 0.263015\n",
      "\t Params:  tensor([0.8424, 0.5899])\n",
      "\t Grad:  tensor([ 0.0009, -0.0065])\n",
      "Epoch 623, Loss 0.263015\n",
      "\t Params:  tensor([0.8423, 0.5900])\n",
      "\t Grad:  tensor([ 0.0009, -0.0065])\n",
      "Epoch 624, Loss 0.263014\n",
      "\t Params:  tensor([0.8423, 0.5901])\n",
      "\t Grad:  tensor([ 0.0009, -0.0064])\n",
      "Epoch 625, Loss 0.263014\n",
      "\t Params:  tensor([0.8423, 0.5901])\n",
      "\t Grad:  tensor([ 0.0009, -0.0064])\n",
      "Epoch 626, Loss 0.263013\n",
      "\t Params:  tensor([0.8423, 0.5902])\n",
      "\t Grad:  tensor([ 0.0008, -0.0063])\n",
      "Epoch 627, Loss 0.263013\n",
      "\t Params:  tensor([0.8423, 0.5903])\n",
      "\t Grad:  tensor([ 0.0008, -0.0063])\n",
      "Epoch 628, Loss 0.263012\n",
      "\t Params:  tensor([0.8423, 0.5903])\n",
      "\t Grad:  tensor([ 0.0008, -0.0063])\n",
      "Epoch 629, Loss 0.263012\n",
      "\t Params:  tensor([0.8423, 0.5904])\n",
      "\t Grad:  tensor([ 0.0008, -0.0062])\n",
      "Epoch 630, Loss 0.263012\n",
      "\t Params:  tensor([0.8423, 0.5905])\n",
      "\t Grad:  tensor([ 0.0008, -0.0062])\n",
      "Epoch 631, Loss 0.263011\n",
      "\t Params:  tensor([0.8423, 0.5905])\n",
      "\t Grad:  tensor([ 0.0008, -0.0061])\n",
      "Epoch 632, Loss 0.263011\n",
      "\t Params:  tensor([0.8423, 0.5906])\n",
      "\t Grad:  tensor([ 0.0008, -0.0061])\n",
      "Epoch 633, Loss 0.263011\n",
      "\t Params:  tensor([0.8423, 0.5906])\n",
      "\t Grad:  tensor([ 0.0008, -0.0061])\n",
      "Epoch 634, Loss 0.263010\n",
      "\t Params:  tensor([0.8423, 0.5907])\n",
      "\t Grad:  tensor([ 0.0008, -0.0060])\n",
      "Epoch 635, Loss 0.263010\n",
      "\t Params:  tensor([0.8422, 0.5908])\n",
      "\t Grad:  tensor([ 0.0008, -0.0060])\n",
      "Epoch 636, Loss 0.263009\n",
      "\t Params:  tensor([0.8422, 0.5908])\n",
      "\t Grad:  tensor([ 0.0008, -0.0059])\n",
      "Epoch 637, Loss 0.263009\n",
      "\t Params:  tensor([0.8422, 0.5909])\n",
      "\t Grad:  tensor([ 0.0008, -0.0059])\n",
      "Epoch 638, Loss 0.263009\n",
      "\t Params:  tensor([0.8422, 0.5909])\n",
      "\t Grad:  tensor([ 0.0008, -0.0059])\n",
      "Epoch 639, Loss 0.263008\n",
      "\t Params:  tensor([0.8422, 0.5910])\n",
      "\t Grad:  tensor([ 0.0008, -0.0058])\n",
      "Epoch 640, Loss 0.263008\n",
      "\t Params:  tensor([0.8422, 0.5910])\n",
      "\t Grad:  tensor([ 0.0008, -0.0058])\n",
      "Epoch 641, Loss 0.263008\n",
      "\t Params:  tensor([0.8422, 0.5911])\n",
      "\t Grad:  tensor([ 0.0008, -0.0057])\n",
      "Epoch 642, Loss 0.263007\n",
      "\t Params:  tensor([0.8422, 0.5912])\n",
      "\t Grad:  tensor([ 0.0008, -0.0057])\n",
      "Epoch 643, Loss 0.263007\n",
      "\t Params:  tensor([0.8422, 0.5912])\n",
      "\t Grad:  tensor([ 0.0008, -0.0057])\n",
      "Epoch 644, Loss 0.263007\n",
      "\t Params:  tensor([0.8422, 0.5913])\n",
      "\t Grad:  tensor([ 0.0007, -0.0056])\n",
      "Epoch 645, Loss 0.263006\n",
      "\t Params:  tensor([0.8422, 0.5913])\n",
      "\t Grad:  tensor([ 0.0007, -0.0056])\n",
      "Epoch 646, Loss 0.263006\n",
      "\t Params:  tensor([0.8422, 0.5914])\n",
      "\t Grad:  tensor([ 0.0007, -0.0056])\n",
      "Epoch 647, Loss 0.263006\n",
      "\t Params:  tensor([0.8422, 0.5914])\n",
      "\t Grad:  tensor([ 0.0007, -0.0055])\n",
      "Epoch 648, Loss 0.263006\n",
      "\t Params:  tensor([0.8421, 0.5915])\n",
      "\t Grad:  tensor([ 0.0007, -0.0055])\n",
      "Epoch 649, Loss 0.263005\n",
      "\t Params:  tensor([0.8421, 0.5916])\n",
      "\t Grad:  tensor([ 0.0007, -0.0054])\n",
      "Epoch 650, Loss 0.263005\n",
      "\t Params:  tensor([0.8421, 0.5916])\n",
      "\t Grad:  tensor([ 0.0007, -0.0054])\n",
      "Epoch 651, Loss 0.263005\n",
      "\t Params:  tensor([0.8421, 0.5917])\n",
      "\t Grad:  tensor([ 0.0007, -0.0054])\n",
      "Epoch 652, Loss 0.263004\n",
      "\t Params:  tensor([0.8421, 0.5917])\n",
      "\t Grad:  tensor([ 0.0007, -0.0053])\n",
      "Epoch 653, Loss 0.263004\n",
      "\t Params:  tensor([0.8421, 0.5918])\n",
      "\t Grad:  tensor([ 0.0007, -0.0053])\n",
      "Epoch 654, Loss 0.263004\n",
      "\t Params:  tensor([0.8421, 0.5918])\n",
      "\t Grad:  tensor([ 0.0007, -0.0053])\n",
      "Epoch 655, Loss 0.263003\n",
      "\t Params:  tensor([0.8421, 0.5919])\n",
      "\t Grad:  tensor([ 0.0007, -0.0052])\n",
      "Epoch 656, Loss 0.263003\n",
      "\t Params:  tensor([0.8421, 0.5919])\n",
      "\t Grad:  tensor([ 0.0007, -0.0052])\n",
      "Epoch 657, Loss 0.263003\n",
      "\t Params:  tensor([0.8421, 0.5920])\n",
      "\t Grad:  tensor([ 0.0007, -0.0052])\n",
      "Epoch 658, Loss 0.263003\n",
      "\t Params:  tensor([0.8421, 0.5920])\n",
      "\t Grad:  tensor([ 0.0007, -0.0051])\n",
      "Epoch 659, Loss 0.263002\n",
      "\t Params:  tensor([0.8421, 0.5921])\n",
      "\t Grad:  tensor([ 0.0007, -0.0051])\n",
      "Epoch 660, Loss 0.263002\n",
      "\t Params:  tensor([0.8421, 0.5921])\n",
      "\t Grad:  tensor([ 0.0007, -0.0051])\n",
      "Epoch 661, Loss 0.263002\n",
      "\t Params:  tensor([0.8421, 0.5922])\n",
      "\t Grad:  tensor([ 0.0007, -0.0050])\n",
      "Epoch 662, Loss 0.263002\n",
      "\t Params:  tensor([0.8420, 0.5922])\n",
      "\t Grad:  tensor([ 0.0007, -0.0050])\n",
      "Epoch 663, Loss 0.263001\n",
      "\t Params:  tensor([0.8420, 0.5923])\n",
      "\t Grad:  tensor([ 0.0007, -0.0050])\n",
      "Epoch 664, Loss 0.263001\n",
      "\t Params:  tensor([0.8420, 0.5923])\n",
      "\t Grad:  tensor([ 0.0007, -0.0049])\n",
      "Epoch 665, Loss 0.263001\n",
      "\t Params:  tensor([0.8420, 0.5924])\n",
      "\t Grad:  tensor([ 0.0007, -0.0049])\n",
      "Epoch 666, Loss 0.263001\n",
      "\t Params:  tensor([0.8420, 0.5924])\n",
      "\t Grad:  tensor([ 0.0007, -0.0049])\n",
      "Epoch 667, Loss 0.263000\n",
      "\t Params:  tensor([0.8420, 0.5925])\n",
      "\t Grad:  tensor([ 0.0006, -0.0048])\n",
      "Epoch 668, Loss 0.263000\n",
      "\t Params:  tensor([0.8420, 0.5925])\n",
      "\t Grad:  tensor([ 0.0006, -0.0048])\n",
      "Epoch 669, Loss 0.263000\n",
      "\t Params:  tensor([0.8420, 0.5926])\n",
      "\t Grad:  tensor([ 0.0006, -0.0048])\n",
      "Epoch 670, Loss 0.263000\n",
      "\t Params:  tensor([0.8420, 0.5926])\n",
      "\t Grad:  tensor([ 0.0006, -0.0047])\n",
      "Epoch 671, Loss 0.262999\n",
      "\t Params:  tensor([0.8420, 0.5927])\n",
      "\t Grad:  tensor([ 0.0006, -0.0047])\n",
      "Epoch 672, Loss 0.262999\n",
      "\t Params:  tensor([0.8420, 0.5927])\n",
      "\t Grad:  tensor([ 0.0006, -0.0047])\n",
      "Epoch 673, Loss 0.262999\n",
      "\t Params:  tensor([0.8420, 0.5928])\n",
      "\t Grad:  tensor([ 0.0006, -0.0046])\n",
      "Epoch 674, Loss 0.262999\n",
      "\t Params:  tensor([0.8420, 0.5928])\n",
      "\t Grad:  tensor([ 0.0006, -0.0046])\n",
      "Epoch 675, Loss 0.262998\n",
      "\t Params:  tensor([0.8420, 0.5928])\n",
      "\t Grad:  tensor([ 0.0006, -0.0046])\n",
      "Epoch 676, Loss 0.262998\n",
      "\t Params:  tensor([0.8420, 0.5929])\n",
      "\t Grad:  tensor([ 0.0006, -0.0045])\n",
      "Epoch 677, Loss 0.262998\n",
      "\t Params:  tensor([0.8420, 0.5929])\n",
      "\t Grad:  tensor([ 0.0006, -0.0045])\n",
      "Epoch 678, Loss 0.262998\n",
      "\t Params:  tensor([0.8419, 0.5930])\n",
      "\t Grad:  tensor([ 0.0006, -0.0045])\n",
      "Epoch 679, Loss 0.262998\n",
      "\t Params:  tensor([0.8419, 0.5930])\n",
      "\t Grad:  tensor([ 0.0006, -0.0045])\n",
      "Epoch 680, Loss 0.262998\n",
      "\t Params:  tensor([0.8419, 0.5931])\n",
      "\t Grad:  tensor([ 0.0006, -0.0044])\n",
      "Epoch 681, Loss 0.262997\n",
      "\t Params:  tensor([0.8419, 0.5931])\n",
      "\t Grad:  tensor([ 0.0006, -0.0044])\n",
      "Epoch 682, Loss 0.262997\n",
      "\t Params:  tensor([0.8419, 0.5932])\n",
      "\t Grad:  tensor([ 0.0006, -0.0044])\n",
      "Epoch 683, Loss 0.262997\n",
      "\t Params:  tensor([0.8419, 0.5932])\n",
      "\t Grad:  tensor([ 0.0006, -0.0043])\n",
      "Epoch 684, Loss 0.262997\n",
      "\t Params:  tensor([0.8419, 0.5932])\n",
      "\t Grad:  tensor([ 0.0006, -0.0043])\n",
      "Epoch 685, Loss 0.262996\n",
      "\t Params:  tensor([0.8419, 0.5933])\n",
      "\t Grad:  tensor([ 0.0006, -0.0043])\n",
      "Epoch 686, Loss 0.262996\n",
      "\t Params:  tensor([0.8419, 0.5933])\n",
      "\t Grad:  tensor([ 0.0006, -0.0043])\n",
      "Epoch 687, Loss 0.262996\n",
      "\t Params:  tensor([0.8419, 0.5934])\n",
      "\t Grad:  tensor([ 0.0006, -0.0042])\n",
      "Epoch 688, Loss 0.262996\n",
      "\t Params:  tensor([0.8419, 0.5934])\n",
      "\t Grad:  tensor([ 0.0006, -0.0042])\n",
      "Epoch 689, Loss 0.262996\n",
      "\t Params:  tensor([0.8419, 0.5935])\n",
      "\t Grad:  tensor([ 0.0006, -0.0042])\n",
      "Epoch 690, Loss 0.262996\n",
      "\t Params:  tensor([0.8419, 0.5935])\n",
      "\t Grad:  tensor([ 0.0006, -0.0041])\n",
      "Epoch 691, Loss 0.262995\n",
      "\t Params:  tensor([0.8419, 0.5935])\n",
      "\t Grad:  tensor([ 0.0006, -0.0041])\n",
      "Epoch 692, Loss 0.262995\n",
      "\t Params:  tensor([0.8419, 0.5936])\n",
      "\t Grad:  tensor([ 0.0005, -0.0041])\n",
      "Epoch 693, Loss 0.262995\n",
      "\t Params:  tensor([0.8419, 0.5936])\n",
      "\t Grad:  tensor([ 0.0005, -0.0041])\n",
      "Epoch 694, Loss 0.262995\n",
      "\t Params:  tensor([0.8419, 0.5937])\n",
      "\t Grad:  tensor([ 0.0005, -0.0040])\n",
      "Epoch 695, Loss 0.262995\n",
      "\t Params:  tensor([0.8418, 0.5937])\n",
      "\t Grad:  tensor([ 0.0005, -0.0040])\n",
      "Epoch 696, Loss 0.262995\n",
      "\t Params:  tensor([0.8418, 0.5937])\n",
      "\t Grad:  tensor([ 0.0005, -0.0040])\n",
      "Epoch 697, Loss 0.262994\n",
      "\t Params:  tensor([0.8418, 0.5938])\n",
      "\t Grad:  tensor([ 0.0005, -0.0040])\n",
      "Epoch 698, Loss 0.262994\n",
      "\t Params:  tensor([0.8418, 0.5938])\n",
      "\t Grad:  tensor([ 0.0005, -0.0039])\n",
      "Epoch 699, Loss 0.262994\n",
      "\t Params:  tensor([0.8418, 0.5939])\n",
      "\t Grad:  tensor([ 0.0005, -0.0039])\n",
      "Epoch 700, Loss 0.262994\n",
      "\t Params:  tensor([0.8418, 0.5939])\n",
      "\t Grad:  tensor([ 0.0005, -0.0039])\n",
      "Epoch 701, Loss 0.262994\n",
      "\t Params:  tensor([0.8418, 0.5939])\n",
      "\t Grad:  tensor([ 0.0005, -0.0039])\n",
      "Epoch 702, Loss 0.262994\n",
      "\t Params:  tensor([0.8418, 0.5940])\n",
      "\t Grad:  tensor([ 0.0005, -0.0038])\n",
      "Epoch 703, Loss 0.262994\n",
      "\t Params:  tensor([0.8418, 0.5940])\n",
      "\t Grad:  tensor([ 0.0005, -0.0038])\n",
      "Epoch 704, Loss 0.262993\n",
      "\t Params:  tensor([0.8418, 0.5941])\n",
      "\t Grad:  tensor([ 0.0005, -0.0038])\n",
      "Epoch 705, Loss 0.262993\n",
      "\t Params:  tensor([0.8418, 0.5941])\n",
      "\t Grad:  tensor([ 0.0005, -0.0038])\n",
      "Epoch 706, Loss 0.262993\n",
      "\t Params:  tensor([0.8418, 0.5941])\n",
      "\t Grad:  tensor([ 0.0005, -0.0037])\n",
      "Epoch 707, Loss 0.262993\n",
      "\t Params:  tensor([0.8418, 0.5942])\n",
      "\t Grad:  tensor([ 0.0005, -0.0037])\n",
      "Epoch 708, Loss 0.262993\n",
      "\t Params:  tensor([0.8418, 0.5942])\n",
      "\t Grad:  tensor([ 0.0005, -0.0037])\n",
      "Epoch 709, Loss 0.262993\n",
      "\t Params:  tensor([0.8418, 0.5942])\n",
      "\t Grad:  tensor([ 0.0005, -0.0037])\n",
      "Epoch 710, Loss 0.262993\n",
      "\t Params:  tensor([0.8418, 0.5943])\n",
      "\t Grad:  tensor([ 0.0005, -0.0036])\n",
      "Epoch 711, Loss 0.262992\n",
      "\t Params:  tensor([0.8418, 0.5943])\n",
      "\t Grad:  tensor([ 0.0005, -0.0036])\n",
      "Epoch 712, Loss 0.262992\n",
      "\t Params:  tensor([0.8418, 0.5943])\n",
      "\t Grad:  tensor([ 0.0005, -0.0036])\n",
      "Epoch 713, Loss 0.262992\n",
      "\t Params:  tensor([0.8418, 0.5944])\n",
      "\t Grad:  tensor([ 0.0005, -0.0036])\n",
      "Epoch 714, Loss 0.262992\n",
      "\t Params:  tensor([0.8418, 0.5944])\n",
      "\t Grad:  tensor([ 0.0005, -0.0035])\n",
      "Epoch 715, Loss 0.262992\n",
      "\t Params:  tensor([0.8417, 0.5944])\n",
      "\t Grad:  tensor([ 0.0005, -0.0035])\n",
      "Epoch 716, Loss 0.262992\n",
      "\t Params:  tensor([0.8417, 0.5945])\n",
      "\t Grad:  tensor([ 0.0005, -0.0035])\n",
      "Epoch 717, Loss 0.262992\n",
      "\t Params:  tensor([0.8417, 0.5945])\n",
      "\t Grad:  tensor([ 0.0005, -0.0035])\n",
      "Epoch 718, Loss 0.262992\n",
      "\t Params:  tensor([0.8417, 0.5946])\n",
      "\t Grad:  tensor([ 0.0005, -0.0034])\n",
      "Epoch 719, Loss 0.262991\n",
      "\t Params:  tensor([0.8417, 0.5946])\n",
      "\t Grad:  tensor([ 0.0005, -0.0034])\n",
      "Epoch 720, Loss 0.262991\n",
      "\t Params:  tensor([0.8417, 0.5946])\n",
      "\t Grad:  tensor([ 0.0005, -0.0034])\n",
      "Epoch 721, Loss 0.262991\n",
      "\t Params:  tensor([0.8417, 0.5947])\n",
      "\t Grad:  tensor([ 0.0005, -0.0034])\n",
      "Epoch 722, Loss 0.262991\n",
      "\t Params:  tensor([0.8417, 0.5947])\n",
      "\t Grad:  tensor([ 0.0004, -0.0034])\n",
      "Epoch 723, Loss 0.262991\n",
      "\t Params:  tensor([0.8417, 0.5947])\n",
      "\t Grad:  tensor([ 0.0004, -0.0033])\n",
      "Epoch 724, Loss 0.262991\n",
      "\t Params:  tensor([0.8417, 0.5948])\n",
      "\t Grad:  tensor([ 0.0004, -0.0033])\n",
      "Epoch 725, Loss 0.262991\n",
      "\t Params:  tensor([0.8417, 0.5948])\n",
      "\t Grad:  tensor([ 0.0004, -0.0033])\n",
      "Epoch 726, Loss 0.262991\n",
      "\t Params:  tensor([0.8417, 0.5948])\n",
      "\t Grad:  tensor([ 0.0004, -0.0033])\n",
      "Epoch 727, Loss 0.262990\n",
      "\t Params:  tensor([0.8417, 0.5949])\n",
      "\t Grad:  tensor([ 0.0004, -0.0032])\n",
      "Epoch 728, Loss 0.262990\n",
      "\t Params:  tensor([0.8417, 0.5949])\n",
      "\t Grad:  tensor([ 0.0004, -0.0032])\n",
      "Epoch 729, Loss 0.262990\n",
      "\t Params:  tensor([0.8417, 0.5949])\n",
      "\t Grad:  tensor([ 0.0004, -0.0032])\n",
      "Epoch 730, Loss 0.262990\n",
      "\t Params:  tensor([0.8417, 0.5949])\n",
      "\t Grad:  tensor([ 0.0004, -0.0032])\n",
      "Epoch 731, Loss 0.262990\n",
      "\t Params:  tensor([0.8417, 0.5950])\n",
      "\t Grad:  tensor([ 0.0004, -0.0032])\n",
      "Epoch 732, Loss 0.262990\n",
      "\t Params:  tensor([0.8417, 0.5950])\n",
      "\t Grad:  tensor([ 0.0004, -0.0031])\n",
      "Epoch 733, Loss 0.262990\n",
      "\t Params:  tensor([0.8417, 0.5950])\n",
      "\t Grad:  tensor([ 0.0004, -0.0031])\n",
      "Epoch 734, Loss 0.262990\n",
      "\t Params:  tensor([0.8417, 0.5951])\n",
      "\t Grad:  tensor([ 0.0004, -0.0031])\n",
      "Epoch 735, Loss 0.262990\n",
      "\t Params:  tensor([0.8417, 0.5951])\n",
      "\t Grad:  tensor([ 0.0004, -0.0031])\n",
      "Epoch 736, Loss 0.262989\n",
      "\t Params:  tensor([0.8417, 0.5951])\n",
      "\t Grad:  tensor([ 0.0004, -0.0031])\n",
      "Epoch 737, Loss 0.262989\n",
      "\t Params:  tensor([0.8417, 0.5952])\n",
      "\t Grad:  tensor([ 0.0004, -0.0030])\n",
      "Epoch 738, Loss 0.262989\n",
      "\t Params:  tensor([0.8416, 0.5952])\n",
      "\t Grad:  tensor([ 0.0004, -0.0030])\n",
      "Epoch 739, Loss 0.262989\n",
      "\t Params:  tensor([0.8416, 0.5952])\n",
      "\t Grad:  tensor([ 0.0004, -0.0030])\n",
      "Epoch 740, Loss 0.262989\n",
      "\t Params:  tensor([0.8416, 0.5953])\n",
      "\t Grad:  tensor([ 0.0004, -0.0030])\n",
      "Epoch 741, Loss 0.262989\n",
      "\t Params:  tensor([0.8416, 0.5953])\n",
      "\t Grad:  tensor([ 0.0004, -0.0030])\n",
      "Epoch 742, Loss 0.262989\n",
      "\t Params:  tensor([0.8416, 0.5953])\n",
      "\t Grad:  tensor([ 0.0004, -0.0029])\n",
      "Epoch 743, Loss 0.262989\n",
      "\t Params:  tensor([0.8416, 0.5953])\n",
      "\t Grad:  tensor([ 0.0004, -0.0029])\n",
      "Epoch 744, Loss 0.262989\n",
      "\t Params:  tensor([0.8416, 0.5954])\n",
      "\t Grad:  tensor([ 0.0004, -0.0029])\n",
      "Epoch 745, Loss 0.262989\n",
      "\t Params:  tensor([0.8416, 0.5954])\n",
      "\t Grad:  tensor([ 0.0004, -0.0029])\n",
      "Epoch 746, Loss 0.262989\n",
      "\t Params:  tensor([0.8416, 0.5954])\n",
      "\t Grad:  tensor([ 0.0004, -0.0029])\n",
      "Epoch 747, Loss 0.262989\n",
      "\t Params:  tensor([0.8416, 0.5955])\n",
      "\t Grad:  tensor([ 0.0004, -0.0028])\n",
      "Epoch 748, Loss 0.262989\n",
      "\t Params:  tensor([0.8416, 0.5955])\n",
      "\t Grad:  tensor([ 0.0004, -0.0028])\n",
      "Epoch 749, Loss 0.262988\n",
      "\t Params:  tensor([0.8416, 0.5955])\n",
      "\t Grad:  tensor([ 0.0004, -0.0028])\n",
      "Epoch 750, Loss 0.262988\n",
      "\t Params:  tensor([0.8416, 0.5955])\n",
      "\t Grad:  tensor([ 0.0004, -0.0028])\n",
      "Epoch 751, Loss 0.262988\n",
      "\t Params:  tensor([0.8416, 0.5956])\n",
      "\t Grad:  tensor([ 0.0004, -0.0028])\n",
      "Epoch 752, Loss 0.262988\n",
      "\t Params:  tensor([0.8416, 0.5956])\n",
      "\t Grad:  tensor([ 0.0004, -0.0027])\n",
      "Epoch 753, Loss 0.262988\n",
      "\t Params:  tensor([0.8416, 0.5956])\n",
      "\t Grad:  tensor([ 0.0004, -0.0027])\n",
      "Epoch 754, Loss 0.262988\n",
      "\t Params:  tensor([0.8416, 0.5957])\n",
      "\t Grad:  tensor([ 0.0004, -0.0027])\n",
      "Epoch 755, Loss 0.262988\n",
      "\t Params:  tensor([0.8416, 0.5957])\n",
      "\t Grad:  tensor([ 0.0004, -0.0027])\n",
      "Epoch 756, Loss 0.262988\n",
      "\t Params:  tensor([0.8416, 0.5957])\n",
      "\t Grad:  tensor([ 0.0004, -0.0027])\n",
      "Epoch 757, Loss 0.262988\n",
      "\t Params:  tensor([0.8416, 0.5957])\n",
      "\t Grad:  tensor([ 0.0004, -0.0027])\n",
      "Epoch 758, Loss 0.262988\n",
      "\t Params:  tensor([0.8416, 0.5958])\n",
      "\t Grad:  tensor([ 0.0004, -0.0026])\n",
      "Epoch 759, Loss 0.262988\n",
      "\t Params:  tensor([0.8416, 0.5958])\n",
      "\t Grad:  tensor([ 0.0003, -0.0026])\n",
      "Epoch 760, Loss 0.262988\n",
      "\t Params:  tensor([0.8416, 0.5958])\n",
      "\t Grad:  tensor([ 0.0003, -0.0026])\n",
      "Epoch 761, Loss 0.262988\n",
      "\t Params:  tensor([0.8416, 0.5958])\n",
      "\t Grad:  tensor([ 0.0003, -0.0026])\n",
      "Epoch 762, Loss 0.262987\n",
      "\t Params:  tensor([0.8416, 0.5959])\n",
      "\t Grad:  tensor([ 0.0003, -0.0026])\n",
      "Epoch 763, Loss 0.262987\n",
      "\t Params:  tensor([0.8416, 0.5959])\n",
      "\t Grad:  tensor([ 0.0003, -0.0026])\n",
      "Epoch 764, Loss 0.262987\n",
      "\t Params:  tensor([0.8416, 0.5959])\n",
      "\t Grad:  tensor([ 0.0003, -0.0025])\n",
      "Epoch 765, Loss 0.262987\n",
      "\t Params:  tensor([0.8416, 0.5959])\n",
      "\t Grad:  tensor([ 0.0003, -0.0025])\n",
      "Epoch 766, Loss 0.262987\n",
      "\t Params:  tensor([0.8415, 0.5960])\n",
      "\t Grad:  tensor([ 0.0003, -0.0025])\n",
      "Epoch 767, Loss 0.262987\n",
      "\t Params:  tensor([0.8415, 0.5960])\n",
      "\t Grad:  tensor([ 0.0003, -0.0025])\n",
      "Epoch 768, Loss 0.262987\n",
      "\t Params:  tensor([0.8415, 0.5960])\n",
      "\t Grad:  tensor([ 0.0003, -0.0025])\n",
      "Epoch 769, Loss 0.262987\n",
      "\t Params:  tensor([0.8415, 0.5960])\n",
      "\t Grad:  tensor([ 0.0003, -0.0025])\n",
      "Epoch 770, Loss 0.262987\n",
      "\t Params:  tensor([0.8415, 0.5961])\n",
      "\t Grad:  tensor([ 0.0003, -0.0024])\n",
      "Epoch 771, Loss 0.262987\n",
      "\t Params:  tensor([0.8415, 0.5961])\n",
      "\t Grad:  tensor([ 0.0003, -0.0024])\n",
      "Epoch 772, Loss 0.262987\n",
      "\t Params:  tensor([0.8415, 0.5961])\n",
      "\t Grad:  tensor([ 0.0003, -0.0024])\n",
      "Epoch 773, Loss 0.262987\n",
      "\t Params:  tensor([0.8415, 0.5961])\n",
      "\t Grad:  tensor([ 0.0003, -0.0024])\n",
      "Epoch 774, Loss 0.262987\n",
      "\t Params:  tensor([0.8415, 0.5962])\n",
      "\t Grad:  tensor([ 0.0003, -0.0024])\n",
      "Epoch 775, Loss 0.262987\n",
      "\t Params:  tensor([0.8415, 0.5962])\n",
      "\t Grad:  tensor([ 0.0003, -0.0024])\n",
      "Epoch 776, Loss 0.262987\n",
      "\t Params:  tensor([0.8415, 0.5962])\n",
      "\t Grad:  tensor([ 0.0003, -0.0023])\n",
      "Epoch 777, Loss 0.262987\n",
      "\t Params:  tensor([0.8415, 0.5962])\n",
      "\t Grad:  tensor([ 0.0003, -0.0023])\n",
      "Epoch 778, Loss 0.262986\n",
      "\t Params:  tensor([0.8415, 0.5963])\n",
      "\t Grad:  tensor([ 0.0003, -0.0023])\n",
      "Epoch 779, Loss 0.262987\n",
      "\t Params:  tensor([0.8415, 0.5963])\n",
      "\t Grad:  tensor([ 0.0003, -0.0023])\n",
      "Epoch 780, Loss 0.262986\n",
      "\t Params:  tensor([0.8415, 0.5963])\n",
      "\t Grad:  tensor([ 0.0003, -0.0023])\n",
      "Epoch 781, Loss 0.262986\n",
      "\t Params:  tensor([0.8415, 0.5963])\n",
      "\t Grad:  tensor([ 0.0003, -0.0023])\n",
      "Epoch 782, Loss 0.262986\n",
      "\t Params:  tensor([0.8415, 0.5963])\n",
      "\t Grad:  tensor([ 0.0003, -0.0022])\n",
      "Epoch 783, Loss 0.262986\n",
      "\t Params:  tensor([0.8415, 0.5964])\n",
      "\t Grad:  tensor([ 0.0003, -0.0022])\n",
      "Epoch 784, Loss 0.262986\n",
      "\t Params:  tensor([0.8415, 0.5964])\n",
      "\t Grad:  tensor([ 0.0003, -0.0022])\n",
      "Epoch 785, Loss 0.262986\n",
      "\t Params:  tensor([0.8415, 0.5964])\n",
      "\t Grad:  tensor([ 0.0003, -0.0022])\n",
      "Epoch 786, Loss 0.262986\n",
      "\t Params:  tensor([0.8415, 0.5964])\n",
      "\t Grad:  tensor([ 0.0003, -0.0022])\n",
      "Epoch 787, Loss 0.262986\n",
      "\t Params:  tensor([0.8415, 0.5965])\n",
      "\t Grad:  tensor([ 0.0003, -0.0022])\n",
      "Epoch 788, Loss 0.262986\n",
      "\t Params:  tensor([0.8415, 0.5965])\n",
      "\t Grad:  tensor([ 0.0003, -0.0022])\n",
      "Epoch 789, Loss 0.262986\n",
      "\t Params:  tensor([0.8415, 0.5965])\n",
      "\t Grad:  tensor([ 0.0003, -0.0021])\n",
      "Epoch 790, Loss 0.262986\n",
      "\t Params:  tensor([0.8415, 0.5965])\n",
      "\t Grad:  tensor([ 0.0003, -0.0021])\n",
      "Epoch 791, Loss 0.262986\n",
      "\t Params:  tensor([0.8415, 0.5965])\n",
      "\t Grad:  tensor([ 0.0003, -0.0021])\n",
      "Epoch 792, Loss 0.262986\n",
      "\t Params:  tensor([0.8415, 0.5966])\n",
      "\t Grad:  tensor([ 0.0003, -0.0021])\n",
      "Epoch 793, Loss 0.262986\n",
      "\t Params:  tensor([0.8415, 0.5966])\n",
      "\t Grad:  tensor([ 0.0003, -0.0021])\n",
      "Epoch 794, Loss 0.262986\n",
      "\t Params:  tensor([0.8415, 0.5966])\n",
      "\t Grad:  tensor([ 0.0003, -0.0021])\n",
      "Epoch 795, Loss 0.262986\n",
      "\t Params:  tensor([0.8415, 0.5966])\n",
      "\t Grad:  tensor([ 0.0003, -0.0021])\n",
      "Epoch 796, Loss 0.262986\n",
      "\t Params:  tensor([0.8415, 0.5966])\n",
      "\t Grad:  tensor([ 0.0003, -0.0020])\n",
      "Epoch 797, Loss 0.262986\n",
      "\t Params:  tensor([0.8415, 0.5967])\n",
      "\t Grad:  tensor([ 0.0003, -0.0020])\n",
      "Epoch 798, Loss 0.262985\n",
      "\t Params:  tensor([0.8415, 0.5967])\n",
      "\t Grad:  tensor([ 0.0003, -0.0020])\n",
      "Epoch 799, Loss 0.262985\n",
      "\t Params:  tensor([0.8414, 0.5967])\n",
      "\t Grad:  tensor([ 0.0003, -0.0020])\n",
      "Epoch 800, Loss 0.262986\n",
      "\t Params:  tensor([0.8414, 0.5967])\n",
      "\t Grad:  tensor([ 0.0003, -0.0020])\n",
      "Epoch 801, Loss 0.262985\n",
      "\t Params:  tensor([0.8414, 0.5967])\n",
      "\t Grad:  tensor([ 0.0003, -0.0020])\n",
      "Epoch 802, Loss 0.262985\n",
      "\t Params:  tensor([0.8414, 0.5968])\n",
      "\t Grad:  tensor([ 0.0003, -0.0020])\n",
      "Epoch 803, Loss 0.262985\n",
      "\t Params:  tensor([0.8414, 0.5968])\n",
      "\t Grad:  tensor([ 0.0003, -0.0020])\n",
      "Epoch 804, Loss 0.262985\n",
      "\t Params:  tensor([0.8414, 0.5968])\n",
      "\t Grad:  tensor([ 0.0003, -0.0019])\n",
      "Epoch 805, Loss 0.262985\n",
      "\t Params:  tensor([0.8414, 0.5968])\n",
      "\t Grad:  tensor([ 0.0003, -0.0019])\n",
      "Epoch 806, Loss 0.262985\n",
      "\t Params:  tensor([0.8414, 0.5968])\n",
      "\t Grad:  tensor([ 0.0003, -0.0019])\n",
      "Epoch 807, Loss 0.262985\n",
      "\t Params:  tensor([0.8414, 0.5969])\n",
      "\t Grad:  tensor([ 0.0003, -0.0019])\n",
      "Epoch 808, Loss 0.262985\n",
      "\t Params:  tensor([0.8414, 0.5969])\n",
      "\t Grad:  tensor([ 0.0003, -0.0019])\n",
      "Epoch 809, Loss 0.262985\n",
      "\t Params:  tensor([0.8414, 0.5969])\n",
      "\t Grad:  tensor([ 0.0002, -0.0019])\n",
      "Epoch 810, Loss 0.262985\n",
      "\t Params:  tensor([0.8414, 0.5969])\n",
      "\t Grad:  tensor([ 0.0002, -0.0019])\n",
      "Epoch 811, Loss 0.262985\n",
      "\t Params:  tensor([0.8414, 0.5969])\n",
      "\t Grad:  tensor([ 0.0002, -0.0019])\n",
      "Epoch 812, Loss 0.262985\n",
      "\t Params:  tensor([0.8414, 0.5970])\n",
      "\t Grad:  tensor([ 0.0002, -0.0018])\n",
      "Epoch 813, Loss 0.262985\n",
      "\t Params:  tensor([0.8414, 0.5970])\n",
      "\t Grad:  tensor([ 0.0002, -0.0018])\n",
      "Epoch 814, Loss 0.262985\n",
      "\t Params:  tensor([0.8414, 0.5970])\n",
      "\t Grad:  tensor([ 0.0002, -0.0018])\n",
      "Epoch 815, Loss 0.262985\n",
      "\t Params:  tensor([0.8414, 0.5970])\n",
      "\t Grad:  tensor([ 0.0002, -0.0018])\n",
      "Epoch 816, Loss 0.262985\n",
      "\t Params:  tensor([0.8414, 0.5970])\n",
      "\t Grad:  tensor([ 0.0002, -0.0018])\n",
      "Epoch 817, Loss 0.262985\n",
      "\t Params:  tensor([0.8414, 0.5970])\n",
      "\t Grad:  tensor([ 0.0002, -0.0018])\n",
      "Epoch 818, Loss 0.262985\n",
      "\t Params:  tensor([0.8414, 0.5971])\n",
      "\t Grad:  tensor([ 0.0002, -0.0018])\n",
      "Epoch 819, Loss 0.262985\n",
      "\t Params:  tensor([0.8414, 0.5971])\n",
      "\t Grad:  tensor([ 0.0002, -0.0018])\n",
      "Epoch 820, Loss 0.262985\n",
      "\t Params:  tensor([0.8414, 0.5971])\n",
      "\t Grad:  tensor([ 0.0002, -0.0017])\n",
      "Epoch 821, Loss 0.262985\n",
      "\t Params:  tensor([0.8414, 0.5971])\n",
      "\t Grad:  tensor([ 0.0002, -0.0017])\n",
      "Epoch 822, Loss 0.262985\n",
      "\t Params:  tensor([0.8414, 0.5971])\n",
      "\t Grad:  tensor([ 0.0002, -0.0017])\n",
      "Epoch 823, Loss 0.262985\n",
      "\t Params:  tensor([0.8414, 0.5971])\n",
      "\t Grad:  tensor([ 0.0002, -0.0017])\n",
      "Epoch 824, Loss 0.262985\n",
      "\t Params:  tensor([0.8414, 0.5972])\n",
      "\t Grad:  tensor([ 0.0002, -0.0017])\n",
      "Epoch 825, Loss 0.262985\n",
      "\t Params:  tensor([0.8414, 0.5972])\n",
      "\t Grad:  tensor([ 0.0002, -0.0017])\n",
      "Epoch 826, Loss 0.262985\n",
      "\t Params:  tensor([0.8414, 0.5972])\n",
      "\t Grad:  tensor([ 0.0002, -0.0017])\n",
      "Epoch 827, Loss 0.262985\n",
      "\t Params:  tensor([0.8414, 0.5972])\n",
      "\t Grad:  tensor([ 0.0002, -0.0017])\n",
      "Epoch 828, Loss 0.262985\n",
      "\t Params:  tensor([0.8414, 0.5972])\n",
      "\t Grad:  tensor([ 0.0002, -0.0017])\n",
      "Epoch 829, Loss 0.262985\n",
      "\t Params:  tensor([0.8414, 0.5972])\n",
      "\t Grad:  tensor([ 0.0002, -0.0016])\n",
      "Epoch 830, Loss 0.262985\n",
      "\t Params:  tensor([0.8414, 0.5973])\n",
      "\t Grad:  tensor([ 0.0002, -0.0016])\n",
      "Epoch 831, Loss 0.262984\n",
      "\t Params:  tensor([0.8414, 0.5973])\n",
      "\t Grad:  tensor([ 0.0002, -0.0016])\n",
      "Epoch 832, Loss 0.262984\n",
      "\t Params:  tensor([0.8414, 0.5973])\n",
      "\t Grad:  tensor([ 0.0002, -0.0016])\n",
      "Epoch 833, Loss 0.262984\n",
      "\t Params:  tensor([0.8414, 0.5973])\n",
      "\t Grad:  tensor([ 0.0002, -0.0016])\n",
      "Epoch 834, Loss 0.262984\n",
      "\t Params:  tensor([0.8414, 0.5973])\n",
      "\t Grad:  tensor([ 0.0002, -0.0016])\n",
      "Epoch 835, Loss 0.262984\n",
      "\t Params:  tensor([0.8414, 0.5973])\n",
      "\t Grad:  tensor([ 0.0002, -0.0016])\n",
      "Epoch 836, Loss 0.262984\n",
      "\t Params:  tensor([0.8414, 0.5974])\n",
      "\t Grad:  tensor([ 0.0002, -0.0016])\n",
      "Epoch 837, Loss 0.262984\n",
      "\t Params:  tensor([0.8414, 0.5974])\n",
      "\t Grad:  tensor([ 0.0002, -0.0016])\n",
      "Epoch 838, Loss 0.262984\n",
      "\t Params:  tensor([0.8414, 0.5974])\n",
      "\t Grad:  tensor([ 0.0002, -0.0015])\n",
      "Epoch 839, Loss 0.262984\n",
      "\t Params:  tensor([0.8414, 0.5974])\n",
      "\t Grad:  tensor([ 0.0002, -0.0015])\n",
      "Epoch 840, Loss 0.262984\n",
      "\t Params:  tensor([0.8414, 0.5974])\n",
      "\t Grad:  tensor([ 0.0002, -0.0015])\n",
      "Epoch 841, Loss 0.262984\n",
      "\t Params:  tensor([0.8414, 0.5974])\n",
      "\t Grad:  tensor([ 0.0002, -0.0015])\n",
      "Epoch 842, Loss 0.262984\n",
      "\t Params:  tensor([0.8413, 0.5974])\n",
      "\t Grad:  tensor([ 0.0002, -0.0015])\n",
      "Epoch 843, Loss 0.262984\n",
      "\t Params:  tensor([0.8413, 0.5975])\n",
      "\t Grad:  tensor([ 0.0002, -0.0015])\n",
      "Epoch 844, Loss 0.262984\n",
      "\t Params:  tensor([0.8413, 0.5975])\n",
      "\t Grad:  tensor([ 0.0002, -0.0015])\n",
      "Epoch 845, Loss 0.262984\n",
      "\t Params:  tensor([0.8413, 0.5975])\n",
      "\t Grad:  tensor([ 0.0002, -0.0015])\n",
      "Epoch 846, Loss 0.262984\n",
      "\t Params:  tensor([0.8413, 0.5975])\n",
      "\t Grad:  tensor([ 0.0002, -0.0015])\n",
      "Epoch 847, Loss 0.262984\n",
      "\t Params:  tensor([0.8413, 0.5975])\n",
      "\t Grad:  tensor([ 0.0002, -0.0015])\n",
      "Epoch 848, Loss 0.262984\n",
      "\t Params:  tensor([0.8413, 0.5975])\n",
      "\t Grad:  tensor([ 0.0002, -0.0015])\n",
      "Epoch 849, Loss 0.262984\n",
      "\t Params:  tensor([0.8413, 0.5976])\n",
      "\t Grad:  tensor([ 0.0002, -0.0014])\n",
      "Epoch 850, Loss 0.262984\n",
      "\t Params:  tensor([0.8413, 0.5976])\n",
      "\t Grad:  tensor([ 0.0002, -0.0014])\n",
      "Epoch 851, Loss 0.262984\n",
      "\t Params:  tensor([0.8413, 0.5976])\n",
      "\t Grad:  tensor([ 0.0002, -0.0014])\n",
      "Epoch 852, Loss 0.262984\n",
      "\t Params:  tensor([0.8413, 0.5976])\n",
      "\t Grad:  tensor([ 0.0002, -0.0014])\n",
      "Epoch 853, Loss 0.262984\n",
      "\t Params:  tensor([0.8413, 0.5976])\n",
      "\t Grad:  tensor([ 0.0002, -0.0014])\n",
      "Epoch 854, Loss 0.262984\n",
      "\t Params:  tensor([0.8413, 0.5976])\n",
      "\t Grad:  tensor([ 0.0002, -0.0014])\n",
      "Epoch 855, Loss 0.262984\n",
      "\t Params:  tensor([0.8413, 0.5976])\n",
      "\t Grad:  tensor([ 0.0002, -0.0014])\n",
      "Epoch 856, Loss 0.262984\n",
      "\t Params:  tensor([0.8413, 0.5977])\n",
      "\t Grad:  tensor([ 0.0002, -0.0014])\n",
      "Epoch 857, Loss 0.262984\n",
      "\t Params:  tensor([0.8413, 0.5977])\n",
      "\t Grad:  tensor([ 0.0002, -0.0014])\n",
      "Epoch 858, Loss 0.262984\n",
      "\t Params:  tensor([0.8413, 0.5977])\n",
      "\t Grad:  tensor([ 0.0002, -0.0014])\n",
      "Epoch 859, Loss 0.262984\n",
      "\t Params:  tensor([0.8413, 0.5977])\n",
      "\t Grad:  tensor([ 0.0002, -0.0013])\n",
      "Epoch 860, Loss 0.262984\n",
      "\t Params:  tensor([0.8413, 0.5977])\n",
      "\t Grad:  tensor([ 0.0002, -0.0013])\n",
      "Epoch 861, Loss 0.262984\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Params:  tensor([0.8413, 0.5977])\n",
      "\t Grad:  tensor([ 0.0002, -0.0013])\n",
      "Epoch 862, Loss 0.262984\n",
      "\t Params:  tensor([0.8413, 0.5977])\n",
      "\t Grad:  tensor([ 0.0002, -0.0013])\n",
      "Epoch 863, Loss 0.262984\n",
      "\t Params:  tensor([0.8413, 0.5977])\n",
      "\t Grad:  tensor([ 0.0002, -0.0013])\n",
      "Epoch 864, Loss 0.262984\n",
      "\t Params:  tensor([0.8413, 0.5978])\n",
      "\t Grad:  tensor([ 0.0002, -0.0013])\n",
      "Epoch 865, Loss 0.262984\n",
      "\t Params:  tensor([0.8413, 0.5978])\n",
      "\t Grad:  tensor([ 0.0002, -0.0013])\n",
      "Epoch 866, Loss 0.262984\n",
      "\t Params:  tensor([0.8413, 0.5978])\n",
      "\t Grad:  tensor([ 0.0002, -0.0013])\n",
      "Epoch 867, Loss 0.262984\n",
      "\t Params:  tensor([0.8413, 0.5978])\n",
      "\t Grad:  tensor([ 0.0002, -0.0013])\n",
      "Epoch 868, Loss 0.262984\n",
      "\t Params:  tensor([0.8413, 0.5978])\n",
      "\t Grad:  tensor([ 0.0002, -0.0013])\n",
      "Epoch 869, Loss 0.262984\n",
      "\t Params:  tensor([0.8413, 0.5978])\n",
      "\t Grad:  tensor([ 0.0002, -0.0013])\n",
      "Epoch 870, Loss 0.262984\n",
      "\t Params:  tensor([0.8413, 0.5978])\n",
      "\t Grad:  tensor([ 0.0002, -0.0013])\n",
      "Epoch 871, Loss 0.262984\n",
      "\t Params:  tensor([0.8413, 0.5978])\n",
      "\t Grad:  tensor([ 0.0002, -0.0012])\n",
      "Epoch 872, Loss 0.262983\n",
      "\t Params:  tensor([0.8413, 0.5979])\n",
      "\t Grad:  tensor([ 0.0002, -0.0012])\n",
      "Epoch 873, Loss 0.262984\n",
      "\t Params:  tensor([0.8413, 0.5979])\n",
      "\t Grad:  tensor([ 0.0002, -0.0012])\n",
      "Epoch 874, Loss 0.262984\n",
      "\t Params:  tensor([0.8413, 0.5979])\n",
      "\t Grad:  tensor([ 0.0002, -0.0012])\n",
      "Epoch 875, Loss 0.262984\n",
      "\t Params:  tensor([0.8413, 0.5979])\n",
      "\t Grad:  tensor([ 0.0002, -0.0012])\n",
      "Epoch 876, Loss 0.262984\n",
      "\t Params:  tensor([0.8413, 0.5979])\n",
      "\t Grad:  tensor([ 0.0002, -0.0012])\n",
      "Epoch 877, Loss 0.262983\n",
      "\t Params:  tensor([0.8413, 0.5979])\n",
      "\t Grad:  tensor([ 0.0002, -0.0012])\n",
      "Epoch 878, Loss 0.262983\n",
      "\t Params:  tensor([0.8413, 0.5979])\n",
      "\t Grad:  tensor([ 0.0002, -0.0012])\n",
      "Epoch 879, Loss 0.262983\n",
      "\t Params:  tensor([0.8413, 0.5979])\n",
      "\t Grad:  tensor([ 0.0002, -0.0012])\n",
      "Epoch 880, Loss 0.262983\n",
      "\t Params:  tensor([0.8413, 0.5980])\n",
      "\t Grad:  tensor([ 0.0002, -0.0012])\n",
      "Epoch 881, Loss 0.262984\n",
      "\t Params:  tensor([0.8413, 0.5980])\n",
      "\t Grad:  tensor([ 0.0002, -0.0012])\n",
      "Epoch 882, Loss 0.262983\n",
      "\t Params:  tensor([0.8413, 0.5980])\n",
      "\t Grad:  tensor([ 0.0002, -0.0012])\n",
      "Epoch 883, Loss 0.262983\n",
      "\t Params:  tensor([0.8413, 0.5980])\n",
      "\t Grad:  tensor([ 0.0002, -0.0011])\n",
      "Epoch 884, Loss 0.262983\n",
      "\t Params:  tensor([0.8413, 0.5980])\n",
      "\t Grad:  tensor([ 0.0002, -0.0011])\n",
      "Epoch 885, Loss 0.262983\n",
      "\t Params:  tensor([0.8413, 0.5980])\n",
      "\t Grad:  tensor([ 0.0001, -0.0011])\n",
      "Epoch 886, Loss 0.262983\n",
      "\t Params:  tensor([0.8413, 0.5980])\n",
      "\t Grad:  tensor([ 0.0002, -0.0011])\n",
      "Epoch 887, Loss 0.262983\n",
      "\t Params:  tensor([0.8413, 0.5980])\n",
      "\t Grad:  tensor([ 0.0001, -0.0011])\n",
      "Epoch 888, Loss 0.262983\n",
      "\t Params:  tensor([0.8413, 0.5980])\n",
      "\t Grad:  tensor([ 0.0001, -0.0011])\n",
      "Epoch 889, Loss 0.262983\n",
      "\t Params:  tensor([0.8413, 0.5981])\n",
      "\t Grad:  tensor([ 0.0002, -0.0011])\n",
      "Epoch 890, Loss 0.262984\n",
      "\t Params:  tensor([0.8413, 0.5981])\n",
      "\t Grad:  tensor([ 0.0001, -0.0011])\n",
      "Epoch 891, Loss 0.262983\n",
      "\t Params:  tensor([0.8413, 0.5981])\n",
      "\t Grad:  tensor([ 0.0001, -0.0011])\n",
      "Epoch 892, Loss 0.262983\n",
      "\t Params:  tensor([0.8413, 0.5981])\n",
      "\t Grad:  tensor([ 0.0001, -0.0011])\n",
      "Epoch 893, Loss 0.262983\n",
      "\t Params:  tensor([0.8413, 0.5981])\n",
      "\t Grad:  tensor([ 0.0001, -0.0011])\n",
      "Epoch 894, Loss 0.262983\n",
      "\t Params:  tensor([0.8413, 0.5981])\n",
      "\t Grad:  tensor([ 0.0001, -0.0011])\n",
      "Epoch 895, Loss 0.262983\n",
      "\t Params:  tensor([0.8413, 0.5981])\n",
      "\t Grad:  tensor([ 0.0001, -0.0011])\n",
      "Epoch 896, Loss 0.262983\n",
      "\t Params:  tensor([0.8413, 0.5981])\n",
      "\t Grad:  tensor([ 0.0001, -0.0011])\n",
      "Epoch 897, Loss 0.262983\n",
      "\t Params:  tensor([0.8413, 0.5981])\n",
      "\t Grad:  tensor([ 0.0001, -0.0010])\n",
      "Epoch 898, Loss 0.262983\n",
      "\t Params:  tensor([0.8413, 0.5982])\n",
      "\t Grad:  tensor([ 0.0001, -0.0010])\n",
      "Epoch 899, Loss 0.262983\n",
      "\t Params:  tensor([0.8413, 0.5982])\n",
      "\t Grad:  tensor([ 0.0001, -0.0010])\n",
      "Epoch 900, Loss 0.262983\n",
      "\t Params:  tensor([0.8413, 0.5982])\n",
      "\t Grad:  tensor([ 0.0001, -0.0010])\n",
      "Epoch 901, Loss 0.262983\n",
      "\t Params:  tensor([0.8413, 0.5982])\n",
      "\t Grad:  tensor([ 0.0001, -0.0010])\n",
      "Epoch 902, Loss 0.262983\n",
      "\t Params:  tensor([0.8412, 0.5982])\n",
      "\t Grad:  tensor([ 0.0001, -0.0010])\n",
      "Epoch 903, Loss 0.262983\n",
      "\t Params:  tensor([0.8412, 0.5982])\n",
      "\t Grad:  tensor([ 0.0001, -0.0010])\n",
      "Epoch 904, Loss 0.262983\n",
      "\t Params:  tensor([0.8412, 0.5982])\n",
      "\t Grad:  tensor([ 0.0001, -0.0010])\n",
      "Epoch 905, Loss 0.262983\n",
      "\t Params:  tensor([0.8412, 0.5982])\n",
      "\t Grad:  tensor([ 0.0001, -0.0010])\n",
      "Epoch 906, Loss 0.262983\n",
      "\t Params:  tensor([0.8412, 0.5982])\n",
      "\t Grad:  tensor([ 0.0001, -0.0010])\n",
      "Epoch 907, Loss 0.262983\n",
      "\t Params:  tensor([0.8412, 0.5982])\n",
      "\t Grad:  tensor([ 0.0001, -0.0010])\n",
      "Epoch 908, Loss 0.262983\n",
      "\t Params:  tensor([0.8412, 0.5983])\n",
      "\t Grad:  tensor([ 0.0001, -0.0010])\n",
      "Epoch 909, Loss 0.262983\n",
      "\t Params:  tensor([0.8412, 0.5983])\n",
      "\t Grad:  tensor([ 0.0001, -0.0010])\n",
      "Epoch 910, Loss 0.262983\n",
      "\t Params:  tensor([0.8412, 0.5983])\n",
      "\t Grad:  tensor([ 0.0001, -0.0010])\n",
      "Epoch 911, Loss 0.262983\n",
      "\t Params:  tensor([0.8412, 0.5983])\n",
      "\t Grad:  tensor([ 0.0001, -0.0010])\n",
      "Epoch 912, Loss 0.262983\n",
      "\t Params:  tensor([0.8412, 0.5983])\n",
      "\t Grad:  tensor([ 0.0001, -0.0009])\n",
      "Epoch 913, Loss 0.262983\n",
      "\t Params:  tensor([0.8412, 0.5983])\n",
      "\t Grad:  tensor([ 0.0001, -0.0009])\n",
      "Epoch 914, Loss 0.262983\n",
      "\t Params:  tensor([0.8412, 0.5983])\n",
      "\t Grad:  tensor([ 0.0001, -0.0009])\n",
      "Epoch 915, Loss 0.262983\n",
      "\t Params:  tensor([0.8412, 0.5983])\n",
      "\t Grad:  tensor([ 0.0001, -0.0009])\n",
      "Epoch 916, Loss 0.262983\n",
      "\t Params:  tensor([0.8412, 0.5983])\n",
      "\t Grad:  tensor([ 0.0001, -0.0009])\n",
      "Epoch 917, Loss 0.262983\n",
      "\t Params:  tensor([0.8412, 0.5983])\n",
      "\t Grad:  tensor([ 0.0001, -0.0009])\n",
      "Epoch 918, Loss 0.262983\n",
      "\t Params:  tensor([0.8412, 0.5983])\n",
      "\t Grad:  tensor([ 0.0001, -0.0009])\n",
      "Epoch 919, Loss 0.262983\n",
      "\t Params:  tensor([0.8412, 0.5984])\n",
      "\t Grad:  tensor([ 0.0001, -0.0009])\n",
      "Epoch 920, Loss 0.262983\n",
      "\t Params:  tensor([0.8412, 0.5984])\n",
      "\t Grad:  tensor([ 0.0001, -0.0009])\n",
      "Epoch 921, Loss 0.262983\n",
      "\t Params:  tensor([0.8412, 0.5984])\n",
      "\t Grad:  tensor([ 0.0001, -0.0009])\n",
      "Epoch 922, Loss 0.262983\n",
      "\t Params:  tensor([0.8412, 0.5984])\n",
      "\t Grad:  tensor([ 0.0001, -0.0009])\n",
      "Epoch 923, Loss 0.262983\n",
      "\t Params:  tensor([0.8412, 0.5984])\n",
      "\t Grad:  tensor([ 0.0001, -0.0009])\n",
      "Epoch 924, Loss 0.262983\n",
      "\t Params:  tensor([0.8412, 0.5984])\n",
      "\t Grad:  tensor([ 0.0001, -0.0009])\n",
      "Epoch 925, Loss 0.262983\n",
      "\t Params:  tensor([0.8412, 0.5984])\n",
      "\t Grad:  tensor([ 0.0001, -0.0009])\n",
      "Epoch 926, Loss 0.262983\n",
      "\t Params:  tensor([0.8412, 0.5984])\n",
      "\t Grad:  tensor([ 0.0001, -0.0009])\n",
      "Epoch 927, Loss 0.262983\n",
      "\t Params:  tensor([0.8412, 0.5984])\n",
      "\t Grad:  tensor([ 0.0001, -0.0009])\n",
      "Epoch 928, Loss 0.262983\n",
      "\t Params:  tensor([0.8412, 0.5984])\n",
      "\t Grad:  tensor([ 0.0001, -0.0009])\n",
      "Epoch 929, Loss 0.262983\n",
      "\t Params:  tensor([0.8412, 0.5984])\n",
      "\t Grad:  tensor([ 0.0001, -0.0008])\n",
      "Epoch 930, Loss 0.262983\n",
      "\t Params:  tensor([0.8412, 0.5985])\n",
      "\t Grad:  tensor([ 0.0001, -0.0008])\n",
      "Epoch 931, Loss 0.262983\n",
      "\t Params:  tensor([0.8412, 0.5985])\n",
      "\t Grad:  tensor([ 0.0001, -0.0008])\n",
      "Epoch 932, Loss 0.262983\n",
      "\t Params:  tensor([0.8412, 0.5985])\n",
      "\t Grad:  tensor([ 0.0001, -0.0008])\n",
      "Epoch 933, Loss 0.262983\n",
      "\t Params:  tensor([0.8412, 0.5985])\n",
      "\t Grad:  tensor([ 0.0001, -0.0008])\n",
      "Epoch 934, Loss 0.262983\n",
      "\t Params:  tensor([0.8412, 0.5985])\n",
      "\t Grad:  tensor([ 0.0001, -0.0008])\n",
      "Epoch 935, Loss 0.262983\n",
      "\t Params:  tensor([0.8412, 0.5985])\n",
      "\t Grad:  tensor([ 0.0001, -0.0008])\n",
      "Epoch 936, Loss 0.262983\n",
      "\t Params:  tensor([0.8412, 0.5985])\n",
      "\t Grad:  tensor([ 0.0001, -0.0008])\n",
      "Epoch 937, Loss 0.262983\n",
      "\t Params:  tensor([0.8412, 0.5985])\n",
      "\t Grad:  tensor([ 0.0001, -0.0008])\n",
      "Epoch 938, Loss 0.262983\n",
      "\t Params:  tensor([0.8412, 0.5985])\n",
      "\t Grad:  tensor([ 0.0001, -0.0008])\n",
      "Epoch 939, Loss 0.262983\n",
      "\t Params:  tensor([0.8412, 0.5985])\n",
      "\t Grad:  tensor([ 0.0001, -0.0008])\n",
      "Epoch 940, Loss 0.262983\n",
      "\t Params:  tensor([0.8412, 0.5985])\n",
      "\t Grad:  tensor([ 0.0001, -0.0008])\n",
      "Epoch 941, Loss 0.262983\n",
      "\t Params:  tensor([0.8412, 0.5985])\n",
      "\t Grad:  tensor([ 0.0001, -0.0008])\n",
      "Epoch 942, Loss 0.262983\n",
      "\t Params:  tensor([0.8412, 0.5985])\n",
      "\t Grad:  tensor([ 0.0001, -0.0008])\n",
      "Epoch 943, Loss 0.262983\n",
      "\t Params:  tensor([0.8412, 0.5986])\n",
      "\t Grad:  tensor([ 0.0001, -0.0008])\n",
      "Epoch 944, Loss 0.262983\n",
      "\t Params:  tensor([0.8412, 0.5986])\n",
      "\t Grad:  tensor([ 0.0001, -0.0008])\n",
      "Epoch 945, Loss 0.262983\n",
      "\t Params:  tensor([0.8412, 0.5986])\n",
      "\t Grad:  tensor([ 0.0001, -0.0008])\n",
      "Epoch 946, Loss 0.262983\n",
      "\t Params:  tensor([0.8412, 0.5986])\n",
      "\t Grad:  tensor([ 0.0001, -0.0008])\n",
      "Epoch 947, Loss 0.262983\n",
      "\t Params:  tensor([0.8412, 0.5986])\n",
      "\t Grad:  tensor([ 0.0001, -0.0008])\n",
      "Epoch 948, Loss 0.262983\n",
      "\t Params:  tensor([0.8412, 0.5986])\n",
      "\t Grad:  tensor([ 0.0001, -0.0007])\n",
      "Epoch 949, Loss 0.262983\n",
      "\t Params:  tensor([0.8412, 0.5986])\n",
      "\t Grad:  tensor([ 9.8348e-05, -7.4108e-04])\n",
      "Epoch 950, Loss 0.262983\n",
      "\t Params:  tensor([0.8412, 0.5986])\n",
      "\t Grad:  tensor([ 9.5844e-05, -7.3629e-04])\n",
      "Epoch 951, Loss 0.262983\n",
      "\t Params:  tensor([0.8412, 0.5986])\n",
      "\t Grad:  tensor([ 9.5069e-05, -7.3139e-04])\n",
      "Epoch 952, Loss 0.262983\n",
      "\t Params:  tensor([0.8412, 0.5986])\n",
      "\t Grad:  tensor([ 0.0001, -0.0007])\n",
      "Epoch 953, Loss 0.262983\n",
      "\t Params:  tensor([0.8412, 0.5986])\n",
      "\t Grad:  tensor([ 9.2506e-05, -7.2181e-04])\n",
      "Epoch 954, Loss 0.262983\n",
      "\t Params:  tensor([0.8412, 0.5986])\n",
      "\t Grad:  tensor([ 9.6619e-05, -7.1650e-04])\n",
      "Epoch 955, Loss 0.262983\n",
      "\t Params:  tensor([0.8412, 0.5986])\n",
      "\t Grad:  tensor([ 9.3102e-05, -7.1204e-04])\n",
      "Epoch 956, Loss 0.262983\n",
      "\t Params:  tensor([0.8412, 0.5987])\n",
      "\t Grad:  tensor([ 9.5487e-05, -7.0708e-04])\n",
      "Epoch 957, Loss 0.262983\n",
      "\t Params:  tensor([0.8412, 0.5987])\n",
      "\t Grad:  tensor([ 9.1553e-05, -7.0278e-04])\n",
      "Epoch 958, Loss 0.262983\n",
      "\t Params:  tensor([0.8412, 0.5987])\n",
      "\t Grad:  tensor([ 9.5665e-05, -6.9757e-04])\n",
      "Epoch 959, Loss 0.262983\n",
      "\t Params:  tensor([0.8412, 0.5987])\n",
      "\t Grad:  tensor([ 9.2626e-05, -6.9327e-04])\n",
      "Epoch 960, Loss 0.262983\n",
      "\t Params:  tensor([0.8412, 0.5987])\n",
      "\t Grad:  tensor([ 9.0599e-05, -6.8879e-04])\n",
      "Epoch 961, Loss 0.262983\n",
      "\t Params:  tensor([0.8412, 0.5987])\n",
      "\t Grad:  tensor([ 9.1970e-05, -6.8394e-04])\n",
      "Epoch 962, Loss 0.262983\n",
      "\t Params:  tensor([0.8412, 0.5987])\n",
      "\t Grad:  tensor([ 9.2924e-05, -6.7926e-04])\n",
      "Epoch 963, Loss 0.262983\n",
      "\t Params:  tensor([0.8412, 0.5987])\n",
      "\t Grad:  tensor([ 8.9884e-05, -6.7502e-04])\n",
      "Epoch 964, Loss 0.262983\n",
      "\t Params:  tensor([0.8412, 0.5987])\n",
      "\t Grad:  tensor([ 8.9347e-05, -6.7058e-04])\n",
      "Epoch 965, Loss 0.262983\n",
      "\t Params:  tensor([0.8412, 0.5987])\n",
      "\t Grad:  tensor([ 8.8751e-05, -6.6607e-04])\n",
      "Epoch 966, Loss 0.262983\n",
      "\t Params:  tensor([0.8412, 0.5987])\n",
      "\t Grad:  tensor([ 8.7261e-05, -6.6175e-04])\n",
      "Epoch 967, Loss 0.262983\n",
      "\t Params:  tensor([0.8412, 0.5987])\n",
      "\t Grad:  tensor([ 8.7917e-05, -6.5726e-04])\n",
      "Epoch 968, Loss 0.262983\n",
      "\t Params:  tensor([0.8412, 0.5987])\n",
      "\t Grad:  tensor([ 8.6784e-05, -6.5302e-04])\n",
      "Epoch 969, Loss 0.262983\n",
      "\t Params:  tensor([0.8412, 0.5987])\n",
      "\t Grad:  tensor([ 8.3506e-05, -6.4907e-04])\n",
      "Epoch 970, Loss 0.262983\n",
      "\t Params:  tensor([0.8412, 0.5987])\n",
      "\t Grad:  tensor([ 8.3148e-05, -6.4461e-04])\n",
      "Epoch 971, Loss 0.262983\n",
      "\t Params:  tensor([0.8412, 0.5988])\n",
      "\t Grad:  tensor([ 8.5950e-05, -6.3997e-04])\n",
      "Epoch 972, Loss 0.262983\n",
      "\t Params:  tensor([0.8412, 0.5988])\n",
      "\t Grad:  tensor([ 8.8453e-05, -6.3530e-04])\n",
      "Epoch 973, Loss 0.262983\n",
      "\t Params:  tensor([0.8412, 0.5988])\n",
      "\t Grad:  tensor([ 8.5056e-05, -6.3155e-04])\n",
      "Epoch 974, Loss 0.262983\n",
      "\t Params:  tensor([0.8412, 0.5988])\n",
      "\t Grad:  tensor([ 8.1778e-05, -6.2758e-04])\n",
      "Epoch 975, Loss 0.262983\n",
      "\t Params:  tensor([0.8412, 0.5988])\n",
      "\t Grad:  tensor([ 8.4221e-05, -6.2314e-04])\n",
      "Epoch 976, Loss 0.262983\n",
      "\t Params:  tensor([0.8412, 0.5988])\n",
      "\t Grad:  tensor([ 8.2850e-05, -6.1906e-04])\n",
      "Epoch 977, Loss 0.262983\n",
      "\t Params:  tensor([0.8412, 0.5988])\n",
      "\t Grad:  tensor([ 8.3148e-05, -6.1484e-04])\n",
      "Epoch 978, Loss 0.262983\n",
      "\t Params:  tensor([0.8412, 0.5988])\n",
      "\t Grad:  tensor([ 7.9036e-05, -6.1122e-04])\n",
      "Epoch 979, Loss 0.262983\n",
      "\t Params:  tensor([0.8412, 0.5988])\n",
      "\t Grad:  tensor([ 8.2493e-05, -6.0672e-04])\n",
      "Epoch 980, Loss 0.262983\n",
      "\t Params:  tensor([0.8412, 0.5988])\n",
      "\t Grad:  tensor([ 7.8857e-05, -6.0302e-04])\n",
      "Epoch 981, Loss 0.262983\n",
      "\t Params:  tensor([0.8412, 0.5988])\n",
      "\t Grad:  tensor([ 8.3148e-05, -5.9841e-04])\n",
      "Epoch 982, Loss 0.262983\n",
      "\t Params:  tensor([0.8412, 0.5988])\n",
      "\t Grad:  tensor([ 7.9393e-05, -5.9495e-04])\n",
      "Epoch 983, Loss 0.262983\n",
      "\t Params:  tensor([0.8412, 0.5988])\n",
      "\t Grad:  tensor([ 8.1897e-05, -5.9059e-04])\n",
      "Epoch 984, Loss 0.262983\n",
      "\t Params:  tensor([0.8412, 0.5988])\n",
      "\t Grad:  tensor([ 7.4744e-05, -5.8749e-04])\n",
      "Epoch 985, Loss 0.262983\n",
      "\t Params:  tensor([0.8412, 0.5988])\n",
      "\t Grad:  tensor([ 7.6354e-05, -5.8331e-04])\n",
      "Epoch 986, Loss 0.262983\n",
      "\t Params:  tensor([0.8412, 0.5988])\n",
      "\t Grad:  tensor([ 7.5579e-05, -5.7938e-04])\n",
      "Epoch 987, Loss 0.262983\n",
      "\t Params:  tensor([0.8412, 0.5988])\n",
      "\t Grad:  tensor([ 7.4983e-05, -5.7555e-04])\n",
      "Epoch 988, Loss 0.262983\n",
      "\t Params:  tensor([0.8412, 0.5989])\n",
      "\t Grad:  tensor([ 7.8440e-05, -5.7139e-04])\n",
      "Epoch 989, Loss 0.262983\n",
      "\t Params:  tensor([0.8412, 0.5989])\n",
      "\t Grad:  tensor([ 7.4208e-05, -5.6801e-04])\n",
      "Epoch 990, Loss 0.262983\n",
      "\t Params:  tensor([0.8412, 0.5989])\n",
      "\t Grad:  tensor([ 7.5877e-05, -5.6387e-04])\n",
      "Epoch 991, Loss 0.262983\n",
      "\t Params:  tensor([0.8412, 0.5989])\n",
      "\t Grad:  tensor([ 7.4685e-05, -5.6029e-04])\n",
      "Epoch 992, Loss 0.262983\n",
      "\t Params:  tensor([0.8412, 0.5989])\n",
      "\t Grad:  tensor([ 7.0751e-05, -5.5705e-04])\n",
      "Epoch 993, Loss 0.262983\n",
      "\t Params:  tensor([0.8412, 0.5989])\n",
      "\t Grad:  tensor([ 7.3373e-05, -5.5298e-04])\n",
      "Epoch 994, Loss 0.262983\n",
      "\t Params:  tensor([0.8412, 0.5989])\n",
      "\t Grad:  tensor([ 7.7307e-05, -5.4870e-04])\n",
      "Epoch 995, Loss 0.262983\n",
      "\t Params:  tensor([0.8412, 0.5989])\n",
      "\t Grad:  tensor([ 7.2122e-05, -5.4572e-04])\n",
      "Epoch 996, Loss 0.262983\n",
      "\t Params:  tensor([0.8412, 0.5989])\n",
      "\t Grad:  tensor([ 7.1526e-05, -5.4202e-04])\n",
      "Epoch 997, Loss 0.262983\n",
      "\t Params:  tensor([0.8412, 0.5989])\n",
      "\t Grad:  tensor([ 7.2956e-05, -5.3823e-04])\n",
      "Epoch 998, Loss 0.262983\n",
      "\t Params:  tensor([0.8412, 0.5989])\n",
      "\t Grad:  tensor([ 7.3016e-05, -5.3463e-04])\n",
      "Epoch 999, Loss 0.262983\n",
      "\t Params:  tensor([0.8412, 0.5989])\n",
      "\t Grad:  tensor([ 7.2479e-05, -5.3101e-04])\n",
      "Epoch 1000, Loss 0.262983\n",
      "\t Params:  tensor([0.8412, 0.5989])\n",
      "\t Grad:  tensor([ 7.3373e-05, -5.2740e-04])\n",
      "Epoch 1001, Loss 0.262983\n",
      "\t Params:  tensor([0.8412, 0.5989])\n",
      "\t Grad:  tensor([ 7.0810e-05, -5.2416e-04])\n",
      "Epoch 1002, Loss 0.262983\n",
      "\t Params:  tensor([0.8412, 0.5989])\n",
      "\t Grad:  tensor([ 7.0989e-05, -5.2063e-04])\n",
      "Epoch 1003, Loss 0.262983\n",
      "\t Params:  tensor([0.8411, 0.5989])\n",
      "\t Grad:  tensor([ 6.7949e-05, -5.1751e-04])\n",
      "Epoch 1004, Loss 0.262983\n",
      "\t Params:  tensor([0.8411, 0.5989])\n",
      "\t Grad:  tensor([ 6.9320e-05, -5.1384e-04])\n",
      "Epoch 1005, Loss 0.262983\n",
      "\t Params:  tensor([0.8411, 0.5989])\n",
      "\t Grad:  tensor([ 6.4433e-05, -5.1095e-04])\n",
      "Epoch 1006, Loss 0.262983\n",
      "\t Params:  tensor([0.8411, 0.5990])\n",
      "\t Grad:  tensor([ 6.6042e-05, -5.0728e-04])\n",
      "Epoch 1007, Loss 0.262983\n",
      "\t Params:  tensor([0.8411, 0.5990])\n",
      "\t Grad:  tensor([ 7.2122e-05, -5.0320e-04])\n",
      "Epoch 1008, Loss 0.262983\n",
      "\t Params:  tensor([0.8411, 0.5990])\n",
      "\t Grad:  tensor([ 6.6400e-05, -5.0062e-04])\n",
      "Epoch 1009, Loss 0.262983\n",
      "\t Params:  tensor([0.8411, 0.5990])\n",
      "\t Grad:  tensor([ 6.7294e-05, -4.9700e-04])\n",
      "Epoch 1010, Loss 0.262983\n",
      "\t Params:  tensor([0.8411, 0.5990])\n",
      "\t Grad:  tensor([ 6.4671e-05, -4.9403e-04])\n",
      "Epoch 1011, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5990])\n",
      "\t Grad:  tensor([ 6.6757e-05, -4.9046e-04])\n",
      "Epoch 1012, Loss 0.262983\n",
      "\t Params:  tensor([0.8411, 0.5990])\n",
      "\t Grad:  tensor([ 6.7174e-05, -4.8719e-04])\n",
      "Epoch 1013, Loss 0.262983\n",
      "\t Params:  tensor([0.8411, 0.5990])\n",
      "\t Grad:  tensor([ 6.5565e-05, -4.8397e-04])\n",
      "Epoch 1014, Loss 0.262983\n",
      "\t Params:  tensor([0.8411, 0.5990])\n",
      "\t Grad:  tensor([ 6.3837e-05, -4.8098e-04])\n",
      "Epoch 1015, Loss 0.262983\n",
      "\t Params:  tensor([0.8411, 0.5990])\n",
      "\t Grad:  tensor([ 6.1870e-05, -4.7799e-04])\n",
      "Epoch 1016, Loss 0.262983\n",
      "\t Params:  tensor([0.8411, 0.5990])\n",
      "\t Grad:  tensor([ 6.4552e-05, -4.7440e-04])\n",
      "Epoch 1017, Loss 0.262983\n",
      "\t Params:  tensor([0.8411, 0.5990])\n",
      "\t Grad:  tensor([ 6.3598e-05, -4.7140e-04])\n",
      "Epoch 1018, Loss 0.262983\n",
      "\t Params:  tensor([0.8411, 0.5990])\n",
      "\t Grad:  tensor([ 5.9664e-05, -4.6859e-04])\n",
      "Epoch 1019, Loss 0.262983\n",
      "\t Params:  tensor([0.8411, 0.5990])\n",
      "\t Grad:  tensor([ 6.3419e-05, -4.6500e-04])\n",
      "Epoch 1020, Loss 0.262983\n",
      "\t Params:  tensor([0.8411, 0.5990])\n",
      "\t Grad:  tensor([ 6.3717e-05, -4.6188e-04])\n",
      "Epoch 1021, Loss 0.262983\n",
      "\t Params:  tensor([0.8411, 0.5990])\n",
      "\t Grad:  tensor([ 5.6505e-05, -4.5969e-04])\n",
      "Epoch 1022, Loss 0.262983\n",
      "\t Params:  tensor([0.8411, 0.5990])\n",
      "\t Grad:  tensor([ 6.0976e-05, -4.5587e-04])\n",
      "Epoch 1023, Loss 0.262983\n",
      "\t Params:  tensor([0.8411, 0.5990])\n",
      "\t Grad:  tensor([ 6.3002e-05, -4.5268e-04])\n",
      "Epoch 1024, Loss 0.262983\n",
      "\t Params:  tensor([0.8411, 0.5990])\n",
      "\t Grad:  tensor([ 5.8055e-05, -4.5023e-04])\n",
      "Epoch 1025, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5990])\n",
      "\t Grad:  tensor([ 5.9783e-05, -4.4702e-04])\n",
      "Epoch 1026, Loss 0.262983\n",
      "\t Params:  tensor([0.8411, 0.5990])\n",
      "\t Grad:  tensor([ 5.7280e-05, -4.4425e-04])\n",
      "Epoch 1027, Loss 0.262983\n",
      "\t Params:  tensor([0.8411, 0.5991])\n",
      "\t Grad:  tensor([ 5.6028e-05, -4.4142e-04])\n",
      "Epoch 1028, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5991])\n",
      "\t Grad:  tensor([ 6.1691e-05, -4.3778e-04])\n",
      "Epoch 1029, Loss 0.262983\n",
      "\t Params:  tensor([0.8411, 0.5991])\n",
      "\t Grad:  tensor([ 6.0380e-05, -4.3499e-04])\n",
      "Epoch 1030, Loss 0.262983\n",
      "\t Params:  tensor([0.8411, 0.5991])\n",
      "\t Grad:  tensor([ 5.9903e-05, -4.3211e-04])\n",
      "Epoch 1031, Loss 0.262983\n",
      "\t Params:  tensor([0.8411, 0.5991])\n",
      "\t Grad:  tensor([ 5.6207e-05, -4.2967e-04])\n",
      "Epoch 1032, Loss 0.262983\n",
      "\t Params:  tensor([0.8411, 0.5991])\n",
      "\t Grad:  tensor([ 6.1333e-05, -4.2616e-04])\n",
      "Epoch 1033, Loss 0.262983\n",
      "\t Params:  tensor([0.8411, 0.5991])\n",
      "\t Grad:  tensor([ 5.6505e-05, -4.2392e-04])\n",
      "Epoch 1034, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5991])\n",
      "\t Grad:  tensor([ 5.8293e-05, -4.2088e-04])\n",
      "Epoch 1035, Loss 0.262983\n",
      "\t Params:  tensor([0.8411, 0.5991])\n",
      "\t Grad:  tensor([ 5.4002e-05, -4.1849e-04])\n",
      "Epoch 1036, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5991])\n",
      "\t Grad:  tensor([ 5.6624e-05, -4.1538e-04])\n",
      "Epoch 1037, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5991])\n",
      "\t Grad:  tensor([ 5.5850e-05, -4.1265e-04])\n",
      "Epoch 1038, Loss 0.262983\n",
      "\t Params:  tensor([0.8411, 0.5991])\n",
      "\t Grad:  tensor([ 5.2869e-05, -4.1027e-04])\n",
      "Epoch 1039, Loss 0.262983\n",
      "\t Params:  tensor([0.8411, 0.5991])\n",
      "\t Grad:  tensor([ 5.3585e-05, -4.0741e-04])\n",
      "Epoch 1040, Loss 0.262983\n",
      "\t Params:  tensor([0.8411, 0.5991])\n",
      "\t Grad:  tensor([ 5.7757e-05, -4.0422e-04])\n",
      "Epoch 1041, Loss 0.262983\n",
      "\t Params:  tensor([0.8411, 0.5991])\n",
      "\t Grad:  tensor([ 5.2094e-05, -4.0214e-04])\n",
      "Epoch 1042, Loss 0.262983\n",
      "\t Params:  tensor([0.8411, 0.5991])\n",
      "\t Grad:  tensor([ 4.8161e-05, -3.9992e-04])\n",
      "Epoch 1043, Loss 0.262983\n",
      "\t Params:  tensor([0.8411, 0.5991])\n",
      "\t Grad:  tensor([ 5.3108e-05, -3.9655e-04])\n",
      "Epoch 1044, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5991])\n",
      "\t Grad:  tensor([ 5.4419e-05, -3.9387e-04])\n",
      "Epoch 1045, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5991])\n",
      "\t Grad:  tensor([ 5.3644e-05, -3.9127e-04])\n",
      "Epoch 1046, Loss 0.262983\n",
      "\t Params:  tensor([0.8411, 0.5991])\n",
      "\t Grad:  tensor([ 5.2929e-05, -3.8866e-04])\n",
      "Epoch 1047, Loss 0.262983\n",
      "\t Params:  tensor([0.8411, 0.5991])\n",
      "\t Grad:  tensor([ 5.0366e-05, -3.8642e-04])\n",
      "Epoch 1048, Loss 0.262983\n",
      "\t Params:  tensor([0.8411, 0.5991])\n",
      "\t Grad:  tensor([ 5.3883e-05, -3.8331e-04])\n",
      "Epoch 1049, Loss 0.262983\n",
      "\t Params:  tensor([0.8411, 0.5991])\n",
      "\t Grad:  tensor([ 5.2094e-05, -3.8105e-04])\n",
      "Epoch 1050, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5991])\n",
      "\t Grad:  tensor([ 4.8518e-05, -3.7888e-04])\n",
      "Epoch 1051, Loss 0.262983\n",
      "\t Params:  tensor([0.8411, 0.5991])\n",
      "\t Grad:  tensor([ 5.0962e-05, -3.7596e-04])\n",
      "Epoch 1052, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5992])\n",
      "\t Grad:  tensor([ 4.6670e-05, -3.7396e-04])\n",
      "Epoch 1053, Loss 0.262983\n",
      "\t Params:  tensor([0.8411, 0.5992])\n",
      "\t Grad:  tensor([ 4.8995e-05, -3.7117e-04])\n",
      "Epoch 1054, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5992])\n",
      "\t Grad:  tensor([ 5.0902e-05, -3.6850e-04])\n",
      "Epoch 1055, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5992])\n",
      "\t Grad:  tensor([ 4.6849e-05, -3.6659e-04])\n",
      "Epoch 1056, Loss 0.262983\n",
      "\t Params:  tensor([0.8411, 0.5992])\n",
      "\t Grad:  tensor([ 4.4525e-05, -3.6414e-04])\n",
      "Epoch 1057, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5992])\n",
      "\t Grad:  tensor([ 5.0366e-05, -3.6102e-04])\n",
      "Epoch 1058, Loss 0.262983\n",
      "\t Params:  tensor([0.8411, 0.5992])\n",
      "\t Grad:  tensor([ 5.0426e-05, -3.5854e-04])\n",
      "Epoch 1059, Loss 0.262983\n",
      "\t Params:  tensor([0.8411, 0.5992])\n",
      "\t Grad:  tensor([ 5.3287e-05, -3.5588e-04])\n",
      "Epoch 1060, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5992])\n",
      "\t Grad:  tensor([ 4.8339e-05, -3.5410e-04])\n",
      "Epoch 1061, Loss 0.262983\n",
      "\t Params:  tensor([0.8411, 0.5992])\n",
      "\t Grad:  tensor([ 4.5419e-05, -3.5203e-04])\n",
      "Epoch 1062, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5992])\n",
      "\t Grad:  tensor([ 4.7088e-05, -3.4950e-04])\n",
      "Epoch 1063, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5992])\n",
      "\t Grad:  tensor([ 4.4525e-05, -3.4749e-04])\n",
      "Epoch 1064, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5992])\n",
      "\t Grad:  tensor([ 4.7624e-05, -3.4475e-04])\n",
      "Epoch 1065, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5992])\n",
      "\t Grad:  tensor([ 4.7445e-05, -3.4236e-04])\n",
      "Epoch 1066, Loss 0.262983\n",
      "\t Params:  tensor([0.8411, 0.5992])\n",
      "\t Grad:  tensor([ 4.3273e-05, -3.4058e-04])\n",
      "Epoch 1067, Loss 0.262983\n",
      "\t Params:  tensor([0.8411, 0.5992])\n",
      "\t Grad:  tensor([ 4.6670e-05, -3.3788e-04])\n",
      "Epoch 1068, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5992])\n",
      "\t Grad:  tensor([ 4.5657e-05, -3.3587e-04])\n",
      "Epoch 1069, Loss 0.262983\n",
      "\t Params:  tensor([0.8411, 0.5992])\n",
      "\t Grad:  tensor([ 4.2439e-05, -3.3396e-04])\n",
      "Epoch 1070, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5992])\n",
      "\t Grad:  tensor([ 4.5657e-05, -3.3134e-04])\n",
      "Epoch 1071, Loss 0.262983\n",
      "\t Params:  tensor([0.8411, 0.5992])\n",
      "\t Grad:  tensor([ 4.1723e-05, -3.2958e-04])\n",
      "Epoch 1072, Loss 0.262983\n",
      "\t Params:  tensor([0.8411, 0.5992])\n",
      "\t Grad:  tensor([ 3.9935e-05, -3.2748e-04])\n",
      "Epoch 1073, Loss 0.262983\n",
      "\t Params:  tensor([0.8411, 0.5992])\n",
      "\t Grad:  tensor([ 4.3869e-05, -3.2480e-04])\n",
      "Epoch 1074, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5992])\n",
      "\t Grad:  tensor([ 4.5061e-05, -3.2249e-04])\n",
      "Epoch 1075, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5992])\n",
      "\t Grad:  tensor([ 4.0710e-05, -3.2094e-04])\n",
      "Epoch 1076, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5992])\n",
      "\t Grad:  tensor([ 4.1962e-05, -3.1862e-04])\n",
      "Epoch 1077, Loss 0.262983\n",
      "\t Params:  tensor([0.8411, 0.5992])\n",
      "\t Grad:  tensor([ 4.2081e-05, -3.1637e-04])\n",
      "Epoch 1078, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5992])\n",
      "\t Grad:  tensor([ 4.2200e-05, -3.1429e-04])\n",
      "Epoch 1079, Loss 0.262983\n",
      "\t Params:  tensor([0.8411, 0.5992])\n",
      "\t Grad:  tensor([ 4.2200e-05, -3.1213e-04])\n",
      "Epoch 1080, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5992])\n",
      "\t Grad:  tensor([ 3.9995e-05, -3.1028e-04])\n",
      "Epoch 1081, Loss 0.262983\n",
      "\t Params:  tensor([0.8411, 0.5993])\n",
      "\t Grad:  tensor([ 4.0829e-05, -3.0813e-04])\n",
      "Epoch 1082, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5993])\n",
      "\t Grad:  tensor([ 4.1902e-05, -3.0598e-04])\n",
      "Epoch 1083, Loss 0.262983\n",
      "\t Params:  tensor([0.8411, 0.5993])\n",
      "\t Grad:  tensor([ 3.9697e-05, -3.0410e-04])\n",
      "Epoch 1084, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5993])\n",
      "\t Grad:  tensor([ 3.7372e-05, -3.0234e-04])\n",
      "Epoch 1085, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5993])\n",
      "\t Grad:  tensor([ 4.3035e-05, -2.9960e-04])\n",
      "Epoch 1086, Loss 0.262983\n",
      "\t Params:  tensor([0.8411, 0.5993])\n",
      "\t Grad:  tensor([ 3.8147e-05, -2.9817e-04])\n",
      "Epoch 1087, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5993])\n",
      "\t Grad:  tensor([ 4.4346e-05, -2.9551e-04])\n",
      "Epoch 1088, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5993])\n",
      "\t Grad:  tensor([ 4.1306e-05, -2.9387e-04])\n",
      "Epoch 1089, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5993])\n",
      "\t Grad:  tensor([ 3.9518e-05, -2.9211e-04])\n",
      "Epoch 1090, Loss 0.262983\n",
      "\t Params:  tensor([0.8411, 0.5993])\n",
      "\t Grad:  tensor([ 3.7909e-05, -2.9033e-04])\n",
      "Epoch 1091, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5993])\n",
      "\t Grad:  tensor([ 4.0412e-05, -2.8802e-04])\n",
      "Epoch 1092, Loss 0.262983\n",
      "\t Params:  tensor([0.8411, 0.5993])\n",
      "\t Grad:  tensor([ 3.7611e-05, -2.8646e-04])\n",
      "Epoch 1093, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5993])\n",
      "\t Grad:  tensor([ 3.7968e-05, -2.8452e-04])\n",
      "Epoch 1094, Loss 0.262983\n",
      "\t Params:  tensor([0.8411, 0.5993])\n",
      "\t Grad:  tensor([ 4.1306e-05, -2.8215e-04])\n",
      "Epoch 1095, Loss 0.262983\n",
      "\t Params:  tensor([0.8411, 0.5993])\n",
      "\t Grad:  tensor([ 3.7551e-05, -2.8076e-04])\n",
      "Epoch 1096, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5993])\n",
      "\t Grad:  tensor([ 3.8385e-05, -2.7880e-04])\n",
      "Epoch 1097, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5993])\n",
      "\t Grad:  tensor([ 3.8445e-05, -2.7688e-04])\n",
      "Epoch 1098, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5993])\n",
      "\t Grad:  tensor([ 3.8803e-05, -2.7496e-04])\n",
      "Epoch 1099, Loss 0.262983\n",
      "\t Params:  tensor([0.8411, 0.5993])\n",
      "\t Grad:  tensor([ 3.2604e-05, -2.7384e-04])\n",
      "Epoch 1100, Loss 0.262983\n",
      "\t Params:  tensor([0.8411, 0.5993])\n",
      "\t Grad:  tensor([ 3.8564e-05, -2.7121e-04])\n",
      "Epoch 1101, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5993])\n",
      "\t Grad:  tensor([ 3.9577e-05, -2.6929e-04])\n",
      "Epoch 1102, Loss 0.262983\n",
      "\t Params:  tensor([0.8411, 0.5993])\n",
      "\t Grad:  tensor([ 3.3617e-05, -2.6826e-04])\n",
      "Epoch 1103, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5993])\n",
      "\t Grad:  tensor([ 3.6180e-05, -2.6612e-04])\n",
      "Epoch 1104, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5993])\n",
      "\t Grad:  tensor([ 3.4630e-05, -2.6456e-04])\n",
      "Epoch 1105, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5993])\n",
      "\t Grad:  tensor([ 3.3438e-05, -2.6286e-04])\n",
      "Epoch 1106, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5993])\n",
      "\t Grad:  tensor([ 3.2008e-05, -2.6125e-04])\n",
      "Epoch 1107, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5993])\n",
      "\t Grad:  tensor([ 3.8087e-05, -2.5875e-04])\n",
      "Epoch 1108, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5993])\n",
      "\t Grad:  tensor([ 3.6001e-05, -2.5737e-04])\n",
      "Epoch 1109, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5993])\n",
      "\t Grad:  tensor([ 3.4332e-05, -2.5576e-04])\n",
      "Epoch 1110, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5993])\n",
      "\t Grad:  tensor([ 3.2365e-05, -2.5430e-04])\n",
      "Epoch 1111, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5993])\n",
      "\t Grad:  tensor([ 3.6895e-05, -2.5203e-04])\n",
      "Epoch 1112, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5993])\n",
      "\t Grad:  tensor([ 3.3796e-05, -2.5072e-04])\n",
      "Epoch 1113, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5993])\n",
      "\t Grad:  tensor([ 3.3140e-05, -2.4912e-04])\n",
      "Epoch 1114, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5993])\n",
      "\t Grad:  tensor([ 2.9683e-05, -2.4790e-04])\n",
      "Epoch 1115, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5993])\n",
      "\t Grad:  tensor([ 3.1710e-05, -2.4589e-04])\n",
      "Epoch 1116, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5993])\n",
      "\t Grad:  tensor([ 3.5107e-05, -2.4389e-04])\n",
      "Epoch 1117, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5993])\n",
      "\t Grad:  tensor([ 3.1471e-05, -2.4259e-04])\n",
      "Epoch 1118, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5994])\n",
      "\t Grad:  tensor([ 3.6120e-05, -2.4040e-04])\n",
      "Epoch 1119, Loss 0.262983\n",
      "\t Params:  tensor([0.8411, 0.5994])\n",
      "\t Grad:  tensor([ 3.0339e-05, -2.3950e-04])\n",
      "Epoch 1120, Loss 0.262983\n",
      "\t Params:  tensor([0.8411, 0.5994])\n",
      "\t Grad:  tensor([ 3.4034e-05, -2.3755e-04])\n",
      "Epoch 1121, Loss 0.262983\n",
      "\t Params:  tensor([0.8411, 0.5994])\n",
      "\t Grad:  tensor([ 3.0994e-05, -2.3628e-04])\n",
      "Epoch 1122, Loss 0.262983\n",
      "\t Params:  tensor([0.8411, 0.5994])\n",
      "\t Grad:  tensor([ 3.2187e-05, -2.3449e-04])\n",
      "Epoch 1123, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5994])\n",
      "\t Grad:  tensor([ 3.1173e-05, -2.3304e-04])\n",
      "Epoch 1124, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5994])\n",
      "\t Grad:  tensor([ 3.3796e-05, -2.3122e-04])\n",
      "Epoch 1125, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5994])\n",
      "\t Grad:  tensor([ 2.7537e-05, -2.3048e-04])\n",
      "Epoch 1126, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5994])\n",
      "\t Grad:  tensor([ 2.8908e-05, -2.2869e-04])\n",
      "Epoch 1127, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5994])\n",
      "\t Grad:  tensor([ 2.9504e-05, -2.2706e-04])\n",
      "Epoch 1128, Loss 0.262983\n",
      "\t Params:  tensor([0.8411, 0.5994])\n",
      "\t Grad:  tensor([ 2.8968e-05, -2.2560e-04])\n",
      "Epoch 1129, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5994])\n",
      "\t Grad:  tensor([ 2.8610e-05, -2.2411e-04])\n",
      "Epoch 1130, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5994])\n",
      "\t Grad:  tensor([ 2.8849e-05, -2.2258e-04])\n",
      "Epoch 1131, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5994])\n",
      "\t Grad:  tensor([ 2.9147e-05, -2.2095e-04])\n",
      "Epoch 1132, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5994])\n",
      "\t Grad:  tensor([ 2.7716e-05, -2.1977e-04])\n",
      "Epoch 1133, Loss 0.262983\n",
      "\t Params:  tensor([0.8411, 0.5994])\n",
      "\t Grad:  tensor([ 2.8074e-05, -2.1816e-04])\n",
      "Epoch 1134, Loss 0.262983\n",
      "\t Params:  tensor([0.8411, 0.5994])\n",
      "\t Grad:  tensor([ 2.7537e-05, -2.1676e-04])\n",
      "Epoch 1135, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5994])\n",
      "\t Grad:  tensor([ 2.6822e-05, -2.1545e-04])\n",
      "Epoch 1136, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5994])\n",
      "\t Grad:  tensor([ 2.7418e-05, -2.1391e-04])\n",
      "Epoch 1137, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5994])\n",
      "\t Grad:  tensor([ 2.4378e-05, -2.1289e-04])\n",
      "Epoch 1138, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5994])\n",
      "\t Grad:  tensor([ 2.8670e-05, -2.1080e-04])\n",
      "Epoch 1139, Loss 0.262983\n",
      "\t Params:  tensor([0.8411, 0.5994])\n",
      "\t Grad:  tensor([ 2.5988e-05, -2.0976e-04])\n",
      "Epoch 1140, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5994])\n",
      "\t Grad:  tensor([ 2.9087e-05, -2.0798e-04])\n",
      "Epoch 1141, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5994])\n",
      "\t Grad:  tensor([ 2.7716e-05, -2.0675e-04])\n",
      "Epoch 1142, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5994])\n",
      "\t Grad:  tensor([ 2.5332e-05, -2.0571e-04])\n",
      "Epoch 1143, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5994])\n",
      "\t Grad:  tensor([ 2.7657e-05, -2.0391e-04])\n",
      "Epoch 1144, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5994])\n",
      "\t Grad:  tensor([ 2.6345e-05, -2.0271e-04])\n",
      "Epoch 1145, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5994])\n",
      "\t Grad:  tensor([ 3.1710e-05, -2.0079e-04])\n",
      "Epoch 1146, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5994])\n",
      "\t Grad:  tensor([ 2.8968e-05, -1.9976e-04])\n",
      "Epoch 1147, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5994])\n",
      "\t Grad:  tensor([ 2.5630e-05, -1.9879e-04])\n",
      "Epoch 1148, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5994])\n",
      "\t Grad:  tensor([ 2.8551e-05, -1.9708e-04])\n",
      "Epoch 1149, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5994])\n",
      "\t Grad:  tensor([ 2.3782e-05, -1.9628e-04])\n",
      "Epoch 1150, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5994])\n",
      "\t Grad:  tensor([ 2.6762e-05, -1.9462e-04])\n",
      "Epoch 1151, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5994])\n",
      "\t Grad:  tensor([ 2.8491e-05, -1.9310e-04])\n",
      "Epoch 1152, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5994])\n",
      "\t Grad:  tensor([ 2.5153e-05, -1.9228e-04])\n",
      "Epoch 1153, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5994])\n",
      "\t Grad:  tensor([ 2.4974e-05, -1.9094e-04])\n",
      "Epoch 1154, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5994])\n",
      "\t Grad:  tensor([ 2.5868e-05, -1.8955e-04])\n",
      "Epoch 1155, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5994])\n",
      "\t Grad:  tensor([ 2.7657e-05, -1.8805e-04])\n",
      "Epoch 1156, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5994])\n",
      "\t Grad:  tensor([ 2.2352e-05, -1.8743e-04])\n",
      "Epoch 1157, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5994])\n",
      "\t Grad:  tensor([ 2.5690e-05, -1.8578e-04])\n",
      "Epoch 1158, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5994])\n",
      "\t Grad:  tensor([ 2.7120e-05, -1.8435e-04])\n",
      "Epoch 1159, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5994])\n",
      "\t Grad:  tensor([ 2.2948e-05, -1.8367e-04])\n",
      "Epoch 1160, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5994])\n",
      "\t Grad:  tensor([ 2.1935e-05, -1.8253e-04])\n",
      "Epoch 1161, Loss 0.262982\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Params:  tensor([0.8411, 0.5994])\n",
      "\t Grad:  tensor([ 2.1458e-05, -1.8132e-04])\n",
      "Epoch 1162, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5994])\n",
      "\t Grad:  tensor([ 2.2173e-05, -1.8009e-04])\n",
      "Epoch 1163, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5994])\n",
      "\t Grad:  tensor([ 1.9729e-05, -1.7903e-04])\n",
      "Epoch 1164, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5994])\n",
      "\t Grad:  tensor([ 2.4378e-05, -1.7730e-04])\n",
      "Epoch 1165, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5994])\n",
      "\t Grad:  tensor([ 2.3663e-05, -1.7621e-04])\n",
      "Epoch 1166, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5994])\n",
      "\t Grad:  tensor([ 2.4319e-05, -1.7497e-04])\n",
      "Epoch 1167, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5995])\n",
      "\t Grad:  tensor([ 2.5153e-05, -1.7369e-04])\n",
      "Epoch 1168, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5995])\n",
      "\t Grad:  tensor([ 2.5868e-05, -1.7251e-04])\n",
      "Epoch 1169, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5995])\n",
      "\t Grad:  tensor([ 2.5392e-05, -1.7134e-04])\n",
      "Epoch 1170, Loss 0.262983\n",
      "\t Params:  tensor([0.8411, 0.5995])\n",
      "\t Grad:  tensor([ 2.2352e-05, -1.7052e-04])\n",
      "Epoch 1171, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5995])\n",
      "\t Grad:  tensor([ 2.0981e-05, -1.6954e-04])\n",
      "Epoch 1172, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5995])\n",
      "\t Grad:  tensor([ 2.1219e-05, -1.6845e-04])\n",
      "Epoch 1173, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5995])\n",
      "\t Grad:  tensor([ 2.0146e-05, -1.6750e-04])\n",
      "Epoch 1174, Loss 0.262983\n",
      "\t Params:  tensor([0.8411, 0.5995])\n",
      "\t Grad:  tensor([ 2.1875e-05, -1.6602e-04])\n",
      "Epoch 1175, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5995])\n",
      "\t Grad:  tensor([ 1.9670e-05, -1.6525e-04])\n",
      "Epoch 1176, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5995])\n",
      "\t Grad:  tensor([ 2.6524e-05, -1.6328e-04])\n",
      "Epoch 1177, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5995])\n",
      "\t Grad:  tensor([ 2.4080e-05, -1.6249e-04])\n",
      "Epoch 1178, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5995])\n",
      "\t Grad:  tensor([ 1.9908e-05, -1.6186e-04])\n",
      "Epoch 1179, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5995])\n",
      "\t Grad:  tensor([ 2.3723e-05, -1.6025e-04])\n",
      "Epoch 1180, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5995])\n",
      "\t Grad:  tensor([ 2.3603e-05, -1.5938e-04])\n",
      "Epoch 1181, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5995])\n",
      "\t Grad:  tensor([ 1.8775e-05, -1.5871e-04])\n",
      "Epoch 1182, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5995])\n",
      "\t Grad:  tensor([ 2.0802e-05, -1.5742e-04])\n",
      "Epoch 1183, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5995])\n",
      "\t Grad:  tensor([ 2.4974e-05, -1.5588e-04])\n",
      "Epoch 1184, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5995])\n",
      "\t Grad:  tensor([ 1.9550e-05, -1.5543e-04])\n",
      "Epoch 1185, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5995])\n",
      "\t Grad:  tensor([ 2.3603e-05, -1.5385e-04])\n",
      "Epoch 1186, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5995])\n",
      "\t Grad:  tensor([ 2.3127e-05, -1.5302e-04])\n",
      "Epoch 1187, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5995])\n",
      "\t Grad:  tensor([ 1.7941e-05, -1.5255e-04])\n",
      "Epoch 1188, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5995])\n",
      "\t Grad:  tensor([ 2.0325e-05, -1.5125e-04])\n",
      "Epoch 1189, Loss 0.262983\n",
      "\t Params:  tensor([0.8411, 0.5995])\n",
      "\t Grad:  tensor([ 2.1756e-05, -1.4997e-04])\n",
      "Epoch 1190, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5995])\n",
      "\t Grad:  tensor([ 1.9789e-05, -1.4926e-04])\n",
      "Epoch 1191, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5995])\n",
      "\t Grad:  tensor([ 1.8597e-05, -1.4833e-04])\n",
      "Epoch 1192, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5995])\n",
      "\t Grad:  tensor([ 2.0385e-05, -1.4716e-04])\n",
      "Epoch 1193, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5995])\n",
      "\t Grad:  tensor([ 2.3484e-05, -1.4583e-04])\n",
      "Epoch 1194, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5995])\n",
      "\t Grad:  tensor([ 1.9550e-05, -1.4536e-04])\n",
      "Epoch 1195, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5995])\n",
      "\t Grad:  tensor([ 2.1815e-05, -1.4417e-04])\n",
      "Epoch 1196, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5995])\n",
      "\t Grad:  tensor([ 1.6272e-05, -1.4387e-04])\n",
      "Epoch 1197, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5995])\n",
      "\t Grad:  tensor([ 1.8537e-05, -1.4265e-04])\n",
      "Epoch 1198, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5995])\n",
      "\t Grad:  tensor([ 1.7583e-05, -1.4175e-04])\n",
      "Epoch 1199, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5995])\n",
      "\t Grad:  tensor([ 1.9729e-05, -1.4053e-04])\n",
      "Epoch 1200, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5995])\n",
      "\t Grad:  tensor([ 1.9908e-05, -1.3953e-04])\n",
      "Epoch 1201, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5995])\n",
      "\t Grad:  tensor([ 2.1458e-05, -1.3844e-04])\n",
      "Epoch 1202, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5995])\n",
      "\t Grad:  tensor([ 1.7405e-05, -1.3798e-04])\n",
      "Epoch 1203, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5995])\n",
      "\t Grad:  tensor([ 1.7464e-05, -1.3704e-04])\n",
      "Epoch 1204, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5995])\n",
      "\t Grad:  tensor([ 1.9252e-05, -1.3594e-04])\n",
      "Epoch 1205, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5995])\n",
      "\t Grad:  tensor([ 1.5140e-05, -1.3552e-04])\n",
      "Epoch 1206, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5995])\n",
      "\t Grad:  tensor([ 1.6928e-05, -1.3440e-04])\n",
      "Epoch 1207, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5995])\n",
      "\t Grad:  tensor([ 1.6212e-05, -1.3352e-04])\n",
      "Epoch 1208, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5995])\n",
      "\t Grad:  tensor([ 1.7762e-05, -1.3251e-04])\n",
      "Epoch 1209, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5995])\n",
      "\t Grad:  tensor([ 1.8179e-05, -1.3158e-04])\n",
      "Epoch 1210, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5995])\n",
      "\t Grad:  tensor([ 1.4663e-05, -1.3108e-04])\n",
      "Epoch 1211, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5995])\n",
      "\t Grad:  tensor([ 1.9491e-05, -1.2963e-04])\n",
      "Epoch 1212, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5995])\n",
      "\t Grad:  tensor([ 1.9193e-05, -1.2872e-04])\n",
      "Epoch 1213, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5995])\n",
      "\t Grad:  tensor([ 1.7762e-05, -1.2793e-04])\n",
      "Epoch 1214, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5995])\n",
      "\t Grad:  tensor([ 1.6928e-05, -1.2729e-04])\n",
      "Epoch 1215, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5995])\n",
      "\t Grad:  tensor([ 1.7285e-05, -1.2641e-04])\n",
      "Epoch 1216, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5995])\n",
      "\t Grad:  tensor([ 1.5676e-05, -1.2569e-04])\n",
      "Epoch 1217, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5995])\n",
      "\t Grad:  tensor([ 1.5795e-05, -1.2495e-04])\n",
      "Epoch 1218, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5995])\n",
      "\t Grad:  tensor([ 1.3828e-05, -1.2430e-04])\n",
      "Epoch 1219, Loss 0.262983\n",
      "\t Params:  tensor([0.8411, 0.5995])\n",
      "\t Grad:  tensor([ 1.7405e-05, -1.2297e-04])\n",
      "Epoch 1220, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5995])\n",
      "\t Grad:  tensor([ 1.7524e-05, -1.2221e-04])\n",
      "Epoch 1221, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5995])\n",
      "\t Grad:  tensor([ 1.5974e-05, -1.2150e-04])\n",
      "Epoch 1222, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5995])\n",
      "\t Grad:  tensor([ 1.4246e-05, -1.2098e-04])\n",
      "Epoch 1223, Loss 0.262983\n",
      "\t Params:  tensor([0.8411, 0.5995])\n",
      "\t Grad:  tensor([ 1.6391e-05, -1.1984e-04])\n",
      "Epoch 1224, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5995])\n",
      "\t Grad:  tensor([ 1.7583e-05, -1.1899e-04])\n",
      "Epoch 1225, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5995])\n",
      "\t Grad:  tensor([ 1.6153e-05, -1.1829e-04])\n",
      "Epoch 1226, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5995])\n",
      "\t Grad:  tensor([ 1.1623e-05, -1.1803e-04])\n",
      "Epoch 1227, Loss 0.262983\n",
      "\t Params:  tensor([0.8411, 0.5995])\n",
      "\t Grad:  tensor([ 1.4663e-05, -1.1682e-04])\n",
      "Epoch 1228, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5995])\n",
      "\t Grad:  tensor([ 1.9729e-05, -1.1550e-04])\n",
      "Epoch 1229, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5995])\n",
      "\t Grad:  tensor([ 1.8120e-05, -1.1486e-04])\n",
      "Epoch 1230, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5995])\n",
      "\t Grad:  tensor([ 1.4067e-05, -1.1460e-04])\n",
      "Epoch 1231, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5995])\n",
      "\t Grad:  tensor([ 1.5378e-05, -1.1368e-04])\n",
      "Epoch 1232, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5995])\n",
      "\t Grad:  tensor([ 1.3709e-05, -1.1310e-04])\n",
      "Epoch 1233, Loss 0.262983\n",
      "\t Params:  tensor([0.8411, 0.5995])\n",
      "\t Grad:  tensor([ 1.7524e-05, -1.1189e-04])\n",
      "Epoch 1234, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5995])\n",
      "\t Grad:  tensor([ 1.4961e-05, -1.1139e-04])\n",
      "Epoch 1235, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5995])\n",
      "\t Grad:  tensor([ 1.1623e-05, -1.1104e-04])\n",
      "Epoch 1236, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5995])\n",
      "\t Grad:  tensor([ 1.2457e-05, -1.1025e-04])\n",
      "Epoch 1237, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5995])\n",
      "\t Grad:  tensor([ 1.6510e-05, -1.0897e-04])\n",
      "Epoch 1238, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5995])\n",
      "\t Grad:  tensor([ 1.2219e-05, -1.0882e-04])\n",
      "Epoch 1239, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5996])\n",
      "\t Grad:  tensor([ 1.4305e-05, -1.0782e-04])\n",
      "Epoch 1240, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5996])\n",
      "\t Grad:  tensor([ 1.7703e-05, -1.0672e-04])\n",
      "Epoch 1241, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5996])\n",
      "\t Grad:  tensor([ 1.5616e-05, -1.0625e-04])\n",
      "Epoch 1242, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5996])\n",
      "\t Grad:  tensor([ 1.2338e-05, -1.0591e-04])\n",
      "Epoch 1243, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5996])\n",
      "\t Grad:  tensor([ 1.3292e-05, -1.0508e-04])\n",
      "Epoch 1244, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5996])\n",
      "\t Grad:  tensor([ 1.3590e-05, -1.0426e-04])\n",
      "Epoch 1245, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5996])\n",
      "\t Grad:  tensor([ 1.5020e-05, -1.0344e-04])\n",
      "Epoch 1246, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5996])\n",
      "\t Grad:  tensor([ 1.3173e-05, -1.0297e-04])\n",
      "Epoch 1247, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5996])\n",
      "\t Grad:  tensor([ 1.4484e-05, -1.0211e-04])\n",
      "Epoch 1248, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5996])\n",
      "\t Grad:  tensor([ 1.4305e-05, -1.0151e-04])\n",
      "Epoch 1249, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5996])\n",
      "\t Grad:  tensor([ 1.4067e-05, -1.0073e-04])\n",
      "Epoch 1250, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5996])\n",
      "\t Grad:  tensor([ 1.6093e-05, -9.9823e-05])\n",
      "Epoch 1251, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5996])\n",
      "\t Grad:  tensor([ 1.5378e-05, -9.9346e-05])\n",
      "Epoch 1252, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5996])\n",
      "\t Grad:  tensor([ 1.0669e-05, -9.9242e-05])\n",
      "Epoch 1253, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5996])\n",
      "\t Grad:  tensor([ 1.2696e-05, -9.8318e-05])\n",
      "Epoch 1254, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5996])\n",
      "\t Grad:  tensor([ 1.2398e-05, -9.7692e-05])\n",
      "Epoch 1255, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5996])\n",
      "\t Grad:  tensor([ 1.3590e-05, -9.6925e-05])\n",
      "Epoch 1256, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5996])\n",
      "\t Grad:  tensor([ 1.5020e-05, -9.6105e-05])\n",
      "Epoch 1257, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5996])\n",
      "\t Grad:  tensor([ 1.1384e-05, -9.5919e-05])\n",
      "Epoch 1258, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5996])\n",
      "\t Grad:  tensor([ 1.0371e-05, -9.5405e-05])\n",
      "Epoch 1259, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5996])\n",
      "\t Grad:  tensor([ 1.0669e-05, -9.4675e-05])\n",
      "Epoch 1260, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5996])\n",
      "\t Grad:  tensor([ 1.1504e-05, -9.4004e-05])\n",
      "Epoch 1261, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5996])\n",
      "\t Grad:  tensor([ 1.1742e-05, -9.3251e-05])\n",
      "Epoch 1262, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5996])\n",
      "\t Grad:  tensor([ 1.2934e-05, -9.2521e-05])\n",
      "Epoch 1263, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5996])\n",
      "\t Grad:  tensor([ 1.4603e-05, -9.1672e-05])\n",
      "Epoch 1264, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5996])\n",
      "\t Grad:  tensor([ 1.2636e-05, -9.1240e-05])\n",
      "Epoch 1265, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5996])\n",
      "\t Grad:  tensor([ 1.3828e-05, -9.0554e-05])\n",
      "Epoch 1266, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5996])\n",
      "\t Grad:  tensor([ 1.5140e-05, -8.9787e-05])\n",
      "Epoch 1267, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5996])\n",
      "\t Grad:  tensor([ 7.6294e-06, -9.0018e-05])\n",
      "Epoch 1268, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5996])\n",
      "\t Grad:  tensor([ 1.2636e-05, -8.8811e-05])\n",
      "Epoch 1269, Loss 0.262983\n",
      "\t Params:  tensor([0.8411, 0.5996])\n",
      "\t Grad:  tensor([ 1.2040e-05, -8.8304e-05])\n",
      "Epoch 1270, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5996])\n",
      "\t Grad:  tensor([ 1.0550e-05, -8.7805e-05])\n",
      "Epoch 1271, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5996])\n",
      "\t Grad:  tensor([ 9.8348e-06, -8.7388e-05])\n",
      "Epoch 1272, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5996])\n",
      "\t Grad:  tensor([ 1.1981e-05, -8.6509e-05])\n",
      "Epoch 1273, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5996])\n",
      "\t Grad:  tensor([ 1.1206e-05, -8.6002e-05])\n",
      "Epoch 1274, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5996])\n",
      "\t Grad:  tensor([ 1.1563e-05, -8.5361e-05])\n",
      "Epoch 1275, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5996])\n",
      "\t Grad:  tensor([ 1.2159e-05, -8.4810e-05])\n",
      "Epoch 1276, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5996])\n",
      "\t Grad:  tensor([ 9.7156e-06, -8.4490e-05])\n",
      "Epoch 1277, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5996])\n",
      "\t Grad:  tensor([ 1.0073e-05, -8.4013e-05])\n",
      "Epoch 1278, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5996])\n",
      "\t Grad:  tensor([ 8.5831e-06, -8.3499e-05])\n",
      "Epoch 1279, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5996])\n",
      "\t Grad:  tensor([ 1.2279e-05, -8.2448e-05])\n",
      "Epoch 1280, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5996])\n",
      "\t Grad:  tensor([ 1.1325e-05, -8.2053e-05])\n",
      "Epoch 1281, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5996])\n",
      "\t Grad:  tensor([ 1.0431e-05, -8.1636e-05])\n",
      "Epoch 1282, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5996])\n",
      "\t Grad:  tensor([ 1.0312e-05, -8.1055e-05])\n",
      "Epoch 1283, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5996])\n",
      "\t Grad:  tensor([ 1.1981e-05, -8.0317e-05])\n",
      "Epoch 1284, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5996])\n",
      "\t Grad:  tensor([ 8.2254e-06, -8.0280e-05])\n",
      "Epoch 1285, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5996])\n",
      "\t Grad:  tensor([ 1.0490e-05, -7.9378e-05])\n",
      "Epoch 1286, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5996])\n",
      "\t Grad:  tensor([ 9.2983e-06, -7.9043e-05])\n",
      "Epoch 1287, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5996])\n",
      "\t Grad:  tensor([ 8.9407e-06, -7.8581e-05])\n",
      "Epoch 1288, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5996])\n",
      "\t Grad:  tensor([ 1.0133e-05, -7.7970e-05])\n",
      "Epoch 1289, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5996])\n",
      "\t Grad:  tensor([ 6.9737e-06, -7.7844e-05])\n",
      "Epoch 1290, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5996])\n",
      "\t Grad:  tensor([ 1.0908e-05, -7.6756e-05])\n",
      "Epoch 1291, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5996])\n",
      "\t Grad:  tensor([ 8.2850e-06, -7.6547e-05])\n",
      "Epoch 1292, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5996])\n",
      "\t Grad:  tensor([ 1.1146e-05, -7.5668e-05])\n",
      "Epoch 1293, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5996])\n",
      "\t Grad:  tensor([ 1.0908e-05, -7.5176e-05])\n",
      "Epoch 1294, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5996])\n",
      "\t Grad:  tensor([ 1.0788e-05, -7.4670e-05])\n",
      "Epoch 1295, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5996])\n",
      "\t Grad:  tensor([ 1.1265e-05, -7.4148e-05])\n",
      "Epoch 1296, Loss 0.262983\n",
      "\t Params:  tensor([0.8411, 0.5996])\n",
      "\t Grad:  tensor([ 4.7684e-06, -7.4431e-05])\n",
      "Epoch 1297, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5996])\n",
      "\t Grad:  tensor([ 9.7156e-06, -7.3396e-05])\n",
      "Epoch 1298, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5996])\n",
      "\t Grad:  tensor([ 8.7023e-06, -7.3016e-05])\n",
      "Epoch 1299, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5996])\n",
      "\t Grad:  tensor([ 1.1981e-05, -7.2137e-05])\n",
      "Epoch 1300, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5996])\n",
      "\t Grad:  tensor([ 7.5698e-06, -7.2099e-05])\n",
      "Epoch 1301, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5996])\n",
      "\t Grad:  tensor([ 1.0252e-05, -7.1332e-05])\n",
      "Epoch 1302, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5996])\n",
      "\t Grad:  tensor([ 9.3579e-06, -7.1011e-05])\n",
      "Epoch 1303, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5996])\n",
      "\t Grad:  tensor([ 8.5235e-06, -7.0639e-05])\n",
      "Epoch 1304, Loss 0.262983\n",
      "\t Params:  tensor([0.8411, 0.5996])\n",
      "\t Grad:  tensor([ 9.0003e-06, -6.9968e-05])\n",
      "Epoch 1305, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5996])\n",
      "\t Grad:  tensor([ 7.5698e-06, -6.9790e-05])\n",
      "Epoch 1306, Loss 0.262983\n",
      "\t Params:  tensor([0.8411, 0.5996])\n",
      "\t Grad:  tensor([ 1.1265e-05, -6.8761e-05])\n",
      "Epoch 1307, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5996])\n",
      "\t Grad:  tensor([ 9.5963e-06, -6.8612e-05])\n",
      "Epoch 1308, Loss 0.262983\n",
      "\t Params:  tensor([0.8411, 0.5996])\n",
      "\t Grad:  tensor([ 7.5698e-06, -6.8329e-05])\n",
      "Epoch 1309, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5996])\n",
      "\t Grad:  tensor([ 8.1658e-06, -6.7748e-05])\n",
      "Epoch 1310, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5996])\n",
      "\t Grad:  tensor([ 1.1206e-05, -6.7011e-05])\n",
      "Epoch 1311, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5996])\n",
      "\t Grad:  tensor([ 8.5235e-06, -6.6824e-05])\n",
      "Epoch 1312, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5996])\n",
      "\t Grad:  tensor([ 1.0908e-05, -6.6176e-05])\n",
      "Epoch 1313, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5996])\n",
      "\t Grad:  tensor([ 8.7619e-06, -6.6005e-05])\n",
      "Epoch 1314, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5996])\n",
      "\t Grad:  tensor([ 1.0371e-05, -6.5222e-05])\n",
      "Epoch 1315, Loss 0.262983\n",
      "\t Params:  tensor([0.8411, 0.5996])\n",
      "\t Grad:  tensor([ 7.0333e-06, -6.5297e-05])\n",
      "Epoch 1316, Loss 0.262983\n",
      "\t Params:  tensor([0.8411, 0.5996])\n",
      "\t Grad:  tensor([ 1.1921e-05, -6.4254e-05])\n",
      "Epoch 1317, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5996])\n",
      "\t Grad:  tensor([ 9.2983e-06, -6.4157e-05])\n",
      "Epoch 1318, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5996])\n",
      "\t Grad:  tensor([ 3.0398e-06, -6.4492e-05])\n",
      "Epoch 1319, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5996])\n",
      "\t Grad:  tensor([ 6.7353e-06, -6.3494e-05])\n",
      "Epoch 1320, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5996])\n",
      "\t Grad:  tensor([ 1.1325e-05, -6.2622e-05])\n",
      "Epoch 1321, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5996])\n",
      "\t Grad:  tensor([ 5.7817e-06, -6.2771e-05])\n",
      "Epoch 1322, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5996])\n",
      "\t Grad:  tensor([ 9.5963e-06, -6.1914e-05])\n",
      "Epoch 1323, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5996])\n",
      "\t Grad:  tensor([ 6.8545e-06, -6.1892e-05])\n",
      "Epoch 1324, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5996])\n",
      "\t Grad:  tensor([ 6.3181e-06, -6.1475e-05])\n",
      "Epoch 1325, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5996])\n",
      "\t Grad:  tensor([ 1.0669e-05, -6.0551e-05])\n",
      "Epoch 1326, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5996])\n",
      "\t Grad:  tensor([ 6.6161e-06, -6.0648e-05])\n",
      "Epoch 1327, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5996])\n",
      "\t Grad:  tensor([ 6.8545e-06, -6.0260e-05])\n",
      "Epoch 1328, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5996])\n",
      "\t Grad:  tensor([ 7.5698e-06, -5.9754e-05])\n",
      "Epoch 1329, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5996])\n",
      "\t Grad:  tensor([ 1.1265e-05, -5.8845e-05])\n",
      "Epoch 1330, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5996])\n",
      "\t Grad:  tensor([ 7.9274e-06, -5.8845e-05])\n",
      "Epoch 1331, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5996])\n",
      "\t Grad:  tensor([ 1.0312e-05, -5.8278e-05])\n",
      "Epoch 1332, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5996])\n",
      "\t Grad:  tensor([ 3.8743e-06, -5.8614e-05])\n",
      "Epoch 1333, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5996])\n",
      "\t Grad:  tensor([ 8.6427e-06, -5.7645e-05])\n",
      "Epoch 1334, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5996])\n",
      "\t Grad:  tensor([ 8.1658e-06, -5.7213e-05])\n",
      "Epoch 1335, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5996])\n",
      "\t Grad:  tensor([ 9.2387e-06, -5.6706e-05])\n",
      "Epoch 1336, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5996])\n",
      "\t Grad:  tensor([ 8.0466e-06, -5.6535e-05])\n",
      "Epoch 1337, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5996])\n",
      "\t Grad:  tensor([ 5.9605e-06, -5.6356e-05])\n",
      "Epoch 1338, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5996])\n",
      "\t Grad:  tensor([ 1.1623e-05, -5.5343e-05])\n",
      "Epoch 1339, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5996])\n",
      "\t Grad:  tensor([ 3.8147e-06, -5.5827e-05])\n",
      "Epoch 1340, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5996])\n",
      "\t Grad:  tensor([ 7.8678e-06, -5.5045e-05])\n",
      "Epoch 1341, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5996])\n",
      "\t Grad:  tensor([ 8.2254e-06, -5.4680e-05])\n",
      "Epoch 1342, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5996])\n",
      "\t Grad:  tensor([ 7.2718e-06, -5.4426e-05])\n",
      "Epoch 1343, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5996])\n",
      "\t Grad:  tensor([ 1.0252e-05, -5.3704e-05])\n",
      "Epoch 1344, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5996])\n",
      "\t Grad:  tensor([ 2.7418e-06, -5.4263e-05])\n",
      "Epoch 1345, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5996])\n",
      "\t Grad:  tensor([ 1.0133e-05, -5.3026e-05])\n",
      "Epoch 1346, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5996])\n",
      "\t Grad:  tensor([ 7.7486e-06, -5.2832e-05])\n",
      "Epoch 1347, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5996])\n",
      "\t Grad:  tensor([ 8.3447e-06, -5.2415e-05])\n",
      "Epoch 1348, Loss 0.262983\n",
      "\t Params:  tensor([0.8411, 0.5996])\n",
      "\t Grad:  tensor([ 7.7486e-06, -5.2057e-05])\n",
      "Epoch 1349, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5996])\n",
      "\t Grad:  tensor([ 9.6560e-06, -5.1551e-05])\n",
      "Epoch 1350, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5996])\n",
      "\t Grad:  tensor([ 8.2254e-06, -5.1506e-05])\n",
      "Epoch 1351, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5996])\n",
      "\t Grad:  tensor([ 5.6624e-06, -5.1349e-05])\n",
      "Epoch 1352, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5996])\n",
      "\t Grad:  tensor([ 6.2585e-06, -5.0917e-05])\n",
      "Epoch 1353, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5996])\n",
      "\t Grad:  tensor([ 7.3314e-06, -5.0366e-05])\n",
      "Epoch 1354, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5996])\n",
      "\t Grad:  tensor([ 8.7619e-06, -4.9874e-05])\n",
      "Epoch 1355, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5996])\n",
      "\t Grad:  tensor([ 7.2122e-06, -4.9733e-05])\n",
      "Epoch 1356, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5996])\n",
      "\t Grad:  tensor([ 9.4771e-06, -4.9174e-05])\n",
      "Epoch 1357, Loss 0.262983\n",
      "\t Params:  tensor([0.8411, 0.5996])\n",
      "\t Grad:  tensor([ 4.2319e-06, -4.9487e-05])\n",
      "Epoch 1358, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5996])\n",
      "\t Grad:  tensor([ 6.8545e-06, -4.8898e-05])\n",
      "Epoch 1359, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5996])\n",
      "\t Grad:  tensor([ 5.3048e-06, -4.8742e-05])\n",
      "Epoch 1360, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5996])\n",
      "\t Grad:  tensor([ 5.6624e-06, -4.8406e-05])\n",
      "Epoch 1361, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5996])\n",
      "\t Grad:  tensor([ 5.5432e-06, -4.8019e-05])\n",
      "Epoch 1362, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5996])\n",
      "\t Grad:  tensor([ 7.6890e-06, -4.7512e-05])\n",
      "Epoch 1363, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5996])\n",
      "\t Grad:  tensor([ 5.3644e-06, -4.7341e-05])\n",
      "Epoch 1364, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5996])\n",
      "\t Grad:  tensor([ 9.4771e-06, -4.6633e-05])\n",
      "Epoch 1365, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5996])\n",
      "\t Grad:  tensor([ 3.5763e-07, -4.7393e-05])\n",
      "Epoch 1366, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5996])\n",
      "\t Grad:  tensor([ 6.7949e-06, -4.6283e-05])\n",
      "Epoch 1367, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5996])\n",
      "\t Grad:  tensor([ 7.3910e-06, -4.5896e-05])\n",
      "Epoch 1368, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5996])\n",
      "\t Grad:  tensor([ 7.7486e-06, -4.5568e-05])\n",
      "Epoch 1369, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5996])\n",
      "\t Grad:  tensor([ 7.2718e-06, -4.5232e-05])\n",
      "Epoch 1370, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5996])\n",
      "\t Grad:  tensor([ 1.0490e-05, -4.4569e-05])\n",
      "Epoch 1371, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5996])\n",
      "\t Grad:  tensor([ 1.2517e-06, -4.5404e-05])\n",
      "Epoch 1372, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5996])\n",
      "\t Grad:  tensor([ 7.9870e-06, -4.4279e-05])\n",
      "Epoch 1373, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5996])\n",
      "\t Grad:  tensor([ 7.6294e-06, -4.4025e-05])\n",
      "Epoch 1374, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5996])\n",
      "\t Grad:  tensor([ 1.0252e-05, -4.3459e-05])\n",
      "Epoch 1375, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5996])\n",
      "\t Grad:  tensor([ 2.9206e-06, -4.4122e-05])\n",
      "Epoch 1376, Loss 0.262983\n",
      "\t Params:  tensor([0.8411, 0.5996])\n",
      "\t Grad:  tensor([ 5.7817e-06, -4.3362e-05])\n",
      "Epoch 1377, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5996])\n",
      "\t Grad:  tensor([ 8.2850e-06, -4.2751e-05])\n",
      "Epoch 1378, Loss 0.262983\n",
      "\t Params:  tensor([0.8411, 0.5996])\n",
      "\t Grad:  tensor([ 5.4240e-06, -4.2811e-05])\n",
      "Epoch 1379, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5996])\n",
      "\t Grad:  tensor([ 6.9737e-06, -4.2513e-05])\n",
      "Epoch 1380, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5996])\n",
      "\t Grad:  tensor([ 4.5896e-06, -4.2409e-05])\n",
      "Epoch 1381, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5996])\n",
      "\t Grad:  tensor([ 4.9472e-06, -4.2066e-05])\n",
      "Epoch 1382, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5996])\n",
      "\t Grad:  tensor([ 5.5432e-06, -4.1798e-05])\n",
      "Epoch 1383, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5997])\n",
      "\t Grad:  tensor([ 4.2319e-06, -4.1611e-05])\n",
      "Epoch 1384, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5997])\n",
      "\t Grad:  tensor([ 4.9472e-06, -4.1239e-05])\n",
      "Epoch 1385, Loss 0.262983\n",
      "\t Params:  tensor([0.8411, 0.5997])\n",
      "\t Grad:  tensor([ 3.7551e-06, -4.1135e-05])\n",
      "Epoch 1386, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5997])\n",
      "\t Grad:  tensor([ 4.5896e-06, -4.0777e-05])\n",
      "Epoch 1387, Loss 0.262983\n",
      "\t Params:  tensor([0.8411, 0.5997])\n",
      "\t Grad:  tensor([ 3.0994e-06, -4.0621e-05])\n",
      "Epoch 1388, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5997])\n",
      "\t Grad:  tensor([ 3.3975e-06, -4.0397e-05])\n",
      "Epoch 1389, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5997])\n",
      "\t Grad:  tensor([ 1.7881e-06, -4.0285e-05])\n",
      "Epoch 1390, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5997])\n",
      "\t Grad:  tensor([ 8.4043e-06, -3.9190e-05])\n",
      "Epoch 1391, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5997])\n",
      "\t Grad:  tensor([ 5.6028e-06, -3.9138e-05])\n",
      "Epoch 1392, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5997])\n",
      "\t Grad:  tensor([ 4.7684e-06, -3.8922e-05])\n",
      "Epoch 1393, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5997])\n",
      "\t Grad:  tensor([ 6.6757e-06, -3.8482e-05])\n",
      "Epoch 1394, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5997])\n",
      "\t Grad:  tensor([ 1.6689e-06, -3.8855e-05])\n",
      "Epoch 1395, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5997])\n",
      "\t Grad:  tensor([ 7.6294e-06, -3.7856e-05])\n",
      "Epoch 1396, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5997])\n",
      "\t Grad:  tensor([ 8.2254e-06, -3.7573e-05])\n",
      "Epoch 1397, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5997])\n",
      "\t Grad:  tensor([ 5.8413e-06, -3.7469e-05])\n",
      "Epoch 1398, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5997])\n",
      "\t Grad:  tensor([ 5.9605e-06, -3.7290e-05])\n",
      "Epoch 1399, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5997])\n",
      "\t Grad:  tensor([ 4.4107e-06, -3.7305e-05])\n",
      "Epoch 1400, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5997])\n",
      "\t Grad:  tensor([ 4.6492e-06, -3.7082e-05])\n",
      "Epoch 1401, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5997])\n",
      "\t Grad:  tensor([ 3.8147e-06, -3.6888e-05])\n",
      "Epoch 1402, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5997])\n",
      "\t Grad:  tensor([ 2.2054e-06, -3.6873e-05])\n",
      "Epoch 1403, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5997])\n",
      "\t Grad:  tensor([ 6.3181e-06, -3.6143e-05])\n",
      "Epoch 1404, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5997])\n",
      "\t Grad:  tensor([ 3.3975e-06, -3.6187e-05])\n",
      "Epoch 1405, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5997])\n",
      "\t Grad:  tensor([ 2.6226e-06, -3.6001e-05])\n",
      "Epoch 1406, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5997])\n",
      "\t Grad:  tensor([ 8.9407e-06, -3.4995e-05])\n",
      "Epoch 1407, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5997])\n",
      "\t Grad:  tensor([ 1.9073e-06, -3.5673e-05])\n",
      "Epoch 1408, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5997])\n",
      "\t Grad:  tensor([ 7.8678e-06, -3.4712e-05])\n",
      "Epoch 1409, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5997])\n",
      "\t Grad:  tensor([ 4.8876e-06, -3.4794e-05])\n",
      "Epoch 1410, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5997])\n",
      "\t Grad:  tensor([ 3.2187e-06, -3.4772e-05])\n",
      "Epoch 1411, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5997])\n",
      "\t Grad:  tensor([ 3.6955e-06, -3.4541e-05])\n",
      "Epoch 1412, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5997])\n",
      "\t Grad:  tensor([ 7.1526e-07, -3.4645e-05])\n",
      "Epoch 1413, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5997])\n",
      "\t Grad:  tensor([ 3.8147e-06, -3.3900e-05])\n",
      "Epoch 1414, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5997])\n",
      "\t Grad:  tensor([ 7.2718e-06, -3.3394e-05])\n",
      "Epoch 1415, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5997])\n",
      "\t Grad:  tensor([ 2.0266e-06, -3.3706e-05])\n",
      "Epoch 1416, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5997])\n",
      "\t Grad:  tensor([ 8.2254e-06, -3.2723e-05])\n",
      "Epoch 1417, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5997])\n",
      "\t Grad:  tensor([ 5.3644e-06, -3.2842e-05])\n",
      "Epoch 1418, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5997])\n",
      "\t Grad:  tensor([ 4.7684e-06, -3.2790e-05])\n",
      "Epoch 1419, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5997])\n",
      "\t Grad:  tensor([ 4.2915e-06, -3.2596e-05])\n",
      "Epoch 1420, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5997])\n",
      "\t Grad:  tensor([ 3.0994e-06, -3.2499e-05])\n",
      "Epoch 1421, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-07, -3.2693e-05])\n",
      "Epoch 1422, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5997])\n",
      "\t Grad:  tensor([ 5.2452e-06, -3.1918e-05])\n",
      "Epoch 1423, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5997])\n",
      "\t Grad:  tensor([ 3.2187e-06, -3.1896e-05])\n",
      "Epoch 1424, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5997])\n",
      "\t Grad:  tensor([-5.3644e-07, -3.2142e-05])\n",
      "Epoch 1425, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5997])\n",
      "\t Grad:  tensor([ 5.6028e-06, -3.1196e-05])\n",
      "Epoch 1426, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5997])\n",
      "\t Grad:  tensor([ 4.4107e-06, -3.1121e-05])\n",
      "Epoch 1427, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5997])\n",
      "\t Grad:  tensor([ 4.1723e-07, -3.1479e-05])\n",
      "Epoch 1428, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5997])\n",
      "\t Grad:  tensor([ 3.2783e-06, -3.0808e-05])\n",
      "Epoch 1429, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5997])\n",
      "\t Grad:  tensor([ 4.5300e-06, -3.0540e-05])\n",
      "Epoch 1430, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5997])\n",
      "\t Grad:  tensor([ 6.5565e-07, -3.0845e-05])\n",
      "Epoch 1431, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5997])\n",
      "\t Grad:  tensor([ 3.5167e-06, -3.0130e-05])\n",
      "Epoch 1432, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5997])\n",
      "\t Grad:  tensor([ 2.9206e-06, -3.0160e-05])\n",
      "Epoch 1433, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5997])\n",
      "\t Grad:  tensor([ 5.5432e-06, -2.9571e-05])\n",
      "Epoch 1434, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5997])\n",
      "\t Grad:  tensor([ 4.1127e-06, -2.9549e-05])\n",
      "Epoch 1435, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5997])\n",
      "\t Grad:  tensor([ 4.1127e-06, -2.9445e-05])\n",
      "Epoch 1436, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5997])\n",
      "\t Grad:  tensor([ 1.4901e-06, -2.9482e-05])\n",
      "Epoch 1437, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5997])\n",
      "\t Grad:  tensor([ 2.3246e-06, -2.9184e-05])\n",
      "Epoch 1438, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5997])\n",
      "\t Grad:  tensor([ 8.0466e-06, -2.8282e-05])\n",
      "Epoch 1439, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5997])\n",
      "\t Grad:  tensor([ 6.6161e-06, -2.8223e-05])\n",
      "Epoch 1440, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5997])\n",
      "\t Grad:  tensor([ 2.6822e-06, -2.8558e-05])\n",
      "Epoch 1441, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5997])\n",
      "\t Grad:  tensor([ 5.3048e-06, -2.7947e-05])\n",
      "Epoch 1442, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5997])\n",
      "\t Grad:  tensor([ 5.9009e-06, -2.7709e-05])\n",
      "Epoch 1443, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5997])\n",
      "\t Grad:  tensor([ 4.5896e-06, -2.7694e-05])\n",
      "Epoch 1444, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5997])\n",
      "\t Grad:  tensor([ 1.2517e-06, -2.7895e-05])\n",
      "Epoch 1445, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5997])\n",
      "\t Grad:  tensor([ 5.4240e-06, -2.7232e-05])\n",
      "Epoch 1446, Loss 0.262983\n",
      "\t Params:  tensor([0.8411, 0.5997])\n",
      "\t Grad:  tensor([ 2.5630e-06, -2.7344e-05])\n",
      "Epoch 1447, Loss 0.262983\n",
      "\t Params:  tensor([0.8411, 0.5997])\n",
      "\t Grad:  tensor([ 5.4240e-06, -2.6718e-05])\n",
      "Epoch 1448, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5997])\n",
      "\t Grad:  tensor([ 6.8545e-06, -2.6457e-05])\n",
      "Epoch 1449, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5997])\n",
      "\t Grad:  tensor([ 4.1127e-06, -2.6569e-05])\n",
      "Epoch 1450, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5997])\n",
      "\t Grad:  tensor([ 5.9605e-08, -2.6971e-05])\n",
      "Epoch 1451, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5997])\n",
      "\t Grad:  tensor([ 2.5630e-06, -2.6390e-05])\n",
      "Epoch 1452, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5997])\n",
      "\t Grad:  tensor([ 6.4373e-06, -2.5839e-05])\n",
      "Epoch 1453, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5997])\n",
      "\t Grad:  tensor([ 5.4240e-06, -2.5734e-05])\n",
      "Epoch 1454, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5997])\n",
      "\t Grad:  tensor([-2.9802e-07, -2.6338e-05])\n",
      "Epoch 1455, Loss 0.262983\n",
      "\t Params:  tensor([0.8411, 0.5997])\n",
      "\t Grad:  tensor([ 2.2054e-06, -2.5772e-05])\n",
      "Epoch 1456, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5997])\n",
      "\t Grad:  tensor([ 6.0201e-06, -2.5198e-05])\n",
      "Epoch 1457, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5997])\n",
      "\t Grad:  tensor([ 4.7088e-06, -2.5213e-05])\n",
      "Epoch 1458, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5997])\n",
      "\t Grad:  tensor([ 1.4901e-06, -2.5451e-05])\n",
      "Epoch 1459, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5997])\n",
      "\t Grad:  tensor([ 4.1127e-06, -2.4915e-05])\n",
      "Epoch 1460, Loss 0.262982\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Params:  tensor([0.8411, 0.5997])\n",
      "\t Grad:  tensor([ 3.7551e-06, -2.4900e-05])\n",
      "Epoch 1461, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5997])\n",
      "\t Grad:  tensor([ 4.1723e-07, -2.5116e-05])\n",
      "Epoch 1462, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5997])\n",
      "\t Grad:  tensor([ 7.7486e-07, -2.4900e-05])\n",
      "Epoch 1463, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5997])\n",
      "\t Grad:  tensor([ 3.2783e-06, -2.4356e-05])\n",
      "Epoch 1464, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5997])\n",
      "\t Grad:  tensor([ 3.5167e-06, -2.4281e-05])\n",
      "Epoch 1465, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5997])\n",
      "\t Grad:  tensor([ 2.2054e-06, -2.4281e-05])\n",
      "Epoch 1466, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5997])\n",
      "\t Grad:  tensor([ 2.4438e-06, -2.4058e-05])\n",
      "Epoch 1467, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5997])\n",
      "\t Grad:  tensor([ 5.1856e-06, -2.3499e-05])\n",
      "Epoch 1468, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5997])\n",
      "\t Grad:  tensor([ 2.9206e-06, -2.3708e-05])\n",
      "Epoch 1469, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5997])\n",
      "\t Grad:  tensor([ 5.4240e-06, -2.3142e-05])\n",
      "Epoch 1470, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5997])\n",
      "\t Grad:  tensor([ 3.3975e-06, -2.3335e-05])\n",
      "Epoch 1471, Loss 0.262983\n",
      "\t Params:  tensor([0.8411, 0.5997])\n",
      "\t Grad:  tensor([ 9.5367e-07, -2.3380e-05])\n",
      "Epoch 1472, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5997])\n",
      "\t Grad:  tensor([ 4.7088e-06, -2.2829e-05])\n",
      "Epoch 1473, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5997])\n",
      "\t Grad:  tensor([ 1.9073e-06, -2.2963e-05])\n",
      "Epoch 1474, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5997])\n",
      "\t Grad:  tensor([ 2.1458e-06, -2.2754e-05])\n",
      "Epoch 1475, Loss 0.262983\n",
      "\t Params:  tensor([0.8411, 0.5997])\n",
      "\t Grad:  tensor([ 5.0068e-06, -2.2203e-05])\n",
      "Epoch 1476, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5997])\n",
      "\t Grad:  tensor([ 6.3777e-06, -2.1987e-05])\n",
      "Epoch 1477, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5997])\n",
      "\t Grad:  tensor([ 3.4571e-06, -2.2128e-05])\n",
      "Epoch 1478, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5997])\n",
      "\t Grad:  tensor([-7.1526e-07, -2.2538e-05])\n",
      "Epoch 1479, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5997])\n",
      "\t Grad:  tensor([ 2.1458e-06, -2.1957e-05])\n",
      "Epoch 1480, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5997])\n",
      "\t Grad:  tensor([ 5.8413e-06, -2.1391e-05])\n",
      "Epoch 1481, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 4.8876e-06, -2.1338e-05])\n",
      "Epoch 1482, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([-1.0729e-06, -2.1912e-05])\n",
      "Epoch 1483, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 1.9073e-06, -2.1361e-05])\n",
      "Epoch 1484, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 5.6028e-06, -2.0802e-05])\n",
      "Epoch 1485, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 4.1723e-06, -2.0884e-05])\n",
      "Epoch 1486, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 3.5763e-07, -2.1197e-05])\n",
      "Epoch 1487, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 3.0994e-06, -2.0631e-05])\n",
      "Epoch 1488, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.9802e-06, -2.0631e-05])\n",
      "Epoch 1489, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 4.4107e-06, -2.0273e-05])\n",
      "Epoch 1490, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-07, -2.0646e-05])\n",
      "Epoch 1491, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 4.7684e-07, -2.0437e-05])\n",
      "Epoch 1492, Loss 0.262983\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.9802e-06, -1.9990e-05])\n",
      "Epoch 1493, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 5.9605e-07, -2.0258e-05])\n",
      "Epoch 1494, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.8610e-06, -1.9901e-05])\n",
      "Epoch 1495, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 5.6028e-06, -1.9349e-05])\n",
      "Epoch 1496, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 1.1921e-06, -1.9856e-05])\n",
      "Epoch 1497, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.0266e-06, -1.9602e-05])\n",
      "Epoch 1498, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 4.0531e-06, -1.9141e-05])\n",
      "Epoch 1499, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.1458e-06, -1.9379e-05])\n",
      "Epoch 1500, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 4.7684e-06, -1.8887e-05])\n",
      "Epoch 1501, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 3.4571e-06, -1.8962e-05])\n",
      "Epoch 1502, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([-2.3246e-06, -1.9558e-05])\n",
      "Epoch 1503, Loss 0.262983\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 4.1723e-07, -1.9006e-05])\n",
      "Epoch 1504, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 3.9339e-06, -1.8559e-05])\n",
      "Epoch 1505, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([-2.9802e-07, -1.8954e-05])\n",
      "Epoch 1506, Loss 0.262983\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 1.0133e-06, -1.8597e-05])\n",
      "Epoch 1507, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 1.2517e-06, -1.8448e-05])\n",
      "Epoch 1508, Loss 0.262983\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 3.8743e-06, -1.7963e-05])\n",
      "Epoch 1509, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 5.3644e-06, -1.7799e-05])\n",
      "Epoch 1510, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.9802e-07, -1.8284e-05])\n",
      "Epoch 1511, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.8014e-06, -1.7755e-05])\n",
      "Epoch 1512, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.9206e-06, -1.7650e-05])\n",
      "Epoch 1513, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 4.3511e-06, -1.7293e-05])\n",
      "Epoch 1514, Loss 0.262983\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 1.3709e-06, -1.7546e-05])\n",
      "Epoch 1515, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 5.1856e-06, -1.7032e-05])\n",
      "Epoch 1516, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 3.8743e-06, -1.7099e-05])\n",
      "Epoch 1517, Loss 0.262983\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([-1.7285e-06, -1.7680e-05])\n",
      "Epoch 1518, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([-1.4901e-06, -1.7494e-05])\n",
      "Epoch 1519, Loss 0.262983\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 1.0133e-06, -1.6987e-05])\n",
      "Epoch 1520, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 4.5896e-06, -1.6525e-05])\n",
      "Epoch 1521, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3246e-06, -1.6697e-05])\n",
      "Epoch 1522, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 3.6359e-06, -1.6332e-05])\n",
      "Epoch 1523, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([-2.9802e-07, -1.6823e-05])\n",
      "Epoch 1524, Loss 0.262983\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.0862e-06, -1.6332e-05])\n",
      "Epoch 1525, Loss 0.262983\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3246e-06, -1.6153e-05])\n",
      "Epoch 1526, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 6.0201e-06, -1.5631e-05])\n",
      "Epoch 1527, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 4.2319e-06, -1.5683e-05])\n",
      "Epoch 1528, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([-1.6093e-06, -1.6324e-05])\n",
      "Epoch 1529, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([-4.1723e-07, -1.5989e-05])\n",
      "Epoch 1530, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 1.2517e-06, -1.5609e-05])\n",
      "Epoch 1531, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 4.9472e-06, -1.5125e-05])\n",
      "Epoch 1532, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 3.7551e-06, -1.5184e-05])\n",
      "Epoch 1533, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 1.7881e-07, -1.5475e-05])\n",
      "Epoch 1534, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 4.1723e-07, -1.5296e-05])\n",
      "Epoch 1535, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 3.0398e-06, -1.4782e-05])\n",
      "Epoch 1536, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([-1.4305e-06, -1.5326e-05])\n",
      "Epoch 1537, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 6.5565e-07, -1.4983e-05])\n",
      "Epoch 1538, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 3.2783e-06, -1.4514e-05])\n",
      "Epoch 1539, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 1.6093e-06, -1.4730e-05])\n",
      "Epoch 1540, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 1.6093e-06, -1.4655e-05])\n",
      "Epoch 1541, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.5630e-06, -1.4439e-05])\n",
      "Epoch 1542, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 4.7088e-06, -1.4007e-05])\n",
      "Epoch 1543, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 7.1526e-07, -1.4447e-05])\n",
      "Epoch 1544, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.8014e-06, -1.4171e-05])\n",
      "Epoch 1545, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 4.2319e-06, -1.3888e-05])\n",
      "Epoch 1546, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 1.4305e-06, -1.4164e-05])\n",
      "Epoch 1547, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 1.7881e-06, -1.3985e-05])\n",
      "Epoch 1548, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.0266e-06, -1.3858e-05])\n",
      "Epoch 1549, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 3.8147e-06, -1.3493e-05])\n",
      "Epoch 1550, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 3.5763e-07, -1.3903e-05])\n",
      "Epoch 1551, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.1458e-06, -1.3635e-05])\n",
      "Epoch 1552, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 3.9935e-06, -1.3418e-05])\n",
      "Epoch 1553, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([-1.0729e-06, -1.3918e-05])\n",
      "Epoch 1554, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 5.9605e-07, -1.3545e-05])\n",
      "Epoch 1555, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 1.0729e-06, -1.3441e-05])\n",
      "Epoch 1556, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 1.0729e-06, -1.3337e-05])\n",
      "Epoch 1557, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.8610e-06, -1.2979e-05])\n",
      "Epoch 1558, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 3.8147e-06, -1.2785e-05])\n",
      "Epoch 1559, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 3.3379e-06, -1.2867e-05])\n",
      "Epoch 1560, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([-1.1921e-07, -1.3292e-05])\n",
      "Epoch 1561, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 1.6689e-06, -1.2919e-05])\n",
      "Epoch 1562, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -1.2711e-05])\n",
      "Epoch 1563, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.5034e-06, -1.2606e-05])\n",
      "Epoch 1564, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.7418e-06, -1.2502e-05])\n",
      "Epoch 1565, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 3.6955e-06, -1.2293e-05])\n",
      "Epoch 1566, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 1.3113e-06, -1.2480e-05])\n",
      "Epoch 1567, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 4.7684e-06, -1.2048e-05])\n",
      "Epoch 1568, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([-5.9605e-07, -1.2629e-05])\n",
      "Epoch 1569, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 7.1526e-07, -1.2375e-05])\n",
      "Epoch 1570, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 1.7881e-06, -1.2152e-05])\n",
      "Epoch 1571, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.0266e-06, -1.1988e-05])\n",
      "Epoch 1572, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.5034e-06, -1.1839e-05])\n",
      "Epoch 1573, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 3.6955e-06, -1.1601e-05])\n",
      "Epoch 1574, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 8.3447e-07, -1.1884e-05])\n",
      "Epoch 1575, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.7418e-06, -1.1608e-05])\n",
      "Epoch 1576, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 4.6492e-06, -1.1310e-05])\n",
      "Epoch 1577, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 1.7881e-06, -1.1601e-05])\n",
      "Epoch 1578, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 3.6955e-06, -1.1198e-05])\n",
      "Epoch 1579, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([-2.3246e-06, -1.1921e-05])\n",
      "Epoch 1580, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([-2.0862e-06, -1.1802e-05])\n",
      "Epoch 1581, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([-1.3709e-06, -1.1638e-05])\n",
      "Epoch 1582, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 1.7881e-07, -1.1310e-05])\n",
      "Epoch 1583, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 5.3644e-07, -1.1146e-05])\n",
      "Epoch 1584, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 4.2915e-06, -1.0662e-05])\n",
      "Epoch 1585, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 3.3379e-06, -1.0796e-05])\n",
      "Epoch 1586, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([-1.3709e-06, -1.1265e-05])\n",
      "Epoch 1587, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([-1.1325e-06, -1.1146e-05])\n",
      "Epoch 1588, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([-1.0133e-06, -1.1042e-05])\n",
      "Epoch 1589, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 4.1723e-07, -1.0759e-05])\n",
      "Epoch 1590, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 1.8477e-06, -1.0476e-05])\n",
      "Epoch 1591, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 1.9670e-06, -1.0356e-05])\n",
      "Epoch 1592, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 5.6028e-06, -9.9018e-06])\n",
      "Epoch 1593, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 3.2783e-06, -1.0073e-05])\n",
      "Epoch 1594, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 1.1325e-06, -1.0312e-05])\n",
      "Epoch 1595, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 1.3709e-06, -1.0222e-05])\n",
      "Epoch 1596, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 1.4901e-06, -1.0103e-05])\n",
      "Epoch 1597, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 3.6359e-06, -9.7305e-06])\n",
      "Epoch 1598, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([-1.8477e-06, -1.0267e-05])\n",
      "Epoch 1599, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 1.7285e-06, -9.8199e-06])\n",
      "Epoch 1600, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 1.7285e-06, -9.7752e-06])\n",
      "Epoch 1601, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 3.9935e-06, -9.3430e-06])\n",
      "Epoch 1602, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 1.7881e-07, -9.7454e-06])\n",
      "Epoch 1603, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.9802e-07, -9.6411e-06])\n",
      "Epoch 1604, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 5.3644e-07, -9.5367e-06])\n",
      "Epoch 1605, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.6822e-06, -9.1493e-06])\n",
      "Epoch 1606, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 3.5167e-06, -8.9556e-06])\n",
      "Epoch 1607, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 3.1590e-06, -9.0599e-06])\n",
      "Epoch 1608, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([-2.2054e-06, -9.6709e-06])\n",
      "Epoch 1609, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([-1.3709e-06, -9.4324e-06])\n",
      "Epoch 1610, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 6.5565e-07, -9.0227e-06])\n",
      "Epoch 1611, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 6.5565e-07, -8.9630e-06])\n",
      "Epoch 1612, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 6.5565e-07, -8.8885e-06])\n",
      "Epoch 1613, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 1.0133e-06, -8.7395e-06])\n",
      "Epoch 1614, Loss 0.262983\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.9206e-06, -8.4713e-06])\n",
      "Epoch 1615, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 3.3975e-06, -8.3372e-06])\n",
      "Epoch 1616, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([-1.1325e-06, -8.9556e-06])\n",
      "Epoch 1617, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 8.9407e-07, -8.6054e-06])\n",
      "Epoch 1618, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.8014e-06, -8.4043e-06])\n",
      "Epoch 1619, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 3.0398e-06, -8.2999e-06])\n",
      "Epoch 1620, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([-1.2517e-06, -8.8885e-06])\n",
      "Epoch 1621, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([-6.5565e-07, -8.7544e-06])\n",
      "Epoch 1622, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 1.2517e-06, -8.4415e-06])\n",
      "Epoch 1623, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 1.4901e-06, -8.3447e-06])\n",
      "Epoch 1624, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 1.4901e-06, -8.3223e-06])\n",
      "Epoch 1625, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 1.4901e-06, -8.2776e-06])\n",
      "Epoch 1626, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 1.4901e-06, -8.2329e-06])\n",
      "Epoch 1627, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 1.4901e-06, -8.2180e-06])\n",
      "Epoch 1628, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 1.7285e-06, -8.1137e-06])\n",
      "Epoch 1629, Loss 0.262983\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 3.6359e-06, -7.7859e-06])\n",
      "Epoch 1630, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.9802e-07, -8.1956e-06])\n",
      "Epoch 1631, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.9802e-07, -8.1658e-06])\n",
      "Epoch 1632, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 5.3644e-07, -8.0764e-06])\n",
      "Epoch 1633, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.2054e-06, -7.8306e-06])\n",
      "Epoch 1634, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 4.1127e-06, -7.6294e-06])\n",
      "Epoch 1635, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([-1.0133e-06, -8.2180e-06])\n",
      "Epoch 1636, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([-8.9407e-07, -8.1509e-06])\n",
      "Epoch 1637, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([-4.1723e-07, -8.0317e-06])\n",
      "Epoch 1638, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 1.3709e-06, -7.7337e-06])\n",
      "Epoch 1639, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 1.4901e-06, -7.6592e-06])\n",
      "Epoch 1640, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 1.6093e-06, -7.5847e-06])\n",
      "Epoch 1641, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 1.6093e-06, -7.5549e-06])\n",
      "Epoch 1642, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 1.6093e-06, -7.5251e-06])\n",
      "Epoch 1643, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 1.7285e-06, -7.4431e-06])\n",
      "Epoch 1644, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 1.8477e-06, -7.3686e-06])\n",
      "Epoch 1645, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 3.7551e-06, -7.0855e-06])\n",
      "Epoch 1646, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 5.9605e-08, -7.5474e-06])\n",
      "Epoch 1647, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 4.1723e-07, -7.4208e-06])\n",
      "Epoch 1648, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 4.1723e-07, -7.3761e-06])\n",
      "Epoch 1649, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 4.1723e-07, -7.3314e-06])\n",
      "Epoch 1650, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 3.8743e-06, -6.9886e-06])\n",
      "Epoch 1651, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 4.1723e-07, -7.3984e-06])\n",
      "Epoch 1652, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 4.1723e-07, -7.3388e-06])\n",
      "Epoch 1653, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 4.1723e-07, -7.3090e-06])\n",
      "Epoch 1654, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.5630e-06, -6.9439e-06])\n",
      "Epoch 1655, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.5630e-06, -6.9290e-06])\n",
      "Epoch 1656, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.9206e-06, -6.7800e-06])\n",
      "Epoch 1657, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.9206e-06, -6.7502e-06])\n",
      "Epoch 1658, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.9206e-06, -6.7204e-06])\n",
      "Epoch 1659, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 3.1590e-06, -6.6459e-06])\n",
      "Epoch 1660, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([-2.6226e-06, -7.3239e-06])\n",
      "Epoch 1661, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([-2.6226e-06, -7.2792e-06])\n",
      "Epoch 1662, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([-4.7684e-07, -6.8992e-06])\n",
      "Epoch 1663, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([-2.3842e-07, -6.8545e-06])\n",
      "Epoch 1664, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([-1.1921e-07, -6.7800e-06])\n",
      "Epoch 1665, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 1.8477e-06, -6.5714e-06])\n",
      "Epoch 1666, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 3.5167e-06, -6.3479e-06])\n",
      "Epoch 1667, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 1.7285e-06, -6.6236e-06])\n",
      "Epoch 1668, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 1.8477e-06, -6.5491e-06])\n",
      "Epoch 1669, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3246e-06, -6.4522e-06])\n",
      "Epoch 1670, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 3.9935e-06, -6.1691e-06])\n",
      "Epoch 1671, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([-1.7881e-06, -6.8471e-06])\n",
      "Epoch 1672, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([-1.6689e-06, -6.7726e-06])\n",
      "Epoch 1673, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([-1.6689e-06, -6.7577e-06])\n",
      "Epoch 1674, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([-1.6689e-06, -6.7428e-06])\n",
      "Epoch 1675, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([-1.5497e-06, -6.6236e-06])\n",
      "Epoch 1676, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([-1.3113e-06, -6.5640e-06])\n",
      "Epoch 1677, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([-8.3447e-07, -6.4597e-06])\n",
      "Epoch 1678, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 8.3447e-07, -6.1989e-06])\n",
      "Epoch 1679, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 1.0729e-06, -6.0946e-06])\n",
      "Epoch 1680, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 1.1921e-06, -6.0201e-06])\n",
      "Epoch 1681, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 3.0398e-06, -5.8189e-06])\n",
      "Epoch 1682, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([-1.1921e-07, -6.1840e-06])\n",
      "Epoch 1683, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-07, -6.0722e-06])\n",
      "Epoch 1684, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-07, -6.0275e-06])\n",
      "Epoch 1685, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 1.9073e-06, -5.7295e-06])\n",
      "Epoch 1686, Loss 0.262983\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.5034e-06, -5.6401e-06])\n",
      "Epoch 1687, Loss 0.262983\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.5034e-06, -5.6103e-06])\n",
      "Epoch 1688, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.8610e-06, -5.4836e-06])\n",
      "Epoch 1689, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.8610e-06, -5.4315e-06])\n",
      "Epoch 1690, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.8610e-06, -5.4166e-06])\n",
      "Epoch 1691, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 3.0994e-06, -5.2974e-06])\n",
      "Epoch 1692, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 5.9605e-07, -5.7071e-06])\n",
      "Epoch 1693, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.2650e-06, -5.4166e-06])\n",
      "Epoch 1694, Loss 0.262983\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.8610e-06, -5.2825e-06])\n",
      "Epoch 1695, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 3.0994e-06, -5.1782e-06])\n",
      "Epoch 1696, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([-2.1458e-06, -5.8040e-06])\n",
      "Epoch 1697, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([-4.7684e-07, -5.6326e-06])\n",
      "Epoch 1698, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 1.3113e-06, -5.3868e-06])\n",
      "Epoch 1699, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 1.5497e-06, -5.3123e-06])\n",
      "Epoch 1700, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 1.6689e-06, -5.2378e-06])\n",
      "Epoch 1701, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.8610e-06, -5.0440e-06])\n",
      "Epoch 1702, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 4.0531e-06, -4.8354e-06])\n",
      "Epoch 1703, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-07, -5.2601e-06])\n",
      "Epoch 1704, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 4.7684e-07, -5.1633e-06])\n",
      "Epoch 1705, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 4.7684e-07, -5.1409e-06])\n",
      "Epoch 1706, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 5.9605e-07, -5.0962e-06])\n",
      "Epoch 1707, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 8.3447e-07, -4.9546e-06])\n",
      "Epoch 1708, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 8.3447e-07, -4.9546e-06])\n",
      "Epoch 1709, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 1.5497e-06, -4.8354e-06])\n",
      "Epoch 1710, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 3.0994e-06, -4.5374e-06])\n",
      "Epoch 1711, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([-8.3447e-07, -5.0291e-06])\n",
      "Epoch 1712, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([-8.3447e-07, -4.9993e-06])\n",
      "Epoch 1713, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 8.3447e-07, -4.7907e-06])\n",
      "Epoch 1714, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.6226e-06, -4.6045e-06])\n",
      "Epoch 1715, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.6226e-06, -4.5449e-06])\n",
      "Epoch 1716, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.8610e-06, -4.4331e-06])\n",
      "Epoch 1717, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 3.5763e-06, -4.3064e-06])\n",
      "Epoch 1718, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([-8.3447e-07, -4.8205e-06])\n",
      "Epoch 1719, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([-8.3447e-07, -4.8056e-06])\n",
      "Epoch 1720, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([-3.5763e-07, -4.6864e-06])\n",
      "Epoch 1721, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([-3.5763e-07, -4.6268e-06])\n",
      "Epoch 1722, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([-3.5763e-07, -4.5970e-06])\n",
      "Epoch 1723, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([-1.1921e-07, -4.4927e-06])\n",
      "Epoch 1724, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([-1.1921e-07, -4.4629e-06])\n",
      "Epoch 1725, Loss 0.262983\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 7.1526e-07, -4.3139e-06])\n",
      "Epoch 1726, Loss 0.262983\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.2650e-06, -4.0457e-06])\n",
      "Epoch 1727, Loss 0.262983\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.2650e-06, -4.0159e-06])\n",
      "Epoch 1728, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.5034e-06, -3.8967e-06])\n",
      "Epoch 1729, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 4.2915e-06, -3.6657e-06])\n",
      "Epoch 1730, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -3.9414e-06])\n",
      "Epoch 1731, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.5034e-06, -3.8818e-06])\n",
      "Epoch 1732, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.6226e-06, -3.8221e-06])\n",
      "Epoch 1733, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 3.9339e-06, -3.5837e-06])\n",
      "Epoch 1734, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 1.1921e-06, -3.9414e-06])\n",
      "Epoch 1735, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 1.3113e-06, -3.8669e-06])\n",
      "Epoch 1736, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 1.5497e-06, -3.7774e-06])\n",
      "Epoch 1737, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 1.5497e-06, -3.7625e-06])\n",
      "Epoch 1738, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 1.5497e-06, -3.7178e-06])\n",
      "Epoch 1739, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 1.7881e-06, -3.6433e-06])\n",
      "Epoch 1740, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 1.9073e-06, -3.5539e-06])\n",
      "Epoch 1741, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 3.5763e-06, -3.2708e-06])\n",
      "Epoch 1742, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([-3.5763e-07, -3.7774e-06])\n",
      "Epoch 1743, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([-1.1921e-07, -3.7029e-06])\n",
      "Epoch 1744, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 0.0000e+00, -3.5986e-06])\n",
      "Epoch 1745, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 3.5763e-06, -3.2336e-06])\n",
      "Epoch 1746, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([-2.5034e-06, -3.9712e-06])\n",
      "Epoch 1747, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([-2.2650e-06, -3.8818e-06])\n",
      "Epoch 1748, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([-2.1458e-06, -3.8221e-06])\n",
      "Epoch 1749, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([-1.4305e-06, -3.7029e-06])\n",
      "Epoch 1750, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 1.1921e-07, -3.4049e-06])\n",
      "Epoch 1751, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 1.1921e-07, -3.4049e-06])\n",
      "Epoch 1752, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 3.5763e-07, -3.2708e-06])\n",
      "Epoch 1753, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 3.5763e-07, -3.2410e-06])\n",
      "Epoch 1754, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 3.5763e-07, -3.2112e-06])\n",
      "Epoch 1755, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 7.1526e-07, -3.1069e-06])\n",
      "Epoch 1756, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Grad:  tensor([ 7.1526e-07, -3.0473e-06])\n",
      "Epoch 1757, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1758, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1759, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1760, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1761, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1762, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1763, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1764, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1765, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1766, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1767, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1768, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1769, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1770, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1771, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1772, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1773, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1774, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1775, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1776, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1777, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1778, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1779, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1780, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1781, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1782, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1783, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1784, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1785, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1786, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1787, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1788, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1789, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1790, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1791, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1792, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1793, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1794, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1795, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1796, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1797, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1798, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1799, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1800, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1801, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1802, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1803, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1804, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1805, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1806, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1807, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1808, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1809, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1810, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1811, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1812, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1813, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1814, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1815, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1816, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1817, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1818, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1819, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1820, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1821, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1822, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1823, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1824, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1825, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1826, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1827, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1828, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1829, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1830, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1831, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1832, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1833, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1834, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1835, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1836, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1837, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1838, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1839, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1840, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1841, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1842, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1843, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1844, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1845, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1846, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1847, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1848, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1849, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1850, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1851, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1852, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1853, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1854, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1855, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1856, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1857, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1858, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1859, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1860, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1861, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1862, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1863, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1864, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1865, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1866, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1867, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1868, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1869, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1870, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1871, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1872, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1873, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1874, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1875, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1876, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1877, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1878, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1879, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1880, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1881, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1882, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1883, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1884, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1885, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1886, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1887, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1888, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1889, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1890, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1891, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1892, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1893, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1894, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1895, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1896, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1897, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1898, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1899, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1900, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1901, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1902, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1903, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1904, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1905, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1906, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1907, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1908, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1909, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1910, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1911, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1912, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1913, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1914, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1915, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1916, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1917, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1918, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1919, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1920, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1921, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1922, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1923, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1924, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1925, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1926, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1927, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1928, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1929, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1930, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1931, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1932, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1933, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1934, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1935, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1936, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1937, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1938, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1939, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1940, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1941, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1942, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1943, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1944, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1945, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1946, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1947, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1948, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1949, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1950, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1951, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1952, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1953, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1954, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1955, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1956, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1957, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1958, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1959, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1960, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1961, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1962, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1963, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1964, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1965, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1966, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1967, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1968, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1969, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1970, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1971, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1972, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1973, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1974, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1975, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1976, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1977, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1978, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1979, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1980, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1981, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1982, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1983, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1984, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1985, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1986, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1987, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1988, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1989, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1990, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1991, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1992, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1993, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1994, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1995, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1996, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1997, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1998, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1999, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2000, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2001, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2002, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2003, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2004, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2005, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2006, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2007, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2008, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2009, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2010, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2011, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2012, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2013, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2014, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2015, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2016, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2017, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2018, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2019, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2020, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2021, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2022, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2023, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2024, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2025, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2026, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2027, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2028, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2029, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2030, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2031, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2032, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2033, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2034, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2035, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2036, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2037, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2038, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2039, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2040, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2041, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2042, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2043, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2044, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2045, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2046, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2047, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2048, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2049, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2050, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2051, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2052, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2053, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2054, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2055, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2056, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2057, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2058, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2059, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2060, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2061, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2062, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2063, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2064, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2065, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2066, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2067, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2068, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2069, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2070, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2071, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2072, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2073, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2074, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2075, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2076, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2077, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2078, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2079, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2080, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2081, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2082, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2083, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2084, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2085, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2086, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2087, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2088, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2089, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2090, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2091, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2092, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2093, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2094, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2095, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2096, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2097, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2098, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2099, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2100, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2101, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2102, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2103, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2104, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2105, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2106, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2107, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2108, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2109, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2110, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2111, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2112, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2113, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2114, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2115, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2116, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2117, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2118, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2119, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2120, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2121, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2122, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2123, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2124, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2125, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2126, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2127, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2128, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2129, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2130, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2131, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2132, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2133, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2134, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2135, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2136, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2137, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2138, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2139, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2140, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2141, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2142, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2143, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2144, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2145, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2146, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2147, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2148, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2149, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2150, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2151, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2152, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2153, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2154, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2155, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2156, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2157, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2158, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2159, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2160, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2161, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2162, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2163, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2164, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2165, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2166, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2167, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2168, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2169, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2170, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2171, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2172, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2173, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2174, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2175, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2176, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2177, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2178, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2179, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2180, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2181, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2182, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2183, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2184, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2185, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2186, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2187, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2188, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2189, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2190, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2191, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2192, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2193, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2194, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2195, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2196, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2197, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2198, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2199, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2200, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2201, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2202, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2203, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2204, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2205, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2206, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2207, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2208, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2209, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2210, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2211, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2212, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2213, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2214, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2215, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2216, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2217, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2218, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2219, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2220, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2221, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2222, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2223, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2224, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2225, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2226, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2227, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2228, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2229, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2230, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2231, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2232, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2233, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2234, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2235, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2236, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2237, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2238, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2239, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2240, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2241, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2242, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2243, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2244, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2245, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2246, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2247, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2248, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2249, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2250, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2251, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2252, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2253, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2254, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2255, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2256, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2257, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2258, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2259, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2260, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2261, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2262, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2263, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2264, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2265, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2266, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2267, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2268, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2269, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2270, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2271, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2272, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2273, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2274, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2275, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2276, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2277, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2278, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2279, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2280, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2281, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2282, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2283, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2284, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2285, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2286, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2287, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2288, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2289, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2290, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2291, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2292, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2293, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2294, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2295, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2296, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2297, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2298, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2299, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2300, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2301, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2302, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2303, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2304, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2305, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2306, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2307, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2308, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2309, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2310, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2311, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2312, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2313, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2314, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2315, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2316, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2317, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2318, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2319, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2320, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2321, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2322, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2323, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2324, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2325, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2326, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2327, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2328, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2329, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2330, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2331, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2332, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2333, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2334, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2335, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2336, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2337, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2338, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2339, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2340, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2341, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2342, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2343, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2344, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2345, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2346, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2347, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2348, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2349, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2350, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2351, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2352, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2353, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2354, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2355, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2356, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2357, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2358, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2359, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2360, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2361, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2362, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2363, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2364, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2365, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2366, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2367, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2368, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2369, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2370, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2371, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2372, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2373, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2374, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2375, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2376, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2377, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2378, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2379, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2380, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2381, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2382, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2383, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2384, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2385, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2386, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2387, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2388, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2389, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2390, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2391, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2392, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2393, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2394, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2395, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2396, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2397, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2398, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2399, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2400, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2401, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2402, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2403, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2404, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2405, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2406, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2407, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2408, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2409, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2410, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2411, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2412, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2413, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2414, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2415, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2416, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2417, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2418, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2419, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2420, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2421, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2422, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2423, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2424, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2425, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2426, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2427, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2428, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2429, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2430, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2431, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2432, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2433, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2434, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2435, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2436, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2437, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2438, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2439, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2440, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2441, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2442, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2443, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2444, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2445, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2446, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2447, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2448, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2449, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2450, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2451, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2452, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2453, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2454, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2455, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2456, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2457, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2458, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2459, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2460, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2461, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2462, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2463, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2464, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2465, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2466, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2467, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2468, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2469, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2470, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2471, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2472, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2473, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2474, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2475, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2476, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2477, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2478, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2479, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2480, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2481, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2482, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2483, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2484, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2485, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2486, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2487, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2488, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2489, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2490, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2491, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2492, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2493, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2494, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2495, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2496, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2497, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2498, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2499, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2500, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2501, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2502, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2503, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2504, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2505, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2506, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2507, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2508, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2509, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2510, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2511, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2512, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2513, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2514, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2515, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2516, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2517, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2518, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2519, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2520, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2521, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2522, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2523, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2524, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2525, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2526, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2527, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2528, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2529, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2530, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2531, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2532, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2533, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2534, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2535, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2536, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2537, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2538, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2539, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2540, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2541, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2542, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2543, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2544, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2545, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2546, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2547, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2548, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2549, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2550, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2551, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2552, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2553, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2554, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2555, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2556, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2557, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2558, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2559, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2560, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2561, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2562, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2563, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2564, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2565, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2566, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2567, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2568, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2569, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2570, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2571, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2572, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2573, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2574, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2575, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2576, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2577, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2578, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2579, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2580, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2581, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2582, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2583, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2584, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2585, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2586, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2587, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2588, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2589, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2590, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2591, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2592, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2593, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2594, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2595, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2596, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2597, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2598, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2599, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2600, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2601, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2602, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2603, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2604, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2605, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2606, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2607, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2608, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2609, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2610, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2611, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2612, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2613, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2614, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2615, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2616, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2617, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2618, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2619, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2620, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2621, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2622, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2623, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2624, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2625, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2626, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2627, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2628, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2629, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2630, Loss 0.262982\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2631, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2632, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2633, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2634, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2635, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2636, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2637, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2638, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2639, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2640, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2641, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2642, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2643, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2644, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2645, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2646, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2647, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2648, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2649, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2650, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2651, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2652, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2653, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2654, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2655, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2656, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2657, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2658, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2659, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2660, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2661, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2662, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2663, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2664, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2665, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2666, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2667, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2668, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2669, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2670, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2671, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2672, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2673, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2674, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2675, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2676, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2677, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2678, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2679, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2680, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2681, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2682, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2683, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2684, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2685, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2686, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2687, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2688, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2689, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2690, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2691, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2692, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2693, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2694, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2695, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2696, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2697, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2698, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2699, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2700, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2701, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2702, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2703, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2704, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2705, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2706, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2707, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2708, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2709, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2710, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2711, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2712, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2713, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2714, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2715, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2716, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2717, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2718, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2719, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2720, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2721, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2722, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2723, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2724, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2725, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2726, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2727, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2728, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2729, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2730, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2731, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2732, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2733, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2734, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2735, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2736, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2737, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2738, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2739, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2740, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2741, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2742, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2743, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2744, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2745, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2746, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2747, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2748, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2749, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2750, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2751, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2752, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2753, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2754, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2755, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2756, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2757, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2758, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2759, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2760, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2761, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2762, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2763, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2764, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2765, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2766, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2767, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2768, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2769, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2770, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2771, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2772, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2773, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2774, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2775, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2776, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2777, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2778, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2779, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2780, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2781, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2782, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2783, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2784, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2785, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2786, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2787, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2788, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2789, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2790, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2791, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2792, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2793, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2794, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2795, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2796, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2797, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2798, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2799, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2800, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2801, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2802, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2803, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2804, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2805, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2806, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2807, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2808, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2809, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2810, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2811, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2812, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2813, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2814, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2815, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2816, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2817, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2818, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2819, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2820, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2821, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2822, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2823, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2824, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2825, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2826, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2827, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2828, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2829, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2830, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2831, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2832, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2833, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2834, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2835, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2836, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2837, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2838, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2839, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2840, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2841, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2842, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2843, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2844, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2845, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2846, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2847, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2848, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2849, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2850, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2851, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2852, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2853, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2854, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2855, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2856, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2857, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2858, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2859, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2860, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2861, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2862, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2863, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2864, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2865, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2866, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2867, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2868, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2869, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2870, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2871, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2872, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2873, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2874, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2875, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2876, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2877, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2878, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2879, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2880, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2881, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2882, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2883, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2884, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2885, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2886, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2887, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2888, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2889, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2890, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2891, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2892, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2893, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2894, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2895, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2896, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2897, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2898, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2899, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2900, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2901, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2902, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2903, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2904, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2905, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2906, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2907, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2908, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2909, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2910, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2911, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2912, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2913, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2914, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2915, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2916, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2917, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2918, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2919, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2920, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2921, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2922, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2923, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2924, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2925, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2926, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2927, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2928, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2929, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2930, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2931, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2932, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2933, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2934, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2935, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2936, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2937, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2938, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2939, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2940, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2941, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2942, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2943, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2944, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2945, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2946, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2947, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2948, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2949, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2950, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2951, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2952, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2953, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2954, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2955, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2956, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2957, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2958, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2959, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2960, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2961, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2962, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2963, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2964, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2965, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2966, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2967, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2968, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2969, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2970, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2971, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2972, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2973, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2974, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2975, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2976, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2977, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2978, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2979, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2980, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2981, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2982, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2983, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2984, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2985, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2986, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2987, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2988, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2989, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2990, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2991, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2992, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2993, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2994, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2995, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2996, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2997, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2998, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2999, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3000, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3001, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3002, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3003, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3004, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3005, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3006, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3007, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3008, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3009, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3010, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3011, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3012, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3013, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3014, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3015, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3016, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3017, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3018, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3019, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3020, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3021, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3022, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3023, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3024, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3025, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3026, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3027, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3028, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3029, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3030, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3031, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3032, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3033, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3034, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3035, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3036, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3037, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3038, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3039, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3040, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3041, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3042, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3043, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3044, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3045, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3046, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3047, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3048, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3049, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3050, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3051, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3052, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3053, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3054, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3055, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3056, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3057, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3058, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3059, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3060, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3061, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3062, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3063, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3064, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3065, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3066, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3067, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3068, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3069, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3070, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3071, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3072, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3073, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3074, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3075, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3076, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3077, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3078, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3079, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3080, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3081, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3082, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3083, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3084, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3085, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3086, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3087, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3088, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3089, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3090, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3091, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3092, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3093, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3094, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3095, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3096, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3097, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3098, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3099, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3100, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3101, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3102, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3103, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3104, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3105, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3106, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3107, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3108, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3109, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3110, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3111, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3112, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3113, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3114, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3115, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3116, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3117, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3118, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3119, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3120, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3121, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3122, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3123, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3124, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3125, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3126, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3127, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3128, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3129, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3130, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3131, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3132, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3133, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3134, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3135, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3136, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3137, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3138, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3139, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3140, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3141, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3142, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3143, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3144, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3145, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3146, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3147, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3148, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3149, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3150, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3151, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3152, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3153, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3154, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3155, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3156, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3157, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3158, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3159, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3160, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3161, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3162, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3163, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3164, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3165, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3166, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3167, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3168, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3169, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3170, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3171, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3172, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3173, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3174, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3175, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3176, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3177, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3178, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3179, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3180, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3181, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3182, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3183, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3184, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3185, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3186, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3187, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3188, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3189, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3190, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3191, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3192, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3193, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3194, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3195, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3196, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3197, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3198, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3199, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3200, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3201, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3202, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3203, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3204, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3205, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3206, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3207, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3208, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3209, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3210, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3211, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3212, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3213, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3214, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3215, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3216, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3217, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3218, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3219, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3220, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3221, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3222, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3223, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3224, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3225, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3226, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3227, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3228, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3229, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3230, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3231, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3232, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3233, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3234, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3235, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3236, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3237, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3238, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3239, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3240, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3241, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3242, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3243, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3244, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3245, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3246, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3247, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3248, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3249, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3250, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3251, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3252, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3253, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3254, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3255, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3256, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3257, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3258, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3259, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3260, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3261, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3262, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3263, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3264, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3265, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3266, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3267, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3268, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3269, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3270, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3271, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3272, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3273, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3274, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3275, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3276, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3277, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3278, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3279, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3280, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3281, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3282, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3283, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3284, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3285, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3286, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3287, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3288, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3289, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3290, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3291, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3292, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3293, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3294, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3295, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3296, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3297, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3298, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3299, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3300, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3301, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3302, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3303, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3304, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3305, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3306, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3307, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3308, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3309, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3310, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3311, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3312, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3313, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3314, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3315, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3316, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3317, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3318, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3319, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3320, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3321, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3322, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3323, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3324, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3325, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3326, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3327, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3328, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3329, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3330, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3331, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3332, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3333, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3334, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3335, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3336, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3337, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3338, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3339, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3340, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3341, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3342, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3343, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3344, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3345, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3346, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3347, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3348, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3349, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3350, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3351, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3352, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3353, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3354, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3355, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3356, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3357, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3358, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3359, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3360, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3361, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3362, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3363, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3364, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3365, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3366, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3367, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3368, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3369, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3370, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3371, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3372, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3373, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3374, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3375, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3376, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3377, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3378, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3379, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3380, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3381, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3382, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3383, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3384, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3385, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3386, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3387, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3388, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3389, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3390, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3391, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3392, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3393, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3394, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3395, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3396, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3397, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3398, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3399, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3400, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3401, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3402, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3403, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3404, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3405, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3406, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3407, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3408, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3409, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3410, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3411, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3412, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3413, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3414, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3415, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3416, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3417, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3418, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3419, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3420, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3421, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3422, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3423, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3424, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3425, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3426, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3427, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3428, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3429, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3430, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3431, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3432, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3433, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3434, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3435, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3436, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3437, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3438, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3439, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3440, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3441, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3442, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3443, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3444, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3445, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3446, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3447, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3448, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3449, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3450, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3451, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3452, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3453, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3454, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3455, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3456, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3457, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3458, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3459, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3460, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3461, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3462, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3463, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3464, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3465, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3466, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3467, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3468, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3469, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3470, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3471, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3472, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3473, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3474, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3475, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3476, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3477, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3478, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3479, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3480, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3481, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3482, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3483, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3484, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3485, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3486, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3487, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3488, Loss 0.262982\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3489, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3490, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3491, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3492, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3493, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3494, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3495, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3496, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3497, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3498, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3499, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3500, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3501, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3502, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3503, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3504, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3505, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3506, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3507, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3508, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3509, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3510, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3511, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3512, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3513, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3514, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3515, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3516, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3517, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3518, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3519, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3520, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3521, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3522, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3523, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3524, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3525, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3526, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3527, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3528, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3529, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3530, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3531, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3532, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3533, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3534, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3535, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3536, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3537, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3538, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3539, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3540, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3541, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3542, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3543, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3544, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3545, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3546, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3547, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3548, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3549, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3550, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3551, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3552, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3553, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3554, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3555, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3556, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3557, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3558, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3559, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3560, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3561, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3562, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3563, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3564, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3565, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3566, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3567, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3568, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3569, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3570, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3571, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3572, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3573, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3574, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3575, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3576, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3577, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3578, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3579, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3580, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3581, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3582, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3583, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3584, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3585, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3586, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3587, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3588, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3589, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3590, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3591, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3592, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3593, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3594, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3595, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3596, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3597, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3598, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3599, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3600, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3601, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3602, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3603, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3604, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3605, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3606, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3607, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3608, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3609, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3610, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3611, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3612, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3613, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3614, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3615, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3616, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3617, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3618, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3619, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3620, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3621, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3622, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3623, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3624, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3625, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3626, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3627, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3628, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3629, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3630, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3631, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3632, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3633, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3634, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3635, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3636, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3637, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3638, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3639, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3640, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3641, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3642, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3643, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3644, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3645, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3646, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3647, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3648, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3649, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3650, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3651, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3652, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3653, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3654, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3655, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3656, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3657, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3658, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3659, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3660, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3661, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3662, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3663, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3664, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3665, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3666, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3667, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3668, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3669, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3670, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3671, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3672, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3673, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3674, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3675, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3676, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3677, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3678, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3679, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3680, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3681, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3682, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3683, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3684, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3685, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3686, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3687, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3688, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3689, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3690, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3691, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3692, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3693, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3694, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3695, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3696, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3697, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3698, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3699, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3700, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3701, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3702, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3703, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3704, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3705, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3706, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3707, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3708, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3709, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3710, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3711, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3712, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3713, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3714, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3715, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3716, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3717, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3718, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3719, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3720, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3721, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3722, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3723, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3724, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3725, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3726, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3727, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3728, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3729, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3730, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3731, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3732, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3733, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3734, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3735, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3736, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3737, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3738, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3739, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3740, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3741, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3742, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3743, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3744, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3745, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3746, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3747, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3748, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3749, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3750, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3751, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3752, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3753, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3754, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3755, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3756, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3757, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3758, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3759, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3760, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3761, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3762, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3763, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3764, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3765, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3766, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3767, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3768, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3769, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3770, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3771, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3772, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3773, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3774, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3775, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3776, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3777, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3778, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3779, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3780, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3781, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3782, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3783, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3784, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3785, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3786, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3787, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3788, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3789, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3790, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3791, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3792, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3793, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3794, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3795, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3796, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3797, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3798, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3799, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3800, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3801, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3802, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3803, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3804, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3805, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3806, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3807, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3808, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3809, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3810, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3811, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3812, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3813, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3814, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3815, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3816, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3817, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3818, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3819, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3820, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3821, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3822, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3823, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3824, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3825, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3826, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3827, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3828, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3829, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3830, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3831, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3832, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3833, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3834, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3835, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3836, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3837, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3838, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3839, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3840, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3841, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3842, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3843, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3844, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3845, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3846, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3847, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3848, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3849, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3850, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3851, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3852, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3853, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3854, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3855, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3856, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3857, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3858, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3859, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3860, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3861, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3862, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3863, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3864, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3865, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3866, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3867, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3868, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3869, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3870, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3871, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3872, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3873, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3874, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3875, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3876, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3877, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3878, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3879, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3880, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3881, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3882, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3883, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3884, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3885, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3886, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3887, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3888, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3889, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3890, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3891, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3892, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3893, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3894, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3895, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3896, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3897, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3898, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3899, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3900, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3901, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3902, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3903, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3904, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3905, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3906, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3907, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3908, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3909, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3910, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3911, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3912, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3913, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3914, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3915, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3916, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3917, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3918, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3919, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3920, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3921, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3922, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3923, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3924, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3925, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3926, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3927, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3928, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3929, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3930, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3931, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3932, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3933, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3934, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3935, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3936, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3937, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3938, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3939, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3940, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3941, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3942, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3943, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3944, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3945, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3946, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3947, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3948, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3949, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3950, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3951, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3952, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3953, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3954, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3955, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3956, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3957, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3958, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3959, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3960, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3961, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3962, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3963, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3964, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3965, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3966, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3967, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3968, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3969, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3970, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3971, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3972, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3973, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3974, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3975, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3976, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3977, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3978, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3979, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3980, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3981, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3982, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3983, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3984, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3985, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3986, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3987, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3988, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3989, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3990, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3991, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3992, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3993, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3994, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3995, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3996, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3997, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3998, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3999, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4000, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4001, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4002, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4003, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4004, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4005, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4006, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4007, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4008, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4009, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4010, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4011, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4012, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4013, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4014, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4015, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4016, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4017, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4018, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4019, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4020, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4021, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4022, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4023, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4024, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4025, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4026, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4027, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4028, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4029, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4030, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4031, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4032, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4033, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4034, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4035, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4036, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4037, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4038, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4039, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4040, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4041, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4042, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4043, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4044, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4045, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4046, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4047, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4048, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4049, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4050, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4051, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4052, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4053, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4054, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4055, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4056, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4057, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4058, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4059, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4060, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4061, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4062, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4063, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4064, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4065, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4066, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4067, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4068, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4069, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4070, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4071, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4072, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4073, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4074, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4075, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4076, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4077, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4078, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4079, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4080, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4081, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4082, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4083, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4084, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4085, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4086, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4087, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4088, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4089, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4090, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4091, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4092, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4093, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4094, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4095, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4096, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4097, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4098, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4099, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4100, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4101, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4102, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4103, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4104, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4105, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4106, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4107, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4108, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4109, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4110, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4111, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4112, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4113, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4114, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4115, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4116, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4117, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4118, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4119, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4120, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4121, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4122, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4123, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4124, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4125, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4126, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4127, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4128, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4129, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4130, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4131, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4132, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4133, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4134, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4135, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4136, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4137, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4138, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4139, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4140, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4141, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4142, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4143, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4144, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4145, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4146, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4147, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4148, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4149, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4150, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4151, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4152, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4153, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4154, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4155, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4156, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4157, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4158, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4159, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4160, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4161, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4162, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4163, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4164, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4165, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4166, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4167, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4168, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4169, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4170, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4171, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4172, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4173, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4174, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4175, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4176, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4177, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4178, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4179, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4180, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4181, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4182, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4183, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4184, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4185, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4186, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4187, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4188, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4189, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4190, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4191, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4192, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4193, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4194, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4195, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4196, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4197, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4198, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4199, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4200, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4201, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4202, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4203, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4204, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4205, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4206, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4207, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4208, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4209, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4210, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4211, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4212, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4213, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4214, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4215, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4216, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4217, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4218, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4219, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4220, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4221, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4222, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4223, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4224, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4225, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4226, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4227, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4228, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4229, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4230, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4231, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4232, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4233, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4234, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4235, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4236, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4237, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4238, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4239, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4240, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4241, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4242, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4243, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4244, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4245, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4246, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4247, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4248, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4249, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4250, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4251, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4252, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4253, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4254, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4255, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4256, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4257, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4258, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4259, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4260, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4261, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4262, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4263, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4264, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4265, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4266, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4267, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4268, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4269, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4270, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4271, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4272, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4273, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4274, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4275, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4276, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4277, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4278, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4279, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4280, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4281, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4282, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4283, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4284, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4285, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4286, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4287, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4288, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4289, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4290, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4291, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4292, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4293, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4294, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4295, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4296, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4297, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4298, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4299, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4300, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4301, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4302, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4303, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4304, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4305, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4306, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4307, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4308, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4309, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4310, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4311, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4312, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4313, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4314, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4315, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4316, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4317, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4318, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4319, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4320, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4321, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4322, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4323, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4324, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4325, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4326, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4327, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4328, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4329, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4330, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4331, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4332, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4333, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4334, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4335, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4336, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4337, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4338, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4339, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4340, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4341, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4342, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4343, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4344, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4345, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4346, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4347, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4348, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4349, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4350, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4351, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4352, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4353, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4354, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4355, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4356, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4357, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4358, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4359, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4360, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4361, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4362, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4363, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4364, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4365, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4366, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4367, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4368, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4369, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4370, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4371, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4372, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4373, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4374, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4375, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4376, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4377, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4378, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4379, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4380, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4381, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4382, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4383, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4384, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4385, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4386, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4387, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4388, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4389, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4390, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4391, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4392, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4393, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4394, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4395, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4396, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4397, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4398, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4399, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4400, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4401, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4402, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4403, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4404, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4405, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4406, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4407, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4408, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4409, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4410, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4411, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4412, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4413, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4414, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4415, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4416, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4417, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4418, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4419, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4420, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4421, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4422, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4423, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4424, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4425, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4426, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4427, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4428, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4429, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4430, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4431, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4432, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4433, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4434, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4435, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4436, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4437, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4438, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4439, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4440, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4441, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4442, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4443, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4444, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4445, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4446, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4447, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4448, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4449, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4450, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4451, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4452, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4453, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4454, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4455, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4456, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4457, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4458, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4459, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4460, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4461, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4462, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4463, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4464, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4465, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4466, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4467, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4468, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4469, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4470, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4471, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4472, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4473, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4474, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4475, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4476, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4477, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4478, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4479, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4480, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4481, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4482, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4483, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4484, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4485, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4486, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4487, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4488, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4489, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4490, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4491, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4492, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4493, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4494, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4495, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4496, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4497, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4498, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4499, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4500, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4501, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4502, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4503, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4504, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4505, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4506, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4507, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4508, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4509, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4510, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4511, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4512, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4513, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4514, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4515, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4516, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4517, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4518, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4519, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4520, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4521, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4522, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4523, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4524, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4525, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4526, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4527, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4528, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4529, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4530, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4531, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4532, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4533, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4534, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4535, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4536, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4537, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4538, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4539, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4540, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4541, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4542, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4543, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4544, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4545, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4546, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4547, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4548, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4549, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4550, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4551, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4552, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4553, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4554, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4555, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4556, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4557, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4558, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4559, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4560, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4561, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4562, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4563, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4564, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4565, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4566, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4567, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4568, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4569, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4570, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4571, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4572, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4573, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4574, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4575, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4576, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4577, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4578, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4579, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4580, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4581, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4582, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4583, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4584, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4585, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4586, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4587, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4588, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4589, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4590, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4591, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4592, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4593, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4594, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4595, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4596, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4597, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4598, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4599, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4600, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4601, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4602, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4603, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4604, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4605, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4606, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4607, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4608, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4609, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4610, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4611, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4612, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4613, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4614, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4615, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4616, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4617, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4618, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4619, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4620, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4621, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4622, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4623, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4624, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4625, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4626, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4627, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4628, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4629, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4630, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4631, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4632, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4633, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4634, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4635, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4636, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4637, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4638, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4639, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4640, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4641, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4642, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4643, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4644, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4645, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4646, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4647, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4648, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4649, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4650, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4651, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4652, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4653, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4654, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4655, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4656, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4657, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4658, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4659, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4660, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4661, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4662, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4663, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4664, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4665, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4666, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4667, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4668, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4669, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4670, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4671, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4672, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4673, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4674, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4675, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4676, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4677, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4678, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4679, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4680, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4681, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4682, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4683, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4684, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4685, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4686, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4687, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4688, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4689, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4690, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4691, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4692, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4693, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4694, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4695, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4696, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4697, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4698, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4699, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4700, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4701, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4702, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4703, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4704, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4705, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4706, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4707, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4708, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4709, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4710, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4711, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4712, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4713, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4714, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4715, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4716, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4717, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4718, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4719, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4720, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4721, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4722, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4723, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4724, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4725, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4726, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4727, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4728, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4729, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4730, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4731, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4732, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4733, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4734, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4735, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4736, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4737, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4738, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4739, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4740, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4741, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4742, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4743, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4744, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4745, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4746, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4747, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4748, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4749, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4750, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4751, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4752, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4753, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4754, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4755, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4756, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4757, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4758, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4759, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4760, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4761, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4762, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4763, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4764, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4765, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4766, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4767, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4768, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4769, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4770, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4771, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4772, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4773, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4774, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4775, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4776, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4777, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4778, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4779, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4780, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4781, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4782, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4783, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4784, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4785, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4786, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4787, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4788, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4789, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4790, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4791, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4792, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4793, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4794, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4795, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4796, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4797, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4798, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4799, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4800, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4801, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4802, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4803, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4804, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4805, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4806, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4807, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4808, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4809, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4810, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4811, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4812, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4813, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4814, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4815, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4816, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4817, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4818, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4819, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4820, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4821, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4822, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4823, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4824, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4825, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4826, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4827, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4828, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4829, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4830, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4831, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4832, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4833, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4834, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4835, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4836, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4837, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4838, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4839, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4840, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4841, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4842, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4843, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4844, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4845, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4846, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4847, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4848, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4849, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4850, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4851, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4852, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4853, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4854, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4855, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4856, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4857, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4858, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4859, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4860, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4861, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4862, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4863, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4864, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4865, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4866, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4867, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4868, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4869, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4870, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4871, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4872, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4873, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4874, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4875, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4876, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4877, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4878, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4879, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4880, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4881, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4882, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4883, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4884, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4885, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4886, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4887, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4888, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4889, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4890, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4891, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4892, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4893, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4894, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4895, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4896, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4897, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4898, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4899, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4900, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4901, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4902, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4903, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4904, Loss 0.262982\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4905, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4906, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4907, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4908, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4909, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4910, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4911, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4912, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4913, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4914, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4915, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4916, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4917, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4918, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4919, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4920, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4921, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4922, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4923, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4924, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4925, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4926, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4927, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4928, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4929, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4930, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4931, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4932, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4933, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4934, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4935, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4936, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4937, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4938, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4939, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4940, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4941, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4942, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4943, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4944, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4945, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4946, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4947, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4948, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4949, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4950, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4951, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4952, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4953, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4954, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4955, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4956, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4957, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4958, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4959, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4960, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4961, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4962, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4963, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4964, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4965, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4966, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4967, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4968, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4969, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4970, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4971, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4972, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4973, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4974, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4975, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4976, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4977, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4978, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4979, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4980, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4981, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4982, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4983, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4984, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4985, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4986, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4987, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4988, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4989, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4990, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4991, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4992, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4993, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4994, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4995, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4996, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4997, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4998, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4999, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 5000, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n"
     ]
    }
   ],
   "source": [
    "params = training_loop(\n",
    "    n_epochs = 5000,\n",
    "    learning_rate = 1e-2,\n",
    "    params = torch.tensor([1.0, 0.0]),\n",
    "    x = x,\n",
    "    y = y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "authorized-manor",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 3600x2400 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "yy = model(x, *params)\n",
    "fig = plt.figure(dpi=600)\n",
    "plt.xlabel(\"x label\")\n",
    "plt.ylabel(\"y label\")\n",
    "plt.plot(x.numpy(), yy.detach().numpy())\n",
    "plt.plot(x.numpy(), y.numpy(), 'o')\n",
    "plt.savefig(\"dl_ch1t2_plot01.png\", format=\"png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daily-phenomenon",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
