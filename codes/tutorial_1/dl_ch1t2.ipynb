{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fiscal-moral",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "subtle-artwork",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = [4.209015  , 6.0251656 , 6.586659  , 1.0785204 , 5.323591,   2.9644287, 8.885769  , 9.895647  ,  6.464806  , 0.18034637, 1.2534696]\n",
    "x = [34.552039 , 74.45411  , 80.987488 ,  3.458197 , 56.4778655, 26.98163  , 95.79415  , 106.228316 , 61.169422 , 1.089516 , 8.962632]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "retained-adams",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 34.5520,  74.4541,  80.9875,   3.4582,  56.4779,  26.9816,  95.7942,\n",
       "        106.2283,  61.1694,   1.0895,   8.9626])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = torch.tensor(y)\n",
    "x = torch.tensor(x)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "different-trading",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(x, w, b):\n",
    "    return w * x + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "united-catholic",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn(yy, y):\n",
    "    squared_diffs = (yy - y)**2\n",
    "    return squared_diffs.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "constant-animation",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dloss_fn(yy, y):\n",
    "    dsq_diffs = 2 * (yy - y) / yy.size(0)\n",
    "    return dsq_diffs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "clear-surname",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dmodel_dw(x, w, b):\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "facial-least",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dmodel_db(x, w, b):\n",
    "    return 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "quiet-monaco",
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad_fn(x, y, yy, w, b):\n",
    "    dloss_dyy = dloss_fn(yy,y)\n",
    "    dloss_dw = dloss_dyy * dmodel_dw(x, w, b)\n",
    "    dloss_db = dloss_dyy * dmodel_db(x, w, b)\n",
    "    return torch.stack([dloss_dw.sum(), dloss_db.sum()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "composite-fairy",
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_loop(n_epochs, learning_rate, params, x, y):\n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        w, b = params\n",
    "        yy = model(x, w, b)\n",
    "        loss = loss_fn(yy, y)                \n",
    "        grad = grad_fn(x, y, yy, w, b)        \n",
    "        params = params - learning_rate * grad\n",
    "        print('Epoch %d, Loss %f' % (epoch, float(loss)))\n",
    "        print('\\t Params: ', params)\n",
    "        print('\\t Grad: ', grad)\n",
    "        \n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "medieval-beverage",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 3.4552,  7.4454,  8.0987,  0.3458,  5.6478,  2.6982,  9.5794, 10.6228,\n",
       "         6.1169,  0.1090,  0.8963])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = x * 0.1\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "altered-overview",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss 0.622568\n",
      "\t Params:  tensor([ 0.9400, -0.0039])\n",
      "\t Grad:  tensor([5.9979, 0.3906])\n",
      "Epoch 2, Loss 0.399406\n",
      "\t Params:  tensor([ 0.9257, -0.0017])\n",
      "\t Grad:  tensor([ 1.4320, -0.2172])\n",
      "Epoch 3, Loss 0.385859\n",
      "\t Params:  tensor([0.9220, 0.0018])\n",
      "\t Grad:  tensor([ 0.3730, -0.3561])\n",
      "Epoch 4, Loss 0.383605\n",
      "\t Params:  tensor([0.9207, 0.0057])\n",
      "\t Grad:  tensor([ 0.1271, -0.3863])\n",
      "Epoch 5, Loss 0.381978\n",
      "\t Params:  tensor([0.9200, 0.0096])\n",
      "\t Grad:  tensor([ 0.0698, -0.3913])\n",
      "Epoch 6, Loss 0.380404\n",
      "\t Params:  tensor([0.9194, 0.0135])\n",
      "\t Grad:  tensor([ 0.0563, -0.3904])\n",
      "Epoch 7, Loss 0.378853\n",
      "\t Params:  tensor([0.9189, 0.0174])\n",
      "\t Grad:  tensor([ 0.0529, -0.3883])\n",
      "Epoch 8, Loss 0.377323\n",
      "\t Params:  tensor([0.9184, 0.0212])\n",
      "\t Grad:  tensor([ 0.0518, -0.3858])\n",
      "Epoch 9, Loss 0.375813\n",
      "\t Params:  tensor([0.9179, 0.0251])\n",
      "\t Grad:  tensor([ 0.0513, -0.3832])\n",
      "Epoch 10, Loss 0.374323\n",
      "\t Params:  tensor([0.9174, 0.0289])\n",
      "\t Grad:  tensor([ 0.0509, -0.3807])\n",
      "Epoch 11, Loss 0.372852\n",
      "\t Params:  tensor([0.9169, 0.0327])\n",
      "\t Grad:  tensor([ 0.0506, -0.3782])\n",
      "Epoch 12, Loss 0.371401\n",
      "\t Params:  tensor([0.9164, 0.0364])\n",
      "\t Grad:  tensor([ 0.0502, -0.3757])\n",
      "Epoch 13, Loss 0.369969\n",
      "\t Params:  tensor([0.9159, 0.0402])\n",
      "\t Grad:  tensor([ 0.0499, -0.3732])\n",
      "Epoch 14, Loss 0.368557\n",
      "\t Params:  tensor([0.9154, 0.0439])\n",
      "\t Grad:  tensor([ 0.0496, -0.3707])\n",
      "Epoch 15, Loss 0.367162\n",
      "\t Params:  tensor([0.9149, 0.0475])\n",
      "\t Grad:  tensor([ 0.0492, -0.3683])\n",
      "Epoch 16, Loss 0.365786\n",
      "\t Params:  tensor([0.9144, 0.0512])\n",
      "\t Grad:  tensor([ 0.0489, -0.3658])\n",
      "Epoch 17, Loss 0.364429\n",
      "\t Params:  tensor([0.9139, 0.0548])\n",
      "\t Grad:  tensor([ 0.0486, -0.3634])\n",
      "Epoch 18, Loss 0.363089\n",
      "\t Params:  tensor([0.9134, 0.0585])\n",
      "\t Grad:  tensor([ 0.0483, -0.3610])\n",
      "Epoch 19, Loss 0.361767\n",
      "\t Params:  tensor([0.9129, 0.0620])\n",
      "\t Grad:  tensor([ 0.0480, -0.3586])\n",
      "Epoch 20, Loss 0.360462\n",
      "\t Params:  tensor([0.9125, 0.0656])\n",
      "\t Grad:  tensor([ 0.0476, -0.3562])\n",
      "Epoch 21, Loss 0.359175\n",
      "\t Params:  tensor([0.9120, 0.0691])\n",
      "\t Grad:  tensor([ 0.0473, -0.3539])\n",
      "Epoch 22, Loss 0.357905\n",
      "\t Params:  tensor([0.9115, 0.0727])\n",
      "\t Grad:  tensor([ 0.0470, -0.3515])\n",
      "Epoch 23, Loss 0.356651\n",
      "\t Params:  tensor([0.9111, 0.0761])\n",
      "\t Grad:  tensor([ 0.0467, -0.3492])\n",
      "Epoch 24, Loss 0.355414\n",
      "\t Params:  tensor([0.9106, 0.0796])\n",
      "\t Grad:  tensor([ 0.0464, -0.3469])\n",
      "Epoch 25, Loss 0.354193\n",
      "\t Params:  tensor([0.9101, 0.0831])\n",
      "\t Grad:  tensor([ 0.0461, -0.3446])\n",
      "Epoch 26, Loss 0.352988\n",
      "\t Params:  tensor([0.9097, 0.0865])\n",
      "\t Grad:  tensor([ 0.0458, -0.3423])\n",
      "Epoch 27, Loss 0.351800\n",
      "\t Params:  tensor([0.9092, 0.0899])\n",
      "\t Grad:  tensor([ 0.0455, -0.3400])\n",
      "Epoch 28, Loss 0.350627\n",
      "\t Params:  tensor([0.9088, 0.0933])\n",
      "\t Grad:  tensor([ 0.0452, -0.3378])\n",
      "Epoch 29, Loss 0.349469\n",
      "\t Params:  tensor([0.9083, 0.0966])\n",
      "\t Grad:  tensor([ 0.0449, -0.3355])\n",
      "Epoch 30, Loss 0.348327\n",
      "\t Params:  tensor([0.9079, 0.0999])\n",
      "\t Grad:  tensor([ 0.0446, -0.3333])\n",
      "Epoch 31, Loss 0.347200\n",
      "\t Params:  tensor([0.9074, 0.1033])\n",
      "\t Grad:  tensor([ 0.0443, -0.3311])\n",
      "Epoch 32, Loss 0.346088\n",
      "\t Params:  tensor([0.9070, 0.1065])\n",
      "\t Grad:  tensor([ 0.0440, -0.3289])\n",
      "Epoch 33, Loss 0.344990\n",
      "\t Params:  tensor([0.9065, 0.1098])\n",
      "\t Grad:  tensor([ 0.0437, -0.3267])\n",
      "Epoch 34, Loss 0.343907\n",
      "\t Params:  tensor([0.9061, 0.1131])\n",
      "\t Grad:  tensor([ 0.0434, -0.3246])\n",
      "Epoch 35, Loss 0.342838\n",
      "\t Params:  tensor([0.9057, 0.1163])\n",
      "\t Grad:  tensor([ 0.0431, -0.3224])\n",
      "Epoch 36, Loss 0.341784\n",
      "\t Params:  tensor([0.9053, 0.1195])\n",
      "\t Grad:  tensor([ 0.0428, -0.3203])\n",
      "Epoch 37, Loss 0.340743\n",
      "\t Params:  tensor([0.9048, 0.1227])\n",
      "\t Grad:  tensor([ 0.0425, -0.3182])\n",
      "Epoch 38, Loss 0.339716\n",
      "\t Params:  tensor([0.9044, 0.1258])\n",
      "\t Grad:  tensor([ 0.0423, -0.3161])\n",
      "Epoch 39, Loss 0.338703\n",
      "\t Params:  tensor([0.9040, 0.1290])\n",
      "\t Grad:  tensor([ 0.0420, -0.3140])\n",
      "Epoch 40, Loss 0.337703\n",
      "\t Params:  tensor([0.9036, 0.1321])\n",
      "\t Grad:  tensor([ 0.0417, -0.3119])\n",
      "Epoch 41, Loss 0.336716\n",
      "\t Params:  tensor([0.9032, 0.1352])\n",
      "\t Grad:  tensor([ 0.0414, -0.3098])\n",
      "Epoch 42, Loss 0.335742\n",
      "\t Params:  tensor([0.9027, 0.1383])\n",
      "\t Grad:  tensor([ 0.0412, -0.3078])\n",
      "Epoch 43, Loss 0.334781\n",
      "\t Params:  tensor([0.9023, 0.1413])\n",
      "\t Grad:  tensor([ 0.0409, -0.3057])\n",
      "Epoch 44, Loss 0.333833\n",
      "\t Params:  tensor([0.9019, 0.1444])\n",
      "\t Grad:  tensor([ 0.0406, -0.3037])\n",
      "Epoch 45, Loss 0.332897\n",
      "\t Params:  tensor([0.9015, 0.1474])\n",
      "\t Grad:  tensor([ 0.0403, -0.3017])\n",
      "Epoch 46, Loss 0.331974\n",
      "\t Params:  tensor([0.9011, 0.1504])\n",
      "\t Grad:  tensor([ 0.0401, -0.2997])\n",
      "Epoch 47, Loss 0.331063\n",
      "\t Params:  tensor([0.9007, 0.1534])\n",
      "\t Grad:  tensor([ 0.0398, -0.2977])\n",
      "Epoch 48, Loss 0.330164\n",
      "\t Params:  tensor([0.9003, 0.1563])\n",
      "\t Grad:  tensor([ 0.0395, -0.2957])\n",
      "Epoch 49, Loss 0.329276\n",
      "\t Params:  tensor([0.8999, 0.1592])\n",
      "\t Grad:  tensor([ 0.0393, -0.2938])\n",
      "Epoch 50, Loss 0.328401\n",
      "\t Params:  tensor([0.8995, 0.1622])\n",
      "\t Grad:  tensor([ 0.0390, -0.2918])\n",
      "Epoch 51, Loss 0.327537\n",
      "\t Params:  tensor([0.8992, 0.1651])\n",
      "\t Grad:  tensor([ 0.0388, -0.2899])\n",
      "Epoch 52, Loss 0.326684\n",
      "\t Params:  tensor([0.8988, 0.1679])\n",
      "\t Grad:  tensor([ 0.0385, -0.2880])\n",
      "Epoch 53, Loss 0.325843\n",
      "\t Params:  tensor([0.8984, 0.1708])\n",
      "\t Grad:  tensor([ 0.0383, -0.2861])\n",
      "Epoch 54, Loss 0.325013\n",
      "\t Params:  tensor([0.8980, 0.1736])\n",
      "\t Grad:  tensor([ 0.0380, -0.2842])\n",
      "Epoch 55, Loss 0.324194\n",
      "\t Params:  tensor([0.8976, 0.1765])\n",
      "\t Grad:  tensor([ 0.0377, -0.2823])\n",
      "Epoch 56, Loss 0.323385\n",
      "\t Params:  tensor([0.8973, 0.1793])\n",
      "\t Grad:  tensor([ 0.0375, -0.2804])\n",
      "Epoch 57, Loss 0.322587\n",
      "\t Params:  tensor([0.8969, 0.1821])\n",
      "\t Grad:  tensor([ 0.0372, -0.2786])\n",
      "Epoch 58, Loss 0.321800\n",
      "\t Params:  tensor([0.8965, 0.1848])\n",
      "\t Grad:  tensor([ 0.0370, -0.2767])\n",
      "Epoch 59, Loss 0.321023\n",
      "\t Params:  tensor([0.8962, 0.1876])\n",
      "\t Grad:  tensor([ 0.0368, -0.2749])\n",
      "Epoch 60, Loss 0.320257\n",
      "\t Params:  tensor([0.8958, 0.1903])\n",
      "\t Grad:  tensor([ 0.0365, -0.2731])\n",
      "Epoch 61, Loss 0.319500\n",
      "\t Params:  tensor([0.8954, 0.1930])\n",
      "\t Grad:  tensor([ 0.0363, -0.2712])\n",
      "Epoch 62, Loss 0.318754\n",
      "\t Params:  tensor([0.8951, 0.1957])\n",
      "\t Grad:  tensor([ 0.0360, -0.2694])\n",
      "Epoch 63, Loss 0.318018\n",
      "\t Params:  tensor([0.8947, 0.1984])\n",
      "\t Grad:  tensor([ 0.0358, -0.2677])\n",
      "Epoch 64, Loss 0.317291\n",
      "\t Params:  tensor([0.8944, 0.2010])\n",
      "\t Grad:  tensor([ 0.0356, -0.2659])\n",
      "Epoch 65, Loss 0.316574\n",
      "\t Params:  tensor([0.8940, 0.2037])\n",
      "\t Grad:  tensor([ 0.0353, -0.2641])\n",
      "Epoch 66, Loss 0.315866\n",
      "\t Params:  tensor([0.8936, 0.2063])\n",
      "\t Grad:  tensor([ 0.0351, -0.2624])\n",
      "Epoch 67, Loss 0.315167\n",
      "\t Params:  tensor([0.8933, 0.2089])\n",
      "\t Grad:  tensor([ 0.0348, -0.2606])\n",
      "Epoch 68, Loss 0.314478\n",
      "\t Params:  tensor([0.8930, 0.2115])\n",
      "\t Grad:  tensor([ 0.0346, -0.2589])\n",
      "Epoch 69, Loss 0.313798\n",
      "\t Params:  tensor([0.8926, 0.2141])\n",
      "\t Grad:  tensor([ 0.0344, -0.2572])\n",
      "Epoch 70, Loss 0.313127\n",
      "\t Params:  tensor([0.8923, 0.2166])\n",
      "\t Grad:  tensor([ 0.0342, -0.2555])\n",
      "Epoch 71, Loss 0.312465\n",
      "\t Params:  tensor([0.8919, 0.2192])\n",
      "\t Grad:  tensor([ 0.0339, -0.2538])\n",
      "Epoch 72, Loss 0.311811\n",
      "\t Params:  tensor([0.8916, 0.2217])\n",
      "\t Grad:  tensor([ 0.0337, -0.2521])\n",
      "Epoch 73, Loss 0.311166\n",
      "\t Params:  tensor([0.8913, 0.2242])\n",
      "\t Grad:  tensor([ 0.0335, -0.2505])\n",
      "Epoch 74, Loss 0.310530\n",
      "\t Params:  tensor([0.8909, 0.2267])\n",
      "\t Grad:  tensor([ 0.0333, -0.2488])\n",
      "Epoch 75, Loss 0.309902\n",
      "\t Params:  tensor([0.8906, 0.2292])\n",
      "\t Grad:  tensor([ 0.0330, -0.2471])\n",
      "Epoch 76, Loss 0.309282\n",
      "\t Params:  tensor([0.8903, 0.2316])\n",
      "\t Grad:  tensor([ 0.0328, -0.2455])\n",
      "Epoch 77, Loss 0.308671\n",
      "\t Params:  tensor([0.8899, 0.2341])\n",
      "\t Grad:  tensor([ 0.0326, -0.2439])\n",
      "Epoch 78, Loss 0.308067\n",
      "\t Params:  tensor([0.8896, 0.2365])\n",
      "\t Grad:  tensor([ 0.0324, -0.2423])\n",
      "Epoch 79, Loss 0.307472\n",
      "\t Params:  tensor([0.8893, 0.2389])\n",
      "\t Grad:  tensor([ 0.0322, -0.2407])\n",
      "Epoch 80, Loss 0.306884\n",
      "\t Params:  tensor([0.8890, 0.2413])\n",
      "\t Grad:  tensor([ 0.0320, -0.2391])\n",
      "Epoch 81, Loss 0.306305\n",
      "\t Params:  tensor([0.8887, 0.2436])\n",
      "\t Grad:  tensor([ 0.0318, -0.2375])\n",
      "Epoch 82, Loss 0.305732\n",
      "\t Params:  tensor([0.8883, 0.2460])\n",
      "\t Grad:  tensor([ 0.0315, -0.2359])\n",
      "Epoch 83, Loss 0.305168\n",
      "\t Params:  tensor([0.8880, 0.2483])\n",
      "\t Grad:  tensor([ 0.0313, -0.2343])\n",
      "Epoch 84, Loss 0.304611\n",
      "\t Params:  tensor([0.8877, 0.2507])\n",
      "\t Grad:  tensor([ 0.0311, -0.2328])\n",
      "Epoch 85, Loss 0.304061\n",
      "\t Params:  tensor([0.8874, 0.2530])\n",
      "\t Grad:  tensor([ 0.0309, -0.2312])\n",
      "Epoch 86, Loss 0.303519\n",
      "\t Params:  tensor([0.8871, 0.2553])\n",
      "\t Grad:  tensor([ 0.0307, -0.2297])\n",
      "Epoch 87, Loss 0.302983\n",
      "\t Params:  tensor([0.8868, 0.2576])\n",
      "\t Grad:  tensor([ 0.0305, -0.2282])\n",
      "Epoch 88, Loss 0.302455\n",
      "\t Params:  tensor([0.8865, 0.2598])\n",
      "\t Grad:  tensor([ 0.0303, -0.2267])\n",
      "Epoch 89, Loss 0.301933\n",
      "\t Params:  tensor([0.8862, 0.2621])\n",
      "\t Grad:  tensor([ 0.0301, -0.2252])\n",
      "Epoch 90, Loss 0.301419\n",
      "\t Params:  tensor([0.8859, 0.2643])\n",
      "\t Grad:  tensor([ 0.0299, -0.2237])\n",
      "Epoch 91, Loss 0.300911\n",
      "\t Params:  tensor([0.8856, 0.2665])\n",
      "\t Grad:  tensor([ 0.0297, -0.2222])\n",
      "Epoch 92, Loss 0.300411\n",
      "\t Params:  tensor([0.8853, 0.2688])\n",
      "\t Grad:  tensor([ 0.0295, -0.2207])\n",
      "Epoch 93, Loss 0.299916\n",
      "\t Params:  tensor([0.8850, 0.2709])\n",
      "\t Grad:  tensor([ 0.0293, -0.2193])\n",
      "Epoch 94, Loss 0.299428\n",
      "\t Params:  tensor([0.8847, 0.2731])\n",
      "\t Grad:  tensor([ 0.0291, -0.2178])\n",
      "Epoch 95, Loss 0.298947\n",
      "\t Params:  tensor([0.8844, 0.2753])\n",
      "\t Grad:  tensor([ 0.0289, -0.2164])\n",
      "Epoch 96, Loss 0.298472\n",
      "\t Params:  tensor([0.8841, 0.2774])\n",
      "\t Grad:  tensor([ 0.0287, -0.2149])\n",
      "Epoch 97, Loss 0.298003\n",
      "\t Params:  tensor([0.8839, 0.2796])\n",
      "\t Grad:  tensor([ 0.0285, -0.2135])\n",
      "Epoch 98, Loss 0.297541\n",
      "\t Params:  tensor([0.8836, 0.2817])\n",
      "\t Grad:  tensor([ 0.0284, -0.2121])\n",
      "Epoch 99, Loss 0.297085\n",
      "\t Params:  tensor([0.8833, 0.2838])\n",
      "\t Grad:  tensor([ 0.0282, -0.2107])\n",
      "Epoch 100, Loss 0.296634\n",
      "\t Params:  tensor([0.8830, 0.2859])\n",
      "\t Grad:  tensor([ 0.0280, -0.2093])\n",
      "Epoch 101, Loss 0.296190\n",
      "\t Params:  tensor([0.8827, 0.2880])\n",
      "\t Grad:  tensor([ 0.0278, -0.2079])\n",
      "Epoch 102, Loss 0.295751\n",
      "\t Params:  tensor([0.8825, 0.2900])\n",
      "\t Grad:  tensor([ 0.0276, -0.2065])\n",
      "Epoch 103, Loss 0.295318\n",
      "\t Params:  tensor([0.8822, 0.2921])\n",
      "\t Grad:  tensor([ 0.0274, -0.2052])\n",
      "Epoch 104, Loss 0.294891\n",
      "\t Params:  tensor([0.8819, 0.2941])\n",
      "\t Grad:  tensor([ 0.0273, -0.2038])\n",
      "Epoch 105, Loss 0.294470\n",
      "\t Params:  tensor([0.8816, 0.2962])\n",
      "\t Grad:  tensor([ 0.0271, -0.2025])\n",
      "Epoch 106, Loss 0.294054\n",
      "\t Params:  tensor([0.8814, 0.2982])\n",
      "\t Grad:  tensor([ 0.0269, -0.2011])\n",
      "Epoch 107, Loss 0.293644\n",
      "\t Params:  tensor([0.8811, 0.3002])\n",
      "\t Grad:  tensor([ 0.0267, -0.1998])\n",
      "Epoch 108, Loss 0.293239\n",
      "\t Params:  tensor([0.8808, 0.3021])\n",
      "\t Grad:  tensor([ 0.0265, -0.1985])\n",
      "Epoch 109, Loss 0.292839\n",
      "\t Params:  tensor([0.8806, 0.3041])\n",
      "\t Grad:  tensor([ 0.0264, -0.1971])\n",
      "Epoch 110, Loss 0.292445\n",
      "\t Params:  tensor([0.8803, 0.3061])\n",
      "\t Grad:  tensor([ 0.0262, -0.1958])\n",
      "Epoch 111, Loss 0.292056\n",
      "\t Params:  tensor([0.8800, 0.3080])\n",
      "\t Grad:  tensor([ 0.0260, -0.1945])\n",
      "Epoch 112, Loss 0.291672\n",
      "\t Params:  tensor([0.8798, 0.3100])\n",
      "\t Grad:  tensor([ 0.0258, -0.1933])\n",
      "Epoch 113, Loss 0.291293\n",
      "\t Params:  tensor([0.8795, 0.3119])\n",
      "\t Grad:  tensor([ 0.0257, -0.1920])\n",
      "Epoch 114, Loss 0.290919\n",
      "\t Params:  tensor([0.8793, 0.3138])\n",
      "\t Grad:  tensor([ 0.0255, -0.1907])\n",
      "Epoch 115, Loss 0.290550\n",
      "\t Params:  tensor([0.8790, 0.3157])\n",
      "\t Grad:  tensor([ 0.0253, -0.1894])\n",
      "Epoch 116, Loss 0.290186\n",
      "\t Params:  tensor([0.8788, 0.3176])\n",
      "\t Grad:  tensor([ 0.0252, -0.1882])\n",
      "Epoch 117, Loss 0.289827\n",
      "\t Params:  tensor([0.8785, 0.3194])\n",
      "\t Grad:  tensor([ 0.0250, -0.1869])\n",
      "Epoch 118, Loss 0.289472\n",
      "\t Params:  tensor([0.8783, 0.3213])\n",
      "\t Grad:  tensor([ 0.0248, -0.1857])\n",
      "Epoch 119, Loss 0.289122\n",
      "\t Params:  tensor([0.8780, 0.3231])\n",
      "\t Grad:  tensor([ 0.0247, -0.1845])\n",
      "Epoch 120, Loss 0.288777\n",
      "\t Params:  tensor([0.8778, 0.3250])\n",
      "\t Grad:  tensor([ 0.0245, -0.1832])\n",
      "Epoch 121, Loss 0.288436\n",
      "\t Params:  tensor([0.8775, 0.3268])\n",
      "\t Grad:  tensor([ 0.0243, -0.1820])\n",
      "Epoch 122, Loss 0.288100\n",
      "\t Params:  tensor([0.8773, 0.3286])\n",
      "\t Grad:  tensor([ 0.0242, -0.1808])\n",
      "Epoch 123, Loss 0.287769\n",
      "\t Params:  tensor([0.8771, 0.3304])\n",
      "\t Grad:  tensor([ 0.0240, -0.1796])\n",
      "Epoch 124, Loss 0.287441\n",
      "\t Params:  tensor([0.8768, 0.3322])\n",
      "\t Grad:  tensor([ 0.0239, -0.1784])\n",
      "Epoch 125, Loss 0.287118\n",
      "\t Params:  tensor([0.8766, 0.3339])\n",
      "\t Grad:  tensor([ 0.0237, -0.1773])\n",
      "Epoch 126, Loss 0.286800\n",
      "\t Params:  tensor([0.8763, 0.3357])\n",
      "\t Grad:  tensor([ 0.0235, -0.1761])\n",
      "Epoch 127, Loss 0.286485\n",
      "\t Params:  tensor([0.8761, 0.3375])\n",
      "\t Grad:  tensor([ 0.0234, -0.1749])\n",
      "Epoch 128, Loss 0.286174\n",
      "\t Params:  tensor([0.8759, 0.3392])\n",
      "\t Grad:  tensor([ 0.0232, -0.1738])\n",
      "Epoch 129, Loss 0.285868\n",
      "\t Params:  tensor([0.8756, 0.3409])\n",
      "\t Grad:  tensor([ 0.0231, -0.1726])\n",
      "Epoch 130, Loss 0.285566\n",
      "\t Params:  tensor([0.8754, 0.3426])\n",
      "\t Grad:  tensor([ 0.0229, -0.1715])\n",
      "Epoch 131, Loss 0.285268\n",
      "\t Params:  tensor([0.8752, 0.3443])\n",
      "\t Grad:  tensor([ 0.0228, -0.1703])\n",
      "Epoch 132, Loss 0.284974\n",
      "\t Params:  tensor([0.8750, 0.3460])\n",
      "\t Grad:  tensor([ 0.0226, -0.1692])\n",
      "Epoch 133, Loss 0.284683\n",
      "\t Params:  tensor([0.8747, 0.3477])\n",
      "\t Grad:  tensor([ 0.0225, -0.1681])\n",
      "Epoch 134, Loss 0.284396\n",
      "\t Params:  tensor([0.8745, 0.3494])\n",
      "\t Grad:  tensor([ 0.0223, -0.1670])\n",
      "Epoch 135, Loss 0.284114\n",
      "\t Params:  tensor([0.8743, 0.3510])\n",
      "\t Grad:  tensor([ 0.0222, -0.1659])\n",
      "Epoch 136, Loss 0.283835\n",
      "\t Params:  tensor([0.8741, 0.3527])\n",
      "\t Grad:  tensor([ 0.0220, -0.1648])\n",
      "Epoch 137, Loss 0.283559\n",
      "\t Params:  tensor([0.8739, 0.3543])\n",
      "\t Grad:  tensor([ 0.0219, -0.1637])\n",
      "Epoch 138, Loss 0.283287\n",
      "\t Params:  tensor([0.8736, 0.3559])\n",
      "\t Grad:  tensor([ 0.0217, -0.1626])\n",
      "Epoch 139, Loss 0.283019\n",
      "\t Params:  tensor([0.8734, 0.3576])\n",
      "\t Grad:  tensor([ 0.0216, -0.1615])\n",
      "Epoch 140, Loss 0.282755\n",
      "\t Params:  tensor([0.8732, 0.3592])\n",
      "\t Grad:  tensor([ 0.0215, -0.1604])\n",
      "Epoch 141, Loss 0.282494\n",
      "\t Params:  tensor([0.8730, 0.3608])\n",
      "\t Grad:  tensor([ 0.0213, -0.1594])\n",
      "Epoch 142, Loss 0.282236\n",
      "\t Params:  tensor([0.8728, 0.3623])\n",
      "\t Grad:  tensor([ 0.0212, -0.1583])\n",
      "Epoch 143, Loss 0.281981\n",
      "\t Params:  tensor([0.8726, 0.3639])\n",
      "\t Grad:  tensor([ 0.0210, -0.1573])\n",
      "Epoch 144, Loss 0.281731\n",
      "\t Params:  tensor([0.8724, 0.3655])\n",
      "\t Grad:  tensor([ 0.0209, -0.1562])\n",
      "Epoch 145, Loss 0.281483\n",
      "\t Params:  tensor([0.8722, 0.3670])\n",
      "\t Grad:  tensor([ 0.0207, -0.1552])\n",
      "Epoch 146, Loss 0.281239\n",
      "\t Params:  tensor([0.8720, 0.3686])\n",
      "\t Grad:  tensor([ 0.0206, -0.1542])\n",
      "Epoch 147, Loss 0.280998\n",
      "\t Params:  tensor([0.8717, 0.3701])\n",
      "\t Grad:  tensor([ 0.0205, -0.1531])\n",
      "Epoch 148, Loss 0.280760\n",
      "\t Params:  tensor([0.8715, 0.3716])\n",
      "\t Grad:  tensor([ 0.0203, -0.1521])\n",
      "Epoch 149, Loss 0.280525\n",
      "\t Params:  tensor([0.8713, 0.3731])\n",
      "\t Grad:  tensor([ 0.0202, -0.1511])\n",
      "Epoch 150, Loss 0.280293\n",
      "\t Params:  tensor([0.8711, 0.3746])\n",
      "\t Grad:  tensor([ 0.0201, -0.1501])\n",
      "Epoch 151, Loss 0.280065\n",
      "\t Params:  tensor([0.8709, 0.3761])\n",
      "\t Grad:  tensor([ 0.0199, -0.1491])\n",
      "Epoch 152, Loss 0.279839\n",
      "\t Params:  tensor([0.8707, 0.3776])\n",
      "\t Grad:  tensor([ 0.0198, -0.1481])\n",
      "Epoch 153, Loss 0.279616\n",
      "\t Params:  tensor([0.8705, 0.3791])\n",
      "\t Grad:  tensor([ 0.0197, -0.1472])\n",
      "Epoch 154, Loss 0.279397\n",
      "\t Params:  tensor([0.8704, 0.3805])\n",
      "\t Grad:  tensor([ 0.0195, -0.1462])\n",
      "Epoch 155, Loss 0.279180\n",
      "\t Params:  tensor([0.8702, 0.3820])\n",
      "\t Grad:  tensor([ 0.0194, -0.1452])\n",
      "Epoch 156, Loss 0.278966\n",
      "\t Params:  tensor([0.8700, 0.3834])\n",
      "\t Grad:  tensor([ 0.0193, -0.1442])\n",
      "Epoch 157, Loss 0.278755\n",
      "\t Params:  tensor([0.8698, 0.3849])\n",
      "\t Grad:  tensor([ 0.0192, -0.1433])\n",
      "Epoch 158, Loss 0.278547\n",
      "\t Params:  tensor([0.8696, 0.3863])\n",
      "\t Grad:  tensor([ 0.0190, -0.1423])\n",
      "Epoch 159, Loss 0.278341\n",
      "\t Params:  tensor([0.8694, 0.3877])\n",
      "\t Grad:  tensor([ 0.0189, -0.1414])\n",
      "Epoch 160, Loss 0.278138\n",
      "\t Params:  tensor([0.8692, 0.3891])\n",
      "\t Grad:  tensor([ 0.0188, -0.1405])\n",
      "Epoch 161, Loss 0.277938\n",
      "\t Params:  tensor([0.8690, 0.3905])\n",
      "\t Grad:  tensor([ 0.0187, -0.1395])\n",
      "Epoch 162, Loss 0.277741\n",
      "\t Params:  tensor([0.8688, 0.3919])\n",
      "\t Grad:  tensor([ 0.0185, -0.1386])\n",
      "Epoch 163, Loss 0.277546\n",
      "\t Params:  tensor([0.8686, 0.3933])\n",
      "\t Grad:  tensor([ 0.0184, -0.1377])\n",
      "Epoch 164, Loss 0.277353\n",
      "\t Params:  tensor([0.8685, 0.3946])\n",
      "\t Grad:  tensor([ 0.0183, -0.1368])\n",
      "Epoch 165, Loss 0.277164\n",
      "\t Params:  tensor([0.8683, 0.3960])\n",
      "\t Grad:  tensor([ 0.0182, -0.1359])\n",
      "Epoch 166, Loss 0.276976\n",
      "\t Params:  tensor([0.8681, 0.3973])\n",
      "\t Grad:  tensor([ 0.0180, -0.1350])\n",
      "Epoch 167, Loss 0.276791\n",
      "\t Params:  tensor([0.8679, 0.3987])\n",
      "\t Grad:  tensor([ 0.0179, -0.1341])\n",
      "Epoch 168, Loss 0.276609\n",
      "\t Params:  tensor([0.8677, 0.4000])\n",
      "\t Grad:  tensor([ 0.0178, -0.1332])\n",
      "Epoch 169, Loss 0.276429\n",
      "\t Params:  tensor([0.8676, 0.4013])\n",
      "\t Grad:  tensor([ 0.0177, -0.1323])\n",
      "Epoch 170, Loss 0.276251\n",
      "\t Params:  tensor([0.8674, 0.4027])\n",
      "\t Grad:  tensor([ 0.0176, -0.1314])\n",
      "Epoch 171, Loss 0.276076\n",
      "\t Params:  tensor([0.8672, 0.4040])\n",
      "\t Grad:  tensor([ 0.0175, -0.1306])\n",
      "Epoch 172, Loss 0.275903\n",
      "\t Params:  tensor([0.8670, 0.4053])\n",
      "\t Grad:  tensor([ 0.0173, -0.1297])\n",
      "Epoch 173, Loss 0.275733\n",
      "\t Params:  tensor([0.8669, 0.4065])\n",
      "\t Grad:  tensor([ 0.0172, -0.1288])\n",
      "Epoch 174, Loss 0.275564\n",
      "\t Params:  tensor([0.8667, 0.4078])\n",
      "\t Grad:  tensor([ 0.0171, -0.1280])\n",
      "Epoch 175, Loss 0.275398\n",
      "\t Params:  tensor([0.8665, 0.4091])\n",
      "\t Grad:  tensor([ 0.0170, -0.1271])\n",
      "Epoch 176, Loss 0.275234\n",
      "\t Params:  tensor([0.8664, 0.4104])\n",
      "\t Grad:  tensor([ 0.0169, -0.1263])\n",
      "Epoch 177, Loss 0.275072\n",
      "\t Params:  tensor([0.8662, 0.4116])\n",
      "\t Grad:  tensor([ 0.0168, -0.1255])\n",
      "Epoch 178, Loss 0.274913\n",
      "\t Params:  tensor([0.8660, 0.4129])\n",
      "\t Grad:  tensor([ 0.0167, -0.1246])\n",
      "Epoch 179, Loss 0.274755\n",
      "\t Params:  tensor([0.8659, 0.4141])\n",
      "\t Grad:  tensor([ 0.0166, -0.1238])\n",
      "Epoch 180, Loss 0.274600\n",
      "\t Params:  tensor([0.8657, 0.4153])\n",
      "\t Grad:  tensor([ 0.0164, -0.1230])\n",
      "Epoch 181, Loss 0.274446\n",
      "\t Params:  tensor([0.8655, 0.4165])\n",
      "\t Grad:  tensor([ 0.0163, -0.1222])\n",
      "Epoch 182, Loss 0.274295\n",
      "\t Params:  tensor([0.8654, 0.4178])\n",
      "\t Grad:  tensor([ 0.0162, -0.1214])\n",
      "Epoch 183, Loss 0.274145\n",
      "\t Params:  tensor([0.8652, 0.4190])\n",
      "\t Grad:  tensor([ 0.0161, -0.1205])\n",
      "Epoch 184, Loss 0.273998\n",
      "\t Params:  tensor([0.8651, 0.4202])\n",
      "\t Grad:  tensor([ 0.0160, -0.1197])\n",
      "Epoch 185, Loss 0.273853\n",
      "\t Params:  tensor([0.8649, 0.4214])\n",
      "\t Grad:  tensor([ 0.0159, -0.1190])\n",
      "Epoch 186, Loss 0.273709\n",
      "\t Params:  tensor([0.8647, 0.4225])\n",
      "\t Grad:  tensor([ 0.0158, -0.1182])\n",
      "Epoch 187, Loss 0.273567\n",
      "\t Params:  tensor([0.8646, 0.4237])\n",
      "\t Grad:  tensor([ 0.0157, -0.1174])\n",
      "Epoch 188, Loss 0.273427\n",
      "\t Params:  tensor([0.8644, 0.4249])\n",
      "\t Grad:  tensor([ 0.0156, -0.1166])\n",
      "Epoch 189, Loss 0.273290\n",
      "\t Params:  tensor([0.8643, 0.4260])\n",
      "\t Grad:  tensor([ 0.0155, -0.1158])\n",
      "Epoch 190, Loss 0.273153\n",
      "\t Params:  tensor([0.8641, 0.4272])\n",
      "\t Grad:  tensor([ 0.0154, -0.1151])\n",
      "Epoch 191, Loss 0.273019\n",
      "\t Params:  tensor([0.8640, 0.4283])\n",
      "\t Grad:  tensor([ 0.0153, -0.1143])\n",
      "Epoch 192, Loss 0.272886\n",
      "\t Params:  tensor([0.8638, 0.4295])\n",
      "\t Grad:  tensor([ 0.0152, -0.1135])\n",
      "Epoch 193, Loss 0.272756\n",
      "\t Params:  tensor([0.8637, 0.4306])\n",
      "\t Grad:  tensor([ 0.0151, -0.1128])\n",
      "Epoch 194, Loss 0.272627\n",
      "\t Params:  tensor([0.8635, 0.4317])\n",
      "\t Grad:  tensor([ 0.0150, -0.1120])\n",
      "Epoch 195, Loss 0.272499\n",
      "\t Params:  tensor([0.8634, 0.4328])\n",
      "\t Grad:  tensor([ 0.0149, -0.1113])\n",
      "Epoch 196, Loss 0.272374\n",
      "\t Params:  tensor([0.8632, 0.4339])\n",
      "\t Grad:  tensor([ 0.0148, -0.1106])\n",
      "Epoch 197, Loss 0.272250\n",
      "\t Params:  tensor([0.8631, 0.4350])\n",
      "\t Grad:  tensor([ 0.0147, -0.1098])\n",
      "Epoch 198, Loss 0.272127\n",
      "\t Params:  tensor([0.8629, 0.4361])\n",
      "\t Grad:  tensor([ 0.0146, -0.1091])\n",
      "Epoch 199, Loss 0.272006\n",
      "\t Params:  tensor([0.8628, 0.4372])\n",
      "\t Grad:  tensor([ 0.0145, -0.1084])\n",
      "Epoch 200, Loss 0.271887\n",
      "\t Params:  tensor([0.8626, 0.4383])\n",
      "\t Grad:  tensor([ 0.0144, -0.1077])\n",
      "Epoch 201, Loss 0.271770\n",
      "\t Params:  tensor([0.8625, 0.4394])\n",
      "\t Grad:  tensor([ 0.0143, -0.1070])\n",
      "Epoch 202, Loss 0.271654\n",
      "\t Params:  tensor([0.8623, 0.4404])\n",
      "\t Grad:  tensor([ 0.0142, -0.1062])\n",
      "Epoch 203, Loss 0.271539\n",
      "\t Params:  tensor([0.8622, 0.4415])\n",
      "\t Grad:  tensor([ 0.0141, -0.1055])\n",
      "Epoch 204, Loss 0.271426\n",
      "\t Params:  tensor([0.8621, 0.4425])\n",
      "\t Grad:  tensor([ 0.0140, -0.1048])\n",
      "Epoch 205, Loss 0.271315\n",
      "\t Params:  tensor([0.8619, 0.4436])\n",
      "\t Grad:  tensor([ 0.0139, -0.1041])\n",
      "Epoch 206, Loss 0.271205\n",
      "\t Params:  tensor([0.8618, 0.4446])\n",
      "\t Grad:  tensor([ 0.0138, -0.1035])\n",
      "Epoch 207, Loss 0.271096\n",
      "\t Params:  tensor([0.8616, 0.4456])\n",
      "\t Grad:  tensor([ 0.0137, -0.1028])\n",
      "Epoch 208, Loss 0.270989\n",
      "\t Params:  tensor([0.8615, 0.4466])\n",
      "\t Grad:  tensor([ 0.0137, -0.1021])\n",
      "Epoch 209, Loss 0.270883\n",
      "\t Params:  tensor([0.8614, 0.4477])\n",
      "\t Grad:  tensor([ 0.0136, -0.1014])\n",
      "Epoch 210, Loss 0.270779\n",
      "\t Params:  tensor([0.8612, 0.4487])\n",
      "\t Grad:  tensor([ 0.0135, -0.1007])\n",
      "Epoch 211, Loss 0.270676\n",
      "\t Params:  tensor([0.8611, 0.4497])\n",
      "\t Grad:  tensor([ 0.0134, -0.1001])\n",
      "Epoch 212, Loss 0.270574\n",
      "\t Params:  tensor([0.8610, 0.4507])\n",
      "\t Grad:  tensor([ 0.0133, -0.0994])\n",
      "Epoch 213, Loss 0.270474\n",
      "\t Params:  tensor([0.8608, 0.4516])\n",
      "\t Grad:  tensor([ 0.0132, -0.0988])\n",
      "Epoch 214, Loss 0.270375\n",
      "\t Params:  tensor([0.8607, 0.4526])\n",
      "\t Grad:  tensor([ 0.0131, -0.0981])\n",
      "Epoch 215, Loss 0.270277\n",
      "\t Params:  tensor([0.8606, 0.4536])\n",
      "\t Grad:  tensor([ 0.0130, -0.0974])\n",
      "Epoch 216, Loss 0.270181\n",
      "\t Params:  tensor([0.8605, 0.4546])\n",
      "\t Grad:  tensor([ 0.0129, -0.0968])\n",
      "Epoch 217, Loss 0.270086\n",
      "\t Params:  tensor([0.8603, 0.4555])\n",
      "\t Grad:  tensor([ 0.0129, -0.0962])\n",
      "Epoch 218, Loss 0.269992\n",
      "\t Params:  tensor([0.8602, 0.4565])\n",
      "\t Grad:  tensor([ 0.0128, -0.0955])\n",
      "Epoch 219, Loss 0.269899\n",
      "\t Params:  tensor([0.8601, 0.4574])\n",
      "\t Grad:  tensor([ 0.0127, -0.0949])\n",
      "Epoch 220, Loss 0.269808\n",
      "\t Params:  tensor([0.8599, 0.4584])\n",
      "\t Grad:  tensor([ 0.0126, -0.0943])\n",
      "Epoch 221, Loss 0.269718\n",
      "\t Params:  tensor([0.8598, 0.4593])\n",
      "\t Grad:  tensor([ 0.0125, -0.0936])\n",
      "Epoch 222, Loss 0.269629\n",
      "\t Params:  tensor([0.8597, 0.4602])\n",
      "\t Grad:  tensor([ 0.0124, -0.0930])\n",
      "Epoch 223, Loss 0.269541\n",
      "\t Params:  tensor([0.8596, 0.4612])\n",
      "\t Grad:  tensor([ 0.0124, -0.0924])\n",
      "Epoch 224, Loss 0.269455\n",
      "\t Params:  tensor([0.8594, 0.4621])\n",
      "\t Grad:  tensor([ 0.0123, -0.0918])\n",
      "Epoch 225, Loss 0.269369\n",
      "\t Params:  tensor([0.8593, 0.4630])\n",
      "\t Grad:  tensor([ 0.0122, -0.0912])\n",
      "Epoch 226, Loss 0.269285\n",
      "\t Params:  tensor([0.8592, 0.4639])\n",
      "\t Grad:  tensor([ 0.0121, -0.0906])\n",
      "Epoch 227, Loss 0.269202\n",
      "\t Params:  tensor([0.8591, 0.4648])\n",
      "\t Grad:  tensor([ 0.0120, -0.0900])\n",
      "Epoch 228, Loss 0.269119\n",
      "\t Params:  tensor([0.8590, 0.4657])\n",
      "\t Grad:  tensor([ 0.0119, -0.0894])\n",
      "Epoch 229, Loss 0.269038\n",
      "\t Params:  tensor([0.8588, 0.4666])\n",
      "\t Grad:  tensor([ 0.0119, -0.0888])\n",
      "Epoch 230, Loss 0.268958\n",
      "\t Params:  tensor([0.8587, 0.4675])\n",
      "\t Grad:  tensor([ 0.0118, -0.0882])\n",
      "Epoch 231, Loss 0.268879\n",
      "\t Params:  tensor([0.8586, 0.4683])\n",
      "\t Grad:  tensor([ 0.0117, -0.0876])\n",
      "Epoch 232, Loss 0.268802\n",
      "\t Params:  tensor([0.8585, 0.4692])\n",
      "\t Grad:  tensor([ 0.0116, -0.0870])\n",
      "Epoch 233, Loss 0.268725\n",
      "\t Params:  tensor([0.8584, 0.4701])\n",
      "\t Grad:  tensor([ 0.0116, -0.0865])\n",
      "Epoch 234, Loss 0.268649\n",
      "\t Params:  tensor([0.8583, 0.4709])\n",
      "\t Grad:  tensor([ 0.0115, -0.0859])\n",
      "Epoch 235, Loss 0.268574\n",
      "\t Params:  tensor([0.8582, 0.4718])\n",
      "\t Grad:  tensor([ 0.0114, -0.0853])\n",
      "Epoch 236, Loss 0.268500\n",
      "\t Params:  tensor([0.8580, 0.4726])\n",
      "\t Grad:  tensor([ 0.0113, -0.0848])\n",
      "Epoch 237, Loss 0.268427\n",
      "\t Params:  tensor([0.8579, 0.4735])\n",
      "\t Grad:  tensor([ 0.0113, -0.0842])\n",
      "Epoch 238, Loss 0.268355\n",
      "\t Params:  tensor([0.8578, 0.4743])\n",
      "\t Grad:  tensor([ 0.0112, -0.0836])\n",
      "Epoch 239, Loss 0.268285\n",
      "\t Params:  tensor([0.8577, 0.4751])\n",
      "\t Grad:  tensor([ 0.0111, -0.0831])\n",
      "Epoch 240, Loss 0.268215\n",
      "\t Params:  tensor([0.8576, 0.4760])\n",
      "\t Grad:  tensor([ 0.0110, -0.0825])\n",
      "Epoch 241, Loss 0.268145\n",
      "\t Params:  tensor([0.8575, 0.4768])\n",
      "\t Grad:  tensor([ 0.0110, -0.0820])\n",
      "Epoch 242, Loss 0.268077\n",
      "\t Params:  tensor([0.8574, 0.4776])\n",
      "\t Grad:  tensor([ 0.0109, -0.0814])\n",
      "Epoch 243, Loss 0.268010\n",
      "\t Params:  tensor([0.8573, 0.4784])\n",
      "\t Grad:  tensor([ 0.0108, -0.0809])\n",
      "Epoch 244, Loss 0.267943\n",
      "\t Params:  tensor([0.8572, 0.4792])\n",
      "\t Grad:  tensor([ 0.0107, -0.0804])\n",
      "Epoch 245, Loss 0.267878\n",
      "\t Params:  tensor([0.8571, 0.4800])\n",
      "\t Grad:  tensor([ 0.0107, -0.0798])\n",
      "Epoch 246, Loss 0.267813\n",
      "\t Params:  tensor([0.8569, 0.4808])\n",
      "\t Grad:  tensor([ 0.0106, -0.0793])\n",
      "Epoch 247, Loss 0.267749\n",
      "\t Params:  tensor([0.8568, 0.4816])\n",
      "\t Grad:  tensor([ 0.0105, -0.0788])\n",
      "Epoch 248, Loss 0.267687\n",
      "\t Params:  tensor([0.8567, 0.4824])\n",
      "\t Grad:  tensor([ 0.0105, -0.0783])\n",
      "Epoch 249, Loss 0.267624\n",
      "\t Params:  tensor([0.8566, 0.4832])\n",
      "\t Grad:  tensor([ 0.0104, -0.0777])\n",
      "Epoch 250, Loss 0.267563\n",
      "\t Params:  tensor([0.8565, 0.4839])\n",
      "\t Grad:  tensor([ 0.0103, -0.0772])\n",
      "Epoch 251, Loss 0.267503\n",
      "\t Params:  tensor([0.8564, 0.4847])\n",
      "\t Grad:  tensor([ 0.0103, -0.0767])\n",
      "Epoch 252, Loss 0.267443\n",
      "\t Params:  tensor([0.8563, 0.4855])\n",
      "\t Grad:  tensor([ 0.0102, -0.0762])\n",
      "Epoch 253, Loss 0.267384\n",
      "\t Params:  tensor([0.8562, 0.4862])\n",
      "\t Grad:  tensor([ 0.0101, -0.0757])\n",
      "Epoch 254, Loss 0.267326\n",
      "\t Params:  tensor([0.8561, 0.4870])\n",
      "\t Grad:  tensor([ 0.0101, -0.0752])\n",
      "Epoch 255, Loss 0.267269\n",
      "\t Params:  tensor([0.8560, 0.4877])\n",
      "\t Grad:  tensor([ 0.0100, -0.0747])\n",
      "Epoch 256, Loss 0.267212\n",
      "\t Params:  tensor([0.8559, 0.4885])\n",
      "\t Grad:  tensor([ 0.0099, -0.0742])\n",
      "Epoch 257, Loss 0.267156\n",
      "\t Params:  tensor([0.8558, 0.4892])\n",
      "\t Grad:  tensor([ 0.0099, -0.0737])\n",
      "Epoch 258, Loss 0.267101\n",
      "\t Params:  tensor([0.8557, 0.4899])\n",
      "\t Grad:  tensor([ 0.0098, -0.0732])\n",
      "Epoch 259, Loss 0.267047\n",
      "\t Params:  tensor([0.8556, 0.4907])\n",
      "\t Grad:  tensor([ 0.0097, -0.0727])\n",
      "Epoch 260, Loss 0.266993\n",
      "\t Params:  tensor([0.8555, 0.4914])\n",
      "\t Grad:  tensor([ 0.0097, -0.0723])\n",
      "Epoch 261, Loss 0.266940\n",
      "\t Params:  tensor([0.8554, 0.4921])\n",
      "\t Grad:  tensor([ 0.0096, -0.0718])\n",
      "Epoch 262, Loss 0.266888\n",
      "\t Params:  tensor([0.8553, 0.4928])\n",
      "\t Grad:  tensor([ 0.0095, -0.0713])\n",
      "Epoch 263, Loss 0.266836\n",
      "\t Params:  tensor([0.8552, 0.4935])\n",
      "\t Grad:  tensor([ 0.0095, -0.0708])\n",
      "Epoch 264, Loss 0.266785\n",
      "\t Params:  tensor([0.8552, 0.4942])\n",
      "\t Grad:  tensor([ 0.0094, -0.0704])\n",
      "Epoch 265, Loss 0.266735\n",
      "\t Params:  tensor([0.8551, 0.4949])\n",
      "\t Grad:  tensor([ 0.0093, -0.0699])\n",
      "Epoch 266, Loss 0.266685\n",
      "\t Params:  tensor([0.8550, 0.4956])\n",
      "\t Grad:  tensor([ 0.0093, -0.0694])\n",
      "Epoch 267, Loss 0.266636\n",
      "\t Params:  tensor([0.8549, 0.4963])\n",
      "\t Grad:  tensor([ 0.0092, -0.0690])\n",
      "Epoch 268, Loss 0.266588\n",
      "\t Params:  tensor([0.8548, 0.4970])\n",
      "\t Grad:  tensor([ 0.0092, -0.0685])\n",
      "Epoch 269, Loss 0.266541\n",
      "\t Params:  tensor([0.8547, 0.4977])\n",
      "\t Grad:  tensor([ 0.0091, -0.0681])\n",
      "Epoch 270, Loss 0.266494\n",
      "\t Params:  tensor([0.8546, 0.4983])\n",
      "\t Grad:  tensor([ 0.0090, -0.0676])\n",
      "Epoch 271, Loss 0.266447\n",
      "\t Params:  tensor([0.8545, 0.4990])\n",
      "\t Grad:  tensor([ 0.0090, -0.0672])\n",
      "Epoch 272, Loss 0.266401\n",
      "\t Params:  tensor([0.8544, 0.4997])\n",
      "\t Grad:  tensor([ 0.0089, -0.0667])\n",
      "Epoch 273, Loss 0.266356\n",
      "\t Params:  tensor([0.8543, 0.5003])\n",
      "\t Grad:  tensor([ 0.0089, -0.0663])\n",
      "Epoch 274, Loss 0.266312\n",
      "\t Params:  tensor([0.8542, 0.5010])\n",
      "\t Grad:  tensor([ 0.0088, -0.0658])\n",
      "Epoch 275, Loss 0.266268\n",
      "\t Params:  tensor([0.8542, 0.5017])\n",
      "\t Grad:  tensor([ 0.0087, -0.0654])\n",
      "Epoch 276, Loss 0.266224\n",
      "\t Params:  tensor([0.8541, 0.5023])\n",
      "\t Grad:  tensor([ 0.0087, -0.0650])\n",
      "Epoch 277, Loss 0.266182\n",
      "\t Params:  tensor([0.8540, 0.5030])\n",
      "\t Grad:  tensor([ 0.0086, -0.0645])\n",
      "Epoch 278, Loss 0.266139\n",
      "\t Params:  tensor([0.8539, 0.5036])\n",
      "\t Grad:  tensor([ 0.0086, -0.0641])\n",
      "Epoch 279, Loss 0.266098\n",
      "\t Params:  tensor([0.8538, 0.5042])\n",
      "\t Grad:  tensor([ 0.0085, -0.0637])\n",
      "Epoch 280, Loss 0.266056\n",
      "\t Params:  tensor([0.8537, 0.5049])\n",
      "\t Grad:  tensor([ 0.0085, -0.0633])\n",
      "Epoch 281, Loss 0.266016\n",
      "\t Params:  tensor([0.8536, 0.5055])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Grad:  tensor([ 0.0084, -0.0628])\n",
      "Epoch 282, Loss 0.265976\n",
      "\t Params:  tensor([0.8536, 0.5061])\n",
      "\t Grad:  tensor([ 0.0083, -0.0624])\n",
      "Epoch 283, Loss 0.265936\n",
      "\t Params:  tensor([0.8535, 0.5067])\n",
      "\t Grad:  tensor([ 0.0083, -0.0620])\n",
      "Epoch 284, Loss 0.265897\n",
      "\t Params:  tensor([0.8534, 0.5074])\n",
      "\t Grad:  tensor([ 0.0082, -0.0616])\n",
      "Epoch 285, Loss 0.265859\n",
      "\t Params:  tensor([0.8533, 0.5080])\n",
      "\t Grad:  tensor([ 0.0082, -0.0612])\n",
      "Epoch 286, Loss 0.265821\n",
      "\t Params:  tensor([0.8532, 0.5086])\n",
      "\t Grad:  tensor([ 0.0081, -0.0608])\n",
      "Epoch 287, Loss 0.265783\n",
      "\t Params:  tensor([0.8532, 0.5092])\n",
      "\t Grad:  tensor([ 0.0081, -0.0604])\n",
      "Epoch 288, Loss 0.265746\n",
      "\t Params:  tensor([0.8531, 0.5098])\n",
      "\t Grad:  tensor([ 0.0080, -0.0600])\n",
      "Epoch 289, Loss 0.265710\n",
      "\t Params:  tensor([0.8530, 0.5104])\n",
      "\t Grad:  tensor([ 0.0080, -0.0596])\n",
      "Epoch 290, Loss 0.265674\n",
      "\t Params:  tensor([0.8529, 0.5110])\n",
      "\t Grad:  tensor([ 0.0079, -0.0592])\n",
      "Epoch 291, Loss 0.265638\n",
      "\t Params:  tensor([0.8528, 0.5116])\n",
      "\t Grad:  tensor([ 0.0079, -0.0588])\n",
      "Epoch 292, Loss 0.265603\n",
      "\t Params:  tensor([0.8528, 0.5121])\n",
      "\t Grad:  tensor([ 0.0078, -0.0584])\n",
      "Epoch 293, Loss 0.265569\n",
      "\t Params:  tensor([0.8527, 0.5127])\n",
      "\t Grad:  tensor([ 0.0078, -0.0580])\n",
      "Epoch 294, Loss 0.265534\n",
      "\t Params:  tensor([0.8526, 0.5133])\n",
      "\t Grad:  tensor([ 0.0077, -0.0576])\n",
      "Epoch 295, Loss 0.265501\n",
      "\t Params:  tensor([0.8525, 0.5139])\n",
      "\t Grad:  tensor([ 0.0077, -0.0573])\n",
      "Epoch 296, Loss 0.265467\n",
      "\t Params:  tensor([0.8524, 0.5144])\n",
      "\t Grad:  tensor([ 0.0076, -0.0569])\n",
      "Epoch 297, Loss 0.265435\n",
      "\t Params:  tensor([0.8524, 0.5150])\n",
      "\t Grad:  tensor([ 0.0076, -0.0565])\n",
      "Epoch 298, Loss 0.265402\n",
      "\t Params:  tensor([0.8523, 0.5156])\n",
      "\t Grad:  tensor([ 0.0075, -0.0561])\n",
      "Epoch 299, Loss 0.265370\n",
      "\t Params:  tensor([0.8522, 0.5161])\n",
      "\t Grad:  tensor([ 0.0075, -0.0558])\n",
      "Epoch 300, Loss 0.265339\n",
      "\t Params:  tensor([0.8521, 0.5167])\n",
      "\t Grad:  tensor([ 0.0074, -0.0554])\n",
      "Epoch 301, Loss 0.265308\n",
      "\t Params:  tensor([0.8521, 0.5172])\n",
      "\t Grad:  tensor([ 0.0074, -0.0550])\n",
      "Epoch 302, Loss 0.265277\n",
      "\t Params:  tensor([0.8520, 0.5178])\n",
      "\t Grad:  tensor([ 0.0073, -0.0547])\n",
      "Epoch 303, Loss 0.265247\n",
      "\t Params:  tensor([0.8519, 0.5183])\n",
      "\t Grad:  tensor([ 0.0073, -0.0543])\n",
      "Epoch 304, Loss 0.265217\n",
      "\t Params:  tensor([0.8519, 0.5188])\n",
      "\t Grad:  tensor([ 0.0072, -0.0539])\n",
      "Epoch 305, Loss 0.265187\n",
      "\t Params:  tensor([0.8518, 0.5194])\n",
      "\t Grad:  tensor([ 0.0072, -0.0536])\n",
      "Epoch 306, Loss 0.265158\n",
      "\t Params:  tensor([0.8517, 0.5199])\n",
      "\t Grad:  tensor([ 0.0071, -0.0532])\n",
      "Epoch 307, Loss 0.265129\n",
      "\t Params:  tensor([0.8516, 0.5204])\n",
      "\t Grad:  tensor([ 0.0071, -0.0529])\n",
      "Epoch 308, Loss 0.265101\n",
      "\t Params:  tensor([0.8516, 0.5210])\n",
      "\t Grad:  tensor([ 0.0070, -0.0525])\n",
      "Epoch 309, Loss 0.265073\n",
      "\t Params:  tensor([0.8515, 0.5215])\n",
      "\t Grad:  tensor([ 0.0070, -0.0522])\n",
      "Epoch 310, Loss 0.265045\n",
      "\t Params:  tensor([0.8514, 0.5220])\n",
      "\t Grad:  tensor([ 0.0069, -0.0518])\n",
      "Epoch 311, Loss 0.265018\n",
      "\t Params:  tensor([0.8514, 0.5225])\n",
      "\t Grad:  tensor([ 0.0069, -0.0515])\n",
      "Epoch 312, Loss 0.264991\n",
      "\t Params:  tensor([0.8513, 0.5230])\n",
      "\t Grad:  tensor([ 0.0068, -0.0511])\n",
      "Epoch 313, Loss 0.264965\n",
      "\t Params:  tensor([0.8512, 0.5235])\n",
      "\t Grad:  tensor([ 0.0068, -0.0508])\n",
      "Epoch 314, Loss 0.264939\n",
      "\t Params:  tensor([0.8512, 0.5240])\n",
      "\t Grad:  tensor([ 0.0067, -0.0505])\n",
      "Epoch 315, Loss 0.264913\n",
      "\t Params:  tensor([0.8511, 0.5246])\n",
      "\t Grad:  tensor([ 0.0067, -0.0501])\n",
      "Epoch 316, Loss 0.264887\n",
      "\t Params:  tensor([0.8510, 0.5250])\n",
      "\t Grad:  tensor([ 0.0067, -0.0498])\n",
      "Epoch 317, Loss 0.264862\n",
      "\t Params:  tensor([0.8510, 0.5255])\n",
      "\t Grad:  tensor([ 0.0066, -0.0495])\n",
      "Epoch 318, Loss 0.264837\n",
      "\t Params:  tensor([0.8509, 0.5260])\n",
      "\t Grad:  tensor([ 0.0066, -0.0491])\n",
      "Epoch 319, Loss 0.264813\n",
      "\t Params:  tensor([0.8508, 0.5265])\n",
      "\t Grad:  tensor([ 0.0065, -0.0488])\n",
      "Epoch 320, Loss 0.264789\n",
      "\t Params:  tensor([0.8508, 0.5270])\n",
      "\t Grad:  tensor([ 0.0065, -0.0485])\n",
      "Epoch 321, Loss 0.264765\n",
      "\t Params:  tensor([0.8507, 0.5275])\n",
      "\t Grad:  tensor([ 0.0064, -0.0482])\n",
      "Epoch 322, Loss 0.264741\n",
      "\t Params:  tensor([0.8506, 0.5280])\n",
      "\t Grad:  tensor([ 0.0064, -0.0478])\n",
      "Epoch 323, Loss 0.264718\n",
      "\t Params:  tensor([0.8506, 0.5284])\n",
      "\t Grad:  tensor([ 0.0064, -0.0475])\n",
      "Epoch 324, Loss 0.264695\n",
      "\t Params:  tensor([0.8505, 0.5289])\n",
      "\t Grad:  tensor([ 0.0063, -0.0472])\n",
      "Epoch 325, Loss 0.264672\n",
      "\t Params:  tensor([0.8504, 0.5294])\n",
      "\t Grad:  tensor([ 0.0063, -0.0469])\n",
      "Epoch 326, Loss 0.264650\n",
      "\t Params:  tensor([0.8504, 0.5299])\n",
      "\t Grad:  tensor([ 0.0062, -0.0466])\n",
      "Epoch 327, Loss 0.264628\n",
      "\t Params:  tensor([0.8503, 0.5303])\n",
      "\t Grad:  tensor([ 0.0062, -0.0463])\n",
      "Epoch 328, Loss 0.264606\n",
      "\t Params:  tensor([0.8503, 0.5308])\n",
      "\t Grad:  tensor([ 0.0061, -0.0460])\n",
      "Epoch 329, Loss 0.264585\n",
      "\t Params:  tensor([0.8502, 0.5312])\n",
      "\t Grad:  tensor([ 0.0061, -0.0457])\n",
      "Epoch 330, Loss 0.264564\n",
      "\t Params:  tensor([0.8501, 0.5317])\n",
      "\t Grad:  tensor([ 0.0061, -0.0454])\n",
      "Epoch 331, Loss 0.264543\n",
      "\t Params:  tensor([0.8501, 0.5321])\n",
      "\t Grad:  tensor([ 0.0060, -0.0451])\n",
      "Epoch 332, Loss 0.264522\n",
      "\t Params:  tensor([0.8500, 0.5326])\n",
      "\t Grad:  tensor([ 0.0060, -0.0448])\n",
      "Epoch 333, Loss 0.264502\n",
      "\t Params:  tensor([0.8500, 0.5330])\n",
      "\t Grad:  tensor([ 0.0059, -0.0445])\n",
      "Epoch 334, Loss 0.264482\n",
      "\t Params:  tensor([0.8499, 0.5335])\n",
      "\t Grad:  tensor([ 0.0059, -0.0442])\n",
      "Epoch 335, Loss 0.264462\n",
      "\t Params:  tensor([0.8498, 0.5339])\n",
      "\t Grad:  tensor([ 0.0059, -0.0439])\n",
      "Epoch 336, Loss 0.264443\n",
      "\t Params:  tensor([0.8498, 0.5343])\n",
      "\t Grad:  tensor([ 0.0058, -0.0436])\n",
      "Epoch 337, Loss 0.264423\n",
      "\t Params:  tensor([0.8497, 0.5348])\n",
      "\t Grad:  tensor([ 0.0058, -0.0433])\n",
      "Epoch 338, Loss 0.264404\n",
      "\t Params:  tensor([0.8497, 0.5352])\n",
      "\t Grad:  tensor([ 0.0058, -0.0430])\n",
      "Epoch 339, Loss 0.264385\n",
      "\t Params:  tensor([0.8496, 0.5356])\n",
      "\t Grad:  tensor([ 0.0057, -0.0427])\n",
      "Epoch 340, Loss 0.264367\n",
      "\t Params:  tensor([0.8496, 0.5361])\n",
      "\t Grad:  tensor([ 0.0057, -0.0425])\n",
      "Epoch 341, Loss 0.264349\n",
      "\t Params:  tensor([0.8495, 0.5365])\n",
      "\t Grad:  tensor([ 0.0056, -0.0422])\n",
      "Epoch 342, Loss 0.264331\n",
      "\t Params:  tensor([0.8494, 0.5369])\n",
      "\t Grad:  tensor([ 0.0056, -0.0419])\n",
      "Epoch 343, Loss 0.264313\n",
      "\t Params:  tensor([0.8494, 0.5373])\n",
      "\t Grad:  tensor([ 0.0056, -0.0416])\n",
      "Epoch 344, Loss 0.264295\n",
      "\t Params:  tensor([0.8493, 0.5377])\n",
      "\t Grad:  tensor([ 0.0055, -0.0413])\n",
      "Epoch 345, Loss 0.264278\n",
      "\t Params:  tensor([0.8493, 0.5381])\n",
      "\t Grad:  tensor([ 0.0055, -0.0411])\n",
      "Epoch 346, Loss 0.264261\n",
      "\t Params:  tensor([0.8492, 0.5385])\n",
      "\t Grad:  tensor([ 0.0055, -0.0408])\n",
      "Epoch 347, Loss 0.264244\n",
      "\t Params:  tensor([0.8492, 0.5390])\n",
      "\t Grad:  tensor([ 0.0054, -0.0405])\n",
      "Epoch 348, Loss 0.264227\n",
      "\t Params:  tensor([0.8491, 0.5394])\n",
      "\t Grad:  tensor([ 0.0054, -0.0403])\n",
      "Epoch 349, Loss 0.264211\n",
      "\t Params:  tensor([0.8491, 0.5398])\n",
      "\t Grad:  tensor([ 0.0053, -0.0400])\n",
      "Epoch 350, Loss 0.264195\n",
      "\t Params:  tensor([0.8490, 0.5402])\n",
      "\t Grad:  tensor([ 0.0053, -0.0397])\n",
      "Epoch 351, Loss 0.264179\n",
      "\t Params:  tensor([0.8490, 0.5405])\n",
      "\t Grad:  tensor([ 0.0053, -0.0395])\n",
      "Epoch 352, Loss 0.264163\n",
      "\t Params:  tensor([0.8489, 0.5409])\n",
      "\t Grad:  tensor([ 0.0052, -0.0392])\n",
      "Epoch 353, Loss 0.264147\n",
      "\t Params:  tensor([0.8489, 0.5413])\n",
      "\t Grad:  tensor([ 0.0052, -0.0389])\n",
      "Epoch 354, Loss 0.264132\n",
      "\t Params:  tensor([0.8488, 0.5417])\n",
      "\t Grad:  tensor([ 0.0052, -0.0387])\n",
      "Epoch 355, Loss 0.264117\n",
      "\t Params:  tensor([0.8487, 0.5421])\n",
      "\t Grad:  tensor([ 0.0051, -0.0384])\n",
      "Epoch 356, Loss 0.264102\n",
      "\t Params:  tensor([0.8487, 0.5425])\n",
      "\t Grad:  tensor([ 0.0051, -0.0382])\n",
      "Epoch 357, Loss 0.264087\n",
      "\t Params:  tensor([0.8486, 0.5429])\n",
      "\t Grad:  tensor([ 0.0051, -0.0379])\n",
      "Epoch 358, Loss 0.264072\n",
      "\t Params:  tensor([0.8486, 0.5432])\n",
      "\t Grad:  tensor([ 0.0050, -0.0377])\n",
      "Epoch 359, Loss 0.264058\n",
      "\t Params:  tensor([0.8485, 0.5436])\n",
      "\t Grad:  tensor([ 0.0050, -0.0374])\n",
      "Epoch 360, Loss 0.264044\n",
      "\t Params:  tensor([0.8485, 0.5440])\n",
      "\t Grad:  tensor([ 0.0050, -0.0372])\n",
      "Epoch 361, Loss 0.264030\n",
      "\t Params:  tensor([0.8484, 0.5444])\n",
      "\t Grad:  tensor([ 0.0049, -0.0369])\n",
      "Epoch 362, Loss 0.264016\n",
      "\t Params:  tensor([0.8484, 0.5447])\n",
      "\t Grad:  tensor([ 0.0049, -0.0367])\n",
      "Epoch 363, Loss 0.264002\n",
      "\t Params:  tensor([0.8483, 0.5451])\n",
      "\t Grad:  tensor([ 0.0049, -0.0364])\n",
      "Epoch 364, Loss 0.263989\n",
      "\t Params:  tensor([0.8483, 0.5454])\n",
      "\t Grad:  tensor([ 0.0048, -0.0362])\n",
      "Epoch 365, Loss 0.263975\n",
      "\t Params:  tensor([0.8483, 0.5458])\n",
      "\t Grad:  tensor([ 0.0048, -0.0360])\n",
      "Epoch 366, Loss 0.263962\n",
      "\t Params:  tensor([0.8482, 0.5462])\n",
      "\t Grad:  tensor([ 0.0048, -0.0357])\n",
      "Epoch 367, Loss 0.263949\n",
      "\t Params:  tensor([0.8482, 0.5465])\n",
      "\t Grad:  tensor([ 0.0047, -0.0355])\n",
      "Epoch 368, Loss 0.263937\n",
      "\t Params:  tensor([0.8481, 0.5469])\n",
      "\t Grad:  tensor([ 0.0047, -0.0352])\n",
      "Epoch 369, Loss 0.263924\n",
      "\t Params:  tensor([0.8481, 0.5472])\n",
      "\t Grad:  tensor([ 0.0047, -0.0350])\n",
      "Epoch 370, Loss 0.263912\n",
      "\t Params:  tensor([0.8480, 0.5476])\n",
      "\t Grad:  tensor([ 0.0046, -0.0348])\n",
      "Epoch 371, Loss 0.263899\n",
      "\t Params:  tensor([0.8480, 0.5479])\n",
      "\t Grad:  tensor([ 0.0046, -0.0345])\n",
      "Epoch 372, Loss 0.263887\n",
      "\t Params:  tensor([0.8479, 0.5483])\n",
      "\t Grad:  tensor([ 0.0046, -0.0343])\n",
      "Epoch 373, Loss 0.263875\n",
      "\t Params:  tensor([0.8479, 0.5486])\n",
      "\t Grad:  tensor([ 0.0046, -0.0341])\n",
      "Epoch 374, Loss 0.263863\n",
      "\t Params:  tensor([0.8478, 0.5489])\n",
      "\t Grad:  tensor([ 0.0045, -0.0339])\n",
      "Epoch 375, Loss 0.263852\n",
      "\t Params:  tensor([0.8478, 0.5493])\n",
      "\t Grad:  tensor([ 0.0045, -0.0336])\n",
      "Epoch 376, Loss 0.263840\n",
      "\t Params:  tensor([0.8477, 0.5496])\n",
      "\t Grad:  tensor([ 0.0045, -0.0334])\n",
      "Epoch 377, Loss 0.263829\n",
      "\t Params:  tensor([0.8477, 0.5499])\n",
      "\t Grad:  tensor([ 0.0044, -0.0332])\n",
      "Epoch 378, Loss 0.263818\n",
      "\t Params:  tensor([0.8477, 0.5503])\n",
      "\t Grad:  tensor([ 0.0044, -0.0330])\n",
      "Epoch 379, Loss 0.263807\n",
      "\t Params:  tensor([0.8476, 0.5506])\n",
      "\t Grad:  tensor([ 0.0044, -0.0328])\n",
      "Epoch 380, Loss 0.263796\n",
      "\t Params:  tensor([0.8476, 0.5509])\n",
      "\t Grad:  tensor([ 0.0044, -0.0325])\n",
      "Epoch 381, Loss 0.263785\n",
      "\t Params:  tensor([0.8475, 0.5512])\n",
      "\t Grad:  tensor([ 0.0043, -0.0323])\n",
      "Epoch 382, Loss 0.263775\n",
      "\t Params:  tensor([0.8475, 0.5516])\n",
      "\t Grad:  tensor([ 0.0043, -0.0321])\n",
      "Epoch 383, Loss 0.263764\n",
      "\t Params:  tensor([0.8474, 0.5519])\n",
      "\t Grad:  tensor([ 0.0043, -0.0319])\n",
      "Epoch 384, Loss 0.263754\n",
      "\t Params:  tensor([0.8474, 0.5522])\n",
      "\t Grad:  tensor([ 0.0042, -0.0317])\n",
      "Epoch 385, Loss 0.263744\n",
      "\t Params:  tensor([0.8474, 0.5525])\n",
      "\t Grad:  tensor([ 0.0042, -0.0315])\n",
      "Epoch 386, Loss 0.263733\n",
      "\t Params:  tensor([0.8473, 0.5528])\n",
      "\t Grad:  tensor([ 0.0042, -0.0313])\n",
      "Epoch 387, Loss 0.263724\n",
      "\t Params:  tensor([0.8473, 0.5531])\n",
      "\t Grad:  tensor([ 0.0042, -0.0311])\n",
      "Epoch 388, Loss 0.263714\n",
      "\t Params:  tensor([0.8472, 0.5534])\n",
      "\t Grad:  tensor([ 0.0041, -0.0309])\n",
      "Epoch 389, Loss 0.263704\n",
      "\t Params:  tensor([0.8472, 0.5538])\n",
      "\t Grad:  tensor([ 0.0041, -0.0307])\n",
      "Epoch 390, Loss 0.263695\n",
      "\t Params:  tensor([0.8471, 0.5541])\n",
      "\t Grad:  tensor([ 0.0041, -0.0304])\n",
      "Epoch 391, Loss 0.263685\n",
      "\t Params:  tensor([0.8471, 0.5544])\n",
      "\t Grad:  tensor([ 0.0040, -0.0302])\n",
      "Epoch 392, Loss 0.263676\n",
      "\t Params:  tensor([0.8471, 0.5547])\n",
      "\t Grad:  tensor([ 0.0040, -0.0300])\n",
      "Epoch 393, Loss 0.263667\n",
      "\t Params:  tensor([0.8470, 0.5550])\n",
      "\t Grad:  tensor([ 0.0040, -0.0298])\n",
      "Epoch 394, Loss 0.263658\n",
      "\t Params:  tensor([0.8470, 0.5553])\n",
      "\t Grad:  tensor([ 0.0040, -0.0296])\n",
      "Epoch 395, Loss 0.263649\n",
      "\t Params:  tensor([0.8470, 0.5556])\n",
      "\t Grad:  tensor([ 0.0039, -0.0295])\n",
      "Epoch 396, Loss 0.263640\n",
      "\t Params:  tensor([0.8469, 0.5558])\n",
      "\t Grad:  tensor([ 0.0039, -0.0293])\n",
      "Epoch 397, Loss 0.263631\n",
      "\t Params:  tensor([0.8469, 0.5561])\n",
      "\t Grad:  tensor([ 0.0039, -0.0291])\n",
      "Epoch 398, Loss 0.263623\n",
      "\t Params:  tensor([0.8468, 0.5564])\n",
      "\t Grad:  tensor([ 0.0039, -0.0289])\n",
      "Epoch 399, Loss 0.263614\n",
      "\t Params:  tensor([0.8468, 0.5567])\n",
      "\t Grad:  tensor([ 0.0038, -0.0287])\n",
      "Epoch 400, Loss 0.263606\n",
      "\t Params:  tensor([0.8468, 0.5570])\n",
      "\t Grad:  tensor([ 0.0038, -0.0285])\n",
      "Epoch 401, Loss 0.263598\n",
      "\t Params:  tensor([0.8467, 0.5573])\n",
      "\t Grad:  tensor([ 0.0038, -0.0283])\n",
      "Epoch 402, Loss 0.263590\n",
      "\t Params:  tensor([0.8467, 0.5576])\n",
      "\t Grad:  tensor([ 0.0038, -0.0281])\n",
      "Epoch 403, Loss 0.263582\n",
      "\t Params:  tensor([0.8466, 0.5578])\n",
      "\t Grad:  tensor([ 0.0037, -0.0279])\n",
      "Epoch 404, Loss 0.263574\n",
      "\t Params:  tensor([0.8466, 0.5581])\n",
      "\t Grad:  tensor([ 0.0037, -0.0277])\n",
      "Epoch 405, Loss 0.263566\n",
      "\t Params:  tensor([0.8466, 0.5584])\n",
      "\t Grad:  tensor([ 0.0037, -0.0276])\n",
      "Epoch 406, Loss 0.263558\n",
      "\t Params:  tensor([0.8465, 0.5587])\n",
      "\t Grad:  tensor([ 0.0037, -0.0274])\n",
      "Epoch 407, Loss 0.263551\n",
      "\t Params:  tensor([0.8465, 0.5589])\n",
      "\t Grad:  tensor([ 0.0036, -0.0272])\n",
      "Epoch 408, Loss 0.263543\n",
      "\t Params:  tensor([0.8465, 0.5592])\n",
      "\t Grad:  tensor([ 0.0036, -0.0270])\n",
      "Epoch 409, Loss 0.263536\n",
      "\t Params:  tensor([0.8464, 0.5595])\n",
      "\t Grad:  tensor([ 0.0036, -0.0268])\n",
      "Epoch 410, Loss 0.263528\n",
      "\t Params:  tensor([0.8464, 0.5597])\n",
      "\t Grad:  tensor([ 0.0036, -0.0267])\n",
      "Epoch 411, Loss 0.263521\n",
      "\t Params:  tensor([0.8464, 0.5600])\n",
      "\t Grad:  tensor([ 0.0035, -0.0265])\n",
      "Epoch 412, Loss 0.263514\n",
      "\t Params:  tensor([0.8463, 0.5603])\n",
      "\t Grad:  tensor([ 0.0035, -0.0263])\n",
      "Epoch 413, Loss 0.263507\n",
      "\t Params:  tensor([0.8463, 0.5605])\n",
      "\t Grad:  tensor([ 0.0035, -0.0261])\n",
      "Epoch 414, Loss 0.263500\n",
      "\t Params:  tensor([0.8462, 0.5608])\n",
      "\t Grad:  tensor([ 0.0035, -0.0260])\n",
      "Epoch 415, Loss 0.263493\n",
      "\t Params:  tensor([0.8462, 0.5610])\n",
      "\t Grad:  tensor([ 0.0034, -0.0258])\n",
      "Epoch 416, Loss 0.263486\n",
      "\t Params:  tensor([0.8462, 0.5613])\n",
      "\t Grad:  tensor([ 0.0034, -0.0256])\n",
      "Epoch 417, Loss 0.263480\n",
      "\t Params:  tensor([0.8461, 0.5616])\n",
      "\t Grad:  tensor([ 0.0034, -0.0254])\n",
      "Epoch 418, Loss 0.263473\n",
      "\t Params:  tensor([0.8461, 0.5618])\n",
      "\t Grad:  tensor([ 0.0034, -0.0253])\n",
      "Epoch 419, Loss 0.263467\n",
      "\t Params:  tensor([0.8461, 0.5621])\n",
      "\t Grad:  tensor([ 0.0034, -0.0251])\n",
      "Epoch 420, Loss 0.263460\n",
      "\t Params:  tensor([0.8460, 0.5623])\n",
      "\t Grad:  tensor([ 0.0033, -0.0249])\n",
      "Epoch 421, Loss 0.263454\n",
      "\t Params:  tensor([0.8460, 0.5626])\n",
      "\t Grad:  tensor([ 0.0033, -0.0248])\n",
      "Epoch 422, Loss 0.263448\n",
      "\t Params:  tensor([0.8460, 0.5628])\n",
      "\t Grad:  tensor([ 0.0033, -0.0246])\n",
      "Epoch 423, Loss 0.263442\n",
      "\t Params:  tensor([0.8459, 0.5631])\n",
      "\t Grad:  tensor([ 0.0033, -0.0245])\n",
      "Epoch 424, Loss 0.263436\n",
      "\t Params:  tensor([0.8459, 0.5633])\n",
      "\t Grad:  tensor([ 0.0032, -0.0243])\n",
      "Epoch 425, Loss 0.263430\n",
      "\t Params:  tensor([0.8459, 0.5635])\n",
      "\t Grad:  tensor([ 0.0032, -0.0241])\n",
      "Epoch 426, Loss 0.263424\n",
      "\t Params:  tensor([0.8459, 0.5638])\n",
      "\t Grad:  tensor([ 0.0032, -0.0240])\n",
      "Epoch 427, Loss 0.263418\n",
      "\t Params:  tensor([0.8458, 0.5640])\n",
      "\t Grad:  tensor([ 0.0032, -0.0238])\n",
      "Epoch 428, Loss 0.263412\n",
      "\t Params:  tensor([0.8458, 0.5642])\n",
      "\t Grad:  tensor([ 0.0032, -0.0237])\n",
      "Epoch 429, Loss 0.263406\n",
      "\t Params:  tensor([0.8458, 0.5645])\n",
      "\t Grad:  tensor([ 0.0031, -0.0235])\n",
      "Epoch 430, Loss 0.263401\n",
      "\t Params:  tensor([0.8457, 0.5647])\n",
      "\t Grad:  tensor([ 0.0031, -0.0233])\n",
      "Epoch 431, Loss 0.263395\n",
      "\t Params:  tensor([0.8457, 0.5650])\n",
      "\t Grad:  tensor([ 0.0031, -0.0232])\n",
      "Epoch 432, Loss 0.263390\n",
      "\t Params:  tensor([0.8457, 0.5652])\n",
      "\t Grad:  tensor([ 0.0031, -0.0230])\n",
      "Epoch 433, Loss 0.263384\n",
      "\t Params:  tensor([0.8456, 0.5654])\n",
      "\t Grad:  tensor([ 0.0031, -0.0229])\n",
      "Epoch 434, Loss 0.263379\n",
      "\t Params:  tensor([0.8456, 0.5656])\n",
      "\t Grad:  tensor([ 0.0030, -0.0227])\n",
      "Epoch 435, Loss 0.263374\n",
      "\t Params:  tensor([0.8456, 0.5659])\n",
      "\t Grad:  tensor([ 0.0030, -0.0226])\n",
      "Epoch 436, Loss 0.263369\n",
      "\t Params:  tensor([0.8455, 0.5661])\n",
      "\t Grad:  tensor([ 0.0030, -0.0224])\n",
      "Epoch 437, Loss 0.263364\n",
      "\t Params:  tensor([0.8455, 0.5663])\n",
      "\t Grad:  tensor([ 0.0030, -0.0223])\n",
      "Epoch 438, Loss 0.263359\n",
      "\t Params:  tensor([0.8455, 0.5665])\n",
      "\t Grad:  tensor([ 0.0030, -0.0221])\n",
      "Epoch 439, Loss 0.263354\n",
      "\t Params:  tensor([0.8455, 0.5668])\n",
      "\t Grad:  tensor([ 0.0029, -0.0220])\n",
      "Epoch 440, Loss 0.263349\n",
      "\t Params:  tensor([0.8454, 0.5670])\n",
      "\t Grad:  tensor([ 0.0029, -0.0218])\n",
      "Epoch 441, Loss 0.263344\n",
      "\t Params:  tensor([0.8454, 0.5672])\n",
      "\t Grad:  tensor([ 0.0029, -0.0217])\n",
      "Epoch 442, Loss 0.263339\n",
      "\t Params:  tensor([0.8454, 0.5674])\n",
      "\t Grad:  tensor([ 0.0029, -0.0215])\n",
      "Epoch 443, Loss 0.263334\n",
      "\t Params:  tensor([0.8453, 0.5676])\n",
      "\t Grad:  tensor([ 0.0029, -0.0214])\n",
      "Epoch 444, Loss 0.263330\n",
      "\t Params:  tensor([0.8453, 0.5678])\n",
      "\t Grad:  tensor([ 0.0028, -0.0213])\n",
      "Epoch 445, Loss 0.263325\n",
      "\t Params:  tensor([0.8453, 0.5680])\n",
      "\t Grad:  tensor([ 0.0028, -0.0211])\n",
      "Epoch 446, Loss 0.263321\n",
      "\t Params:  tensor([0.8453, 0.5682])\n",
      "\t Grad:  tensor([ 0.0028, -0.0210])\n",
      "Epoch 447, Loss 0.263316\n",
      "\t Params:  tensor([0.8452, 0.5685])\n",
      "\t Grad:  tensor([ 0.0028, -0.0208])\n",
      "Epoch 448, Loss 0.263312\n",
      "\t Params:  tensor([0.8452, 0.5687])\n",
      "\t Grad:  tensor([ 0.0028, -0.0207])\n",
      "Epoch 449, Loss 0.263308\n",
      "\t Params:  tensor([0.8452, 0.5689])\n",
      "\t Grad:  tensor([ 0.0027, -0.0206])\n",
      "Epoch 450, Loss 0.263303\n",
      "\t Params:  tensor([0.8451, 0.5691])\n",
      "\t Grad:  tensor([ 0.0027, -0.0204])\n",
      "Epoch 451, Loss 0.263299\n",
      "\t Params:  tensor([0.8451, 0.5693])\n",
      "\t Grad:  tensor([ 0.0027, -0.0203])\n",
      "Epoch 452, Loss 0.263295\n",
      "\t Params:  tensor([0.8451, 0.5695])\n",
      "\t Grad:  tensor([ 0.0027, -0.0202])\n",
      "Epoch 453, Loss 0.263291\n",
      "\t Params:  tensor([0.8451, 0.5697])\n",
      "\t Grad:  tensor([ 0.0027, -0.0200])\n",
      "Epoch 454, Loss 0.263287\n",
      "\t Params:  tensor([0.8450, 0.5699])\n",
      "\t Grad:  tensor([ 0.0027, -0.0199])\n",
      "Epoch 455, Loss 0.263282\n",
      "\t Params:  tensor([0.8450, 0.5701])\n",
      "\t Grad:  tensor([ 0.0026, -0.0198])\n",
      "Epoch 456, Loss 0.263279\n",
      "\t Params:  tensor([0.8450, 0.5703])\n",
      "\t Grad:  tensor([ 0.0026, -0.0196])\n",
      "Epoch 457, Loss 0.263275\n",
      "\t Params:  tensor([0.8450, 0.5705])\n",
      "\t Grad:  tensor([ 0.0026, -0.0195])\n",
      "Epoch 458, Loss 0.263271\n",
      "\t Params:  tensor([0.8449, 0.5707])\n",
      "\t Grad:  tensor([ 0.0026, -0.0194])\n",
      "Epoch 459, Loss 0.263267\n",
      "\t Params:  tensor([0.8449, 0.5709])\n",
      "\t Grad:  tensor([ 0.0026, -0.0192])\n",
      "Epoch 460, Loss 0.263263\n",
      "\t Params:  tensor([0.8449, 0.5710])\n",
      "\t Grad:  tensor([ 0.0026, -0.0191])\n",
      "Epoch 461, Loss 0.263260\n",
      "\t Params:  tensor([0.8449, 0.5712])\n",
      "\t Grad:  tensor([ 0.0025, -0.0190])\n",
      "Epoch 462, Loss 0.263256\n",
      "\t Params:  tensor([0.8448, 0.5714])\n",
      "\t Grad:  tensor([ 0.0025, -0.0189])\n",
      "Epoch 463, Loss 0.263252\n",
      "\t Params:  tensor([0.8448, 0.5716])\n",
      "\t Grad:  tensor([ 0.0025, -0.0187])\n",
      "Epoch 464, Loss 0.263249\n",
      "\t Params:  tensor([0.8448, 0.5718])\n",
      "\t Grad:  tensor([ 0.0025, -0.0186])\n",
      "Epoch 465, Loss 0.263245\n",
      "\t Params:  tensor([0.8448, 0.5720])\n",
      "\t Grad:  tensor([ 0.0025, -0.0185])\n",
      "Epoch 466, Loss 0.263242\n",
      "\t Params:  tensor([0.8447, 0.5722])\n",
      "\t Grad:  tensor([ 0.0025, -0.0184])\n",
      "Epoch 467, Loss 0.263238\n",
      "\t Params:  tensor([0.8447, 0.5723])\n",
      "\t Grad:  tensor([ 0.0024, -0.0183])\n",
      "Epoch 468, Loss 0.263235\n",
      "\t Params:  tensor([0.8447, 0.5725])\n",
      "\t Grad:  tensor([ 0.0024, -0.0181])\n",
      "Epoch 469, Loss 0.263232\n",
      "\t Params:  tensor([0.8447, 0.5727])\n",
      "\t Grad:  tensor([ 0.0024, -0.0180])\n",
      "Epoch 470, Loss 0.263228\n",
      "\t Params:  tensor([0.8446, 0.5729])\n",
      "\t Grad:  tensor([ 0.0024, -0.0179])\n",
      "Epoch 471, Loss 0.263225\n",
      "\t Params:  tensor([0.8446, 0.5731])\n",
      "\t Grad:  tensor([ 0.0024, -0.0178])\n",
      "Epoch 472, Loss 0.263222\n",
      "\t Params:  tensor([0.8446, 0.5732])\n",
      "\t Grad:  tensor([ 0.0024, -0.0177])\n",
      "Epoch 473, Loss 0.263219\n",
      "\t Params:  tensor([0.8446, 0.5734])\n",
      "\t Grad:  tensor([ 0.0023, -0.0175])\n",
      "Epoch 474, Loss 0.263216\n",
      "\t Params:  tensor([0.8445, 0.5736])\n",
      "\t Grad:  tensor([ 0.0023, -0.0174])\n",
      "Epoch 475, Loss 0.263213\n",
      "\t Params:  tensor([0.8445, 0.5738])\n",
      "\t Grad:  tensor([ 0.0023, -0.0173])\n",
      "Epoch 476, Loss 0.263209\n",
      "\t Params:  tensor([0.8445, 0.5739])\n",
      "\t Grad:  tensor([ 0.0023, -0.0172])\n",
      "Epoch 477, Loss 0.263207\n",
      "\t Params:  tensor([0.8445, 0.5741])\n",
      "\t Grad:  tensor([ 0.0023, -0.0171])\n",
      "Epoch 478, Loss 0.263203\n",
      "\t Params:  tensor([0.8444, 0.5743])\n",
      "\t Grad:  tensor([ 0.0023, -0.0170])\n",
      "Epoch 479, Loss 0.263201\n",
      "\t Params:  tensor([0.8444, 0.5744])\n",
      "\t Grad:  tensor([ 0.0023, -0.0169])\n",
      "Epoch 480, Loss 0.263198\n",
      "\t Params:  tensor([0.8444, 0.5746])\n",
      "\t Grad:  tensor([ 0.0022, -0.0167])\n",
      "Epoch 481, Loss 0.263195\n",
      "\t Params:  tensor([0.8444, 0.5748])\n",
      "\t Grad:  tensor([ 0.0022, -0.0166])\n",
      "Epoch 482, Loss 0.263192\n",
      "\t Params:  tensor([0.8444, 0.5749])\n",
      "\t Grad:  tensor([ 0.0022, -0.0165])\n",
      "Epoch 483, Loss 0.263189\n",
      "\t Params:  tensor([0.8443, 0.5751])\n",
      "\t Grad:  tensor([ 0.0022, -0.0164])\n",
      "Epoch 484, Loss 0.263187\n",
      "\t Params:  tensor([0.8443, 0.5753])\n",
      "\t Grad:  tensor([ 0.0022, -0.0163])\n",
      "Epoch 485, Loss 0.263184\n",
      "\t Params:  tensor([0.8443, 0.5754])\n",
      "\t Grad:  tensor([ 0.0022, -0.0162])\n",
      "Epoch 486, Loss 0.263181\n",
      "\t Params:  tensor([0.8443, 0.5756])\n",
      "\t Grad:  tensor([ 0.0021, -0.0161])\n",
      "Epoch 487, Loss 0.263179\n",
      "\t Params:  tensor([0.8442, 0.5758])\n",
      "\t Grad:  tensor([ 0.0021, -0.0160])\n",
      "Epoch 488, Loss 0.263176\n",
      "\t Params:  tensor([0.8442, 0.5759])\n",
      "\t Grad:  tensor([ 0.0021, -0.0159])\n",
      "Epoch 489, Loss 0.263173\n",
      "\t Params:  tensor([0.8442, 0.5761])\n",
      "\t Grad:  tensor([ 0.0021, -0.0158])\n",
      "Epoch 490, Loss 0.263171\n",
      "\t Params:  tensor([0.8442, 0.5762])\n",
      "\t Grad:  tensor([ 0.0021, -0.0157])\n",
      "Epoch 491, Loss 0.263168\n",
      "\t Params:  tensor([0.8442, 0.5764])\n",
      "\t Grad:  tensor([ 0.0021, -0.0156])\n",
      "Epoch 492, Loss 0.263166\n",
      "\t Params:  tensor([0.8441, 0.5765])\n",
      "\t Grad:  tensor([ 0.0021, -0.0155])\n",
      "Epoch 493, Loss 0.263164\n",
      "\t Params:  tensor([0.8441, 0.5767])\n",
      "\t Grad:  tensor([ 0.0021, -0.0154])\n",
      "Epoch 494, Loss 0.263161\n",
      "\t Params:  tensor([0.8441, 0.5768])\n",
      "\t Grad:  tensor([ 0.0020, -0.0153])\n",
      "Epoch 495, Loss 0.263159\n",
      "\t Params:  tensor([0.8441, 0.5770])\n",
      "\t Grad:  tensor([ 0.0020, -0.0152])\n",
      "Epoch 496, Loss 0.263156\n",
      "\t Params:  tensor([0.8441, 0.5771])\n",
      "\t Grad:  tensor([ 0.0020, -0.0151])\n",
      "Epoch 497, Loss 0.263154\n",
      "\t Params:  tensor([0.8440, 0.5773])\n",
      "\t Grad:  tensor([ 0.0020, -0.0150])\n",
      "Epoch 498, Loss 0.263152\n",
      "\t Params:  tensor([0.8440, 0.5774])\n",
      "\t Grad:  tensor([ 0.0020, -0.0149])\n",
      "Epoch 499, Loss 0.263150\n",
      "\t Params:  tensor([0.8440, 0.5776])\n",
      "\t Grad:  tensor([ 0.0020, -0.0148])\n",
      "Epoch 500, Loss 0.263147\n",
      "\t Params:  tensor([0.8440, 0.5777])\n",
      "\t Grad:  tensor([ 0.0020, -0.0147])\n",
      "Epoch 501, Loss 0.263145\n",
      "\t Params:  tensor([0.8440, 0.5779])\n",
      "\t Grad:  tensor([ 0.0019, -0.0146])\n",
      "Epoch 502, Loss 0.263143\n",
      "\t Params:  tensor([0.8439, 0.5780])\n",
      "\t Grad:  tensor([ 0.0019, -0.0145])\n",
      "Epoch 503, Loss 0.263141\n",
      "\t Params:  tensor([0.8439, 0.5782])\n",
      "\t Grad:  tensor([ 0.0019, -0.0144])\n",
      "Epoch 504, Loss 0.263139\n",
      "\t Params:  tensor([0.8439, 0.5783])\n",
      "\t Grad:  tensor([ 0.0019, -0.0143])\n",
      "Epoch 505, Loss 0.263137\n",
      "\t Params:  tensor([0.8439, 0.5785])\n",
      "\t Grad:  tensor([ 0.0019, -0.0142])\n",
      "Epoch 506, Loss 0.263135\n",
      "\t Params:  tensor([0.8439, 0.5786])\n",
      "\t Grad:  tensor([ 0.0019, -0.0141])\n",
      "Epoch 507, Loss 0.263133\n",
      "\t Params:  tensor([0.8439, 0.5787])\n",
      "\t Grad:  tensor([ 0.0019, -0.0140])\n",
      "Epoch 508, Loss 0.263131\n",
      "\t Params:  tensor([0.8438, 0.5789])\n",
      "\t Grad:  tensor([ 0.0019, -0.0139])\n",
      "Epoch 509, Loss 0.263129\n",
      "\t Params:  tensor([0.8438, 0.5790])\n",
      "\t Grad:  tensor([ 0.0018, -0.0138])\n",
      "Epoch 510, Loss 0.263127\n",
      "\t Params:  tensor([0.8438, 0.5792])\n",
      "\t Grad:  tensor([ 0.0018, -0.0137])\n",
      "Epoch 511, Loss 0.263125\n",
      "\t Params:  tensor([0.8438, 0.5793])\n",
      "\t Grad:  tensor([ 0.0018, -0.0136])\n",
      "Epoch 512, Loss 0.263123\n",
      "\t Params:  tensor([0.8438, 0.5794])\n",
      "\t Grad:  tensor([ 0.0018, -0.0135])\n",
      "Epoch 513, Loss 0.263121\n",
      "\t Params:  tensor([0.8437, 0.5796])\n",
      "\t Grad:  tensor([ 0.0018, -0.0134])\n",
      "Epoch 514, Loss 0.263119\n",
      "\t Params:  tensor([0.8437, 0.5797])\n",
      "\t Grad:  tensor([ 0.0018, -0.0134])\n",
      "Epoch 515, Loss 0.263118\n",
      "\t Params:  tensor([0.8437, 0.5798])\n",
      "\t Grad:  tensor([ 0.0018, -0.0133])\n",
      "Epoch 516, Loss 0.263116\n",
      "\t Params:  tensor([0.8437, 0.5800])\n",
      "\t Grad:  tensor([ 0.0018, -0.0132])\n",
      "Epoch 517, Loss 0.263114\n",
      "\t Params:  tensor([0.8437, 0.5801])\n",
      "\t Grad:  tensor([ 0.0017, -0.0131])\n",
      "Epoch 518, Loss 0.263112\n",
      "\t Params:  tensor([0.8437, 0.5802])\n",
      "\t Grad:  tensor([ 0.0017, -0.0130])\n",
      "Epoch 519, Loss 0.263111\n",
      "\t Params:  tensor([0.8436, 0.5803])\n",
      "\t Grad:  tensor([ 0.0017, -0.0129])\n",
      "Epoch 520, Loss 0.263109\n",
      "\t Params:  tensor([0.8436, 0.5805])\n",
      "\t Grad:  tensor([ 0.0017, -0.0128])\n",
      "Epoch 521, Loss 0.263107\n",
      "\t Params:  tensor([0.8436, 0.5806])\n",
      "\t Grad:  tensor([ 0.0017, -0.0127])\n",
      "Epoch 522, Loss 0.263106\n",
      "\t Params:  tensor([0.8436, 0.5807])\n",
      "\t Grad:  tensor([ 0.0017, -0.0127])\n",
      "Epoch 523, Loss 0.263104\n",
      "\t Params:  tensor([0.8436, 0.5809])\n",
      "\t Grad:  tensor([ 0.0017, -0.0126])\n",
      "Epoch 524, Loss 0.263102\n",
      "\t Params:  tensor([0.8436, 0.5810])\n",
      "\t Grad:  tensor([ 0.0017, -0.0125])\n",
      "Epoch 525, Loss 0.263101\n",
      "\t Params:  tensor([0.8435, 0.5811])\n",
      "\t Grad:  tensor([ 0.0017, -0.0124])\n",
      "Epoch 526, Loss 0.263099\n",
      "\t Params:  tensor([0.8435, 0.5812])\n",
      "\t Grad:  tensor([ 0.0016, -0.0123])\n",
      "Epoch 527, Loss 0.263098\n",
      "\t Params:  tensor([0.8435, 0.5813])\n",
      "\t Grad:  tensor([ 0.0016, -0.0122])\n",
      "Epoch 528, Loss 0.263096\n",
      "\t Params:  tensor([0.8435, 0.5815])\n",
      "\t Grad:  tensor([ 0.0016, -0.0122])\n",
      "Epoch 529, Loss 0.263095\n",
      "\t Params:  tensor([0.8435, 0.5816])\n",
      "\t Grad:  tensor([ 0.0016, -0.0121])\n",
      "Epoch 530, Loss 0.263093\n",
      "\t Params:  tensor([0.8435, 0.5817])\n",
      "\t Grad:  tensor([ 0.0016, -0.0120])\n",
      "Epoch 531, Loss 0.263092\n",
      "\t Params:  tensor([0.8434, 0.5818])\n",
      "\t Grad:  tensor([ 0.0016, -0.0119])\n",
      "Epoch 532, Loss 0.263090\n",
      "\t Params:  tensor([0.8434, 0.5819])\n",
      "\t Grad:  tensor([ 0.0016, -0.0118])\n",
      "Epoch 533, Loss 0.263089\n",
      "\t Params:  tensor([0.8434, 0.5821])\n",
      "\t Grad:  tensor([ 0.0016, -0.0118])\n",
      "Epoch 534, Loss 0.263087\n",
      "\t Params:  tensor([0.8434, 0.5822])\n",
      "\t Grad:  tensor([ 0.0016, -0.0117])\n",
      "Epoch 535, Loss 0.263086\n",
      "\t Params:  tensor([0.8434, 0.5823])\n",
      "\t Grad:  tensor([ 0.0015, -0.0116])\n",
      "Epoch 536, Loss 0.263085\n",
      "\t Params:  tensor([0.8434, 0.5824])\n",
      "\t Grad:  tensor([ 0.0015, -0.0115])\n",
      "Epoch 537, Loss 0.263083\n",
      "\t Params:  tensor([0.8433, 0.5825])\n",
      "\t Grad:  tensor([ 0.0015, -0.0115])\n",
      "Epoch 538, Loss 0.263082\n",
      "\t Params:  tensor([0.8433, 0.5826])\n",
      "\t Grad:  tensor([ 0.0015, -0.0114])\n",
      "Epoch 539, Loss 0.263081\n",
      "\t Params:  tensor([0.8433, 0.5828])\n",
      "\t Grad:  tensor([ 0.0015, -0.0113])\n",
      "Epoch 540, Loss 0.263079\n",
      "\t Params:  tensor([0.8433, 0.5829])\n",
      "\t Grad:  tensor([ 0.0015, -0.0112])\n",
      "Epoch 541, Loss 0.263078\n",
      "\t Params:  tensor([0.8433, 0.5830])\n",
      "\t Grad:  tensor([ 0.0015, -0.0112])\n",
      "Epoch 542, Loss 0.263077\n",
      "\t Params:  tensor([0.8433, 0.5831])\n",
      "\t Grad:  tensor([ 0.0015, -0.0111])\n",
      "Epoch 543, Loss 0.263076\n",
      "\t Params:  tensor([0.8433, 0.5832])\n",
      "\t Grad:  tensor([ 0.0015, -0.0110])\n",
      "Epoch 544, Loss 0.263074\n",
      "\t Params:  tensor([0.8432, 0.5833])\n",
      "\t Grad:  tensor([ 0.0015, -0.0109])\n",
      "Epoch 545, Loss 0.263073\n",
      "\t Params:  tensor([0.8432, 0.5834])\n",
      "\t Grad:  tensor([ 0.0015, -0.0109])\n",
      "Epoch 546, Loss 0.263072\n",
      "\t Params:  tensor([0.8432, 0.5835])\n",
      "\t Grad:  tensor([ 0.0014, -0.0108])\n",
      "Epoch 547, Loss 0.263071\n",
      "\t Params:  tensor([0.8432, 0.5836])\n",
      "\t Grad:  tensor([ 0.0014, -0.0107])\n",
      "Epoch 548, Loss 0.263070\n",
      "\t Params:  tensor([0.8432, 0.5837])\n",
      "\t Grad:  tensor([ 0.0014, -0.0107])\n",
      "Epoch 549, Loss 0.263068\n",
      "\t Params:  tensor([0.8432, 0.5838])\n",
      "\t Grad:  tensor([ 0.0014, -0.0106])\n",
      "Epoch 550, Loss 0.263067\n",
      "\t Params:  tensor([0.8432, 0.5840])\n",
      "\t Grad:  tensor([ 0.0014, -0.0105])\n",
      "Epoch 551, Loss 0.263066\n",
      "\t Params:  tensor([0.8431, 0.5841])\n",
      "\t Grad:  tensor([ 0.0014, -0.0104])\n",
      "Epoch 552, Loss 0.263065\n",
      "\t Params:  tensor([0.8431, 0.5842])\n",
      "\t Grad:  tensor([ 0.0014, -0.0104])\n",
      "Epoch 553, Loss 0.263064\n",
      "\t Params:  tensor([0.8431, 0.5843])\n",
      "\t Grad:  tensor([ 0.0014, -0.0103])\n",
      "Epoch 554, Loss 0.263063\n",
      "\t Params:  tensor([0.8431, 0.5844])\n",
      "\t Grad:  tensor([ 0.0014, -0.0102])\n",
      "Epoch 555, Loss 0.263062\n",
      "\t Params:  tensor([0.8431, 0.5845])\n",
      "\t Grad:  tensor([ 0.0014, -0.0102])\n",
      "Epoch 556, Loss 0.263061\n",
      "\t Params:  tensor([0.8431, 0.5846])\n",
      "\t Grad:  tensor([ 0.0014, -0.0101])\n",
      "Epoch 557, Loss 0.263060\n",
      "\t Params:  tensor([0.8431, 0.5847])\n",
      "\t Grad:  tensor([ 0.0013, -0.0100])\n",
      "Epoch 558, Loss 0.263059\n",
      "\t Params:  tensor([0.8430, 0.5848])\n",
      "\t Grad:  tensor([ 0.0013, -0.0100])\n",
      "Epoch 559, Loss 0.263058\n",
      "\t Params:  tensor([0.8430, 0.5849])\n",
      "\t Grad:  tensor([ 0.0013, -0.0099])\n",
      "Epoch 560, Loss 0.263057\n",
      "\t Params:  tensor([0.8430, 0.5850])\n",
      "\t Grad:  tensor([ 0.0013, -0.0098])\n",
      "Epoch 561, Loss 0.263056\n",
      "\t Params:  tensor([0.8430, 0.5851])\n",
      "\t Grad:  tensor([ 0.0013, -0.0098])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 562, Loss 0.263055\n",
      "\t Params:  tensor([0.8430, 0.5852])\n",
      "\t Grad:  tensor([ 0.0013, -0.0097])\n",
      "Epoch 563, Loss 0.263054\n",
      "\t Params:  tensor([0.8430, 0.5853])\n",
      "\t Grad:  tensor([ 0.0013, -0.0096])\n",
      "Epoch 564, Loss 0.263053\n",
      "\t Params:  tensor([0.8430, 0.5854])\n",
      "\t Grad:  tensor([ 0.0013, -0.0096])\n",
      "Epoch 565, Loss 0.263052\n",
      "\t Params:  tensor([0.8430, 0.5854])\n",
      "\t Grad:  tensor([ 0.0013, -0.0095])\n",
      "Epoch 566, Loss 0.263051\n",
      "\t Params:  tensor([0.8429, 0.5855])\n",
      "\t Grad:  tensor([ 0.0013, -0.0095])\n",
      "Epoch 567, Loss 0.263050\n",
      "\t Params:  tensor([0.8429, 0.5856])\n",
      "\t Grad:  tensor([ 0.0013, -0.0094])\n",
      "Epoch 568, Loss 0.263049\n",
      "\t Params:  tensor([0.8429, 0.5857])\n",
      "\t Grad:  tensor([ 0.0012, -0.0093])\n",
      "Epoch 569, Loss 0.263048\n",
      "\t Params:  tensor([0.8429, 0.5858])\n",
      "\t Grad:  tensor([ 0.0012, -0.0093])\n",
      "Epoch 570, Loss 0.263047\n",
      "\t Params:  tensor([0.8429, 0.5859])\n",
      "\t Grad:  tensor([ 0.0012, -0.0092])\n",
      "Epoch 571, Loss 0.263047\n",
      "\t Params:  tensor([0.8429, 0.5860])\n",
      "\t Grad:  tensor([ 0.0012, -0.0091])\n",
      "Epoch 572, Loss 0.263046\n",
      "\t Params:  tensor([0.8429, 0.5861])\n",
      "\t Grad:  tensor([ 0.0012, -0.0091])\n",
      "Epoch 573, Loss 0.263045\n",
      "\t Params:  tensor([0.8429, 0.5862])\n",
      "\t Grad:  tensor([ 0.0012, -0.0090])\n",
      "Epoch 574, Loss 0.263044\n",
      "\t Params:  tensor([0.8428, 0.5863])\n",
      "\t Grad:  tensor([ 0.0012, -0.0090])\n",
      "Epoch 575, Loss 0.263043\n",
      "\t Params:  tensor([0.8428, 0.5864])\n",
      "\t Grad:  tensor([ 0.0012, -0.0089])\n",
      "Epoch 576, Loss 0.263042\n",
      "\t Params:  tensor([0.8428, 0.5865])\n",
      "\t Grad:  tensor([ 0.0012, -0.0088])\n",
      "Epoch 577, Loss 0.263042\n",
      "\t Params:  tensor([0.8428, 0.5865])\n",
      "\t Grad:  tensor([ 0.0012, -0.0088])\n",
      "Epoch 578, Loss 0.263041\n",
      "\t Params:  tensor([0.8428, 0.5866])\n",
      "\t Grad:  tensor([ 0.0012, -0.0087])\n",
      "Epoch 579, Loss 0.263040\n",
      "\t Params:  tensor([0.8428, 0.5867])\n",
      "\t Grad:  tensor([ 0.0012, -0.0087])\n",
      "Epoch 580, Loss 0.263039\n",
      "\t Params:  tensor([0.8428, 0.5868])\n",
      "\t Grad:  tensor([ 0.0011, -0.0086])\n",
      "Epoch 581, Loss 0.263039\n",
      "\t Params:  tensor([0.8428, 0.5869])\n",
      "\t Grad:  tensor([ 0.0011, -0.0086])\n",
      "Epoch 582, Loss 0.263038\n",
      "\t Params:  tensor([0.8427, 0.5870])\n",
      "\t Grad:  tensor([ 0.0011, -0.0085])\n",
      "Epoch 583, Loss 0.263037\n",
      "\t Params:  tensor([0.8427, 0.5871])\n",
      "\t Grad:  tensor([ 0.0011, -0.0084])\n",
      "Epoch 584, Loss 0.263036\n",
      "\t Params:  tensor([0.8427, 0.5871])\n",
      "\t Grad:  tensor([ 0.0011, -0.0084])\n",
      "Epoch 585, Loss 0.263036\n",
      "\t Params:  tensor([0.8427, 0.5872])\n",
      "\t Grad:  tensor([ 0.0011, -0.0083])\n",
      "Epoch 586, Loss 0.263035\n",
      "\t Params:  tensor([0.8427, 0.5873])\n",
      "\t Grad:  tensor([ 0.0011, -0.0083])\n",
      "Epoch 587, Loss 0.263034\n",
      "\t Params:  tensor([0.8427, 0.5874])\n",
      "\t Grad:  tensor([ 0.0011, -0.0082])\n",
      "Epoch 588, Loss 0.263034\n",
      "\t Params:  tensor([0.8427, 0.5875])\n",
      "\t Grad:  tensor([ 0.0011, -0.0082])\n",
      "Epoch 589, Loss 0.263033\n",
      "\t Params:  tensor([0.8427, 0.5876])\n",
      "\t Grad:  tensor([ 0.0011, -0.0081])\n",
      "Epoch 590, Loss 0.263032\n",
      "\t Params:  tensor([0.8427, 0.5876])\n",
      "\t Grad:  tensor([ 0.0011, -0.0081])\n",
      "Epoch 591, Loss 0.263031\n",
      "\t Params:  tensor([0.8427, 0.5877])\n",
      "\t Grad:  tensor([ 0.0011, -0.0080])\n",
      "Epoch 592, Loss 0.263031\n",
      "\t Params:  tensor([0.8426, 0.5878])\n",
      "\t Grad:  tensor([ 0.0011, -0.0080])\n",
      "Epoch 593, Loss 0.263030\n",
      "\t Params:  tensor([0.8426, 0.5879])\n",
      "\t Grad:  tensor([ 0.0011, -0.0079])\n",
      "Epoch 594, Loss 0.263030\n",
      "\t Params:  tensor([0.8426, 0.5879])\n",
      "\t Grad:  tensor([ 0.0011, -0.0078])\n",
      "Epoch 595, Loss 0.263029\n",
      "\t Params:  tensor([0.8426, 0.5880])\n",
      "\t Grad:  tensor([ 0.0010, -0.0078])\n",
      "Epoch 596, Loss 0.263028\n",
      "\t Params:  tensor([0.8426, 0.5881])\n",
      "\t Grad:  tensor([ 0.0010, -0.0077])\n",
      "Epoch 597, Loss 0.263028\n",
      "\t Params:  tensor([0.8426, 0.5882])\n",
      "\t Grad:  tensor([ 0.0010, -0.0077])\n",
      "Epoch 598, Loss 0.263027\n",
      "\t Params:  tensor([0.8426, 0.5883])\n",
      "\t Grad:  tensor([ 0.0010, -0.0076])\n",
      "Epoch 599, Loss 0.263027\n",
      "\t Params:  tensor([0.8426, 0.5883])\n",
      "\t Grad:  tensor([ 0.0010, -0.0076])\n",
      "Epoch 600, Loss 0.263026\n",
      "\t Params:  tensor([0.8426, 0.5884])\n",
      "\t Grad:  tensor([ 0.0010, -0.0075])\n",
      "Epoch 601, Loss 0.263026\n",
      "\t Params:  tensor([0.8425, 0.5885])\n",
      "\t Grad:  tensor([ 0.0010, -0.0075])\n",
      "Epoch 602, Loss 0.263025\n",
      "\t Params:  tensor([0.8425, 0.5886])\n",
      "\t Grad:  tensor([ 0.0010, -0.0074])\n",
      "Epoch 603, Loss 0.263024\n",
      "\t Params:  tensor([0.8425, 0.5886])\n",
      "\t Grad:  tensor([ 0.0010, -0.0074])\n",
      "Epoch 604, Loss 0.263024\n",
      "\t Params:  tensor([0.8425, 0.5887])\n",
      "\t Grad:  tensor([ 0.0010, -0.0073])\n",
      "Epoch 605, Loss 0.263023\n",
      "\t Params:  tensor([0.8425, 0.5888])\n",
      "\t Grad:  tensor([ 0.0010, -0.0073])\n",
      "Epoch 606, Loss 0.263023\n",
      "\t Params:  tensor([0.8425, 0.5889])\n",
      "\t Grad:  tensor([ 0.0010, -0.0072])\n",
      "Epoch 607, Loss 0.263022\n",
      "\t Params:  tensor([0.8425, 0.5889])\n",
      "\t Grad:  tensor([ 0.0010, -0.0072])\n",
      "Epoch 608, Loss 0.263022\n",
      "\t Params:  tensor([0.8425, 0.5890])\n",
      "\t Grad:  tensor([ 0.0010, -0.0071])\n",
      "Epoch 609, Loss 0.263021\n",
      "\t Params:  tensor([0.8425, 0.5891])\n",
      "\t Grad:  tensor([ 0.0009, -0.0071])\n",
      "Epoch 610, Loss 0.263021\n",
      "\t Params:  tensor([0.8425, 0.5891])\n",
      "\t Grad:  tensor([ 0.0009, -0.0071])\n",
      "Epoch 611, Loss 0.263020\n",
      "\t Params:  tensor([0.8425, 0.5892])\n",
      "\t Grad:  tensor([ 0.0009, -0.0070])\n",
      "Epoch 612, Loss 0.263020\n",
      "\t Params:  tensor([0.8424, 0.5893])\n",
      "\t Grad:  tensor([ 0.0009, -0.0070])\n",
      "Epoch 613, Loss 0.263019\n",
      "\t Params:  tensor([0.8424, 0.5893])\n",
      "\t Grad:  tensor([ 0.0009, -0.0069])\n",
      "Epoch 614, Loss 0.263019\n",
      "\t Params:  tensor([0.8424, 0.5894])\n",
      "\t Grad:  tensor([ 0.0009, -0.0069])\n",
      "Epoch 615, Loss 0.263018\n",
      "\t Params:  tensor([0.8424, 0.5895])\n",
      "\t Grad:  tensor([ 0.0009, -0.0068])\n",
      "Epoch 616, Loss 0.263018\n",
      "\t Params:  tensor([0.8424, 0.5895])\n",
      "\t Grad:  tensor([ 0.0009, -0.0068])\n",
      "Epoch 617, Loss 0.263017\n",
      "\t Params:  tensor([0.8424, 0.5896])\n",
      "\t Grad:  tensor([ 0.0009, -0.0067])\n",
      "Epoch 618, Loss 0.263017\n",
      "\t Params:  tensor([0.8424, 0.5897])\n",
      "\t Grad:  tensor([ 0.0009, -0.0067])\n",
      "Epoch 619, Loss 0.263016\n",
      "\t Params:  tensor([0.8424, 0.5898])\n",
      "\t Grad:  tensor([ 0.0009, -0.0066])\n",
      "Epoch 620, Loss 0.263016\n",
      "\t Params:  tensor([0.8424, 0.5898])\n",
      "\t Grad:  tensor([ 0.0009, -0.0066])\n",
      "Epoch 621, Loss 0.263015\n",
      "\t Params:  tensor([0.8424, 0.5899])\n",
      "\t Grad:  tensor([ 0.0009, -0.0066])\n",
      "Epoch 622, Loss 0.263015\n",
      "\t Params:  tensor([0.8424, 0.5899])\n",
      "\t Grad:  tensor([ 0.0009, -0.0065])\n",
      "Epoch 623, Loss 0.263015\n",
      "\t Params:  tensor([0.8423, 0.5900])\n",
      "\t Grad:  tensor([ 0.0009, -0.0065])\n",
      "Epoch 624, Loss 0.263014\n",
      "\t Params:  tensor([0.8423, 0.5901])\n",
      "\t Grad:  tensor([ 0.0009, -0.0064])\n",
      "Epoch 625, Loss 0.263014\n",
      "\t Params:  tensor([0.8423, 0.5901])\n",
      "\t Grad:  tensor([ 0.0009, -0.0064])\n",
      "Epoch 626, Loss 0.263013\n",
      "\t Params:  tensor([0.8423, 0.5902])\n",
      "\t Grad:  tensor([ 0.0008, -0.0063])\n",
      "Epoch 627, Loss 0.263013\n",
      "\t Params:  tensor([0.8423, 0.5903])\n",
      "\t Grad:  tensor([ 0.0008, -0.0063])\n",
      "Epoch 628, Loss 0.263012\n",
      "\t Params:  tensor([0.8423, 0.5903])\n",
      "\t Grad:  tensor([ 0.0008, -0.0063])\n",
      "Epoch 629, Loss 0.263012\n",
      "\t Params:  tensor([0.8423, 0.5904])\n",
      "\t Grad:  tensor([ 0.0008, -0.0062])\n",
      "Epoch 630, Loss 0.263012\n",
      "\t Params:  tensor([0.8423, 0.5905])\n",
      "\t Grad:  tensor([ 0.0008, -0.0062])\n",
      "Epoch 631, Loss 0.263011\n",
      "\t Params:  tensor([0.8423, 0.5905])\n",
      "\t Grad:  tensor([ 0.0008, -0.0061])\n",
      "Epoch 632, Loss 0.263011\n",
      "\t Params:  tensor([0.8423, 0.5906])\n",
      "\t Grad:  tensor([ 0.0008, -0.0061])\n",
      "Epoch 633, Loss 0.263011\n",
      "\t Params:  tensor([0.8423, 0.5906])\n",
      "\t Grad:  tensor([ 0.0008, -0.0061])\n",
      "Epoch 634, Loss 0.263010\n",
      "\t Params:  tensor([0.8423, 0.5907])\n",
      "\t Grad:  tensor([ 0.0008, -0.0060])\n",
      "Epoch 635, Loss 0.263010\n",
      "\t Params:  tensor([0.8422, 0.5908])\n",
      "\t Grad:  tensor([ 0.0008, -0.0060])\n",
      "Epoch 636, Loss 0.263009\n",
      "\t Params:  tensor([0.8422, 0.5908])\n",
      "\t Grad:  tensor([ 0.0008, -0.0059])\n",
      "Epoch 637, Loss 0.263009\n",
      "\t Params:  tensor([0.8422, 0.5909])\n",
      "\t Grad:  tensor([ 0.0008, -0.0059])\n",
      "Epoch 638, Loss 0.263009\n",
      "\t Params:  tensor([0.8422, 0.5909])\n",
      "\t Grad:  tensor([ 0.0008, -0.0059])\n",
      "Epoch 639, Loss 0.263008\n",
      "\t Params:  tensor([0.8422, 0.5910])\n",
      "\t Grad:  tensor([ 0.0008, -0.0058])\n",
      "Epoch 640, Loss 0.263008\n",
      "\t Params:  tensor([0.8422, 0.5910])\n",
      "\t Grad:  tensor([ 0.0008, -0.0058])\n",
      "Epoch 641, Loss 0.263008\n",
      "\t Params:  tensor([0.8422, 0.5911])\n",
      "\t Grad:  tensor([ 0.0008, -0.0057])\n",
      "Epoch 642, Loss 0.263007\n",
      "\t Params:  tensor([0.8422, 0.5912])\n",
      "\t Grad:  tensor([ 0.0008, -0.0057])\n",
      "Epoch 643, Loss 0.263007\n",
      "\t Params:  tensor([0.8422, 0.5912])\n",
      "\t Grad:  tensor([ 0.0008, -0.0057])\n",
      "Epoch 644, Loss 0.263007\n",
      "\t Params:  tensor([0.8422, 0.5913])\n",
      "\t Grad:  tensor([ 0.0007, -0.0056])\n",
      "Epoch 645, Loss 0.263006\n",
      "\t Params:  tensor([0.8422, 0.5913])\n",
      "\t Grad:  tensor([ 0.0007, -0.0056])\n",
      "Epoch 646, Loss 0.263006\n",
      "\t Params:  tensor([0.8422, 0.5914])\n",
      "\t Grad:  tensor([ 0.0007, -0.0056])\n",
      "Epoch 647, Loss 0.263006\n",
      "\t Params:  tensor([0.8422, 0.5914])\n",
      "\t Grad:  tensor([ 0.0007, -0.0055])\n",
      "Epoch 648, Loss 0.263006\n",
      "\t Params:  tensor([0.8421, 0.5915])\n",
      "\t Grad:  tensor([ 0.0007, -0.0055])\n",
      "Epoch 649, Loss 0.263005\n",
      "\t Params:  tensor([0.8421, 0.5916])\n",
      "\t Grad:  tensor([ 0.0007, -0.0054])\n",
      "Epoch 650, Loss 0.263005\n",
      "\t Params:  tensor([0.8421, 0.5916])\n",
      "\t Grad:  tensor([ 0.0007, -0.0054])\n",
      "Epoch 651, Loss 0.263005\n",
      "\t Params:  tensor([0.8421, 0.5917])\n",
      "\t Grad:  tensor([ 0.0007, -0.0054])\n",
      "Epoch 652, Loss 0.263004\n",
      "\t Params:  tensor([0.8421, 0.5917])\n",
      "\t Grad:  tensor([ 0.0007, -0.0053])\n",
      "Epoch 653, Loss 0.263004\n",
      "\t Params:  tensor([0.8421, 0.5918])\n",
      "\t Grad:  tensor([ 0.0007, -0.0053])\n",
      "Epoch 654, Loss 0.263004\n",
      "\t Params:  tensor([0.8421, 0.5918])\n",
      "\t Grad:  tensor([ 0.0007, -0.0053])\n",
      "Epoch 655, Loss 0.263003\n",
      "\t Params:  tensor([0.8421, 0.5919])\n",
      "\t Grad:  tensor([ 0.0007, -0.0052])\n",
      "Epoch 656, Loss 0.263003\n",
      "\t Params:  tensor([0.8421, 0.5919])\n",
      "\t Grad:  tensor([ 0.0007, -0.0052])\n",
      "Epoch 657, Loss 0.263003\n",
      "\t Params:  tensor([0.8421, 0.5920])\n",
      "\t Grad:  tensor([ 0.0007, -0.0052])\n",
      "Epoch 658, Loss 0.263003\n",
      "\t Params:  tensor([0.8421, 0.5920])\n",
      "\t Grad:  tensor([ 0.0007, -0.0051])\n",
      "Epoch 659, Loss 0.263002\n",
      "\t Params:  tensor([0.8421, 0.5921])\n",
      "\t Grad:  tensor([ 0.0007, -0.0051])\n",
      "Epoch 660, Loss 0.263002\n",
      "\t Params:  tensor([0.8421, 0.5921])\n",
      "\t Grad:  tensor([ 0.0007, -0.0051])\n",
      "Epoch 661, Loss 0.263002\n",
      "\t Params:  tensor([0.8421, 0.5922])\n",
      "\t Grad:  tensor([ 0.0007, -0.0050])\n",
      "Epoch 662, Loss 0.263002\n",
      "\t Params:  tensor([0.8420, 0.5922])\n",
      "\t Grad:  tensor([ 0.0007, -0.0050])\n",
      "Epoch 663, Loss 0.263001\n",
      "\t Params:  tensor([0.8420, 0.5923])\n",
      "\t Grad:  tensor([ 0.0007, -0.0050])\n",
      "Epoch 664, Loss 0.263001\n",
      "\t Params:  tensor([0.8420, 0.5923])\n",
      "\t Grad:  tensor([ 0.0007, -0.0049])\n",
      "Epoch 665, Loss 0.263001\n",
      "\t Params:  tensor([0.8420, 0.5924])\n",
      "\t Grad:  tensor([ 0.0007, -0.0049])\n",
      "Epoch 666, Loss 0.263001\n",
      "\t Params:  tensor([0.8420, 0.5924])\n",
      "\t Grad:  tensor([ 0.0007, -0.0049])\n",
      "Epoch 667, Loss 0.263000\n",
      "\t Params:  tensor([0.8420, 0.5925])\n",
      "\t Grad:  tensor([ 0.0006, -0.0048])\n",
      "Epoch 668, Loss 0.263000\n",
      "\t Params:  tensor([0.8420, 0.5925])\n",
      "\t Grad:  tensor([ 0.0006, -0.0048])\n",
      "Epoch 669, Loss 0.263000\n",
      "\t Params:  tensor([0.8420, 0.5926])\n",
      "\t Grad:  tensor([ 0.0006, -0.0048])\n",
      "Epoch 670, Loss 0.263000\n",
      "\t Params:  tensor([0.8420, 0.5926])\n",
      "\t Grad:  tensor([ 0.0006, -0.0047])\n",
      "Epoch 671, Loss 0.262999\n",
      "\t Params:  tensor([0.8420, 0.5927])\n",
      "\t Grad:  tensor([ 0.0006, -0.0047])\n",
      "Epoch 672, Loss 0.262999\n",
      "\t Params:  tensor([0.8420, 0.5927])\n",
      "\t Grad:  tensor([ 0.0006, -0.0047])\n",
      "Epoch 673, Loss 0.262999\n",
      "\t Params:  tensor([0.8420, 0.5928])\n",
      "\t Grad:  tensor([ 0.0006, -0.0046])\n",
      "Epoch 674, Loss 0.262999\n",
      "\t Params:  tensor([0.8420, 0.5928])\n",
      "\t Grad:  tensor([ 0.0006, -0.0046])\n",
      "Epoch 675, Loss 0.262998\n",
      "\t Params:  tensor([0.8420, 0.5928])\n",
      "\t Grad:  tensor([ 0.0006, -0.0046])\n",
      "Epoch 676, Loss 0.262998\n",
      "\t Params:  tensor([0.8420, 0.5929])\n",
      "\t Grad:  tensor([ 0.0006, -0.0045])\n",
      "Epoch 677, Loss 0.262998\n",
      "\t Params:  tensor([0.8420, 0.5929])\n",
      "\t Grad:  tensor([ 0.0006, -0.0045])\n",
      "Epoch 678, Loss 0.262998\n",
      "\t Params:  tensor([0.8419, 0.5930])\n",
      "\t Grad:  tensor([ 0.0006, -0.0045])\n",
      "Epoch 679, Loss 0.262998\n",
      "\t Params:  tensor([0.8419, 0.5930])\n",
      "\t Grad:  tensor([ 0.0006, -0.0045])\n",
      "Epoch 680, Loss 0.262998\n",
      "\t Params:  tensor([0.8419, 0.5931])\n",
      "\t Grad:  tensor([ 0.0006, -0.0044])\n",
      "Epoch 681, Loss 0.262997\n",
      "\t Params:  tensor([0.8419, 0.5931])\n",
      "\t Grad:  tensor([ 0.0006, -0.0044])\n",
      "Epoch 682, Loss 0.262997\n",
      "\t Params:  tensor([0.8419, 0.5932])\n",
      "\t Grad:  tensor([ 0.0006, -0.0044])\n",
      "Epoch 683, Loss 0.262997\n",
      "\t Params:  tensor([0.8419, 0.5932])\n",
      "\t Grad:  tensor([ 0.0006, -0.0043])\n",
      "Epoch 684, Loss 0.262997\n",
      "\t Params:  tensor([0.8419, 0.5932])\n",
      "\t Grad:  tensor([ 0.0006, -0.0043])\n",
      "Epoch 685, Loss 0.262996\n",
      "\t Params:  tensor([0.8419, 0.5933])\n",
      "\t Grad:  tensor([ 0.0006, -0.0043])\n",
      "Epoch 686, Loss 0.262996\n",
      "\t Params:  tensor([0.8419, 0.5933])\n",
      "\t Grad:  tensor([ 0.0006, -0.0043])\n",
      "Epoch 687, Loss 0.262996\n",
      "\t Params:  tensor([0.8419, 0.5934])\n",
      "\t Grad:  tensor([ 0.0006, -0.0042])\n",
      "Epoch 688, Loss 0.262996\n",
      "\t Params:  tensor([0.8419, 0.5934])\n",
      "\t Grad:  tensor([ 0.0006, -0.0042])\n",
      "Epoch 689, Loss 0.262996\n",
      "\t Params:  tensor([0.8419, 0.5935])\n",
      "\t Grad:  tensor([ 0.0006, -0.0042])\n",
      "Epoch 690, Loss 0.262996\n",
      "\t Params:  tensor([0.8419, 0.5935])\n",
      "\t Grad:  tensor([ 0.0006, -0.0041])\n",
      "Epoch 691, Loss 0.262995\n",
      "\t Params:  tensor([0.8419, 0.5935])\n",
      "\t Grad:  tensor([ 0.0006, -0.0041])\n",
      "Epoch 692, Loss 0.262995\n",
      "\t Params:  tensor([0.8419, 0.5936])\n",
      "\t Grad:  tensor([ 0.0005, -0.0041])\n",
      "Epoch 693, Loss 0.262995\n",
      "\t Params:  tensor([0.8419, 0.5936])\n",
      "\t Grad:  tensor([ 0.0005, -0.0041])\n",
      "Epoch 694, Loss 0.262995\n",
      "\t Params:  tensor([0.8419, 0.5937])\n",
      "\t Grad:  tensor([ 0.0005, -0.0040])\n",
      "Epoch 695, Loss 0.262995\n",
      "\t Params:  tensor([0.8418, 0.5937])\n",
      "\t Grad:  tensor([ 0.0005, -0.0040])\n",
      "Epoch 696, Loss 0.262995\n",
      "\t Params:  tensor([0.8418, 0.5937])\n",
      "\t Grad:  tensor([ 0.0005, -0.0040])\n",
      "Epoch 697, Loss 0.262994\n",
      "\t Params:  tensor([0.8418, 0.5938])\n",
      "\t Grad:  tensor([ 0.0005, -0.0040])\n",
      "Epoch 698, Loss 0.262994\n",
      "\t Params:  tensor([0.8418, 0.5938])\n",
      "\t Grad:  tensor([ 0.0005, -0.0039])\n",
      "Epoch 699, Loss 0.262994\n",
      "\t Params:  tensor([0.8418, 0.5939])\n",
      "\t Grad:  tensor([ 0.0005, -0.0039])\n",
      "Epoch 700, Loss 0.262994\n",
      "\t Params:  tensor([0.8418, 0.5939])\n",
      "\t Grad:  tensor([ 0.0005, -0.0039])\n",
      "Epoch 701, Loss 0.262994\n",
      "\t Params:  tensor([0.8418, 0.5939])\n",
      "\t Grad:  tensor([ 0.0005, -0.0039])\n",
      "Epoch 702, Loss 0.262994\n",
      "\t Params:  tensor([0.8418, 0.5940])\n",
      "\t Grad:  tensor([ 0.0005, -0.0038])\n",
      "Epoch 703, Loss 0.262994\n",
      "\t Params:  tensor([0.8418, 0.5940])\n",
      "\t Grad:  tensor([ 0.0005, -0.0038])\n",
      "Epoch 704, Loss 0.262993\n",
      "\t Params:  tensor([0.8418, 0.5941])\n",
      "\t Grad:  tensor([ 0.0005, -0.0038])\n",
      "Epoch 705, Loss 0.262993\n",
      "\t Params:  tensor([0.8418, 0.5941])\n",
      "\t Grad:  tensor([ 0.0005, -0.0038])\n",
      "Epoch 706, Loss 0.262993\n",
      "\t Params:  tensor([0.8418, 0.5941])\n",
      "\t Grad:  tensor([ 0.0005, -0.0037])\n",
      "Epoch 707, Loss 0.262993\n",
      "\t Params:  tensor([0.8418, 0.5942])\n",
      "\t Grad:  tensor([ 0.0005, -0.0037])\n",
      "Epoch 708, Loss 0.262993\n",
      "\t Params:  tensor([0.8418, 0.5942])\n",
      "\t Grad:  tensor([ 0.0005, -0.0037])\n",
      "Epoch 709, Loss 0.262993\n",
      "\t Params:  tensor([0.8418, 0.5942])\n",
      "\t Grad:  tensor([ 0.0005, -0.0037])\n",
      "Epoch 710, Loss 0.262993\n",
      "\t Params:  tensor([0.8418, 0.5943])\n",
      "\t Grad:  tensor([ 0.0005, -0.0036])\n",
      "Epoch 711, Loss 0.262992\n",
      "\t Params:  tensor([0.8418, 0.5943])\n",
      "\t Grad:  tensor([ 0.0005, -0.0036])\n",
      "Epoch 712, Loss 0.262992\n",
      "\t Params:  tensor([0.8418, 0.5943])\n",
      "\t Grad:  tensor([ 0.0005, -0.0036])\n",
      "Epoch 713, Loss 0.262992\n",
      "\t Params:  tensor([0.8418, 0.5944])\n",
      "\t Grad:  tensor([ 0.0005, -0.0036])\n",
      "Epoch 714, Loss 0.262992\n",
      "\t Params:  tensor([0.8418, 0.5944])\n",
      "\t Grad:  tensor([ 0.0005, -0.0035])\n",
      "Epoch 715, Loss 0.262992\n",
      "\t Params:  tensor([0.8417, 0.5944])\n",
      "\t Grad:  tensor([ 0.0005, -0.0035])\n",
      "Epoch 716, Loss 0.262992\n",
      "\t Params:  tensor([0.8417, 0.5945])\n",
      "\t Grad:  tensor([ 0.0005, -0.0035])\n",
      "Epoch 717, Loss 0.262992\n",
      "\t Params:  tensor([0.8417, 0.5945])\n",
      "\t Grad:  tensor([ 0.0005, -0.0035])\n",
      "Epoch 718, Loss 0.262992\n",
      "\t Params:  tensor([0.8417, 0.5946])\n",
      "\t Grad:  tensor([ 0.0005, -0.0034])\n",
      "Epoch 719, Loss 0.262991\n",
      "\t Params:  tensor([0.8417, 0.5946])\n",
      "\t Grad:  tensor([ 0.0005, -0.0034])\n",
      "Epoch 720, Loss 0.262991\n",
      "\t Params:  tensor([0.8417, 0.5946])\n",
      "\t Grad:  tensor([ 0.0005, -0.0034])\n",
      "Epoch 721, Loss 0.262991\n",
      "\t Params:  tensor([0.8417, 0.5947])\n",
      "\t Grad:  tensor([ 0.0005, -0.0034])\n",
      "Epoch 722, Loss 0.262991\n",
      "\t Params:  tensor([0.8417, 0.5947])\n",
      "\t Grad:  tensor([ 0.0004, -0.0034])\n",
      "Epoch 723, Loss 0.262991\n",
      "\t Params:  tensor([0.8417, 0.5947])\n",
      "\t Grad:  tensor([ 0.0004, -0.0033])\n",
      "Epoch 724, Loss 0.262991\n",
      "\t Params:  tensor([0.8417, 0.5948])\n",
      "\t Grad:  tensor([ 0.0004, -0.0033])\n",
      "Epoch 725, Loss 0.262991\n",
      "\t Params:  tensor([0.8417, 0.5948])\n",
      "\t Grad:  tensor([ 0.0004, -0.0033])\n",
      "Epoch 726, Loss 0.262991\n",
      "\t Params:  tensor([0.8417, 0.5948])\n",
      "\t Grad:  tensor([ 0.0004, -0.0033])\n",
      "Epoch 727, Loss 0.262990\n",
      "\t Params:  tensor([0.8417, 0.5949])\n",
      "\t Grad:  tensor([ 0.0004, -0.0032])\n",
      "Epoch 728, Loss 0.262990\n",
      "\t Params:  tensor([0.8417, 0.5949])\n",
      "\t Grad:  tensor([ 0.0004, -0.0032])\n",
      "Epoch 729, Loss 0.262990\n",
      "\t Params:  tensor([0.8417, 0.5949])\n",
      "\t Grad:  tensor([ 0.0004, -0.0032])\n",
      "Epoch 730, Loss 0.262990\n",
      "\t Params:  tensor([0.8417, 0.5949])\n",
      "\t Grad:  tensor([ 0.0004, -0.0032])\n",
      "Epoch 731, Loss 0.262990\n",
      "\t Params:  tensor([0.8417, 0.5950])\n",
      "\t Grad:  tensor([ 0.0004, -0.0032])\n",
      "Epoch 732, Loss 0.262990\n",
      "\t Params:  tensor([0.8417, 0.5950])\n",
      "\t Grad:  tensor([ 0.0004, -0.0031])\n",
      "Epoch 733, Loss 0.262990\n",
      "\t Params:  tensor([0.8417, 0.5950])\n",
      "\t Grad:  tensor([ 0.0004, -0.0031])\n",
      "Epoch 734, Loss 0.262990\n",
      "\t Params:  tensor([0.8417, 0.5951])\n",
      "\t Grad:  tensor([ 0.0004, -0.0031])\n",
      "Epoch 735, Loss 0.262990\n",
      "\t Params:  tensor([0.8417, 0.5951])\n",
      "\t Grad:  tensor([ 0.0004, -0.0031])\n",
      "Epoch 736, Loss 0.262989\n",
      "\t Params:  tensor([0.8417, 0.5951])\n",
      "\t Grad:  tensor([ 0.0004, -0.0031])\n",
      "Epoch 737, Loss 0.262989\n",
      "\t Params:  tensor([0.8417, 0.5952])\n",
      "\t Grad:  tensor([ 0.0004, -0.0030])\n",
      "Epoch 738, Loss 0.262989\n",
      "\t Params:  tensor([0.8416, 0.5952])\n",
      "\t Grad:  tensor([ 0.0004, -0.0030])\n",
      "Epoch 739, Loss 0.262989\n",
      "\t Params:  tensor([0.8416, 0.5952])\n",
      "\t Grad:  tensor([ 0.0004, -0.0030])\n",
      "Epoch 740, Loss 0.262989\n",
      "\t Params:  tensor([0.8416, 0.5953])\n",
      "\t Grad:  tensor([ 0.0004, -0.0030])\n",
      "Epoch 741, Loss 0.262989\n",
      "\t Params:  tensor([0.8416, 0.5953])\n",
      "\t Grad:  tensor([ 0.0004, -0.0030])\n",
      "Epoch 742, Loss 0.262989\n",
      "\t Params:  tensor([0.8416, 0.5953])\n",
      "\t Grad:  tensor([ 0.0004, -0.0029])\n",
      "Epoch 743, Loss 0.262989\n",
      "\t Params:  tensor([0.8416, 0.5953])\n",
      "\t Grad:  tensor([ 0.0004, -0.0029])\n",
      "Epoch 744, Loss 0.262989\n",
      "\t Params:  tensor([0.8416, 0.5954])\n",
      "\t Grad:  tensor([ 0.0004, -0.0029])\n",
      "Epoch 745, Loss 0.262989\n",
      "\t Params:  tensor([0.8416, 0.5954])\n",
      "\t Grad:  tensor([ 0.0004, -0.0029])\n",
      "Epoch 746, Loss 0.262989\n",
      "\t Params:  tensor([0.8416, 0.5954])\n",
      "\t Grad:  tensor([ 0.0004, -0.0029])\n",
      "Epoch 747, Loss 0.262989\n",
      "\t Params:  tensor([0.8416, 0.5955])\n",
      "\t Grad:  tensor([ 0.0004, -0.0028])\n",
      "Epoch 748, Loss 0.262989\n",
      "\t Params:  tensor([0.8416, 0.5955])\n",
      "\t Grad:  tensor([ 0.0004, -0.0028])\n",
      "Epoch 749, Loss 0.262988\n",
      "\t Params:  tensor([0.8416, 0.5955])\n",
      "\t Grad:  tensor([ 0.0004, -0.0028])\n",
      "Epoch 750, Loss 0.262988\n",
      "\t Params:  tensor([0.8416, 0.5955])\n",
      "\t Grad:  tensor([ 0.0004, -0.0028])\n",
      "Epoch 751, Loss 0.262988\n",
      "\t Params:  tensor([0.8416, 0.5956])\n",
      "\t Grad:  tensor([ 0.0004, -0.0028])\n",
      "Epoch 752, Loss 0.262988\n",
      "\t Params:  tensor([0.8416, 0.5956])\n",
      "\t Grad:  tensor([ 0.0004, -0.0027])\n",
      "Epoch 753, Loss 0.262988\n",
      "\t Params:  tensor([0.8416, 0.5956])\n",
      "\t Grad:  tensor([ 0.0004, -0.0027])\n",
      "Epoch 754, Loss 0.262988\n",
      "\t Params:  tensor([0.8416, 0.5957])\n",
      "\t Grad:  tensor([ 0.0004, -0.0027])\n",
      "Epoch 755, Loss 0.262988\n",
      "\t Params:  tensor([0.8416, 0.5957])\n",
      "\t Grad:  tensor([ 0.0004, -0.0027])\n",
      "Epoch 756, Loss 0.262988\n",
      "\t Params:  tensor([0.8416, 0.5957])\n",
      "\t Grad:  tensor([ 0.0004, -0.0027])\n",
      "Epoch 757, Loss 0.262988\n",
      "\t Params:  tensor([0.8416, 0.5957])\n",
      "\t Grad:  tensor([ 0.0004, -0.0027])\n",
      "Epoch 758, Loss 0.262988\n",
      "\t Params:  tensor([0.8416, 0.5958])\n",
      "\t Grad:  tensor([ 0.0004, -0.0026])\n",
      "Epoch 759, Loss 0.262988\n",
      "\t Params:  tensor([0.8416, 0.5958])\n",
      "\t Grad:  tensor([ 0.0003, -0.0026])\n",
      "Epoch 760, Loss 0.262988\n",
      "\t Params:  tensor([0.8416, 0.5958])\n",
      "\t Grad:  tensor([ 0.0003, -0.0026])\n",
      "Epoch 761, Loss 0.262988\n",
      "\t Params:  tensor([0.8416, 0.5958])\n",
      "\t Grad:  tensor([ 0.0003, -0.0026])\n",
      "Epoch 762, Loss 0.262987\n",
      "\t Params:  tensor([0.8416, 0.5959])\n",
      "\t Grad:  tensor([ 0.0003, -0.0026])\n",
      "Epoch 763, Loss 0.262987\n",
      "\t Params:  tensor([0.8416, 0.5959])\n",
      "\t Grad:  tensor([ 0.0003, -0.0026])\n",
      "Epoch 764, Loss 0.262987\n",
      "\t Params:  tensor([0.8416, 0.5959])\n",
      "\t Grad:  tensor([ 0.0003, -0.0025])\n",
      "Epoch 765, Loss 0.262987\n",
      "\t Params:  tensor([0.8416, 0.5959])\n",
      "\t Grad:  tensor([ 0.0003, -0.0025])\n",
      "Epoch 766, Loss 0.262987\n",
      "\t Params:  tensor([0.8415, 0.5960])\n",
      "\t Grad:  tensor([ 0.0003, -0.0025])\n",
      "Epoch 767, Loss 0.262987\n",
      "\t Params:  tensor([0.8415, 0.5960])\n",
      "\t Grad:  tensor([ 0.0003, -0.0025])\n",
      "Epoch 768, Loss 0.262987\n",
      "\t Params:  tensor([0.8415, 0.5960])\n",
      "\t Grad:  tensor([ 0.0003, -0.0025])\n",
      "Epoch 769, Loss 0.262987\n",
      "\t Params:  tensor([0.8415, 0.5960])\n",
      "\t Grad:  tensor([ 0.0003, -0.0025])\n",
      "Epoch 770, Loss 0.262987\n",
      "\t Params:  tensor([0.8415, 0.5961])\n",
      "\t Grad:  tensor([ 0.0003, -0.0024])\n",
      "Epoch 771, Loss 0.262987\n",
      "\t Params:  tensor([0.8415, 0.5961])\n",
      "\t Grad:  tensor([ 0.0003, -0.0024])\n",
      "Epoch 772, Loss 0.262987\n",
      "\t Params:  tensor([0.8415, 0.5961])\n",
      "\t Grad:  tensor([ 0.0003, -0.0024])\n",
      "Epoch 773, Loss 0.262987\n",
      "\t Params:  tensor([0.8415, 0.5961])\n",
      "\t Grad:  tensor([ 0.0003, -0.0024])\n",
      "Epoch 774, Loss 0.262987\n",
      "\t Params:  tensor([0.8415, 0.5962])\n",
      "\t Grad:  tensor([ 0.0003, -0.0024])\n",
      "Epoch 775, Loss 0.262987\n",
      "\t Params:  tensor([0.8415, 0.5962])\n",
      "\t Grad:  tensor([ 0.0003, -0.0024])\n",
      "Epoch 776, Loss 0.262987\n",
      "\t Params:  tensor([0.8415, 0.5962])\n",
      "\t Grad:  tensor([ 0.0003, -0.0023])\n",
      "Epoch 777, Loss 0.262987\n",
      "\t Params:  tensor([0.8415, 0.5962])\n",
      "\t Grad:  tensor([ 0.0003, -0.0023])\n",
      "Epoch 778, Loss 0.262986\n",
      "\t Params:  tensor([0.8415, 0.5963])\n",
      "\t Grad:  tensor([ 0.0003, -0.0023])\n",
      "Epoch 779, Loss 0.262987\n",
      "\t Params:  tensor([0.8415, 0.5963])\n",
      "\t Grad:  tensor([ 0.0003, -0.0023])\n",
      "Epoch 780, Loss 0.262986\n",
      "\t Params:  tensor([0.8415, 0.5963])\n",
      "\t Grad:  tensor([ 0.0003, -0.0023])\n",
      "Epoch 781, Loss 0.262986\n",
      "\t Params:  tensor([0.8415, 0.5963])\n",
      "\t Grad:  tensor([ 0.0003, -0.0023])\n",
      "Epoch 782, Loss 0.262986\n",
      "\t Params:  tensor([0.8415, 0.5963])\n",
      "\t Grad:  tensor([ 0.0003, -0.0022])\n",
      "Epoch 783, Loss 0.262986\n",
      "\t Params:  tensor([0.8415, 0.5964])\n",
      "\t Grad:  tensor([ 0.0003, -0.0022])\n",
      "Epoch 784, Loss 0.262986\n",
      "\t Params:  tensor([0.8415, 0.5964])\n",
      "\t Grad:  tensor([ 0.0003, -0.0022])\n",
      "Epoch 785, Loss 0.262986\n",
      "\t Params:  tensor([0.8415, 0.5964])\n",
      "\t Grad:  tensor([ 0.0003, -0.0022])\n",
      "Epoch 786, Loss 0.262986\n",
      "\t Params:  tensor([0.8415, 0.5964])\n",
      "\t Grad:  tensor([ 0.0003, -0.0022])\n",
      "Epoch 787, Loss 0.262986\n",
      "\t Params:  tensor([0.8415, 0.5965])\n",
      "\t Grad:  tensor([ 0.0003, -0.0022])\n",
      "Epoch 788, Loss 0.262986\n",
      "\t Params:  tensor([0.8415, 0.5965])\n",
      "\t Grad:  tensor([ 0.0003, -0.0022])\n",
      "Epoch 789, Loss 0.262986\n",
      "\t Params:  tensor([0.8415, 0.5965])\n",
      "\t Grad:  tensor([ 0.0003, -0.0021])\n",
      "Epoch 790, Loss 0.262986\n",
      "\t Params:  tensor([0.8415, 0.5965])\n",
      "\t Grad:  tensor([ 0.0003, -0.0021])\n",
      "Epoch 791, Loss 0.262986\n",
      "\t Params:  tensor([0.8415, 0.5965])\n",
      "\t Grad:  tensor([ 0.0003, -0.0021])\n",
      "Epoch 792, Loss 0.262986\n",
      "\t Params:  tensor([0.8415, 0.5966])\n",
      "\t Grad:  tensor([ 0.0003, -0.0021])\n",
      "Epoch 793, Loss 0.262986\n",
      "\t Params:  tensor([0.8415, 0.5966])\n",
      "\t Grad:  tensor([ 0.0003, -0.0021])\n",
      "Epoch 794, Loss 0.262986\n",
      "\t Params:  tensor([0.8415, 0.5966])\n",
      "\t Grad:  tensor([ 0.0003, -0.0021])\n",
      "Epoch 795, Loss 0.262986\n",
      "\t Params:  tensor([0.8415, 0.5966])\n",
      "\t Grad:  tensor([ 0.0003, -0.0021])\n",
      "Epoch 796, Loss 0.262986\n",
      "\t Params:  tensor([0.8415, 0.5966])\n",
      "\t Grad:  tensor([ 0.0003, -0.0020])\n",
      "Epoch 797, Loss 0.262986\n",
      "\t Params:  tensor([0.8415, 0.5967])\n",
      "\t Grad:  tensor([ 0.0003, -0.0020])\n",
      "Epoch 798, Loss 0.262985\n",
      "\t Params:  tensor([0.8415, 0.5967])\n",
      "\t Grad:  tensor([ 0.0003, -0.0020])\n",
      "Epoch 799, Loss 0.262985\n",
      "\t Params:  tensor([0.8414, 0.5967])\n",
      "\t Grad:  tensor([ 0.0003, -0.0020])\n",
      "Epoch 800, Loss 0.262986\n",
      "\t Params:  tensor([0.8414, 0.5967])\n",
      "\t Grad:  tensor([ 0.0003, -0.0020])\n",
      "Epoch 801, Loss 0.262985\n",
      "\t Params:  tensor([0.8414, 0.5967])\n",
      "\t Grad:  tensor([ 0.0003, -0.0020])\n",
      "Epoch 802, Loss 0.262985\n",
      "\t Params:  tensor([0.8414, 0.5968])\n",
      "\t Grad:  tensor([ 0.0003, -0.0020])\n",
      "Epoch 803, Loss 0.262985\n",
      "\t Params:  tensor([0.8414, 0.5968])\n",
      "\t Grad:  tensor([ 0.0003, -0.0020])\n",
      "Epoch 804, Loss 0.262985\n",
      "\t Params:  tensor([0.8414, 0.5968])\n",
      "\t Grad:  tensor([ 0.0003, -0.0019])\n",
      "Epoch 805, Loss 0.262985\n",
      "\t Params:  tensor([0.8414, 0.5968])\n",
      "\t Grad:  tensor([ 0.0003, -0.0019])\n",
      "Epoch 806, Loss 0.262985\n",
      "\t Params:  tensor([0.8414, 0.5968])\n",
      "\t Grad:  tensor([ 0.0003, -0.0019])\n",
      "Epoch 807, Loss 0.262985\n",
      "\t Params:  tensor([0.8414, 0.5969])\n",
      "\t Grad:  tensor([ 0.0003, -0.0019])\n",
      "Epoch 808, Loss 0.262985\n",
      "\t Params:  tensor([0.8414, 0.5969])\n",
      "\t Grad:  tensor([ 0.0003, -0.0019])\n",
      "Epoch 809, Loss 0.262985\n",
      "\t Params:  tensor([0.8414, 0.5969])\n",
      "\t Grad:  tensor([ 0.0002, -0.0019])\n",
      "Epoch 810, Loss 0.262985\n",
      "\t Params:  tensor([0.8414, 0.5969])\n",
      "\t Grad:  tensor([ 0.0002, -0.0019])\n",
      "Epoch 811, Loss 0.262985\n",
      "\t Params:  tensor([0.8414, 0.5969])\n",
      "\t Grad:  tensor([ 0.0002, -0.0019])\n",
      "Epoch 812, Loss 0.262985\n",
      "\t Params:  tensor([0.8414, 0.5970])\n",
      "\t Grad:  tensor([ 0.0002, -0.0018])\n",
      "Epoch 813, Loss 0.262985\n",
      "\t Params:  tensor([0.8414, 0.5970])\n",
      "\t Grad:  tensor([ 0.0002, -0.0018])\n",
      "Epoch 814, Loss 0.262985\n",
      "\t Params:  tensor([0.8414, 0.5970])\n",
      "\t Grad:  tensor([ 0.0002, -0.0018])\n",
      "Epoch 815, Loss 0.262985\n",
      "\t Params:  tensor([0.8414, 0.5970])\n",
      "\t Grad:  tensor([ 0.0002, -0.0018])\n",
      "Epoch 816, Loss 0.262985\n",
      "\t Params:  tensor([0.8414, 0.5970])\n",
      "\t Grad:  tensor([ 0.0002, -0.0018])\n",
      "Epoch 817, Loss 0.262985\n",
      "\t Params:  tensor([0.8414, 0.5970])\n",
      "\t Grad:  tensor([ 0.0002, -0.0018])\n",
      "Epoch 818, Loss 0.262985\n",
      "\t Params:  tensor([0.8414, 0.5971])\n",
      "\t Grad:  tensor([ 0.0002, -0.0018])\n",
      "Epoch 819, Loss 0.262985\n",
      "\t Params:  tensor([0.8414, 0.5971])\n",
      "\t Grad:  tensor([ 0.0002, -0.0018])\n",
      "Epoch 820, Loss 0.262985\n",
      "\t Params:  tensor([0.8414, 0.5971])\n",
      "\t Grad:  tensor([ 0.0002, -0.0017])\n",
      "Epoch 821, Loss 0.262985\n",
      "\t Params:  tensor([0.8414, 0.5971])\n",
      "\t Grad:  tensor([ 0.0002, -0.0017])\n",
      "Epoch 822, Loss 0.262985\n",
      "\t Params:  tensor([0.8414, 0.5971])\n",
      "\t Grad:  tensor([ 0.0002, -0.0017])\n",
      "Epoch 823, Loss 0.262985\n",
      "\t Params:  tensor([0.8414, 0.5971])\n",
      "\t Grad:  tensor([ 0.0002, -0.0017])\n",
      "Epoch 824, Loss 0.262985\n",
      "\t Params:  tensor([0.8414, 0.5972])\n",
      "\t Grad:  tensor([ 0.0002, -0.0017])\n",
      "Epoch 825, Loss 0.262985\n",
      "\t Params:  tensor([0.8414, 0.5972])\n",
      "\t Grad:  tensor([ 0.0002, -0.0017])\n",
      "Epoch 826, Loss 0.262985\n",
      "\t Params:  tensor([0.8414, 0.5972])\n",
      "\t Grad:  tensor([ 0.0002, -0.0017])\n",
      "Epoch 827, Loss 0.262985\n",
      "\t Params:  tensor([0.8414, 0.5972])\n",
      "\t Grad:  tensor([ 0.0002, -0.0017])\n",
      "Epoch 828, Loss 0.262985\n",
      "\t Params:  tensor([0.8414, 0.5972])\n",
      "\t Grad:  tensor([ 0.0002, -0.0017])\n",
      "Epoch 829, Loss 0.262985\n",
      "\t Params:  tensor([0.8414, 0.5972])\n",
      "\t Grad:  tensor([ 0.0002, -0.0016])\n",
      "Epoch 830, Loss 0.262985\n",
      "\t Params:  tensor([0.8414, 0.5973])\n",
      "\t Grad:  tensor([ 0.0002, -0.0016])\n",
      "Epoch 831, Loss 0.262984\n",
      "\t Params:  tensor([0.8414, 0.5973])\n",
      "\t Grad:  tensor([ 0.0002, -0.0016])\n",
      "Epoch 832, Loss 0.262984\n",
      "\t Params:  tensor([0.8414, 0.5973])\n",
      "\t Grad:  tensor([ 0.0002, -0.0016])\n",
      "Epoch 833, Loss 0.262984\n",
      "\t Params:  tensor([0.8414, 0.5973])\n",
      "\t Grad:  tensor([ 0.0002, -0.0016])\n",
      "Epoch 834, Loss 0.262984\n",
      "\t Params:  tensor([0.8414, 0.5973])\n",
      "\t Grad:  tensor([ 0.0002, -0.0016])\n",
      "Epoch 835, Loss 0.262984\n",
      "\t Params:  tensor([0.8414, 0.5973])\n",
      "\t Grad:  tensor([ 0.0002, -0.0016])\n",
      "Epoch 836, Loss 0.262984\n",
      "\t Params:  tensor([0.8414, 0.5974])\n",
      "\t Grad:  tensor([ 0.0002, -0.0016])\n",
      "Epoch 837, Loss 0.262984\n",
      "\t Params:  tensor([0.8414, 0.5974])\n",
      "\t Grad:  tensor([ 0.0002, -0.0016])\n",
      "Epoch 838, Loss 0.262984\n",
      "\t Params:  tensor([0.8414, 0.5974])\n",
      "\t Grad:  tensor([ 0.0002, -0.0015])\n",
      "Epoch 839, Loss 0.262984\n",
      "\t Params:  tensor([0.8414, 0.5974])\n",
      "\t Grad:  tensor([ 0.0002, -0.0015])\n",
      "Epoch 840, Loss 0.262984\n",
      "\t Params:  tensor([0.8414, 0.5974])\n",
      "\t Grad:  tensor([ 0.0002, -0.0015])\n",
      "Epoch 841, Loss 0.262984\n",
      "\t Params:  tensor([0.8414, 0.5974])\n",
      "\t Grad:  tensor([ 0.0002, -0.0015])\n",
      "Epoch 842, Loss 0.262984\n",
      "\t Params:  tensor([0.8413, 0.5974])\n",
      "\t Grad:  tensor([ 0.0002, -0.0015])\n",
      "Epoch 843, Loss 0.262984\n",
      "\t Params:  tensor([0.8413, 0.5975])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Grad:  tensor([ 0.0002, -0.0015])\n",
      "Epoch 844, Loss 0.262984\n",
      "\t Params:  tensor([0.8413, 0.5975])\n",
      "\t Grad:  tensor([ 0.0002, -0.0015])\n",
      "Epoch 845, Loss 0.262984\n",
      "\t Params:  tensor([0.8413, 0.5975])\n",
      "\t Grad:  tensor([ 0.0002, -0.0015])\n",
      "Epoch 846, Loss 0.262984\n",
      "\t Params:  tensor([0.8413, 0.5975])\n",
      "\t Grad:  tensor([ 0.0002, -0.0015])\n",
      "Epoch 847, Loss 0.262984\n",
      "\t Params:  tensor([0.8413, 0.5975])\n",
      "\t Grad:  tensor([ 0.0002, -0.0015])\n",
      "Epoch 848, Loss 0.262984\n",
      "\t Params:  tensor([0.8413, 0.5975])\n",
      "\t Grad:  tensor([ 0.0002, -0.0015])\n",
      "Epoch 849, Loss 0.262984\n",
      "\t Params:  tensor([0.8413, 0.5976])\n",
      "\t Grad:  tensor([ 0.0002, -0.0014])\n",
      "Epoch 850, Loss 0.262984\n",
      "\t Params:  tensor([0.8413, 0.5976])\n",
      "\t Grad:  tensor([ 0.0002, -0.0014])\n",
      "Epoch 851, Loss 0.262984\n",
      "\t Params:  tensor([0.8413, 0.5976])\n",
      "\t Grad:  tensor([ 0.0002, -0.0014])\n",
      "Epoch 852, Loss 0.262984\n",
      "\t Params:  tensor([0.8413, 0.5976])\n",
      "\t Grad:  tensor([ 0.0002, -0.0014])\n",
      "Epoch 853, Loss 0.262984\n",
      "\t Params:  tensor([0.8413, 0.5976])\n",
      "\t Grad:  tensor([ 0.0002, -0.0014])\n",
      "Epoch 854, Loss 0.262984\n",
      "\t Params:  tensor([0.8413, 0.5976])\n",
      "\t Grad:  tensor([ 0.0002, -0.0014])\n",
      "Epoch 855, Loss 0.262984\n",
      "\t Params:  tensor([0.8413, 0.5976])\n",
      "\t Grad:  tensor([ 0.0002, -0.0014])\n",
      "Epoch 856, Loss 0.262984\n",
      "\t Params:  tensor([0.8413, 0.5977])\n",
      "\t Grad:  tensor([ 0.0002, -0.0014])\n",
      "Epoch 857, Loss 0.262984\n",
      "\t Params:  tensor([0.8413, 0.5977])\n",
      "\t Grad:  tensor([ 0.0002, -0.0014])\n",
      "Epoch 858, Loss 0.262984\n",
      "\t Params:  tensor([0.8413, 0.5977])\n",
      "\t Grad:  tensor([ 0.0002, -0.0014])\n",
      "Epoch 859, Loss 0.262984\n",
      "\t Params:  tensor([0.8413, 0.5977])\n",
      "\t Grad:  tensor([ 0.0002, -0.0013])\n",
      "Epoch 860, Loss 0.262984\n",
      "\t Params:  tensor([0.8413, 0.5977])\n",
      "\t Grad:  tensor([ 0.0002, -0.0013])\n",
      "Epoch 861, Loss 0.262984\n",
      "\t Params:  tensor([0.8413, 0.5977])\n",
      "\t Grad:  tensor([ 0.0002, -0.0013])\n",
      "Epoch 862, Loss 0.262984\n",
      "\t Params:  tensor([0.8413, 0.5977])\n",
      "\t Grad:  tensor([ 0.0002, -0.0013])\n",
      "Epoch 863, Loss 0.262984\n",
      "\t Params:  tensor([0.8413, 0.5977])\n",
      "\t Grad:  tensor([ 0.0002, -0.0013])\n",
      "Epoch 864, Loss 0.262984\n",
      "\t Params:  tensor([0.8413, 0.5978])\n",
      "\t Grad:  tensor([ 0.0002, -0.0013])\n",
      "Epoch 865, Loss 0.262984\n",
      "\t Params:  tensor([0.8413, 0.5978])\n",
      "\t Grad:  tensor([ 0.0002, -0.0013])\n",
      "Epoch 866, Loss 0.262984\n",
      "\t Params:  tensor([0.8413, 0.5978])\n",
      "\t Grad:  tensor([ 0.0002, -0.0013])\n",
      "Epoch 867, Loss 0.262984\n",
      "\t Params:  tensor([0.8413, 0.5978])\n",
      "\t Grad:  tensor([ 0.0002, -0.0013])\n",
      "Epoch 868, Loss 0.262984\n",
      "\t Params:  tensor([0.8413, 0.5978])\n",
      "\t Grad:  tensor([ 0.0002, -0.0013])\n",
      "Epoch 869, Loss 0.262984\n",
      "\t Params:  tensor([0.8413, 0.5978])\n",
      "\t Grad:  tensor([ 0.0002, -0.0013])\n",
      "Epoch 870, Loss 0.262984\n",
      "\t Params:  tensor([0.8413, 0.5978])\n",
      "\t Grad:  tensor([ 0.0002, -0.0013])\n",
      "Epoch 871, Loss 0.262984\n",
      "\t Params:  tensor([0.8413, 0.5978])\n",
      "\t Grad:  tensor([ 0.0002, -0.0012])\n",
      "Epoch 872, Loss 0.262983\n",
      "\t Params:  tensor([0.8413, 0.5979])\n",
      "\t Grad:  tensor([ 0.0002, -0.0012])\n",
      "Epoch 873, Loss 0.262984\n",
      "\t Params:  tensor([0.8413, 0.5979])\n",
      "\t Grad:  tensor([ 0.0002, -0.0012])\n",
      "Epoch 874, Loss 0.262984\n",
      "\t Params:  tensor([0.8413, 0.5979])\n",
      "\t Grad:  tensor([ 0.0002, -0.0012])\n",
      "Epoch 875, Loss 0.262984\n",
      "\t Params:  tensor([0.8413, 0.5979])\n",
      "\t Grad:  tensor([ 0.0002, -0.0012])\n",
      "Epoch 876, Loss 0.262984\n",
      "\t Params:  tensor([0.8413, 0.5979])\n",
      "\t Grad:  tensor([ 0.0002, -0.0012])\n",
      "Epoch 877, Loss 0.262983\n",
      "\t Params:  tensor([0.8413, 0.5979])\n",
      "\t Grad:  tensor([ 0.0002, -0.0012])\n",
      "Epoch 878, Loss 0.262983\n",
      "\t Params:  tensor([0.8413, 0.5979])\n",
      "\t Grad:  tensor([ 0.0002, -0.0012])\n",
      "Epoch 879, Loss 0.262983\n",
      "\t Params:  tensor([0.8413, 0.5979])\n",
      "\t Grad:  tensor([ 0.0002, -0.0012])\n",
      "Epoch 880, Loss 0.262983\n",
      "\t Params:  tensor([0.8413, 0.5980])\n",
      "\t Grad:  tensor([ 0.0002, -0.0012])\n",
      "Epoch 881, Loss 0.262984\n",
      "\t Params:  tensor([0.8413, 0.5980])\n",
      "\t Grad:  tensor([ 0.0002, -0.0012])\n",
      "Epoch 882, Loss 0.262983\n",
      "\t Params:  tensor([0.8413, 0.5980])\n",
      "\t Grad:  tensor([ 0.0002, -0.0012])\n",
      "Epoch 883, Loss 0.262983\n",
      "\t Params:  tensor([0.8413, 0.5980])\n",
      "\t Grad:  tensor([ 0.0002, -0.0011])\n",
      "Epoch 884, Loss 0.262983\n",
      "\t Params:  tensor([0.8413, 0.5980])\n",
      "\t Grad:  tensor([ 0.0002, -0.0011])\n",
      "Epoch 885, Loss 0.262983\n",
      "\t Params:  tensor([0.8413, 0.5980])\n",
      "\t Grad:  tensor([ 0.0001, -0.0011])\n",
      "Epoch 886, Loss 0.262983\n",
      "\t Params:  tensor([0.8413, 0.5980])\n",
      "\t Grad:  tensor([ 0.0002, -0.0011])\n",
      "Epoch 887, Loss 0.262983\n",
      "\t Params:  tensor([0.8413, 0.5980])\n",
      "\t Grad:  tensor([ 0.0001, -0.0011])\n",
      "Epoch 888, Loss 0.262983\n",
      "\t Params:  tensor([0.8413, 0.5980])\n",
      "\t Grad:  tensor([ 0.0001, -0.0011])\n",
      "Epoch 889, Loss 0.262983\n",
      "\t Params:  tensor([0.8413, 0.5981])\n",
      "\t Grad:  tensor([ 0.0002, -0.0011])\n",
      "Epoch 890, Loss 0.262984\n",
      "\t Params:  tensor([0.8413, 0.5981])\n",
      "\t Grad:  tensor([ 0.0001, -0.0011])\n",
      "Epoch 891, Loss 0.262983\n",
      "\t Params:  tensor([0.8413, 0.5981])\n",
      "\t Grad:  tensor([ 0.0001, -0.0011])\n",
      "Epoch 892, Loss 0.262983\n",
      "\t Params:  tensor([0.8413, 0.5981])\n",
      "\t Grad:  tensor([ 0.0001, -0.0011])\n",
      "Epoch 893, Loss 0.262983\n",
      "\t Params:  tensor([0.8413, 0.5981])\n",
      "\t Grad:  tensor([ 0.0001, -0.0011])\n",
      "Epoch 894, Loss 0.262983\n",
      "\t Params:  tensor([0.8413, 0.5981])\n",
      "\t Grad:  tensor([ 0.0001, -0.0011])\n",
      "Epoch 895, Loss 0.262983\n",
      "\t Params:  tensor([0.8413, 0.5981])\n",
      "\t Grad:  tensor([ 0.0001, -0.0011])\n",
      "Epoch 896, Loss 0.262983\n",
      "\t Params:  tensor([0.8413, 0.5981])\n",
      "\t Grad:  tensor([ 0.0001, -0.0011])\n",
      "Epoch 897, Loss 0.262983\n",
      "\t Params:  tensor([0.8413, 0.5981])\n",
      "\t Grad:  tensor([ 0.0001, -0.0010])\n",
      "Epoch 898, Loss 0.262983\n",
      "\t Params:  tensor([0.8413, 0.5982])\n",
      "\t Grad:  tensor([ 0.0001, -0.0010])\n",
      "Epoch 899, Loss 0.262983\n",
      "\t Params:  tensor([0.8413, 0.5982])\n",
      "\t Grad:  tensor([ 0.0001, -0.0010])\n",
      "Epoch 900, Loss 0.262983\n",
      "\t Params:  tensor([0.8413, 0.5982])\n",
      "\t Grad:  tensor([ 0.0001, -0.0010])\n",
      "Epoch 901, Loss 0.262983\n",
      "\t Params:  tensor([0.8413, 0.5982])\n",
      "\t Grad:  tensor([ 0.0001, -0.0010])\n",
      "Epoch 902, Loss 0.262983\n",
      "\t Params:  tensor([0.8412, 0.5982])\n",
      "\t Grad:  tensor([ 0.0001, -0.0010])\n",
      "Epoch 903, Loss 0.262983\n",
      "\t Params:  tensor([0.8412, 0.5982])\n",
      "\t Grad:  tensor([ 0.0001, -0.0010])\n",
      "Epoch 904, Loss 0.262983\n",
      "\t Params:  tensor([0.8412, 0.5982])\n",
      "\t Grad:  tensor([ 0.0001, -0.0010])\n",
      "Epoch 905, Loss 0.262983\n",
      "\t Params:  tensor([0.8412, 0.5982])\n",
      "\t Grad:  tensor([ 0.0001, -0.0010])\n",
      "Epoch 906, Loss 0.262983\n",
      "\t Params:  tensor([0.8412, 0.5982])\n",
      "\t Grad:  tensor([ 0.0001, -0.0010])\n",
      "Epoch 907, Loss 0.262983\n",
      "\t Params:  tensor([0.8412, 0.5982])\n",
      "\t Grad:  tensor([ 0.0001, -0.0010])\n",
      "Epoch 908, Loss 0.262983\n",
      "\t Params:  tensor([0.8412, 0.5983])\n",
      "\t Grad:  tensor([ 0.0001, -0.0010])\n",
      "Epoch 909, Loss 0.262983\n",
      "\t Params:  tensor([0.8412, 0.5983])\n",
      "\t Grad:  tensor([ 0.0001, -0.0010])\n",
      "Epoch 910, Loss 0.262983\n",
      "\t Params:  tensor([0.8412, 0.5983])\n",
      "\t Grad:  tensor([ 0.0001, -0.0010])\n",
      "Epoch 911, Loss 0.262983\n",
      "\t Params:  tensor([0.8412, 0.5983])\n",
      "\t Grad:  tensor([ 0.0001, -0.0010])\n",
      "Epoch 912, Loss 0.262983\n",
      "\t Params:  tensor([0.8412, 0.5983])\n",
      "\t Grad:  tensor([ 0.0001, -0.0009])\n",
      "Epoch 913, Loss 0.262983\n",
      "\t Params:  tensor([0.8412, 0.5983])\n",
      "\t Grad:  tensor([ 0.0001, -0.0009])\n",
      "Epoch 914, Loss 0.262983\n",
      "\t Params:  tensor([0.8412, 0.5983])\n",
      "\t Grad:  tensor([ 0.0001, -0.0009])\n",
      "Epoch 915, Loss 0.262983\n",
      "\t Params:  tensor([0.8412, 0.5983])\n",
      "\t Grad:  tensor([ 0.0001, -0.0009])\n",
      "Epoch 916, Loss 0.262983\n",
      "\t Params:  tensor([0.8412, 0.5983])\n",
      "\t Grad:  tensor([ 0.0001, -0.0009])\n",
      "Epoch 917, Loss 0.262983\n",
      "\t Params:  tensor([0.8412, 0.5983])\n",
      "\t Grad:  tensor([ 0.0001, -0.0009])\n",
      "Epoch 918, Loss 0.262983\n",
      "\t Params:  tensor([0.8412, 0.5983])\n",
      "\t Grad:  tensor([ 0.0001, -0.0009])\n",
      "Epoch 919, Loss 0.262983\n",
      "\t Params:  tensor([0.8412, 0.5984])\n",
      "\t Grad:  tensor([ 0.0001, -0.0009])\n",
      "Epoch 920, Loss 0.262983\n",
      "\t Params:  tensor([0.8412, 0.5984])\n",
      "\t Grad:  tensor([ 0.0001, -0.0009])\n",
      "Epoch 921, Loss 0.262983\n",
      "\t Params:  tensor([0.8412, 0.5984])\n",
      "\t Grad:  tensor([ 0.0001, -0.0009])\n",
      "Epoch 922, Loss 0.262983\n",
      "\t Params:  tensor([0.8412, 0.5984])\n",
      "\t Grad:  tensor([ 0.0001, -0.0009])\n",
      "Epoch 923, Loss 0.262983\n",
      "\t Params:  tensor([0.8412, 0.5984])\n",
      "\t Grad:  tensor([ 0.0001, -0.0009])\n",
      "Epoch 924, Loss 0.262983\n",
      "\t Params:  tensor([0.8412, 0.5984])\n",
      "\t Grad:  tensor([ 0.0001, -0.0009])\n",
      "Epoch 925, Loss 0.262983\n",
      "\t Params:  tensor([0.8412, 0.5984])\n",
      "\t Grad:  tensor([ 0.0001, -0.0009])\n",
      "Epoch 926, Loss 0.262983\n",
      "\t Params:  tensor([0.8412, 0.5984])\n",
      "\t Grad:  tensor([ 0.0001, -0.0009])\n",
      "Epoch 927, Loss 0.262983\n",
      "\t Params:  tensor([0.8412, 0.5984])\n",
      "\t Grad:  tensor([ 0.0001, -0.0009])\n",
      "Epoch 928, Loss 0.262983\n",
      "\t Params:  tensor([0.8412, 0.5984])\n",
      "\t Grad:  tensor([ 0.0001, -0.0009])\n",
      "Epoch 929, Loss 0.262983\n",
      "\t Params:  tensor([0.8412, 0.5984])\n",
      "\t Grad:  tensor([ 0.0001, -0.0008])\n",
      "Epoch 930, Loss 0.262983\n",
      "\t Params:  tensor([0.8412, 0.5985])\n",
      "\t Grad:  tensor([ 0.0001, -0.0008])\n",
      "Epoch 931, Loss 0.262983\n",
      "\t Params:  tensor([0.8412, 0.5985])\n",
      "\t Grad:  tensor([ 0.0001, -0.0008])\n",
      "Epoch 932, Loss 0.262983\n",
      "\t Params:  tensor([0.8412, 0.5985])\n",
      "\t Grad:  tensor([ 0.0001, -0.0008])\n",
      "Epoch 933, Loss 0.262983\n",
      "\t Params:  tensor([0.8412, 0.5985])\n",
      "\t Grad:  tensor([ 0.0001, -0.0008])\n",
      "Epoch 934, Loss 0.262983\n",
      "\t Params:  tensor([0.8412, 0.5985])\n",
      "\t Grad:  tensor([ 0.0001, -0.0008])\n",
      "Epoch 935, Loss 0.262983\n",
      "\t Params:  tensor([0.8412, 0.5985])\n",
      "\t Grad:  tensor([ 0.0001, -0.0008])\n",
      "Epoch 936, Loss 0.262983\n",
      "\t Params:  tensor([0.8412, 0.5985])\n",
      "\t Grad:  tensor([ 0.0001, -0.0008])\n",
      "Epoch 937, Loss 0.262983\n",
      "\t Params:  tensor([0.8412, 0.5985])\n",
      "\t Grad:  tensor([ 0.0001, -0.0008])\n",
      "Epoch 938, Loss 0.262983\n",
      "\t Params:  tensor([0.8412, 0.5985])\n",
      "\t Grad:  tensor([ 0.0001, -0.0008])\n",
      "Epoch 939, Loss 0.262983\n",
      "\t Params:  tensor([0.8412, 0.5985])\n",
      "\t Grad:  tensor([ 0.0001, -0.0008])\n",
      "Epoch 940, Loss 0.262983\n",
      "\t Params:  tensor([0.8412, 0.5985])\n",
      "\t Grad:  tensor([ 0.0001, -0.0008])\n",
      "Epoch 941, Loss 0.262983\n",
      "\t Params:  tensor([0.8412, 0.5985])\n",
      "\t Grad:  tensor([ 0.0001, -0.0008])\n",
      "Epoch 942, Loss 0.262983\n",
      "\t Params:  tensor([0.8412, 0.5985])\n",
      "\t Grad:  tensor([ 0.0001, -0.0008])\n",
      "Epoch 943, Loss 0.262983\n",
      "\t Params:  tensor([0.8412, 0.5986])\n",
      "\t Grad:  tensor([ 0.0001, -0.0008])\n",
      "Epoch 944, Loss 0.262983\n",
      "\t Params:  tensor([0.8412, 0.5986])\n",
      "\t Grad:  tensor([ 0.0001, -0.0008])\n",
      "Epoch 945, Loss 0.262983\n",
      "\t Params:  tensor([0.8412, 0.5986])\n",
      "\t Grad:  tensor([ 0.0001, -0.0008])\n",
      "Epoch 946, Loss 0.262983\n",
      "\t Params:  tensor([0.8412, 0.5986])\n",
      "\t Grad:  tensor([ 0.0001, -0.0008])\n",
      "Epoch 947, Loss 0.262983\n",
      "\t Params:  tensor([0.8412, 0.5986])\n",
      "\t Grad:  tensor([ 0.0001, -0.0008])\n",
      "Epoch 948, Loss 0.262983\n",
      "\t Params:  tensor([0.8412, 0.5986])\n",
      "\t Grad:  tensor([ 0.0001, -0.0007])\n",
      "Epoch 949, Loss 0.262983\n",
      "\t Params:  tensor([0.8412, 0.5986])\n",
      "\t Grad:  tensor([ 9.8348e-05, -7.4108e-04])\n",
      "Epoch 950, Loss 0.262983\n",
      "\t Params:  tensor([0.8412, 0.5986])\n",
      "\t Grad:  tensor([ 9.5844e-05, -7.3629e-04])\n",
      "Epoch 951, Loss 0.262983\n",
      "\t Params:  tensor([0.8412, 0.5986])\n",
      "\t Grad:  tensor([ 9.5069e-05, -7.3139e-04])\n",
      "Epoch 952, Loss 0.262983\n",
      "\t Params:  tensor([0.8412, 0.5986])\n",
      "\t Grad:  tensor([ 0.0001, -0.0007])\n",
      "Epoch 953, Loss 0.262983\n",
      "\t Params:  tensor([0.8412, 0.5986])\n",
      "\t Grad:  tensor([ 9.2506e-05, -7.2181e-04])\n",
      "Epoch 954, Loss 0.262983\n",
      "\t Params:  tensor([0.8412, 0.5986])\n",
      "\t Grad:  tensor([ 9.6619e-05, -7.1650e-04])\n",
      "Epoch 955, Loss 0.262983\n",
      "\t Params:  tensor([0.8412, 0.5986])\n",
      "\t Grad:  tensor([ 9.3102e-05, -7.1204e-04])\n",
      "Epoch 956, Loss 0.262983\n",
      "\t Params:  tensor([0.8412, 0.5987])\n",
      "\t Grad:  tensor([ 9.5487e-05, -7.0708e-04])\n",
      "Epoch 957, Loss 0.262983\n",
      "\t Params:  tensor([0.8412, 0.5987])\n",
      "\t Grad:  tensor([ 9.1553e-05, -7.0278e-04])\n",
      "Epoch 958, Loss 0.262983\n",
      "\t Params:  tensor([0.8412, 0.5987])\n",
      "\t Grad:  tensor([ 9.5665e-05, -6.9757e-04])\n",
      "Epoch 959, Loss 0.262983\n",
      "\t Params:  tensor([0.8412, 0.5987])\n",
      "\t Grad:  tensor([ 9.2626e-05, -6.9327e-04])\n",
      "Epoch 960, Loss 0.262983\n",
      "\t Params:  tensor([0.8412, 0.5987])\n",
      "\t Grad:  tensor([ 9.0599e-05, -6.8879e-04])\n",
      "Epoch 961, Loss 0.262983\n",
      "\t Params:  tensor([0.8412, 0.5987])\n",
      "\t Grad:  tensor([ 9.1970e-05, -6.8394e-04])\n",
      "Epoch 962, Loss 0.262983\n",
      "\t Params:  tensor([0.8412, 0.5987])\n",
      "\t Grad:  tensor([ 9.2924e-05, -6.7926e-04])\n",
      "Epoch 963, Loss 0.262983\n",
      "\t Params:  tensor([0.8412, 0.5987])\n",
      "\t Grad:  tensor([ 8.9884e-05, -6.7502e-04])\n",
      "Epoch 964, Loss 0.262983\n",
      "\t Params:  tensor([0.8412, 0.5987])\n",
      "\t Grad:  tensor([ 8.9347e-05, -6.7058e-04])\n",
      "Epoch 965, Loss 0.262983\n",
      "\t Params:  tensor([0.8412, 0.5987])\n",
      "\t Grad:  tensor([ 8.8751e-05, -6.6607e-04])\n",
      "Epoch 966, Loss 0.262983\n",
      "\t Params:  tensor([0.8412, 0.5987])\n",
      "\t Grad:  tensor([ 8.7261e-05, -6.6175e-04])\n",
      "Epoch 967, Loss 0.262983\n",
      "\t Params:  tensor([0.8412, 0.5987])\n",
      "\t Grad:  tensor([ 8.7917e-05, -6.5726e-04])\n",
      "Epoch 968, Loss 0.262983\n",
      "\t Params:  tensor([0.8412, 0.5987])\n",
      "\t Grad:  tensor([ 8.6784e-05, -6.5302e-04])\n",
      "Epoch 969, Loss 0.262983\n",
      "\t Params:  tensor([0.8412, 0.5987])\n",
      "\t Grad:  tensor([ 8.3506e-05, -6.4907e-04])\n",
      "Epoch 970, Loss 0.262983\n",
      "\t Params:  tensor([0.8412, 0.5987])\n",
      "\t Grad:  tensor([ 8.3148e-05, -6.4461e-04])\n",
      "Epoch 971, Loss 0.262983\n",
      "\t Params:  tensor([0.8412, 0.5988])\n",
      "\t Grad:  tensor([ 8.5950e-05, -6.3997e-04])\n",
      "Epoch 972, Loss 0.262983\n",
      "\t Params:  tensor([0.8412, 0.5988])\n",
      "\t Grad:  tensor([ 8.8453e-05, -6.3530e-04])\n",
      "Epoch 973, Loss 0.262983\n",
      "\t Params:  tensor([0.8412, 0.5988])\n",
      "\t Grad:  tensor([ 8.5056e-05, -6.3155e-04])\n",
      "Epoch 974, Loss 0.262983\n",
      "\t Params:  tensor([0.8412, 0.5988])\n",
      "\t Grad:  tensor([ 8.1778e-05, -6.2758e-04])\n",
      "Epoch 975, Loss 0.262983\n",
      "\t Params:  tensor([0.8412, 0.5988])\n",
      "\t Grad:  tensor([ 8.4221e-05, -6.2314e-04])\n",
      "Epoch 976, Loss 0.262983\n",
      "\t Params:  tensor([0.8412, 0.5988])\n",
      "\t Grad:  tensor([ 8.2850e-05, -6.1906e-04])\n",
      "Epoch 977, Loss 0.262983\n",
      "\t Params:  tensor([0.8412, 0.5988])\n",
      "\t Grad:  tensor([ 8.3148e-05, -6.1484e-04])\n",
      "Epoch 978, Loss 0.262983\n",
      "\t Params:  tensor([0.8412, 0.5988])\n",
      "\t Grad:  tensor([ 7.9036e-05, -6.1122e-04])\n",
      "Epoch 979, Loss 0.262983\n",
      "\t Params:  tensor([0.8412, 0.5988])\n",
      "\t Grad:  tensor([ 8.2493e-05, -6.0672e-04])\n",
      "Epoch 980, Loss 0.262983\n",
      "\t Params:  tensor([0.8412, 0.5988])\n",
      "\t Grad:  tensor([ 7.8857e-05, -6.0302e-04])\n",
      "Epoch 981, Loss 0.262983\n",
      "\t Params:  tensor([0.8412, 0.5988])\n",
      "\t Grad:  tensor([ 8.3148e-05, -5.9841e-04])\n",
      "Epoch 982, Loss 0.262983\n",
      "\t Params:  tensor([0.8412, 0.5988])\n",
      "\t Grad:  tensor([ 7.9393e-05, -5.9495e-04])\n",
      "Epoch 983, Loss 0.262983\n",
      "\t Params:  tensor([0.8412, 0.5988])\n",
      "\t Grad:  tensor([ 8.1897e-05, -5.9059e-04])\n",
      "Epoch 984, Loss 0.262983\n",
      "\t Params:  tensor([0.8412, 0.5988])\n",
      "\t Grad:  tensor([ 7.4744e-05, -5.8749e-04])\n",
      "Epoch 985, Loss 0.262983\n",
      "\t Params:  tensor([0.8412, 0.5988])\n",
      "\t Grad:  tensor([ 7.6354e-05, -5.8331e-04])\n",
      "Epoch 986, Loss 0.262983\n",
      "\t Params:  tensor([0.8412, 0.5988])\n",
      "\t Grad:  tensor([ 7.5579e-05, -5.7938e-04])\n",
      "Epoch 987, Loss 0.262983\n",
      "\t Params:  tensor([0.8412, 0.5988])\n",
      "\t Grad:  tensor([ 7.4983e-05, -5.7555e-04])\n",
      "Epoch 988, Loss 0.262983\n",
      "\t Params:  tensor([0.8412, 0.5989])\n",
      "\t Grad:  tensor([ 7.8440e-05, -5.7139e-04])\n",
      "Epoch 989, Loss 0.262983\n",
      "\t Params:  tensor([0.8412, 0.5989])\n",
      "\t Grad:  tensor([ 7.4208e-05, -5.6801e-04])\n",
      "Epoch 990, Loss 0.262983\n",
      "\t Params:  tensor([0.8412, 0.5989])\n",
      "\t Grad:  tensor([ 7.5877e-05, -5.6387e-04])\n",
      "Epoch 991, Loss 0.262983\n",
      "\t Params:  tensor([0.8412, 0.5989])\n",
      "\t Grad:  tensor([ 7.4685e-05, -5.6029e-04])\n",
      "Epoch 992, Loss 0.262983\n",
      "\t Params:  tensor([0.8412, 0.5989])\n",
      "\t Grad:  tensor([ 7.0751e-05, -5.5705e-04])\n",
      "Epoch 993, Loss 0.262983\n",
      "\t Params:  tensor([0.8412, 0.5989])\n",
      "\t Grad:  tensor([ 7.3373e-05, -5.5298e-04])\n",
      "Epoch 994, Loss 0.262983\n",
      "\t Params:  tensor([0.8412, 0.5989])\n",
      "\t Grad:  tensor([ 7.7307e-05, -5.4870e-04])\n",
      "Epoch 995, Loss 0.262983\n",
      "\t Params:  tensor([0.8412, 0.5989])\n",
      "\t Grad:  tensor([ 7.2122e-05, -5.4572e-04])\n",
      "Epoch 996, Loss 0.262983\n",
      "\t Params:  tensor([0.8412, 0.5989])\n",
      "\t Grad:  tensor([ 7.1526e-05, -5.4202e-04])\n",
      "Epoch 997, Loss 0.262983\n",
      "\t Params:  tensor([0.8412, 0.5989])\n",
      "\t Grad:  tensor([ 7.2956e-05, -5.3823e-04])\n",
      "Epoch 998, Loss 0.262983\n",
      "\t Params:  tensor([0.8412, 0.5989])\n",
      "\t Grad:  tensor([ 7.3016e-05, -5.3463e-04])\n",
      "Epoch 999, Loss 0.262983\n",
      "\t Params:  tensor([0.8412, 0.5989])\n",
      "\t Grad:  tensor([ 7.2479e-05, -5.3101e-04])\n",
      "Epoch 1000, Loss 0.262983\n",
      "\t Params:  tensor([0.8412, 0.5989])\n",
      "\t Grad:  tensor([ 7.3373e-05, -5.2740e-04])\n",
      "Epoch 1001, Loss 0.262983\n",
      "\t Params:  tensor([0.8412, 0.5989])\n",
      "\t Grad:  tensor([ 7.0810e-05, -5.2416e-04])\n",
      "Epoch 1002, Loss 0.262983\n",
      "\t Params:  tensor([0.8412, 0.5989])\n",
      "\t Grad:  tensor([ 7.0989e-05, -5.2063e-04])\n",
      "Epoch 1003, Loss 0.262983\n",
      "\t Params:  tensor([0.8411, 0.5989])\n",
      "\t Grad:  tensor([ 6.7949e-05, -5.1751e-04])\n",
      "Epoch 1004, Loss 0.262983\n",
      "\t Params:  tensor([0.8411, 0.5989])\n",
      "\t Grad:  tensor([ 6.9320e-05, -5.1384e-04])\n",
      "Epoch 1005, Loss 0.262983\n",
      "\t Params:  tensor([0.8411, 0.5989])\n",
      "\t Grad:  tensor([ 6.4433e-05, -5.1095e-04])\n",
      "Epoch 1006, Loss 0.262983\n",
      "\t Params:  tensor([0.8411, 0.5990])\n",
      "\t Grad:  tensor([ 6.6042e-05, -5.0728e-04])\n",
      "Epoch 1007, Loss 0.262983\n",
      "\t Params:  tensor([0.8411, 0.5990])\n",
      "\t Grad:  tensor([ 7.2122e-05, -5.0320e-04])\n",
      "Epoch 1008, Loss 0.262983\n",
      "\t Params:  tensor([0.8411, 0.5990])\n",
      "\t Grad:  tensor([ 6.6400e-05, -5.0062e-04])\n",
      "Epoch 1009, Loss 0.262983\n",
      "\t Params:  tensor([0.8411, 0.5990])\n",
      "\t Grad:  tensor([ 6.7294e-05, -4.9700e-04])\n",
      "Epoch 1010, Loss 0.262983\n",
      "\t Params:  tensor([0.8411, 0.5990])\n",
      "\t Grad:  tensor([ 6.4671e-05, -4.9403e-04])\n",
      "Epoch 1011, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5990])\n",
      "\t Grad:  tensor([ 6.6757e-05, -4.9046e-04])\n",
      "Epoch 1012, Loss 0.262983\n",
      "\t Params:  tensor([0.8411, 0.5990])\n",
      "\t Grad:  tensor([ 6.7174e-05, -4.8719e-04])\n",
      "Epoch 1013, Loss 0.262983\n",
      "\t Params:  tensor([0.8411, 0.5990])\n",
      "\t Grad:  tensor([ 6.5565e-05, -4.8397e-04])\n",
      "Epoch 1014, Loss 0.262983\n",
      "\t Params:  tensor([0.8411, 0.5990])\n",
      "\t Grad:  tensor([ 6.3837e-05, -4.8098e-04])\n",
      "Epoch 1015, Loss 0.262983\n",
      "\t Params:  tensor([0.8411, 0.5990])\n",
      "\t Grad:  tensor([ 6.1870e-05, -4.7799e-04])\n",
      "Epoch 1016, Loss 0.262983\n",
      "\t Params:  tensor([0.8411, 0.5990])\n",
      "\t Grad:  tensor([ 6.4552e-05, -4.7440e-04])\n",
      "Epoch 1017, Loss 0.262983\n",
      "\t Params:  tensor([0.8411, 0.5990])\n",
      "\t Grad:  tensor([ 6.3598e-05, -4.7140e-04])\n",
      "Epoch 1018, Loss 0.262983\n",
      "\t Params:  tensor([0.8411, 0.5990])\n",
      "\t Grad:  tensor([ 5.9664e-05, -4.6859e-04])\n",
      "Epoch 1019, Loss 0.262983\n",
      "\t Params:  tensor([0.8411, 0.5990])\n",
      "\t Grad:  tensor([ 6.3419e-05, -4.6500e-04])\n",
      "Epoch 1020, Loss 0.262983\n",
      "\t Params:  tensor([0.8411, 0.5990])\n",
      "\t Grad:  tensor([ 6.3717e-05, -4.6188e-04])\n",
      "Epoch 1021, Loss 0.262983\n",
      "\t Params:  tensor([0.8411, 0.5990])\n",
      "\t Grad:  tensor([ 5.6505e-05, -4.5969e-04])\n",
      "Epoch 1022, Loss 0.262983\n",
      "\t Params:  tensor([0.8411, 0.5990])\n",
      "\t Grad:  tensor([ 6.0976e-05, -4.5587e-04])\n",
      "Epoch 1023, Loss 0.262983\n",
      "\t Params:  tensor([0.8411, 0.5990])\n",
      "\t Grad:  tensor([ 6.3002e-05, -4.5268e-04])\n",
      "Epoch 1024, Loss 0.262983\n",
      "\t Params:  tensor([0.8411, 0.5990])\n",
      "\t Grad:  tensor([ 5.8055e-05, -4.5023e-04])\n",
      "Epoch 1025, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5990])\n",
      "\t Grad:  tensor([ 5.9783e-05, -4.4702e-04])\n",
      "Epoch 1026, Loss 0.262983\n",
      "\t Params:  tensor([0.8411, 0.5990])\n",
      "\t Grad:  tensor([ 5.7280e-05, -4.4425e-04])\n",
      "Epoch 1027, Loss 0.262983\n",
      "\t Params:  tensor([0.8411, 0.5991])\n",
      "\t Grad:  tensor([ 5.6028e-05, -4.4142e-04])\n",
      "Epoch 1028, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5991])\n",
      "\t Grad:  tensor([ 6.1691e-05, -4.3778e-04])\n",
      "Epoch 1029, Loss 0.262983\n",
      "\t Params:  tensor([0.8411, 0.5991])\n",
      "\t Grad:  tensor([ 6.0380e-05, -4.3499e-04])\n",
      "Epoch 1030, Loss 0.262983\n",
      "\t Params:  tensor([0.8411, 0.5991])\n",
      "\t Grad:  tensor([ 5.9903e-05, -4.3211e-04])\n",
      "Epoch 1031, Loss 0.262983\n",
      "\t Params:  tensor([0.8411, 0.5991])\n",
      "\t Grad:  tensor([ 5.6207e-05, -4.2967e-04])\n",
      "Epoch 1032, Loss 0.262983\n",
      "\t Params:  tensor([0.8411, 0.5991])\n",
      "\t Grad:  tensor([ 6.1333e-05, -4.2616e-04])\n",
      "Epoch 1033, Loss 0.262983\n",
      "\t Params:  tensor([0.8411, 0.5991])\n",
      "\t Grad:  tensor([ 5.6505e-05, -4.2392e-04])\n",
      "Epoch 1034, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5991])\n",
      "\t Grad:  tensor([ 5.8293e-05, -4.2088e-04])\n",
      "Epoch 1035, Loss 0.262983\n",
      "\t Params:  tensor([0.8411, 0.5991])\n",
      "\t Grad:  tensor([ 5.4002e-05, -4.1849e-04])\n",
      "Epoch 1036, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5991])\n",
      "\t Grad:  tensor([ 5.6624e-05, -4.1538e-04])\n",
      "Epoch 1037, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5991])\n",
      "\t Grad:  tensor([ 5.5850e-05, -4.1265e-04])\n",
      "Epoch 1038, Loss 0.262983\n",
      "\t Params:  tensor([0.8411, 0.5991])\n",
      "\t Grad:  tensor([ 5.2869e-05, -4.1027e-04])\n",
      "Epoch 1039, Loss 0.262983\n",
      "\t Params:  tensor([0.8411, 0.5991])\n",
      "\t Grad:  tensor([ 5.3585e-05, -4.0741e-04])\n",
      "Epoch 1040, Loss 0.262983\n",
      "\t Params:  tensor([0.8411, 0.5991])\n",
      "\t Grad:  tensor([ 5.7757e-05, -4.0422e-04])\n",
      "Epoch 1041, Loss 0.262983\n",
      "\t Params:  tensor([0.8411, 0.5991])\n",
      "\t Grad:  tensor([ 5.2094e-05, -4.0214e-04])\n",
      "Epoch 1042, Loss 0.262983\n",
      "\t Params:  tensor([0.8411, 0.5991])\n",
      "\t Grad:  tensor([ 4.8161e-05, -3.9992e-04])\n",
      "Epoch 1043, Loss 0.262983\n",
      "\t Params:  tensor([0.8411, 0.5991])\n",
      "\t Grad:  tensor([ 5.3108e-05, -3.9655e-04])\n",
      "Epoch 1044, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5991])\n",
      "\t Grad:  tensor([ 5.4419e-05, -3.9387e-04])\n",
      "Epoch 1045, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5991])\n",
      "\t Grad:  tensor([ 5.3644e-05, -3.9127e-04])\n",
      "Epoch 1046, Loss 0.262983\n",
      "\t Params:  tensor([0.8411, 0.5991])\n",
      "\t Grad:  tensor([ 5.2929e-05, -3.8866e-04])\n",
      "Epoch 1047, Loss 0.262983\n",
      "\t Params:  tensor([0.8411, 0.5991])\n",
      "\t Grad:  tensor([ 5.0366e-05, -3.8642e-04])\n",
      "Epoch 1048, Loss 0.262983\n",
      "\t Params:  tensor([0.8411, 0.5991])\n",
      "\t Grad:  tensor([ 5.3883e-05, -3.8331e-04])\n",
      "Epoch 1049, Loss 0.262983\n",
      "\t Params:  tensor([0.8411, 0.5991])\n",
      "\t Grad:  tensor([ 5.2094e-05, -3.8105e-04])\n",
      "Epoch 1050, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5991])\n",
      "\t Grad:  tensor([ 4.8518e-05, -3.7888e-04])\n",
      "Epoch 1051, Loss 0.262983\n",
      "\t Params:  tensor([0.8411, 0.5991])\n",
      "\t Grad:  tensor([ 5.0962e-05, -3.7596e-04])\n",
      "Epoch 1052, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5992])\n",
      "\t Grad:  tensor([ 4.6670e-05, -3.7396e-04])\n",
      "Epoch 1053, Loss 0.262983\n",
      "\t Params:  tensor([0.8411, 0.5992])\n",
      "\t Grad:  tensor([ 4.8995e-05, -3.7117e-04])\n",
      "Epoch 1054, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5992])\n",
      "\t Grad:  tensor([ 5.0902e-05, -3.6850e-04])\n",
      "Epoch 1055, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5992])\n",
      "\t Grad:  tensor([ 4.6849e-05, -3.6659e-04])\n",
      "Epoch 1056, Loss 0.262983\n",
      "\t Params:  tensor([0.8411, 0.5992])\n",
      "\t Grad:  tensor([ 4.4525e-05, -3.6414e-04])\n",
      "Epoch 1057, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5992])\n",
      "\t Grad:  tensor([ 5.0366e-05, -3.6102e-04])\n",
      "Epoch 1058, Loss 0.262983\n",
      "\t Params:  tensor([0.8411, 0.5992])\n",
      "\t Grad:  tensor([ 5.0426e-05, -3.5854e-04])\n",
      "Epoch 1059, Loss 0.262983\n",
      "\t Params:  tensor([0.8411, 0.5992])\n",
      "\t Grad:  tensor([ 5.3287e-05, -3.5588e-04])\n",
      "Epoch 1060, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5992])\n",
      "\t Grad:  tensor([ 4.8339e-05, -3.5410e-04])\n",
      "Epoch 1061, Loss 0.262983\n",
      "\t Params:  tensor([0.8411, 0.5992])\n",
      "\t Grad:  tensor([ 4.5419e-05, -3.5203e-04])\n",
      "Epoch 1062, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5992])\n",
      "\t Grad:  tensor([ 4.7088e-05, -3.4950e-04])\n",
      "Epoch 1063, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5992])\n",
      "\t Grad:  tensor([ 4.4525e-05, -3.4749e-04])\n",
      "Epoch 1064, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5992])\n",
      "\t Grad:  tensor([ 4.7624e-05, -3.4475e-04])\n",
      "Epoch 1065, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5992])\n",
      "\t Grad:  tensor([ 4.7445e-05, -3.4236e-04])\n",
      "Epoch 1066, Loss 0.262983\n",
      "\t Params:  tensor([0.8411, 0.5992])\n",
      "\t Grad:  tensor([ 4.3273e-05, -3.4058e-04])\n",
      "Epoch 1067, Loss 0.262983\n",
      "\t Params:  tensor([0.8411, 0.5992])\n",
      "\t Grad:  tensor([ 4.6670e-05, -3.3788e-04])\n",
      "Epoch 1068, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5992])\n",
      "\t Grad:  tensor([ 4.5657e-05, -3.3587e-04])\n",
      "Epoch 1069, Loss 0.262983\n",
      "\t Params:  tensor([0.8411, 0.5992])\n",
      "\t Grad:  tensor([ 4.2439e-05, -3.3396e-04])\n",
      "Epoch 1070, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5992])\n",
      "\t Grad:  tensor([ 4.5657e-05, -3.3134e-04])\n",
      "Epoch 1071, Loss 0.262983\n",
      "\t Params:  tensor([0.8411, 0.5992])\n",
      "\t Grad:  tensor([ 4.1723e-05, -3.2958e-04])\n",
      "Epoch 1072, Loss 0.262983\n",
      "\t Params:  tensor([0.8411, 0.5992])\n",
      "\t Grad:  tensor([ 3.9935e-05, -3.2748e-04])\n",
      "Epoch 1073, Loss 0.262983\n",
      "\t Params:  tensor([0.8411, 0.5992])\n",
      "\t Grad:  tensor([ 4.3869e-05, -3.2480e-04])\n",
      "Epoch 1074, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5992])\n",
      "\t Grad:  tensor([ 4.5061e-05, -3.2249e-04])\n",
      "Epoch 1075, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5992])\n",
      "\t Grad:  tensor([ 4.0710e-05, -3.2094e-04])\n",
      "Epoch 1076, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5992])\n",
      "\t Grad:  tensor([ 4.1962e-05, -3.1862e-04])\n",
      "Epoch 1077, Loss 0.262983\n",
      "\t Params:  tensor([0.8411, 0.5992])\n",
      "\t Grad:  tensor([ 4.2081e-05, -3.1637e-04])\n",
      "Epoch 1078, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5992])\n",
      "\t Grad:  tensor([ 4.2200e-05, -3.1429e-04])\n",
      "Epoch 1079, Loss 0.262983\n",
      "\t Params:  tensor([0.8411, 0.5992])\n",
      "\t Grad:  tensor([ 4.2200e-05, -3.1213e-04])\n",
      "Epoch 1080, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5992])\n",
      "\t Grad:  tensor([ 3.9995e-05, -3.1028e-04])\n",
      "Epoch 1081, Loss 0.262983\n",
      "\t Params:  tensor([0.8411, 0.5993])\n",
      "\t Grad:  tensor([ 4.0829e-05, -3.0813e-04])\n",
      "Epoch 1082, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5993])\n",
      "\t Grad:  tensor([ 4.1902e-05, -3.0598e-04])\n",
      "Epoch 1083, Loss 0.262983\n",
      "\t Params:  tensor([0.8411, 0.5993])\n",
      "\t Grad:  tensor([ 3.9697e-05, -3.0410e-04])\n",
      "Epoch 1084, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5993])\n",
      "\t Grad:  tensor([ 3.7372e-05, -3.0234e-04])\n",
      "Epoch 1085, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5993])\n",
      "\t Grad:  tensor([ 4.3035e-05, -2.9960e-04])\n",
      "Epoch 1086, Loss 0.262983\n",
      "\t Params:  tensor([0.8411, 0.5993])\n",
      "\t Grad:  tensor([ 3.8147e-05, -2.9817e-04])\n",
      "Epoch 1087, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5993])\n",
      "\t Grad:  tensor([ 4.4346e-05, -2.9551e-04])\n",
      "Epoch 1088, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5993])\n",
      "\t Grad:  tensor([ 4.1306e-05, -2.9387e-04])\n",
      "Epoch 1089, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5993])\n",
      "\t Grad:  tensor([ 3.9518e-05, -2.9211e-04])\n",
      "Epoch 1090, Loss 0.262983\n",
      "\t Params:  tensor([0.8411, 0.5993])\n",
      "\t Grad:  tensor([ 3.7909e-05, -2.9033e-04])\n",
      "Epoch 1091, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5993])\n",
      "\t Grad:  tensor([ 4.0412e-05, -2.8802e-04])\n",
      "Epoch 1092, Loss 0.262983\n",
      "\t Params:  tensor([0.8411, 0.5993])\n",
      "\t Grad:  tensor([ 3.7611e-05, -2.8646e-04])\n",
      "Epoch 1093, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5993])\n",
      "\t Grad:  tensor([ 3.7968e-05, -2.8452e-04])\n",
      "Epoch 1094, Loss 0.262983\n",
      "\t Params:  tensor([0.8411, 0.5993])\n",
      "\t Grad:  tensor([ 4.1306e-05, -2.8215e-04])\n",
      "Epoch 1095, Loss 0.262983\n",
      "\t Params:  tensor([0.8411, 0.5993])\n",
      "\t Grad:  tensor([ 3.7551e-05, -2.8076e-04])\n",
      "Epoch 1096, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5993])\n",
      "\t Grad:  tensor([ 3.8385e-05, -2.7880e-04])\n",
      "Epoch 1097, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5993])\n",
      "\t Grad:  tensor([ 3.8445e-05, -2.7688e-04])\n",
      "Epoch 1098, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5993])\n",
      "\t Grad:  tensor([ 3.8803e-05, -2.7496e-04])\n",
      "Epoch 1099, Loss 0.262983\n",
      "\t Params:  tensor([0.8411, 0.5993])\n",
      "\t Grad:  tensor([ 3.2604e-05, -2.7384e-04])\n",
      "Epoch 1100, Loss 0.262983\n",
      "\t Params:  tensor([0.8411, 0.5993])\n",
      "\t Grad:  tensor([ 3.8564e-05, -2.7121e-04])\n",
      "Epoch 1101, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5993])\n",
      "\t Grad:  tensor([ 3.9577e-05, -2.6929e-04])\n",
      "Epoch 1102, Loss 0.262983\n",
      "\t Params:  tensor([0.8411, 0.5993])\n",
      "\t Grad:  tensor([ 3.3617e-05, -2.6826e-04])\n",
      "Epoch 1103, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5993])\n",
      "\t Grad:  tensor([ 3.6180e-05, -2.6612e-04])\n",
      "Epoch 1104, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5993])\n",
      "\t Grad:  tensor([ 3.4630e-05, -2.6456e-04])\n",
      "Epoch 1105, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5993])\n",
      "\t Grad:  tensor([ 3.3438e-05, -2.6286e-04])\n",
      "Epoch 1106, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5993])\n",
      "\t Grad:  tensor([ 3.2008e-05, -2.6125e-04])\n",
      "Epoch 1107, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5993])\n",
      "\t Grad:  tensor([ 3.8087e-05, -2.5875e-04])\n",
      "Epoch 1108, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5993])\n",
      "\t Grad:  tensor([ 3.6001e-05, -2.5737e-04])\n",
      "Epoch 1109, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5993])\n",
      "\t Grad:  tensor([ 3.4332e-05, -2.5576e-04])\n",
      "Epoch 1110, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5993])\n",
      "\t Grad:  tensor([ 3.2365e-05, -2.5430e-04])\n",
      "Epoch 1111, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5993])\n",
      "\t Grad:  tensor([ 3.6895e-05, -2.5203e-04])\n",
      "Epoch 1112, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5993])\n",
      "\t Grad:  tensor([ 3.3796e-05, -2.5072e-04])\n",
      "Epoch 1113, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5993])\n",
      "\t Grad:  tensor([ 3.3140e-05, -2.4912e-04])\n",
      "Epoch 1114, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5993])\n",
      "\t Grad:  tensor([ 2.9683e-05, -2.4790e-04])\n",
      "Epoch 1115, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5993])\n",
      "\t Grad:  tensor([ 3.1710e-05, -2.4589e-04])\n",
      "Epoch 1116, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5993])\n",
      "\t Grad:  tensor([ 3.5107e-05, -2.4389e-04])\n",
      "Epoch 1117, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5993])\n",
      "\t Grad:  tensor([ 3.1471e-05, -2.4259e-04])\n",
      "Epoch 1118, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5994])\n",
      "\t Grad:  tensor([ 3.6120e-05, -2.4040e-04])\n",
      "Epoch 1119, Loss 0.262983\n",
      "\t Params:  tensor([0.8411, 0.5994])\n",
      "\t Grad:  tensor([ 3.0339e-05, -2.3950e-04])\n",
      "Epoch 1120, Loss 0.262983\n",
      "\t Params:  tensor([0.8411, 0.5994])\n",
      "\t Grad:  tensor([ 3.4034e-05, -2.3755e-04])\n",
      "Epoch 1121, Loss 0.262983\n",
      "\t Params:  tensor([0.8411, 0.5994])\n",
      "\t Grad:  tensor([ 3.0994e-05, -2.3628e-04])\n",
      "Epoch 1122, Loss 0.262983\n",
      "\t Params:  tensor([0.8411, 0.5994])\n",
      "\t Grad:  tensor([ 3.2187e-05, -2.3449e-04])\n",
      "Epoch 1123, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5994])\n",
      "\t Grad:  tensor([ 3.1173e-05, -2.3304e-04])\n",
      "Epoch 1124, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5994])\n",
      "\t Grad:  tensor([ 3.3796e-05, -2.3122e-04])\n",
      "Epoch 1125, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5994])\n",
      "\t Grad:  tensor([ 2.7537e-05, -2.3048e-04])\n",
      "Epoch 1126, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5994])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Grad:  tensor([ 2.8908e-05, -2.2869e-04])\n",
      "Epoch 1127, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5994])\n",
      "\t Grad:  tensor([ 2.9504e-05, -2.2706e-04])\n",
      "Epoch 1128, Loss 0.262983\n",
      "\t Params:  tensor([0.8411, 0.5994])\n",
      "\t Grad:  tensor([ 2.8968e-05, -2.2560e-04])\n",
      "Epoch 1129, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5994])\n",
      "\t Grad:  tensor([ 2.8610e-05, -2.2411e-04])\n",
      "Epoch 1130, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5994])\n",
      "\t Grad:  tensor([ 2.8849e-05, -2.2258e-04])\n",
      "Epoch 1131, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5994])\n",
      "\t Grad:  tensor([ 2.9147e-05, -2.2095e-04])\n",
      "Epoch 1132, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5994])\n",
      "\t Grad:  tensor([ 2.7716e-05, -2.1977e-04])\n",
      "Epoch 1133, Loss 0.262983\n",
      "\t Params:  tensor([0.8411, 0.5994])\n",
      "\t Grad:  tensor([ 2.8074e-05, -2.1816e-04])\n",
      "Epoch 1134, Loss 0.262983\n",
      "\t Params:  tensor([0.8411, 0.5994])\n",
      "\t Grad:  tensor([ 2.7537e-05, -2.1676e-04])\n",
      "Epoch 1135, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5994])\n",
      "\t Grad:  tensor([ 2.6822e-05, -2.1545e-04])\n",
      "Epoch 1136, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5994])\n",
      "\t Grad:  tensor([ 2.7418e-05, -2.1391e-04])\n",
      "Epoch 1137, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5994])\n",
      "\t Grad:  tensor([ 2.4378e-05, -2.1289e-04])\n",
      "Epoch 1138, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5994])\n",
      "\t Grad:  tensor([ 2.8670e-05, -2.1080e-04])\n",
      "Epoch 1139, Loss 0.262983\n",
      "\t Params:  tensor([0.8411, 0.5994])\n",
      "\t Grad:  tensor([ 2.5988e-05, -2.0976e-04])\n",
      "Epoch 1140, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5994])\n",
      "\t Grad:  tensor([ 2.9087e-05, -2.0798e-04])\n",
      "Epoch 1141, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5994])\n",
      "\t Grad:  tensor([ 2.7716e-05, -2.0675e-04])\n",
      "Epoch 1142, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5994])\n",
      "\t Grad:  tensor([ 2.5332e-05, -2.0571e-04])\n",
      "Epoch 1143, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5994])\n",
      "\t Grad:  tensor([ 2.7657e-05, -2.0391e-04])\n",
      "Epoch 1144, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5994])\n",
      "\t Grad:  tensor([ 2.6345e-05, -2.0271e-04])\n",
      "Epoch 1145, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5994])\n",
      "\t Grad:  tensor([ 3.1710e-05, -2.0079e-04])\n",
      "Epoch 1146, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5994])\n",
      "\t Grad:  tensor([ 2.8968e-05, -1.9976e-04])\n",
      "Epoch 1147, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5994])\n",
      "\t Grad:  tensor([ 2.5630e-05, -1.9879e-04])\n",
      "Epoch 1148, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5994])\n",
      "\t Grad:  tensor([ 2.8551e-05, -1.9708e-04])\n",
      "Epoch 1149, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5994])\n",
      "\t Grad:  tensor([ 2.3782e-05, -1.9628e-04])\n",
      "Epoch 1150, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5994])\n",
      "\t Grad:  tensor([ 2.6762e-05, -1.9462e-04])\n",
      "Epoch 1151, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5994])\n",
      "\t Grad:  tensor([ 2.8491e-05, -1.9310e-04])\n",
      "Epoch 1152, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5994])\n",
      "\t Grad:  tensor([ 2.5153e-05, -1.9228e-04])\n",
      "Epoch 1153, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5994])\n",
      "\t Grad:  tensor([ 2.4974e-05, -1.9094e-04])\n",
      "Epoch 1154, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5994])\n",
      "\t Grad:  tensor([ 2.5868e-05, -1.8955e-04])\n",
      "Epoch 1155, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5994])\n",
      "\t Grad:  tensor([ 2.7657e-05, -1.8805e-04])\n",
      "Epoch 1156, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5994])\n",
      "\t Grad:  tensor([ 2.2352e-05, -1.8743e-04])\n",
      "Epoch 1157, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5994])\n",
      "\t Grad:  tensor([ 2.5690e-05, -1.8578e-04])\n",
      "Epoch 1158, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5994])\n",
      "\t Grad:  tensor([ 2.7120e-05, -1.8435e-04])\n",
      "Epoch 1159, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5994])\n",
      "\t Grad:  tensor([ 2.2948e-05, -1.8367e-04])\n",
      "Epoch 1160, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5994])\n",
      "\t Grad:  tensor([ 2.1935e-05, -1.8253e-04])\n",
      "Epoch 1161, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5994])\n",
      "\t Grad:  tensor([ 2.1458e-05, -1.8132e-04])\n",
      "Epoch 1162, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5994])\n",
      "\t Grad:  tensor([ 2.2173e-05, -1.8009e-04])\n",
      "Epoch 1163, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5994])\n",
      "\t Grad:  tensor([ 1.9729e-05, -1.7903e-04])\n",
      "Epoch 1164, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5994])\n",
      "\t Grad:  tensor([ 2.4378e-05, -1.7730e-04])\n",
      "Epoch 1165, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5994])\n",
      "\t Grad:  tensor([ 2.3663e-05, -1.7621e-04])\n",
      "Epoch 1166, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5994])\n",
      "\t Grad:  tensor([ 2.4319e-05, -1.7497e-04])\n",
      "Epoch 1167, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5995])\n",
      "\t Grad:  tensor([ 2.5153e-05, -1.7369e-04])\n",
      "Epoch 1168, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5995])\n",
      "\t Grad:  tensor([ 2.5868e-05, -1.7251e-04])\n",
      "Epoch 1169, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5995])\n",
      "\t Grad:  tensor([ 2.5392e-05, -1.7134e-04])\n",
      "Epoch 1170, Loss 0.262983\n",
      "\t Params:  tensor([0.8411, 0.5995])\n",
      "\t Grad:  tensor([ 2.2352e-05, -1.7052e-04])\n",
      "Epoch 1171, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5995])\n",
      "\t Grad:  tensor([ 2.0981e-05, -1.6954e-04])\n",
      "Epoch 1172, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5995])\n",
      "\t Grad:  tensor([ 2.1219e-05, -1.6845e-04])\n",
      "Epoch 1173, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5995])\n",
      "\t Grad:  tensor([ 2.0146e-05, -1.6750e-04])\n",
      "Epoch 1174, Loss 0.262983\n",
      "\t Params:  tensor([0.8411, 0.5995])\n",
      "\t Grad:  tensor([ 2.1875e-05, -1.6602e-04])\n",
      "Epoch 1175, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5995])\n",
      "\t Grad:  tensor([ 1.9670e-05, -1.6525e-04])\n",
      "Epoch 1176, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5995])\n",
      "\t Grad:  tensor([ 2.6524e-05, -1.6328e-04])\n",
      "Epoch 1177, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5995])\n",
      "\t Grad:  tensor([ 2.4080e-05, -1.6249e-04])\n",
      "Epoch 1178, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5995])\n",
      "\t Grad:  tensor([ 1.9908e-05, -1.6186e-04])\n",
      "Epoch 1179, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5995])\n",
      "\t Grad:  tensor([ 2.3723e-05, -1.6025e-04])\n",
      "Epoch 1180, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5995])\n",
      "\t Grad:  tensor([ 2.3603e-05, -1.5938e-04])\n",
      "Epoch 1181, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5995])\n",
      "\t Grad:  tensor([ 1.8775e-05, -1.5871e-04])\n",
      "Epoch 1182, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5995])\n",
      "\t Grad:  tensor([ 2.0802e-05, -1.5742e-04])\n",
      "Epoch 1183, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5995])\n",
      "\t Grad:  tensor([ 2.4974e-05, -1.5588e-04])\n",
      "Epoch 1184, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5995])\n",
      "\t Grad:  tensor([ 1.9550e-05, -1.5543e-04])\n",
      "Epoch 1185, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5995])\n",
      "\t Grad:  tensor([ 2.3603e-05, -1.5385e-04])\n",
      "Epoch 1186, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5995])\n",
      "\t Grad:  tensor([ 2.3127e-05, -1.5302e-04])\n",
      "Epoch 1187, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5995])\n",
      "\t Grad:  tensor([ 1.7941e-05, -1.5255e-04])\n",
      "Epoch 1188, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5995])\n",
      "\t Grad:  tensor([ 2.0325e-05, -1.5125e-04])\n",
      "Epoch 1189, Loss 0.262983\n",
      "\t Params:  tensor([0.8411, 0.5995])\n",
      "\t Grad:  tensor([ 2.1756e-05, -1.4997e-04])\n",
      "Epoch 1190, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5995])\n",
      "\t Grad:  tensor([ 1.9789e-05, -1.4926e-04])\n",
      "Epoch 1191, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5995])\n",
      "\t Grad:  tensor([ 1.8597e-05, -1.4833e-04])\n",
      "Epoch 1192, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5995])\n",
      "\t Grad:  tensor([ 2.0385e-05, -1.4716e-04])\n",
      "Epoch 1193, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5995])\n",
      "\t Grad:  tensor([ 2.3484e-05, -1.4583e-04])\n",
      "Epoch 1194, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5995])\n",
      "\t Grad:  tensor([ 1.9550e-05, -1.4536e-04])\n",
      "Epoch 1195, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5995])\n",
      "\t Grad:  tensor([ 2.1815e-05, -1.4417e-04])\n",
      "Epoch 1196, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5995])\n",
      "\t Grad:  tensor([ 1.6272e-05, -1.4387e-04])\n",
      "Epoch 1197, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5995])\n",
      "\t Grad:  tensor([ 1.8537e-05, -1.4265e-04])\n",
      "Epoch 1198, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5995])\n",
      "\t Grad:  tensor([ 1.7583e-05, -1.4175e-04])\n",
      "Epoch 1199, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5995])\n",
      "\t Grad:  tensor([ 1.9729e-05, -1.4053e-04])\n",
      "Epoch 1200, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5995])\n",
      "\t Grad:  tensor([ 1.9908e-05, -1.3953e-04])\n",
      "Epoch 1201, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5995])\n",
      "\t Grad:  tensor([ 2.1458e-05, -1.3844e-04])\n",
      "Epoch 1202, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5995])\n",
      "\t Grad:  tensor([ 1.7405e-05, -1.3798e-04])\n",
      "Epoch 1203, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5995])\n",
      "\t Grad:  tensor([ 1.7464e-05, -1.3704e-04])\n",
      "Epoch 1204, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5995])\n",
      "\t Grad:  tensor([ 1.9252e-05, -1.3594e-04])\n",
      "Epoch 1205, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5995])\n",
      "\t Grad:  tensor([ 1.5140e-05, -1.3552e-04])\n",
      "Epoch 1206, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5995])\n",
      "\t Grad:  tensor([ 1.6928e-05, -1.3440e-04])\n",
      "Epoch 1207, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5995])\n",
      "\t Grad:  tensor([ 1.6212e-05, -1.3352e-04])\n",
      "Epoch 1208, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5995])\n",
      "\t Grad:  tensor([ 1.7762e-05, -1.3251e-04])\n",
      "Epoch 1209, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5995])\n",
      "\t Grad:  tensor([ 1.8179e-05, -1.3158e-04])\n",
      "Epoch 1210, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5995])\n",
      "\t Grad:  tensor([ 1.4663e-05, -1.3108e-04])\n",
      "Epoch 1211, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5995])\n",
      "\t Grad:  tensor([ 1.9491e-05, -1.2963e-04])\n",
      "Epoch 1212, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5995])\n",
      "\t Grad:  tensor([ 1.9193e-05, -1.2872e-04])\n",
      "Epoch 1213, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5995])\n",
      "\t Grad:  tensor([ 1.7762e-05, -1.2793e-04])\n",
      "Epoch 1214, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5995])\n",
      "\t Grad:  tensor([ 1.6928e-05, -1.2729e-04])\n",
      "Epoch 1215, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5995])\n",
      "\t Grad:  tensor([ 1.7285e-05, -1.2641e-04])\n",
      "Epoch 1216, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5995])\n",
      "\t Grad:  tensor([ 1.5676e-05, -1.2569e-04])\n",
      "Epoch 1217, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5995])\n",
      "\t Grad:  tensor([ 1.5795e-05, -1.2495e-04])\n",
      "Epoch 1218, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5995])\n",
      "\t Grad:  tensor([ 1.3828e-05, -1.2430e-04])\n",
      "Epoch 1219, Loss 0.262983\n",
      "\t Params:  tensor([0.8411, 0.5995])\n",
      "\t Grad:  tensor([ 1.7405e-05, -1.2297e-04])\n",
      "Epoch 1220, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5995])\n",
      "\t Grad:  tensor([ 1.7524e-05, -1.2221e-04])\n",
      "Epoch 1221, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5995])\n",
      "\t Grad:  tensor([ 1.5974e-05, -1.2150e-04])\n",
      "Epoch 1222, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5995])\n",
      "\t Grad:  tensor([ 1.4246e-05, -1.2098e-04])\n",
      "Epoch 1223, Loss 0.262983\n",
      "\t Params:  tensor([0.8411, 0.5995])\n",
      "\t Grad:  tensor([ 1.6391e-05, -1.1984e-04])\n",
      "Epoch 1224, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5995])\n",
      "\t Grad:  tensor([ 1.7583e-05, -1.1899e-04])\n",
      "Epoch 1225, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5995])\n",
      "\t Grad:  tensor([ 1.6153e-05, -1.1829e-04])\n",
      "Epoch 1226, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5995])\n",
      "\t Grad:  tensor([ 1.1623e-05, -1.1803e-04])\n",
      "Epoch 1227, Loss 0.262983\n",
      "\t Params:  tensor([0.8411, 0.5995])\n",
      "\t Grad:  tensor([ 1.4663e-05, -1.1682e-04])\n",
      "Epoch 1228, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5995])\n",
      "\t Grad:  tensor([ 1.9729e-05, -1.1550e-04])\n",
      "Epoch 1229, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5995])\n",
      "\t Grad:  tensor([ 1.8120e-05, -1.1486e-04])\n",
      "Epoch 1230, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5995])\n",
      "\t Grad:  tensor([ 1.4067e-05, -1.1460e-04])\n",
      "Epoch 1231, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5995])\n",
      "\t Grad:  tensor([ 1.5378e-05, -1.1368e-04])\n",
      "Epoch 1232, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5995])\n",
      "\t Grad:  tensor([ 1.3709e-05, -1.1310e-04])\n",
      "Epoch 1233, Loss 0.262983\n",
      "\t Params:  tensor([0.8411, 0.5995])\n",
      "\t Grad:  tensor([ 1.7524e-05, -1.1189e-04])\n",
      "Epoch 1234, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5995])\n",
      "\t Grad:  tensor([ 1.4961e-05, -1.1139e-04])\n",
      "Epoch 1235, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5995])\n",
      "\t Grad:  tensor([ 1.1623e-05, -1.1104e-04])\n",
      "Epoch 1236, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5995])\n",
      "\t Grad:  tensor([ 1.2457e-05, -1.1025e-04])\n",
      "Epoch 1237, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5995])\n",
      "\t Grad:  tensor([ 1.6510e-05, -1.0897e-04])\n",
      "Epoch 1238, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5995])\n",
      "\t Grad:  tensor([ 1.2219e-05, -1.0882e-04])\n",
      "Epoch 1239, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5996])\n",
      "\t Grad:  tensor([ 1.4305e-05, -1.0782e-04])\n",
      "Epoch 1240, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5996])\n",
      "\t Grad:  tensor([ 1.7703e-05, -1.0672e-04])\n",
      "Epoch 1241, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5996])\n",
      "\t Grad:  tensor([ 1.5616e-05, -1.0625e-04])\n",
      "Epoch 1242, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5996])\n",
      "\t Grad:  tensor([ 1.2338e-05, -1.0591e-04])\n",
      "Epoch 1243, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5996])\n",
      "\t Grad:  tensor([ 1.3292e-05, -1.0508e-04])\n",
      "Epoch 1244, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5996])\n",
      "\t Grad:  tensor([ 1.3590e-05, -1.0426e-04])\n",
      "Epoch 1245, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5996])\n",
      "\t Grad:  tensor([ 1.5020e-05, -1.0344e-04])\n",
      "Epoch 1246, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5996])\n",
      "\t Grad:  tensor([ 1.3173e-05, -1.0297e-04])\n",
      "Epoch 1247, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5996])\n",
      "\t Grad:  tensor([ 1.4484e-05, -1.0211e-04])\n",
      "Epoch 1248, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5996])\n",
      "\t Grad:  tensor([ 1.4305e-05, -1.0151e-04])\n",
      "Epoch 1249, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5996])\n",
      "\t Grad:  tensor([ 1.4067e-05, -1.0073e-04])\n",
      "Epoch 1250, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5996])\n",
      "\t Grad:  tensor([ 1.6093e-05, -9.9823e-05])\n",
      "Epoch 1251, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5996])\n",
      "\t Grad:  tensor([ 1.5378e-05, -9.9346e-05])\n",
      "Epoch 1252, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5996])\n",
      "\t Grad:  tensor([ 1.0669e-05, -9.9242e-05])\n",
      "Epoch 1253, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5996])\n",
      "\t Grad:  tensor([ 1.2696e-05, -9.8318e-05])\n",
      "Epoch 1254, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5996])\n",
      "\t Grad:  tensor([ 1.2398e-05, -9.7692e-05])\n",
      "Epoch 1255, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5996])\n",
      "\t Grad:  tensor([ 1.3590e-05, -9.6925e-05])\n",
      "Epoch 1256, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5996])\n",
      "\t Grad:  tensor([ 1.5020e-05, -9.6105e-05])\n",
      "Epoch 1257, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5996])\n",
      "\t Grad:  tensor([ 1.1384e-05, -9.5919e-05])\n",
      "Epoch 1258, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5996])\n",
      "\t Grad:  tensor([ 1.0371e-05, -9.5405e-05])\n",
      "Epoch 1259, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5996])\n",
      "\t Grad:  tensor([ 1.0669e-05, -9.4675e-05])\n",
      "Epoch 1260, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5996])\n",
      "\t Grad:  tensor([ 1.1504e-05, -9.4004e-05])\n",
      "Epoch 1261, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5996])\n",
      "\t Grad:  tensor([ 1.1742e-05, -9.3251e-05])\n",
      "Epoch 1262, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5996])\n",
      "\t Grad:  tensor([ 1.2934e-05, -9.2521e-05])\n",
      "Epoch 1263, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5996])\n",
      "\t Grad:  tensor([ 1.4603e-05, -9.1672e-05])\n",
      "Epoch 1264, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5996])\n",
      "\t Grad:  tensor([ 1.2636e-05, -9.1240e-05])\n",
      "Epoch 1265, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5996])\n",
      "\t Grad:  tensor([ 1.3828e-05, -9.0554e-05])\n",
      "Epoch 1266, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5996])\n",
      "\t Grad:  tensor([ 1.5140e-05, -8.9787e-05])\n",
      "Epoch 1267, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5996])\n",
      "\t Grad:  tensor([ 7.6294e-06, -9.0018e-05])\n",
      "Epoch 1268, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5996])\n",
      "\t Grad:  tensor([ 1.2636e-05, -8.8811e-05])\n",
      "Epoch 1269, Loss 0.262983\n",
      "\t Params:  tensor([0.8411, 0.5996])\n",
      "\t Grad:  tensor([ 1.2040e-05, -8.8304e-05])\n",
      "Epoch 1270, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5996])\n",
      "\t Grad:  tensor([ 1.0550e-05, -8.7805e-05])\n",
      "Epoch 1271, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5996])\n",
      "\t Grad:  tensor([ 9.8348e-06, -8.7388e-05])\n",
      "Epoch 1272, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5996])\n",
      "\t Grad:  tensor([ 1.1981e-05, -8.6509e-05])\n",
      "Epoch 1273, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5996])\n",
      "\t Grad:  tensor([ 1.1206e-05, -8.6002e-05])\n",
      "Epoch 1274, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5996])\n",
      "\t Grad:  tensor([ 1.1563e-05, -8.5361e-05])\n",
      "Epoch 1275, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5996])\n",
      "\t Grad:  tensor([ 1.2159e-05, -8.4810e-05])\n",
      "Epoch 1276, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5996])\n",
      "\t Grad:  tensor([ 9.7156e-06, -8.4490e-05])\n",
      "Epoch 1277, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5996])\n",
      "\t Grad:  tensor([ 1.0073e-05, -8.4013e-05])\n",
      "Epoch 1278, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5996])\n",
      "\t Grad:  tensor([ 8.5831e-06, -8.3499e-05])\n",
      "Epoch 1279, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5996])\n",
      "\t Grad:  tensor([ 1.2279e-05, -8.2448e-05])\n",
      "Epoch 1280, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5996])\n",
      "\t Grad:  tensor([ 1.1325e-05, -8.2053e-05])\n",
      "Epoch 1281, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5996])\n",
      "\t Grad:  tensor([ 1.0431e-05, -8.1636e-05])\n",
      "Epoch 1282, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5996])\n",
      "\t Grad:  tensor([ 1.0312e-05, -8.1055e-05])\n",
      "Epoch 1283, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5996])\n",
      "\t Grad:  tensor([ 1.1981e-05, -8.0317e-05])\n",
      "Epoch 1284, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5996])\n",
      "\t Grad:  tensor([ 8.2254e-06, -8.0280e-05])\n",
      "Epoch 1285, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5996])\n",
      "\t Grad:  tensor([ 1.0490e-05, -7.9378e-05])\n",
      "Epoch 1286, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5996])\n",
      "\t Grad:  tensor([ 9.2983e-06, -7.9043e-05])\n",
      "Epoch 1287, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5996])\n",
      "\t Grad:  tensor([ 8.9407e-06, -7.8581e-05])\n",
      "Epoch 1288, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5996])\n",
      "\t Grad:  tensor([ 1.0133e-05, -7.7970e-05])\n",
      "Epoch 1289, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5996])\n",
      "\t Grad:  tensor([ 6.9737e-06, -7.7844e-05])\n",
      "Epoch 1290, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5996])\n",
      "\t Grad:  tensor([ 1.0908e-05, -7.6756e-05])\n",
      "Epoch 1291, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5996])\n",
      "\t Grad:  tensor([ 8.2850e-06, -7.6547e-05])\n",
      "Epoch 1292, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5996])\n",
      "\t Grad:  tensor([ 1.1146e-05, -7.5668e-05])\n",
      "Epoch 1293, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5996])\n",
      "\t Grad:  tensor([ 1.0908e-05, -7.5176e-05])\n",
      "Epoch 1294, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5996])\n",
      "\t Grad:  tensor([ 1.0788e-05, -7.4670e-05])\n",
      "Epoch 1295, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5996])\n",
      "\t Grad:  tensor([ 1.1265e-05, -7.4148e-05])\n",
      "Epoch 1296, Loss 0.262983\n",
      "\t Params:  tensor([0.8411, 0.5996])\n",
      "\t Grad:  tensor([ 4.7684e-06, -7.4431e-05])\n",
      "Epoch 1297, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5996])\n",
      "\t Grad:  tensor([ 9.7156e-06, -7.3396e-05])\n",
      "Epoch 1298, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5996])\n",
      "\t Grad:  tensor([ 8.7023e-06, -7.3016e-05])\n",
      "Epoch 1299, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5996])\n",
      "\t Grad:  tensor([ 1.1981e-05, -7.2137e-05])\n",
      "Epoch 1300, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5996])\n",
      "\t Grad:  tensor([ 7.5698e-06, -7.2099e-05])\n",
      "Epoch 1301, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5996])\n",
      "\t Grad:  tensor([ 1.0252e-05, -7.1332e-05])\n",
      "Epoch 1302, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5996])\n",
      "\t Grad:  tensor([ 9.3579e-06, -7.1011e-05])\n",
      "Epoch 1303, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5996])\n",
      "\t Grad:  tensor([ 8.5235e-06, -7.0639e-05])\n",
      "Epoch 1304, Loss 0.262983\n",
      "\t Params:  tensor([0.8411, 0.5996])\n",
      "\t Grad:  tensor([ 9.0003e-06, -6.9968e-05])\n",
      "Epoch 1305, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5996])\n",
      "\t Grad:  tensor([ 7.5698e-06, -6.9790e-05])\n",
      "Epoch 1306, Loss 0.262983\n",
      "\t Params:  tensor([0.8411, 0.5996])\n",
      "\t Grad:  tensor([ 1.1265e-05, -6.8761e-05])\n",
      "Epoch 1307, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5996])\n",
      "\t Grad:  tensor([ 9.5963e-06, -6.8612e-05])\n",
      "Epoch 1308, Loss 0.262983\n",
      "\t Params:  tensor([0.8411, 0.5996])\n",
      "\t Grad:  tensor([ 7.5698e-06, -6.8329e-05])\n",
      "Epoch 1309, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5996])\n",
      "\t Grad:  tensor([ 8.1658e-06, -6.7748e-05])\n",
      "Epoch 1310, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5996])\n",
      "\t Grad:  tensor([ 1.1206e-05, -6.7011e-05])\n",
      "Epoch 1311, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5996])\n",
      "\t Grad:  tensor([ 8.5235e-06, -6.6824e-05])\n",
      "Epoch 1312, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5996])\n",
      "\t Grad:  tensor([ 1.0908e-05, -6.6176e-05])\n",
      "Epoch 1313, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5996])\n",
      "\t Grad:  tensor([ 8.7619e-06, -6.6005e-05])\n",
      "Epoch 1314, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5996])\n",
      "\t Grad:  tensor([ 1.0371e-05, -6.5222e-05])\n",
      "Epoch 1315, Loss 0.262983\n",
      "\t Params:  tensor([0.8411, 0.5996])\n",
      "\t Grad:  tensor([ 7.0333e-06, -6.5297e-05])\n",
      "Epoch 1316, Loss 0.262983\n",
      "\t Params:  tensor([0.8411, 0.5996])\n",
      "\t Grad:  tensor([ 1.1921e-05, -6.4254e-05])\n",
      "Epoch 1317, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5996])\n",
      "\t Grad:  tensor([ 9.2983e-06, -6.4157e-05])\n",
      "Epoch 1318, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5996])\n",
      "\t Grad:  tensor([ 3.0398e-06, -6.4492e-05])\n",
      "Epoch 1319, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5996])\n",
      "\t Grad:  tensor([ 6.7353e-06, -6.3494e-05])\n",
      "Epoch 1320, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5996])\n",
      "\t Grad:  tensor([ 1.1325e-05, -6.2622e-05])\n",
      "Epoch 1321, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5996])\n",
      "\t Grad:  tensor([ 5.7817e-06, -6.2771e-05])\n",
      "Epoch 1322, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5996])\n",
      "\t Grad:  tensor([ 9.5963e-06, -6.1914e-05])\n",
      "Epoch 1323, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5996])\n",
      "\t Grad:  tensor([ 6.8545e-06, -6.1892e-05])\n",
      "Epoch 1324, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5996])\n",
      "\t Grad:  tensor([ 6.3181e-06, -6.1475e-05])\n",
      "Epoch 1325, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5996])\n",
      "\t Grad:  tensor([ 1.0669e-05, -6.0551e-05])\n",
      "Epoch 1326, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5996])\n",
      "\t Grad:  tensor([ 6.6161e-06, -6.0648e-05])\n",
      "Epoch 1327, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5996])\n",
      "\t Grad:  tensor([ 6.8545e-06, -6.0260e-05])\n",
      "Epoch 1328, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5996])\n",
      "\t Grad:  tensor([ 7.5698e-06, -5.9754e-05])\n",
      "Epoch 1329, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5996])\n",
      "\t Grad:  tensor([ 1.1265e-05, -5.8845e-05])\n",
      "Epoch 1330, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5996])\n",
      "\t Grad:  tensor([ 7.9274e-06, -5.8845e-05])\n",
      "Epoch 1331, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5996])\n",
      "\t Grad:  tensor([ 1.0312e-05, -5.8278e-05])\n",
      "Epoch 1332, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5996])\n",
      "\t Grad:  tensor([ 3.8743e-06, -5.8614e-05])\n",
      "Epoch 1333, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5996])\n",
      "\t Grad:  tensor([ 8.6427e-06, -5.7645e-05])\n",
      "Epoch 1334, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5996])\n",
      "\t Grad:  tensor([ 8.1658e-06, -5.7213e-05])\n",
      "Epoch 1335, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5996])\n",
      "\t Grad:  tensor([ 9.2387e-06, -5.6706e-05])\n",
      "Epoch 1336, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5996])\n",
      "\t Grad:  tensor([ 8.0466e-06, -5.6535e-05])\n",
      "Epoch 1337, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5996])\n",
      "\t Grad:  tensor([ 5.9605e-06, -5.6356e-05])\n",
      "Epoch 1338, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5996])\n",
      "\t Grad:  tensor([ 1.1623e-05, -5.5343e-05])\n",
      "Epoch 1339, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5996])\n",
      "\t Grad:  tensor([ 3.8147e-06, -5.5827e-05])\n",
      "Epoch 1340, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5996])\n",
      "\t Grad:  tensor([ 7.8678e-06, -5.5045e-05])\n",
      "Epoch 1341, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5996])\n",
      "\t Grad:  tensor([ 8.2254e-06, -5.4680e-05])\n",
      "Epoch 1342, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5996])\n",
      "\t Grad:  tensor([ 7.2718e-06, -5.4426e-05])\n",
      "Epoch 1343, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5996])\n",
      "\t Grad:  tensor([ 1.0252e-05, -5.3704e-05])\n",
      "Epoch 1344, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5996])\n",
      "\t Grad:  tensor([ 2.7418e-06, -5.4263e-05])\n",
      "Epoch 1345, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5996])\n",
      "\t Grad:  tensor([ 1.0133e-05, -5.3026e-05])\n",
      "Epoch 1346, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5996])\n",
      "\t Grad:  tensor([ 7.7486e-06, -5.2832e-05])\n",
      "Epoch 1347, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5996])\n",
      "\t Grad:  tensor([ 8.3447e-06, -5.2415e-05])\n",
      "Epoch 1348, Loss 0.262983\n",
      "\t Params:  tensor([0.8411, 0.5996])\n",
      "\t Grad:  tensor([ 7.7486e-06, -5.2057e-05])\n",
      "Epoch 1349, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5996])\n",
      "\t Grad:  tensor([ 9.6560e-06, -5.1551e-05])\n",
      "Epoch 1350, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5996])\n",
      "\t Grad:  tensor([ 8.2254e-06, -5.1506e-05])\n",
      "Epoch 1351, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5996])\n",
      "\t Grad:  tensor([ 5.6624e-06, -5.1349e-05])\n",
      "Epoch 1352, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5996])\n",
      "\t Grad:  tensor([ 6.2585e-06, -5.0917e-05])\n",
      "Epoch 1353, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5996])\n",
      "\t Grad:  tensor([ 7.3314e-06, -5.0366e-05])\n",
      "Epoch 1354, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5996])\n",
      "\t Grad:  tensor([ 8.7619e-06, -4.9874e-05])\n",
      "Epoch 1355, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5996])\n",
      "\t Grad:  tensor([ 7.2122e-06, -4.9733e-05])\n",
      "Epoch 1356, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5996])\n",
      "\t Grad:  tensor([ 9.4771e-06, -4.9174e-05])\n",
      "Epoch 1357, Loss 0.262983\n",
      "\t Params:  tensor([0.8411, 0.5996])\n",
      "\t Grad:  tensor([ 4.2319e-06, -4.9487e-05])\n",
      "Epoch 1358, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5996])\n",
      "\t Grad:  tensor([ 6.8545e-06, -4.8898e-05])\n",
      "Epoch 1359, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5996])\n",
      "\t Grad:  tensor([ 5.3048e-06, -4.8742e-05])\n",
      "Epoch 1360, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5996])\n",
      "\t Grad:  tensor([ 5.6624e-06, -4.8406e-05])\n",
      "Epoch 1361, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5996])\n",
      "\t Grad:  tensor([ 5.5432e-06, -4.8019e-05])\n",
      "Epoch 1362, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5996])\n",
      "\t Grad:  tensor([ 7.6890e-06, -4.7512e-05])\n",
      "Epoch 1363, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5996])\n",
      "\t Grad:  tensor([ 5.3644e-06, -4.7341e-05])\n",
      "Epoch 1364, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5996])\n",
      "\t Grad:  tensor([ 9.4771e-06, -4.6633e-05])\n",
      "Epoch 1365, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5996])\n",
      "\t Grad:  tensor([ 3.5763e-07, -4.7393e-05])\n",
      "Epoch 1366, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5996])\n",
      "\t Grad:  tensor([ 6.7949e-06, -4.6283e-05])\n",
      "Epoch 1367, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5996])\n",
      "\t Grad:  tensor([ 7.3910e-06, -4.5896e-05])\n",
      "Epoch 1368, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5996])\n",
      "\t Grad:  tensor([ 7.7486e-06, -4.5568e-05])\n",
      "Epoch 1369, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5996])\n",
      "\t Grad:  tensor([ 7.2718e-06, -4.5232e-05])\n",
      "Epoch 1370, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5996])\n",
      "\t Grad:  tensor([ 1.0490e-05, -4.4569e-05])\n",
      "Epoch 1371, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5996])\n",
      "\t Grad:  tensor([ 1.2517e-06, -4.5404e-05])\n",
      "Epoch 1372, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5996])\n",
      "\t Grad:  tensor([ 7.9870e-06, -4.4279e-05])\n",
      "Epoch 1373, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5996])\n",
      "\t Grad:  tensor([ 7.6294e-06, -4.4025e-05])\n",
      "Epoch 1374, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5996])\n",
      "\t Grad:  tensor([ 1.0252e-05, -4.3459e-05])\n",
      "Epoch 1375, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5996])\n",
      "\t Grad:  tensor([ 2.9206e-06, -4.4122e-05])\n",
      "Epoch 1376, Loss 0.262983\n",
      "\t Params:  tensor([0.8411, 0.5996])\n",
      "\t Grad:  tensor([ 5.7817e-06, -4.3362e-05])\n",
      "Epoch 1377, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5996])\n",
      "\t Grad:  tensor([ 8.2850e-06, -4.2751e-05])\n",
      "Epoch 1378, Loss 0.262983\n",
      "\t Params:  tensor([0.8411, 0.5996])\n",
      "\t Grad:  tensor([ 5.4240e-06, -4.2811e-05])\n",
      "Epoch 1379, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5996])\n",
      "\t Grad:  tensor([ 6.9737e-06, -4.2513e-05])\n",
      "Epoch 1380, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5996])\n",
      "\t Grad:  tensor([ 4.5896e-06, -4.2409e-05])\n",
      "Epoch 1381, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5996])\n",
      "\t Grad:  tensor([ 4.9472e-06, -4.2066e-05])\n",
      "Epoch 1382, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5996])\n",
      "\t Grad:  tensor([ 5.5432e-06, -4.1798e-05])\n",
      "Epoch 1383, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5997])\n",
      "\t Grad:  tensor([ 4.2319e-06, -4.1611e-05])\n",
      "Epoch 1384, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5997])\n",
      "\t Grad:  tensor([ 4.9472e-06, -4.1239e-05])\n",
      "Epoch 1385, Loss 0.262983\n",
      "\t Params:  tensor([0.8411, 0.5997])\n",
      "\t Grad:  tensor([ 3.7551e-06, -4.1135e-05])\n",
      "Epoch 1386, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5997])\n",
      "\t Grad:  tensor([ 4.5896e-06, -4.0777e-05])\n",
      "Epoch 1387, Loss 0.262983\n",
      "\t Params:  tensor([0.8411, 0.5997])\n",
      "\t Grad:  tensor([ 3.0994e-06, -4.0621e-05])\n",
      "Epoch 1388, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5997])\n",
      "\t Grad:  tensor([ 3.3975e-06, -4.0397e-05])\n",
      "Epoch 1389, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5997])\n",
      "\t Grad:  tensor([ 1.7881e-06, -4.0285e-05])\n",
      "Epoch 1390, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5997])\n",
      "\t Grad:  tensor([ 8.4043e-06, -3.9190e-05])\n",
      "Epoch 1391, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5997])\n",
      "\t Grad:  tensor([ 5.6028e-06, -3.9138e-05])\n",
      "Epoch 1392, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5997])\n",
      "\t Grad:  tensor([ 4.7684e-06, -3.8922e-05])\n",
      "Epoch 1393, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5997])\n",
      "\t Grad:  tensor([ 6.6757e-06, -3.8482e-05])\n",
      "Epoch 1394, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5997])\n",
      "\t Grad:  tensor([ 1.6689e-06, -3.8855e-05])\n",
      "Epoch 1395, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5997])\n",
      "\t Grad:  tensor([ 7.6294e-06, -3.7856e-05])\n",
      "Epoch 1396, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5997])\n",
      "\t Grad:  tensor([ 8.2254e-06, -3.7573e-05])\n",
      "Epoch 1397, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5997])\n",
      "\t Grad:  tensor([ 5.8413e-06, -3.7469e-05])\n",
      "Epoch 1398, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5997])\n",
      "\t Grad:  tensor([ 5.9605e-06, -3.7290e-05])\n",
      "Epoch 1399, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5997])\n",
      "\t Grad:  tensor([ 4.4107e-06, -3.7305e-05])\n",
      "Epoch 1400, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5997])\n",
      "\t Grad:  tensor([ 4.6492e-06, -3.7082e-05])\n",
      "Epoch 1401, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5997])\n",
      "\t Grad:  tensor([ 3.8147e-06, -3.6888e-05])\n",
      "Epoch 1402, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5997])\n",
      "\t Grad:  tensor([ 2.2054e-06, -3.6873e-05])\n",
      "Epoch 1403, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5997])\n",
      "\t Grad:  tensor([ 6.3181e-06, -3.6143e-05])\n",
      "Epoch 1404, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5997])\n",
      "\t Grad:  tensor([ 3.3975e-06, -3.6187e-05])\n",
      "Epoch 1405, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5997])\n",
      "\t Grad:  tensor([ 2.6226e-06, -3.6001e-05])\n",
      "Epoch 1406, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5997])\n",
      "\t Grad:  tensor([ 8.9407e-06, -3.4995e-05])\n",
      "Epoch 1407, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5997])\n",
      "\t Grad:  tensor([ 1.9073e-06, -3.5673e-05])\n",
      "Epoch 1408, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5997])\n",
      "\t Grad:  tensor([ 7.8678e-06, -3.4712e-05])\n",
      "Epoch 1409, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5997])\n",
      "\t Grad:  tensor([ 4.8876e-06, -3.4794e-05])\n",
      "Epoch 1410, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5997])\n",
      "\t Grad:  tensor([ 3.2187e-06, -3.4772e-05])\n",
      "Epoch 1411, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5997])\n",
      "\t Grad:  tensor([ 3.6955e-06, -3.4541e-05])\n",
      "Epoch 1412, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5997])\n",
      "\t Grad:  tensor([ 7.1526e-07, -3.4645e-05])\n",
      "Epoch 1413, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5997])\n",
      "\t Grad:  tensor([ 3.8147e-06, -3.3900e-05])\n",
      "Epoch 1414, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5997])\n",
      "\t Grad:  tensor([ 7.2718e-06, -3.3394e-05])\n",
      "Epoch 1415, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5997])\n",
      "\t Grad:  tensor([ 2.0266e-06, -3.3706e-05])\n",
      "Epoch 1416, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5997])\n",
      "\t Grad:  tensor([ 8.2254e-06, -3.2723e-05])\n",
      "Epoch 1417, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5997])\n",
      "\t Grad:  tensor([ 5.3644e-06, -3.2842e-05])\n",
      "Epoch 1418, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5997])\n",
      "\t Grad:  tensor([ 4.7684e-06, -3.2790e-05])\n",
      "Epoch 1419, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5997])\n",
      "\t Grad:  tensor([ 4.2915e-06, -3.2596e-05])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1420, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5997])\n",
      "\t Grad:  tensor([ 3.0994e-06, -3.2499e-05])\n",
      "Epoch 1421, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-07, -3.2693e-05])\n",
      "Epoch 1422, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5997])\n",
      "\t Grad:  tensor([ 5.2452e-06, -3.1918e-05])\n",
      "Epoch 1423, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5997])\n",
      "\t Grad:  tensor([ 3.2187e-06, -3.1896e-05])\n",
      "Epoch 1424, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5997])\n",
      "\t Grad:  tensor([-5.3644e-07, -3.2142e-05])\n",
      "Epoch 1425, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5997])\n",
      "\t Grad:  tensor([ 5.6028e-06, -3.1196e-05])\n",
      "Epoch 1426, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5997])\n",
      "\t Grad:  tensor([ 4.4107e-06, -3.1121e-05])\n",
      "Epoch 1427, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5997])\n",
      "\t Grad:  tensor([ 4.1723e-07, -3.1479e-05])\n",
      "Epoch 1428, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5997])\n",
      "\t Grad:  tensor([ 3.2783e-06, -3.0808e-05])\n",
      "Epoch 1429, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5997])\n",
      "\t Grad:  tensor([ 4.5300e-06, -3.0540e-05])\n",
      "Epoch 1430, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5997])\n",
      "\t Grad:  tensor([ 6.5565e-07, -3.0845e-05])\n",
      "Epoch 1431, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5997])\n",
      "\t Grad:  tensor([ 3.5167e-06, -3.0130e-05])\n",
      "Epoch 1432, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5997])\n",
      "\t Grad:  tensor([ 2.9206e-06, -3.0160e-05])\n",
      "Epoch 1433, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5997])\n",
      "\t Grad:  tensor([ 5.5432e-06, -2.9571e-05])\n",
      "Epoch 1434, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5997])\n",
      "\t Grad:  tensor([ 4.1127e-06, -2.9549e-05])\n",
      "Epoch 1435, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5997])\n",
      "\t Grad:  tensor([ 4.1127e-06, -2.9445e-05])\n",
      "Epoch 1436, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5997])\n",
      "\t Grad:  tensor([ 1.4901e-06, -2.9482e-05])\n",
      "Epoch 1437, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5997])\n",
      "\t Grad:  tensor([ 2.3246e-06, -2.9184e-05])\n",
      "Epoch 1438, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5997])\n",
      "\t Grad:  tensor([ 8.0466e-06, -2.8282e-05])\n",
      "Epoch 1439, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5997])\n",
      "\t Grad:  tensor([ 6.6161e-06, -2.8223e-05])\n",
      "Epoch 1440, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5997])\n",
      "\t Grad:  tensor([ 2.6822e-06, -2.8558e-05])\n",
      "Epoch 1441, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5997])\n",
      "\t Grad:  tensor([ 5.3048e-06, -2.7947e-05])\n",
      "Epoch 1442, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5997])\n",
      "\t Grad:  tensor([ 5.9009e-06, -2.7709e-05])\n",
      "Epoch 1443, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5997])\n",
      "\t Grad:  tensor([ 4.5896e-06, -2.7694e-05])\n",
      "Epoch 1444, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5997])\n",
      "\t Grad:  tensor([ 1.2517e-06, -2.7895e-05])\n",
      "Epoch 1445, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5997])\n",
      "\t Grad:  tensor([ 5.4240e-06, -2.7232e-05])\n",
      "Epoch 1446, Loss 0.262983\n",
      "\t Params:  tensor([0.8411, 0.5997])\n",
      "\t Grad:  tensor([ 2.5630e-06, -2.7344e-05])\n",
      "Epoch 1447, Loss 0.262983\n",
      "\t Params:  tensor([0.8411, 0.5997])\n",
      "\t Grad:  tensor([ 5.4240e-06, -2.6718e-05])\n",
      "Epoch 1448, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5997])\n",
      "\t Grad:  tensor([ 6.8545e-06, -2.6457e-05])\n",
      "Epoch 1449, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5997])\n",
      "\t Grad:  tensor([ 4.1127e-06, -2.6569e-05])\n",
      "Epoch 1450, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5997])\n",
      "\t Grad:  tensor([ 5.9605e-08, -2.6971e-05])\n",
      "Epoch 1451, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5997])\n",
      "\t Grad:  tensor([ 2.5630e-06, -2.6390e-05])\n",
      "Epoch 1452, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5997])\n",
      "\t Grad:  tensor([ 6.4373e-06, -2.5839e-05])\n",
      "Epoch 1453, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5997])\n",
      "\t Grad:  tensor([ 5.4240e-06, -2.5734e-05])\n",
      "Epoch 1454, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5997])\n",
      "\t Grad:  tensor([-2.9802e-07, -2.6338e-05])\n",
      "Epoch 1455, Loss 0.262983\n",
      "\t Params:  tensor([0.8411, 0.5997])\n",
      "\t Grad:  tensor([ 2.2054e-06, -2.5772e-05])\n",
      "Epoch 1456, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5997])\n",
      "\t Grad:  tensor([ 6.0201e-06, -2.5198e-05])\n",
      "Epoch 1457, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5997])\n",
      "\t Grad:  tensor([ 4.7088e-06, -2.5213e-05])\n",
      "Epoch 1458, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5997])\n",
      "\t Grad:  tensor([ 1.4901e-06, -2.5451e-05])\n",
      "Epoch 1459, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5997])\n",
      "\t Grad:  tensor([ 4.1127e-06, -2.4915e-05])\n",
      "Epoch 1460, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5997])\n",
      "\t Grad:  tensor([ 3.7551e-06, -2.4900e-05])\n",
      "Epoch 1461, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5997])\n",
      "\t Grad:  tensor([ 4.1723e-07, -2.5116e-05])\n",
      "Epoch 1462, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5997])\n",
      "\t Grad:  tensor([ 7.7486e-07, -2.4900e-05])\n",
      "Epoch 1463, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5997])\n",
      "\t Grad:  tensor([ 3.2783e-06, -2.4356e-05])\n",
      "Epoch 1464, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5997])\n",
      "\t Grad:  tensor([ 3.5167e-06, -2.4281e-05])\n",
      "Epoch 1465, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5997])\n",
      "\t Grad:  tensor([ 2.2054e-06, -2.4281e-05])\n",
      "Epoch 1466, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5997])\n",
      "\t Grad:  tensor([ 2.4438e-06, -2.4058e-05])\n",
      "Epoch 1467, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5997])\n",
      "\t Grad:  tensor([ 5.1856e-06, -2.3499e-05])\n",
      "Epoch 1468, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5997])\n",
      "\t Grad:  tensor([ 2.9206e-06, -2.3708e-05])\n",
      "Epoch 1469, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5997])\n",
      "\t Grad:  tensor([ 5.4240e-06, -2.3142e-05])\n",
      "Epoch 1470, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5997])\n",
      "\t Grad:  tensor([ 3.3975e-06, -2.3335e-05])\n",
      "Epoch 1471, Loss 0.262983\n",
      "\t Params:  tensor([0.8411, 0.5997])\n",
      "\t Grad:  tensor([ 9.5367e-07, -2.3380e-05])\n",
      "Epoch 1472, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5997])\n",
      "\t Grad:  tensor([ 4.7088e-06, -2.2829e-05])\n",
      "Epoch 1473, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5997])\n",
      "\t Grad:  tensor([ 1.9073e-06, -2.2963e-05])\n",
      "Epoch 1474, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5997])\n",
      "\t Grad:  tensor([ 2.1458e-06, -2.2754e-05])\n",
      "Epoch 1475, Loss 0.262983\n",
      "\t Params:  tensor([0.8411, 0.5997])\n",
      "\t Grad:  tensor([ 5.0068e-06, -2.2203e-05])\n",
      "Epoch 1476, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5997])\n",
      "\t Grad:  tensor([ 6.3777e-06, -2.1987e-05])\n",
      "Epoch 1477, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5997])\n",
      "\t Grad:  tensor([ 3.4571e-06, -2.2128e-05])\n",
      "Epoch 1478, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5997])\n",
      "\t Grad:  tensor([-7.1526e-07, -2.2538e-05])\n",
      "Epoch 1479, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5997])\n",
      "\t Grad:  tensor([ 2.1458e-06, -2.1957e-05])\n",
      "Epoch 1480, Loss 0.262982\n",
      "\t Params:  tensor([0.8411, 0.5997])\n",
      "\t Grad:  tensor([ 5.8413e-06, -2.1391e-05])\n",
      "Epoch 1481, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 4.8876e-06, -2.1338e-05])\n",
      "Epoch 1482, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([-1.0729e-06, -2.1912e-05])\n",
      "Epoch 1483, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 1.9073e-06, -2.1361e-05])\n",
      "Epoch 1484, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 5.6028e-06, -2.0802e-05])\n",
      "Epoch 1485, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 4.1723e-06, -2.0884e-05])\n",
      "Epoch 1486, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 3.5763e-07, -2.1197e-05])\n",
      "Epoch 1487, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 3.0994e-06, -2.0631e-05])\n",
      "Epoch 1488, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.9802e-06, -2.0631e-05])\n",
      "Epoch 1489, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 4.4107e-06, -2.0273e-05])\n",
      "Epoch 1490, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-07, -2.0646e-05])\n",
      "Epoch 1491, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 4.7684e-07, -2.0437e-05])\n",
      "Epoch 1492, Loss 0.262983\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.9802e-06, -1.9990e-05])\n",
      "Epoch 1493, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 5.9605e-07, -2.0258e-05])\n",
      "Epoch 1494, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.8610e-06, -1.9901e-05])\n",
      "Epoch 1495, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 5.6028e-06, -1.9349e-05])\n",
      "Epoch 1496, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 1.1921e-06, -1.9856e-05])\n",
      "Epoch 1497, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.0266e-06, -1.9602e-05])\n",
      "Epoch 1498, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 4.0531e-06, -1.9141e-05])\n",
      "Epoch 1499, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.1458e-06, -1.9379e-05])\n",
      "Epoch 1500, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 4.7684e-06, -1.8887e-05])\n",
      "Epoch 1501, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 3.4571e-06, -1.8962e-05])\n",
      "Epoch 1502, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([-2.3246e-06, -1.9558e-05])\n",
      "Epoch 1503, Loss 0.262983\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 4.1723e-07, -1.9006e-05])\n",
      "Epoch 1504, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 3.9339e-06, -1.8559e-05])\n",
      "Epoch 1505, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([-2.9802e-07, -1.8954e-05])\n",
      "Epoch 1506, Loss 0.262983\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 1.0133e-06, -1.8597e-05])\n",
      "Epoch 1507, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 1.2517e-06, -1.8448e-05])\n",
      "Epoch 1508, Loss 0.262983\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 3.8743e-06, -1.7963e-05])\n",
      "Epoch 1509, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 5.3644e-06, -1.7799e-05])\n",
      "Epoch 1510, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.9802e-07, -1.8284e-05])\n",
      "Epoch 1511, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.8014e-06, -1.7755e-05])\n",
      "Epoch 1512, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.9206e-06, -1.7650e-05])\n",
      "Epoch 1513, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 4.3511e-06, -1.7293e-05])\n",
      "Epoch 1514, Loss 0.262983\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 1.3709e-06, -1.7546e-05])\n",
      "Epoch 1515, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 5.1856e-06, -1.7032e-05])\n",
      "Epoch 1516, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 3.8743e-06, -1.7099e-05])\n",
      "Epoch 1517, Loss 0.262983\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([-1.7285e-06, -1.7680e-05])\n",
      "Epoch 1518, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([-1.4901e-06, -1.7494e-05])\n",
      "Epoch 1519, Loss 0.262983\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 1.0133e-06, -1.6987e-05])\n",
      "Epoch 1520, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 4.5896e-06, -1.6525e-05])\n",
      "Epoch 1521, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3246e-06, -1.6697e-05])\n",
      "Epoch 1522, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 3.6359e-06, -1.6332e-05])\n",
      "Epoch 1523, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([-2.9802e-07, -1.6823e-05])\n",
      "Epoch 1524, Loss 0.262983\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.0862e-06, -1.6332e-05])\n",
      "Epoch 1525, Loss 0.262983\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3246e-06, -1.6153e-05])\n",
      "Epoch 1526, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 6.0201e-06, -1.5631e-05])\n",
      "Epoch 1527, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 4.2319e-06, -1.5683e-05])\n",
      "Epoch 1528, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([-1.6093e-06, -1.6324e-05])\n",
      "Epoch 1529, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([-4.1723e-07, -1.5989e-05])\n",
      "Epoch 1530, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 1.2517e-06, -1.5609e-05])\n",
      "Epoch 1531, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 4.9472e-06, -1.5125e-05])\n",
      "Epoch 1532, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 3.7551e-06, -1.5184e-05])\n",
      "Epoch 1533, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 1.7881e-07, -1.5475e-05])\n",
      "Epoch 1534, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 4.1723e-07, -1.5296e-05])\n",
      "Epoch 1535, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 3.0398e-06, -1.4782e-05])\n",
      "Epoch 1536, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([-1.4305e-06, -1.5326e-05])\n",
      "Epoch 1537, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 6.5565e-07, -1.4983e-05])\n",
      "Epoch 1538, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 3.2783e-06, -1.4514e-05])\n",
      "Epoch 1539, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 1.6093e-06, -1.4730e-05])\n",
      "Epoch 1540, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 1.6093e-06, -1.4655e-05])\n",
      "Epoch 1541, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.5630e-06, -1.4439e-05])\n",
      "Epoch 1542, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 4.7088e-06, -1.4007e-05])\n",
      "Epoch 1543, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 7.1526e-07, -1.4447e-05])\n",
      "Epoch 1544, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.8014e-06, -1.4171e-05])\n",
      "Epoch 1545, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 4.2319e-06, -1.3888e-05])\n",
      "Epoch 1546, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 1.4305e-06, -1.4164e-05])\n",
      "Epoch 1547, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 1.7881e-06, -1.3985e-05])\n",
      "Epoch 1548, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.0266e-06, -1.3858e-05])\n",
      "Epoch 1549, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 3.8147e-06, -1.3493e-05])\n",
      "Epoch 1550, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 3.5763e-07, -1.3903e-05])\n",
      "Epoch 1551, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.1458e-06, -1.3635e-05])\n",
      "Epoch 1552, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 3.9935e-06, -1.3418e-05])\n",
      "Epoch 1553, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([-1.0729e-06, -1.3918e-05])\n",
      "Epoch 1554, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 5.9605e-07, -1.3545e-05])\n",
      "Epoch 1555, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 1.0729e-06, -1.3441e-05])\n",
      "Epoch 1556, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 1.0729e-06, -1.3337e-05])\n",
      "Epoch 1557, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.8610e-06, -1.2979e-05])\n",
      "Epoch 1558, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 3.8147e-06, -1.2785e-05])\n",
      "Epoch 1559, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 3.3379e-06, -1.2867e-05])\n",
      "Epoch 1560, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([-1.1921e-07, -1.3292e-05])\n",
      "Epoch 1561, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 1.6689e-06, -1.2919e-05])\n",
      "Epoch 1562, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -1.2711e-05])\n",
      "Epoch 1563, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.5034e-06, -1.2606e-05])\n",
      "Epoch 1564, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.7418e-06, -1.2502e-05])\n",
      "Epoch 1565, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 3.6955e-06, -1.2293e-05])\n",
      "Epoch 1566, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 1.3113e-06, -1.2480e-05])\n",
      "Epoch 1567, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 4.7684e-06, -1.2048e-05])\n",
      "Epoch 1568, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([-5.9605e-07, -1.2629e-05])\n",
      "Epoch 1569, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 7.1526e-07, -1.2375e-05])\n",
      "Epoch 1570, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 1.7881e-06, -1.2152e-05])\n",
      "Epoch 1571, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.0266e-06, -1.1988e-05])\n",
      "Epoch 1572, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.5034e-06, -1.1839e-05])\n",
      "Epoch 1573, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 3.6955e-06, -1.1601e-05])\n",
      "Epoch 1574, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 8.3447e-07, -1.1884e-05])\n",
      "Epoch 1575, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.7418e-06, -1.1608e-05])\n",
      "Epoch 1576, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 4.6492e-06, -1.1310e-05])\n",
      "Epoch 1577, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 1.7881e-06, -1.1601e-05])\n",
      "Epoch 1578, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 3.6955e-06, -1.1198e-05])\n",
      "Epoch 1579, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([-2.3246e-06, -1.1921e-05])\n",
      "Epoch 1580, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([-2.0862e-06, -1.1802e-05])\n",
      "Epoch 1581, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([-1.3709e-06, -1.1638e-05])\n",
      "Epoch 1582, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 1.7881e-07, -1.1310e-05])\n",
      "Epoch 1583, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 5.3644e-07, -1.1146e-05])\n",
      "Epoch 1584, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 4.2915e-06, -1.0662e-05])\n",
      "Epoch 1585, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 3.3379e-06, -1.0796e-05])\n",
      "Epoch 1586, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([-1.3709e-06, -1.1265e-05])\n",
      "Epoch 1587, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([-1.1325e-06, -1.1146e-05])\n",
      "Epoch 1588, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([-1.0133e-06, -1.1042e-05])\n",
      "Epoch 1589, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 4.1723e-07, -1.0759e-05])\n",
      "Epoch 1590, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 1.8477e-06, -1.0476e-05])\n",
      "Epoch 1591, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 1.9670e-06, -1.0356e-05])\n",
      "Epoch 1592, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 5.6028e-06, -9.9018e-06])\n",
      "Epoch 1593, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 3.2783e-06, -1.0073e-05])\n",
      "Epoch 1594, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 1.1325e-06, -1.0312e-05])\n",
      "Epoch 1595, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 1.3709e-06, -1.0222e-05])\n",
      "Epoch 1596, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 1.4901e-06, -1.0103e-05])\n",
      "Epoch 1597, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 3.6359e-06, -9.7305e-06])\n",
      "Epoch 1598, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([-1.8477e-06, -1.0267e-05])\n",
      "Epoch 1599, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 1.7285e-06, -9.8199e-06])\n",
      "Epoch 1600, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 1.7285e-06, -9.7752e-06])\n",
      "Epoch 1601, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 3.9935e-06, -9.3430e-06])\n",
      "Epoch 1602, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 1.7881e-07, -9.7454e-06])\n",
      "Epoch 1603, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.9802e-07, -9.6411e-06])\n",
      "Epoch 1604, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 5.3644e-07, -9.5367e-06])\n",
      "Epoch 1605, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.6822e-06, -9.1493e-06])\n",
      "Epoch 1606, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 3.5167e-06, -8.9556e-06])\n",
      "Epoch 1607, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 3.1590e-06, -9.0599e-06])\n",
      "Epoch 1608, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([-2.2054e-06, -9.6709e-06])\n",
      "Epoch 1609, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([-1.3709e-06, -9.4324e-06])\n",
      "Epoch 1610, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 6.5565e-07, -9.0227e-06])\n",
      "Epoch 1611, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 6.5565e-07, -8.9630e-06])\n",
      "Epoch 1612, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 6.5565e-07, -8.8885e-06])\n",
      "Epoch 1613, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 1.0133e-06, -8.7395e-06])\n",
      "Epoch 1614, Loss 0.262983\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.9206e-06, -8.4713e-06])\n",
      "Epoch 1615, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 3.3975e-06, -8.3372e-06])\n",
      "Epoch 1616, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([-1.1325e-06, -8.9556e-06])\n",
      "Epoch 1617, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 8.9407e-07, -8.6054e-06])\n",
      "Epoch 1618, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.8014e-06, -8.4043e-06])\n",
      "Epoch 1619, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 3.0398e-06, -8.2999e-06])\n",
      "Epoch 1620, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([-1.2517e-06, -8.8885e-06])\n",
      "Epoch 1621, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([-6.5565e-07, -8.7544e-06])\n",
      "Epoch 1622, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 1.2517e-06, -8.4415e-06])\n",
      "Epoch 1623, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 1.4901e-06, -8.3447e-06])\n",
      "Epoch 1624, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 1.4901e-06, -8.3223e-06])\n",
      "Epoch 1625, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 1.4901e-06, -8.2776e-06])\n",
      "Epoch 1626, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 1.4901e-06, -8.2329e-06])\n",
      "Epoch 1627, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 1.4901e-06, -8.2180e-06])\n",
      "Epoch 1628, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 1.7285e-06, -8.1137e-06])\n",
      "Epoch 1629, Loss 0.262983\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 3.6359e-06, -7.7859e-06])\n",
      "Epoch 1630, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.9802e-07, -8.1956e-06])\n",
      "Epoch 1631, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.9802e-07, -8.1658e-06])\n",
      "Epoch 1632, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 5.3644e-07, -8.0764e-06])\n",
      "Epoch 1633, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.2054e-06, -7.8306e-06])\n",
      "Epoch 1634, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 4.1127e-06, -7.6294e-06])\n",
      "Epoch 1635, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([-1.0133e-06, -8.2180e-06])\n",
      "Epoch 1636, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([-8.9407e-07, -8.1509e-06])\n",
      "Epoch 1637, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([-4.1723e-07, -8.0317e-06])\n",
      "Epoch 1638, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 1.3709e-06, -7.7337e-06])\n",
      "Epoch 1639, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 1.4901e-06, -7.6592e-06])\n",
      "Epoch 1640, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 1.6093e-06, -7.5847e-06])\n",
      "Epoch 1641, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 1.6093e-06, -7.5549e-06])\n",
      "Epoch 1642, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 1.6093e-06, -7.5251e-06])\n",
      "Epoch 1643, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 1.7285e-06, -7.4431e-06])\n",
      "Epoch 1644, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 1.8477e-06, -7.3686e-06])\n",
      "Epoch 1645, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 3.7551e-06, -7.0855e-06])\n",
      "Epoch 1646, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 5.9605e-08, -7.5474e-06])\n",
      "Epoch 1647, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 4.1723e-07, -7.4208e-06])\n",
      "Epoch 1648, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 4.1723e-07, -7.3761e-06])\n",
      "Epoch 1649, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 4.1723e-07, -7.3314e-06])\n",
      "Epoch 1650, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 3.8743e-06, -6.9886e-06])\n",
      "Epoch 1651, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 4.1723e-07, -7.3984e-06])\n",
      "Epoch 1652, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 4.1723e-07, -7.3388e-06])\n",
      "Epoch 1653, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 4.1723e-07, -7.3090e-06])\n",
      "Epoch 1654, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.5630e-06, -6.9439e-06])\n",
      "Epoch 1655, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.5630e-06, -6.9290e-06])\n",
      "Epoch 1656, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.9206e-06, -6.7800e-06])\n",
      "Epoch 1657, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.9206e-06, -6.7502e-06])\n",
      "Epoch 1658, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.9206e-06, -6.7204e-06])\n",
      "Epoch 1659, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 3.1590e-06, -6.6459e-06])\n",
      "Epoch 1660, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([-2.6226e-06, -7.3239e-06])\n",
      "Epoch 1661, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([-2.6226e-06, -7.2792e-06])\n",
      "Epoch 1662, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([-4.7684e-07, -6.8992e-06])\n",
      "Epoch 1663, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([-2.3842e-07, -6.8545e-06])\n",
      "Epoch 1664, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([-1.1921e-07, -6.7800e-06])\n",
      "Epoch 1665, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 1.8477e-06, -6.5714e-06])\n",
      "Epoch 1666, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 3.5167e-06, -6.3479e-06])\n",
      "Epoch 1667, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 1.7285e-06, -6.6236e-06])\n",
      "Epoch 1668, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 1.8477e-06, -6.5491e-06])\n",
      "Epoch 1669, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3246e-06, -6.4522e-06])\n",
      "Epoch 1670, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 3.9935e-06, -6.1691e-06])\n",
      "Epoch 1671, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([-1.7881e-06, -6.8471e-06])\n",
      "Epoch 1672, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([-1.6689e-06, -6.7726e-06])\n",
      "Epoch 1673, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([-1.6689e-06, -6.7577e-06])\n",
      "Epoch 1674, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([-1.6689e-06, -6.7428e-06])\n",
      "Epoch 1675, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([-1.5497e-06, -6.6236e-06])\n",
      "Epoch 1676, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([-1.3113e-06, -6.5640e-06])\n",
      "Epoch 1677, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([-8.3447e-07, -6.4597e-06])\n",
      "Epoch 1678, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 8.3447e-07, -6.1989e-06])\n",
      "Epoch 1679, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 1.0729e-06, -6.0946e-06])\n",
      "Epoch 1680, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 1.1921e-06, -6.0201e-06])\n",
      "Epoch 1681, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 3.0398e-06, -5.8189e-06])\n",
      "Epoch 1682, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([-1.1921e-07, -6.1840e-06])\n",
      "Epoch 1683, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-07, -6.0722e-06])\n",
      "Epoch 1684, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-07, -6.0275e-06])\n",
      "Epoch 1685, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 1.9073e-06, -5.7295e-06])\n",
      "Epoch 1686, Loss 0.262983\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.5034e-06, -5.6401e-06])\n",
      "Epoch 1687, Loss 0.262983\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.5034e-06, -5.6103e-06])\n",
      "Epoch 1688, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.8610e-06, -5.4836e-06])\n",
      "Epoch 1689, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.8610e-06, -5.4315e-06])\n",
      "Epoch 1690, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.8610e-06, -5.4166e-06])\n",
      "Epoch 1691, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 3.0994e-06, -5.2974e-06])\n",
      "Epoch 1692, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 5.9605e-07, -5.7071e-06])\n",
      "Epoch 1693, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.2650e-06, -5.4166e-06])\n",
      "Epoch 1694, Loss 0.262983\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.8610e-06, -5.2825e-06])\n",
      "Epoch 1695, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 3.0994e-06, -5.1782e-06])\n",
      "Epoch 1696, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([-2.1458e-06, -5.8040e-06])\n",
      "Epoch 1697, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([-4.7684e-07, -5.6326e-06])\n",
      "Epoch 1698, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 1.3113e-06, -5.3868e-06])\n",
      "Epoch 1699, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 1.5497e-06, -5.3123e-06])\n",
      "Epoch 1700, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 1.6689e-06, -5.2378e-06])\n",
      "Epoch 1701, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.8610e-06, -5.0440e-06])\n",
      "Epoch 1702, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 4.0531e-06, -4.8354e-06])\n",
      "Epoch 1703, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-07, -5.2601e-06])\n",
      "Epoch 1704, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 4.7684e-07, -5.1633e-06])\n",
      "Epoch 1705, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 4.7684e-07, -5.1409e-06])\n",
      "Epoch 1706, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 5.9605e-07, -5.0962e-06])\n",
      "Epoch 1707, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Grad:  tensor([ 8.3447e-07, -4.9546e-06])\n",
      "Epoch 1708, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 8.3447e-07, -4.9546e-06])\n",
      "Epoch 1709, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 1.5497e-06, -4.8354e-06])\n",
      "Epoch 1710, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 3.0994e-06, -4.5374e-06])\n",
      "Epoch 1711, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([-8.3447e-07, -5.0291e-06])\n",
      "Epoch 1712, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([-8.3447e-07, -4.9993e-06])\n",
      "Epoch 1713, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 8.3447e-07, -4.7907e-06])\n",
      "Epoch 1714, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.6226e-06, -4.6045e-06])\n",
      "Epoch 1715, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.6226e-06, -4.5449e-06])\n",
      "Epoch 1716, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.8610e-06, -4.4331e-06])\n",
      "Epoch 1717, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 3.5763e-06, -4.3064e-06])\n",
      "Epoch 1718, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([-8.3447e-07, -4.8205e-06])\n",
      "Epoch 1719, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([-8.3447e-07, -4.8056e-06])\n",
      "Epoch 1720, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([-3.5763e-07, -4.6864e-06])\n",
      "Epoch 1721, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([-3.5763e-07, -4.6268e-06])\n",
      "Epoch 1722, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([-3.5763e-07, -4.5970e-06])\n",
      "Epoch 1723, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([-1.1921e-07, -4.4927e-06])\n",
      "Epoch 1724, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([-1.1921e-07, -4.4629e-06])\n",
      "Epoch 1725, Loss 0.262983\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 7.1526e-07, -4.3139e-06])\n",
      "Epoch 1726, Loss 0.262983\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.2650e-06, -4.0457e-06])\n",
      "Epoch 1727, Loss 0.262983\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.2650e-06, -4.0159e-06])\n",
      "Epoch 1728, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.5034e-06, -3.8967e-06])\n",
      "Epoch 1729, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 4.2915e-06, -3.6657e-06])\n",
      "Epoch 1730, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -3.9414e-06])\n",
      "Epoch 1731, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.5034e-06, -3.8818e-06])\n",
      "Epoch 1732, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.6226e-06, -3.8221e-06])\n",
      "Epoch 1733, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 3.9339e-06, -3.5837e-06])\n",
      "Epoch 1734, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 1.1921e-06, -3.9414e-06])\n",
      "Epoch 1735, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 1.3113e-06, -3.8669e-06])\n",
      "Epoch 1736, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 1.5497e-06, -3.7774e-06])\n",
      "Epoch 1737, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 1.5497e-06, -3.7625e-06])\n",
      "Epoch 1738, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 1.5497e-06, -3.7178e-06])\n",
      "Epoch 1739, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 1.7881e-06, -3.6433e-06])\n",
      "Epoch 1740, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 1.9073e-06, -3.5539e-06])\n",
      "Epoch 1741, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 3.5763e-06, -3.2708e-06])\n",
      "Epoch 1742, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([-3.5763e-07, -3.7774e-06])\n",
      "Epoch 1743, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([-1.1921e-07, -3.7029e-06])\n",
      "Epoch 1744, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 0.0000e+00, -3.5986e-06])\n",
      "Epoch 1745, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 3.5763e-06, -3.2336e-06])\n",
      "Epoch 1746, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([-2.5034e-06, -3.9712e-06])\n",
      "Epoch 1747, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([-2.2650e-06, -3.8818e-06])\n",
      "Epoch 1748, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([-2.1458e-06, -3.8221e-06])\n",
      "Epoch 1749, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([-1.4305e-06, -3.7029e-06])\n",
      "Epoch 1750, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 1.1921e-07, -3.4049e-06])\n",
      "Epoch 1751, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 1.1921e-07, -3.4049e-06])\n",
      "Epoch 1752, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 3.5763e-07, -3.2708e-06])\n",
      "Epoch 1753, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 3.5763e-07, -3.2410e-06])\n",
      "Epoch 1754, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 3.5763e-07, -3.2112e-06])\n",
      "Epoch 1755, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 7.1526e-07, -3.1069e-06])\n",
      "Epoch 1756, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 7.1526e-07, -3.0473e-06])\n",
      "Epoch 1757, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1758, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1759, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1760, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1761, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1762, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1763, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1764, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1765, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1766, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1767, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1768, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1769, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1770, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1771, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1772, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1773, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1774, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1775, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1776, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1777, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1778, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1779, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1780, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1781, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1782, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1783, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1784, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1785, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1786, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1787, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1788, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1789, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1790, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1791, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1792, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1793, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1794, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1795, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1796, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1797, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1798, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1799, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1800, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1801, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1802, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1803, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1804, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1805, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1806, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1807, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1808, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1809, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1810, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1811, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1812, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1813, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1814, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1815, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1816, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1817, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1818, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1819, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1820, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1821, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1822, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1823, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1824, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1825, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1826, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1827, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1828, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1829, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1830, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1831, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1832, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1833, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1834, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1835, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1836, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1837, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1838, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1839, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1840, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1841, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1842, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1843, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1844, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1845, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1846, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1847, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1848, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1849, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1850, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1851, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1852, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1853, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1854, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1855, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1856, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1857, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1858, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1859, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1860, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1861, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1862, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1863, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1864, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1865, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1866, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1867, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1868, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1869, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1870, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1871, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1872, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1873, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1874, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1875, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1876, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1877, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1878, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1879, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1880, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1881, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1882, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1883, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1884, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1885, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1886, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1887, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1888, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1889, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1890, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1891, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1892, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1893, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1894, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1895, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1896, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1897, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1898, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1899, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1900, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1901, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1902, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1903, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1904, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1905, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1906, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1907, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1908, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1909, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1910, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1911, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1912, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1913, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1914, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1915, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1916, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1917, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1918, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1919, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1920, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1921, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1922, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1923, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1924, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1925, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1926, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1927, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1928, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1929, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1930, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1931, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1932, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1933, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1934, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1935, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1936, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1937, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1938, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1939, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1940, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1941, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1942, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1943, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1944, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1945, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1946, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1947, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1948, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1949, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1950, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1951, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1952, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1953, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1954, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1955, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1956, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1957, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1958, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1959, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1960, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1961, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1962, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1963, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1964, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1965, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1966, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1967, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1968, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1969, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1970, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1971, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1972, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1973, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1974, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1975, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1976, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1977, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1978, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1979, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1980, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1981, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1982, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1983, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1984, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1985, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1986, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1987, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1988, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1989, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1990, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1991, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1992, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1993, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1994, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1995, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1996, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1997, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1998, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 1999, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2000, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2001, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2002, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2003, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2004, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2005, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2006, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2007, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2008, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2009, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2010, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2011, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2012, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2013, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2014, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2015, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2016, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2017, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2018, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2019, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2020, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2021, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2022, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2023, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2024, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2025, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2026, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2027, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2028, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2029, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2030, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2031, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2032, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2033, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2034, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2035, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2036, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2037, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2038, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2039, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2040, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2041, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2042, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2043, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2044, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2045, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2046, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2047, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2048, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2049, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2050, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2051, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2052, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2053, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2054, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2055, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2056, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2057, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2058, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2059, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2060, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2061, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2062, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2063, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2064, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2065, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2066, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2067, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2068, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2069, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2070, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2071, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2072, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2073, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2074, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2075, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2076, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2077, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2078, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2079, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2080, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2081, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2082, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2083, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2084, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2085, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2086, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2087, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2088, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2089, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2090, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2091, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2092, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2093, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2094, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2095, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2096, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2097, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2098, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2099, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2100, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2101, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2102, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2103, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2104, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2105, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2106, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2107, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2108, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2109, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2110, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2111, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2112, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2113, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2114, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2115, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2116, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2117, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2118, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2119, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2120, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2121, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2122, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2123, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2124, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2125, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2126, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2127, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2128, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2129, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2130, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2131, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2132, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2133, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2134, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2135, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2136, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2137, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2138, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2139, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2140, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2141, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2142, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2143, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2144, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2145, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2146, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2147, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2148, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2149, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2150, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2151, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2152, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2153, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2154, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2155, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2156, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2157, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2158, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2159, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2160, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2161, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2162, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2163, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2164, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2165, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2166, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2167, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2168, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2169, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2170, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2171, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2172, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2173, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2174, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2175, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2176, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2177, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2178, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2179, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2180, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2181, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2182, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2183, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2184, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2185, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2186, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2187, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2188, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2189, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2190, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2191, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2192, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2193, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2194, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2195, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2196, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2197, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2198, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2199, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2200, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2201, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2202, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2203, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2204, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2205, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2206, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2207, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2208, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2209, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2210, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2211, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2212, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2213, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2214, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2215, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2216, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2217, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2218, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2219, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2220, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2221, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2222, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2223, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2224, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2225, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2226, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2227, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2228, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2229, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2230, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2231, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2232, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2233, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2234, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2235, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2236, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2237, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2238, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2239, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2240, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2241, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2242, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2243, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2244, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2245, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2246, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2247, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2248, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2249, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2250, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2251, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2252, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2253, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2254, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2255, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2256, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2257, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2258, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2259, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2260, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2261, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2262, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2263, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2264, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2265, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2266, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2267, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2268, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2269, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2270, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2271, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2272, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2273, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2274, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2275, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2276, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2277, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2278, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2279, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2280, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2281, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2282, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2283, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2284, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2285, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2286, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2287, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2288, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2289, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2290, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2291, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2292, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2293, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2294, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2295, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2296, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2297, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2298, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2299, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2300, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2301, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2302, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2303, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2304, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2305, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2306, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2307, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2308, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2309, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2310, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2311, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2312, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2313, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2314, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2315, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2316, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2317, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2318, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2319, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2320, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2321, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2322, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2323, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2324, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2325, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2326, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2327, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2328, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2329, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2330, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2331, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2332, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2333, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2334, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2335, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2336, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2337, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2338, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2339, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2340, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2341, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2342, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2343, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2344, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2345, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2346, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2347, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2348, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2349, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2350, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2351, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2352, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2353, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2354, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2355, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2356, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2357, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2358, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2359, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2360, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2361, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2362, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2363, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2364, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2365, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2366, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2367, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2368, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2369, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2370, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2371, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2372, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2373, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2374, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2375, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2376, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2377, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2378, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2379, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2380, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2381, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2382, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2383, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2384, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2385, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2386, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2387, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2388, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2389, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2390, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2391, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2392, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2393, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2394, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2395, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2396, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2397, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2398, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2399, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2400, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2401, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2402, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2403, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2404, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2405, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2406, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2407, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2408, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2409, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2410, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2411, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2412, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2413, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2414, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2415, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2416, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2417, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2418, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2419, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2420, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2421, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2422, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2423, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2424, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2425, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2426, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2427, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2428, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2429, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2430, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2431, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2432, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2433, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2434, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2435, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2436, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2437, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2438, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2439, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2440, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2441, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2442, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2443, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2444, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2445, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2446, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2447, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2448, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2449, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2450, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2451, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2452, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2453, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2454, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2455, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2456, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2457, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2458, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2459, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2460, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2461, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2462, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2463, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2464, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2465, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2466, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2467, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2468, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2469, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2470, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2471, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2472, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2473, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2474, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2475, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2476, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2477, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2478, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2479, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2480, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2481, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2482, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2483, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2484, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2485, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2486, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2487, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2488, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2489, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2490, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2491, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2492, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2493, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2494, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2495, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2496, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2497, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2498, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2499, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2500, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2501, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2502, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2503, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2504, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2505, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2506, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2507, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2508, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2509, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2510, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2511, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2512, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2513, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2514, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2515, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2516, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2517, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2518, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2519, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2520, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2521, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2522, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2523, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2524, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2525, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2526, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2527, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2528, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2529, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2530, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2531, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2532, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2533, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2534, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2535, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2536, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2537, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2538, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2539, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2540, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2541, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2542, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2543, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2544, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2545, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2546, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2547, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2548, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2549, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2550, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2551, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2552, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2553, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2554, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2555, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2556, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2557, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2558, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2559, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2560, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2561, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2562, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2563, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2564, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2565, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2566, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2567, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2568, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2569, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2570, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2571, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2572, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2573, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2574, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2575, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2576, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2577, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2578, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2579, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2580, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2581, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2582, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2583, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2584, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2585, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2586, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2587, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2588, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2589, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2590, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2591, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2592, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2593, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2594, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2595, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2596, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2597, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2598, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2599, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2600, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2601, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2602, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2603, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2604, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2605, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2606, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2607, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2608, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2609, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2610, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2611, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2612, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2613, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2614, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2615, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2616, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2617, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2618, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2619, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2620, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2621, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2622, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2623, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2624, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2625, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2626, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2627, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2628, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2629, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2630, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2631, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2632, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2633, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2634, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2635, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2636, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2637, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2638, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2639, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2640, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2641, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2642, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2643, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2644, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2645, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2646, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2647, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2648, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2649, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2650, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2651, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2652, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2653, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2654, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2655, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2656, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2657, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2658, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2659, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2660, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2661, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2662, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2663, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2664, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2665, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2666, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2667, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2668, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2669, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2670, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2671, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2672, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2673, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2674, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2675, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2676, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2677, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2678, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2679, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2680, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2681, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2682, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2683, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2684, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2685, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2686, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2687, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2688, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2689, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2690, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2691, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2692, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2693, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2694, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2695, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2696, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2697, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2698, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2699, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2700, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2701, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2702, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2703, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2704, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2705, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2706, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2707, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2708, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2709, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2710, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2711, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2712, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2713, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2714, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2715, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2716, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2717, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2718, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2719, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2720, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2721, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2722, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2723, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2724, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2725, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2726, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2727, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2728, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2729, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2730, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2731, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2732, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2733, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2734, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2735, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2736, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2737, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2738, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2739, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2740, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2741, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2742, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2743, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2744, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2745, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2746, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2747, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2748, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2749, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2750, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2751, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2752, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2753, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2754, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2755, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2756, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2757, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2758, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2759, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2760, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2761, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2762, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2763, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2764, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2765, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2766, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2767, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2768, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2769, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2770, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2771, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2772, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2773, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2774, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2775, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2776, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2777, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2778, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2779, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2780, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2781, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2782, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2783, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2784, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2785, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2786, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2787, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2788, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2789, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2790, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2791, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2792, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2793, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2794, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2795, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2796, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2797, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2798, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2799, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2800, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2801, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2802, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2803, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2804, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2805, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2806, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2807, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2808, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2809, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2810, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2811, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2812, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2813, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2814, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2815, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2816, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2817, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2818, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2819, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2820, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2821, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2822, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2823, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2824, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2825, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2826, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2827, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2828, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2829, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2830, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2831, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2832, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2833, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2834, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2835, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2836, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2837, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2838, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2839, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2840, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2841, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2842, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2843, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2844, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2845, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2846, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2847, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2848, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2849, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2850, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2851, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2852, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2853, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2854, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2855, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2856, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2857, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2858, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2859, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2860, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2861, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2862, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2863, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2864, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2865, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2866, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2867, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2868, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2869, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2870, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2871, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2872, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2873, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2874, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2875, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2876, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2877, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2878, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2879, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2880, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2881, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2882, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2883, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2884, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2885, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2886, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2887, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2888, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2889, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2890, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2891, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2892, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2893, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2894, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2895, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2896, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2897, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2898, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2899, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2900, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2901, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2902, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2903, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2904, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2905, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2906, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2907, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2908, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2909, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2910, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2911, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2912, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2913, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2914, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2915, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2916, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2917, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2918, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2919, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2920, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2921, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2922, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2923, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2924, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2925, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2926, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2927, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2928, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2929, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2930, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2931, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2932, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2933, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2934, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2935, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2936, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2937, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2938, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2939, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2940, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2941, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2942, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2943, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2944, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2945, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2946, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2947, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2948, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2949, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2950, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2951, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2952, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2953, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2954, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2955, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2956, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2957, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2958, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2959, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2960, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2961, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2962, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2963, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2964, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2965, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2966, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2967, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2968, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2969, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2970, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2971, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2972, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2973, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2974, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2975, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2976, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2977, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2978, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2979, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2980, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2981, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2982, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2983, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2984, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2985, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2986, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2987, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2988, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2989, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2990, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2991, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2992, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2993, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2994, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2995, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2996, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2997, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2998, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 2999, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3000, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3001, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3002, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3003, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3004, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3005, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3006, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3007, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3008, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3009, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3010, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3011, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3012, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3013, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3014, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3015, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3016, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3017, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3018, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3019, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3020, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3021, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3022, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3023, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3024, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3025, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3026, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3027, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3028, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3029, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3030, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3031, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3032, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3033, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3034, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3035, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3036, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3037, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3038, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3039, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3040, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3041, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3042, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3043, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3044, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3045, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3046, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3047, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3048, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3049, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3050, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3051, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3052, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3053, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3054, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3055, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3056, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3057, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3058, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3059, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3060, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3061, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3062, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3063, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3064, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3065, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3066, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3067, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3068, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3069, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3070, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3071, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3072, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3073, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3074, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3075, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3076, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3077, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3078, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3079, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3080, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3081, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3082, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3083, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3084, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3085, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3086, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3087, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3088, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3089, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3090, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3091, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3092, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3093, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3094, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3095, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3096, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3097, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3098, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3099, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3100, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3101, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3102, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3103, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3104, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3105, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3106, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3107, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3108, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3109, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3110, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3111, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3112, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3113, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3114, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3115, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3116, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3117, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3118, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3119, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3120, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3121, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3122, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3123, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3124, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3125, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3126, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3127, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3128, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3129, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3130, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3131, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3132, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3133, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3134, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3135, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3136, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3137, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3138, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3139, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3140, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3141, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3142, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3143, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3144, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3145, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3146, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3147, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3148, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3149, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3150, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3151, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3152, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3153, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3154, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3155, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3156, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3157, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3158, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3159, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3160, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3161, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3162, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3163, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3164, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3165, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3166, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3167, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3168, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3169, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3170, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3171, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3172, Loss 0.262982\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3173, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3174, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3175, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3176, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3177, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3178, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3179, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3180, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3181, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3182, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3183, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3184, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3185, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3186, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3187, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3188, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3189, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3190, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3191, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3192, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3193, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3194, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3195, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3196, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3197, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3198, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3199, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3200, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3201, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3202, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3203, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3204, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3205, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3206, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3207, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3208, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3209, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3210, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3211, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3212, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3213, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3214, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3215, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3216, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3217, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3218, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3219, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3220, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3221, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3222, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3223, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3224, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3225, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3226, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3227, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3228, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3229, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3230, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3231, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3232, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3233, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3234, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3235, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3236, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3237, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3238, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3239, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3240, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3241, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3242, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3243, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3244, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3245, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3246, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3247, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3248, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3249, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3250, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3251, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3252, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3253, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3254, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3255, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3256, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3257, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3258, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3259, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3260, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3261, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3262, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3263, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3264, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3265, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3266, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3267, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3268, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3269, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3270, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3271, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3272, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3273, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3274, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3275, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3276, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3277, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3278, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3279, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3280, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3281, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3282, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3283, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3284, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3285, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3286, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3287, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3288, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3289, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3290, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3291, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3292, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3293, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3294, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3295, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3296, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3297, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3298, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3299, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3300, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3301, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3302, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3303, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3304, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3305, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3306, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3307, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3308, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3309, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3310, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3311, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3312, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3313, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3314, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3315, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3316, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3317, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3318, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3319, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3320, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3321, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3322, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3323, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3324, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3325, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3326, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3327, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3328, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3329, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3330, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3331, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3332, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3333, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3334, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3335, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3336, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3337, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3338, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3339, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3340, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3341, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3342, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3343, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3344, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3345, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3346, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3347, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3348, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3349, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3350, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3351, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3352, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3353, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3354, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3355, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3356, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3357, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3358, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3359, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3360, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3361, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3362, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3363, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3364, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3365, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3366, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3367, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3368, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3369, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3370, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3371, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3372, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3373, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3374, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3375, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3376, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3377, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3378, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3379, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3380, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3381, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3382, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3383, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3384, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3385, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3386, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3387, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3388, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3389, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3390, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3391, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3392, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3393, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3394, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3395, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3396, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3397, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3398, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3399, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3400, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3401, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3402, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3403, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3404, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3405, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3406, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3407, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3408, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3409, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3410, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3411, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3412, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3413, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3414, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3415, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3416, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3417, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3418, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3419, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3420, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3421, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3422, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3423, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3424, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3425, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3426, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3427, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3428, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3429, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3430, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3431, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3432, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3433, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3434, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3435, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3436, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3437, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3438, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3439, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3440, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3441, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3442, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3443, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3444, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3445, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3446, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3447, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3448, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3449, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3450, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3451, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3452, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3453, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3454, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3455, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3456, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3457, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3458, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3459, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3460, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3461, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3462, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3463, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3464, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3465, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3466, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3467, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3468, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3469, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3470, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3471, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3472, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3473, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3474, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3475, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3476, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3477, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3478, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3479, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3480, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3481, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3482, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3483, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3484, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3485, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3486, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3487, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3488, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3489, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3490, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3491, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3492, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3493, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3494, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3495, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3496, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3497, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3498, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3499, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3500, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3501, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3502, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3503, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3504, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3505, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3506, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3507, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3508, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3509, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3510, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3511, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3512, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3513, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3514, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3515, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3516, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3517, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3518, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3519, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3520, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3521, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3522, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3523, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3524, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3525, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3526, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3527, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3528, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3529, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3530, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3531, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3532, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3533, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3534, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3535, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3536, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3537, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3538, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3539, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3540, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3541, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3542, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3543, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3544, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3545, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3546, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3547, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3548, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3549, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3550, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3551, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3552, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3553, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3554, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3555, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3556, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3557, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3558, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3559, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3560, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3561, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3562, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3563, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3564, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3565, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3566, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3567, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3568, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3569, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3570, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3571, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3572, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3573, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3574, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3575, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3576, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3577, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3578, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3579, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3580, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3581, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3582, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3583, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3584, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3585, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3586, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3587, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3588, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3589, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3590, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3591, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3592, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3593, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3594, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3595, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3596, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3597, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3598, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3599, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3600, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3601, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3602, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3603, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3604, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3605, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3606, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3607, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3608, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3609, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3610, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3611, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3612, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3613, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3614, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3615, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3616, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3617, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3618, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3619, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3620, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3621, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3622, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3623, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3624, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3625, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3626, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3627, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3628, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3629, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3630, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3631, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3632, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3633, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3634, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3635, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3636, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3637, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3638, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3639, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3640, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3641, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3642, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3643, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3644, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3645, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3646, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3647, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3648, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3649, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3650, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3651, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3652, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3653, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3654, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3655, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3656, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3657, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3658, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3659, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3660, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3661, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3662, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3663, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3664, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3665, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3666, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3667, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3668, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3669, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3670, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3671, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3672, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3673, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3674, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3675, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3676, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3677, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3678, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3679, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3680, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3681, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3682, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3683, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3684, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3685, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3686, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3687, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3688, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3689, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3690, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3691, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3692, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3693, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3694, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3695, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3696, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3697, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3698, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3699, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3700, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3701, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3702, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3703, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3704, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3705, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3706, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3707, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3708, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3709, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3710, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3711, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3712, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3713, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3714, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3715, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3716, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3717, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3718, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3719, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3720, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3721, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3722, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3723, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3724, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3725, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3726, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3727, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3728, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3729, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3730, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3731, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3732, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3733, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3734, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3735, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3736, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3737, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3738, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3739, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3740, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3741, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3742, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3743, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3744, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3745, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3746, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3747, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3748, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3749, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3750, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3751, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3752, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3753, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3754, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3755, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3756, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3757, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3758, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3759, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3760, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3761, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3762, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3763, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3764, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3765, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3766, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3767, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3768, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3769, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3770, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3771, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3772, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3773, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3774, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3775, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3776, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3777, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3778, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3779, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3780, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3781, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3782, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3783, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3784, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3785, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3786, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3787, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3788, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3789, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3790, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3791, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3792, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3793, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3794, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3795, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3796, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3797, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3798, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3799, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3800, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3801, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3802, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3803, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3804, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3805, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3806, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3807, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3808, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3809, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3810, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3811, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3812, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3813, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3814, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3815, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3816, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3817, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3818, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3819, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3820, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3821, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3822, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3823, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3824, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3825, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3826, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3827, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3828, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3829, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3830, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3831, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3832, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3833, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3834, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3835, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3836, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3837, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3838, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3839, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3840, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3841, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3842, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3843, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3844, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3845, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3846, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3847, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3848, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3849, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3850, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3851, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3852, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3853, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3854, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3855, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3856, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3857, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3858, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3859, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3860, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3861, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3862, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3863, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3864, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3865, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3866, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3867, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3868, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3869, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3870, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3871, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3872, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3873, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3874, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3875, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3876, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3877, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3878, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3879, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3880, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3881, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3882, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3883, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3884, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3885, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3886, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3887, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3888, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3889, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3890, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3891, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3892, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3893, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3894, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3895, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3896, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3897, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3898, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3899, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3900, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3901, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3902, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3903, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3904, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3905, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3906, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3907, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3908, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3909, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3910, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3911, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3912, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3913, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3914, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3915, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3916, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3917, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3918, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3919, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3920, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3921, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3922, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3923, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3924, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3925, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3926, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3927, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3928, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3929, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3930, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3931, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3932, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3933, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3934, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3935, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3936, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3937, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3938, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3939, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3940, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3941, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3942, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3943, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3944, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3945, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3946, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3947, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3948, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3949, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3950, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3951, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3952, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3953, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3954, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3955, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3956, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3957, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3958, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3959, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3960, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3961, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3962, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3963, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3964, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3965, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3966, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3967, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3968, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3969, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3970, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3971, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3972, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3973, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3974, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3975, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3976, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3977, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3978, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3979, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3980, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3981, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3982, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3983, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3984, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3985, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3986, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3987, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3988, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3989, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3990, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3991, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3992, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3993, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3994, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3995, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3996, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3997, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3998, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 3999, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4000, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4001, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4002, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4003, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4004, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4005, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4006, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4007, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4008, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4009, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4010, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4011, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4012, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4013, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4014, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4015, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4016, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4017, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4018, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4019, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4020, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4021, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4022, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4023, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4024, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4025, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4026, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4027, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4028, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4029, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4030, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4031, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4032, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4033, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4034, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4035, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4036, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4037, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4038, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4039, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4040, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4041, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4042, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4043, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4044, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4045, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4046, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4047, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4048, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4049, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4050, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4051, Loss 0.262982\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4052, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4053, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4054, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4055, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4056, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4057, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4058, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4059, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4060, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4061, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4062, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4063, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4064, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4065, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4066, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4067, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4068, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4069, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4070, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4071, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4072, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4073, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4074, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4075, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4076, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4077, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4078, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4079, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4080, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4081, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4082, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4083, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4084, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4085, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4086, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4087, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4088, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4089, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4090, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4091, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4092, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4093, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4094, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4095, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4096, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4097, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4098, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4099, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4100, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4101, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4102, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4103, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4104, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4105, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4106, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4107, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4108, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4109, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4110, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4111, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4112, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4113, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4114, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4115, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4116, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4117, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4118, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4119, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4120, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4121, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4122, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4123, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4124, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4125, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4126, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4127, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4128, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4129, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4130, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4131, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4132, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4133, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4134, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4135, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4136, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4137, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4138, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4139, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4140, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4141, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4142, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4143, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4144, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4145, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4146, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4147, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4148, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4149, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4150, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4151, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4152, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4153, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4154, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4155, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4156, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4157, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4158, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4159, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4160, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4161, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4162, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4163, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4164, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4165, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4166, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4167, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4168, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4169, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4170, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4171, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4172, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4173, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4174, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4175, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4176, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4177, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4178, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4179, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4180, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4181, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4182, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4183, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4184, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4185, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4186, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4187, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4188, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4189, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4190, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4191, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4192, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4193, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4194, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4195, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4196, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4197, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4198, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4199, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4200, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4201, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4202, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4203, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4204, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4205, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4206, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4207, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4208, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4209, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4210, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4211, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4212, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4213, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4214, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4215, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4216, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4217, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4218, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4219, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4220, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4221, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4222, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4223, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4224, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4225, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4226, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4227, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4228, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4229, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4230, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4231, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4232, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4233, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4234, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4235, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4236, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4237, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4238, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4239, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4240, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4241, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4242, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4243, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4244, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4245, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4246, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4247, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4248, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4249, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4250, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4251, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4252, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4253, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4254, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4255, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4256, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4257, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4258, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4259, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4260, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4261, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4262, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4263, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4264, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4265, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4266, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4267, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4268, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4269, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4270, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4271, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4272, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4273, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4274, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4275, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4276, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4277, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4278, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4279, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4280, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4281, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4282, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4283, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4284, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4285, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4286, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4287, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4288, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4289, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4290, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4291, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4292, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4293, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4294, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4295, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4296, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4297, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4298, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4299, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4300, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4301, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4302, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4303, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4304, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4305, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4306, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4307, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4308, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4309, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4310, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4311, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4312, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4313, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4314, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4315, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4316, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4317, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4318, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4319, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4320, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4321, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4322, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4323, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4324, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4325, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4326, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4327, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4328, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4329, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4330, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4331, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4332, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4333, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4334, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4335, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4336, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4337, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4338, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4339, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4340, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4341, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4342, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4343, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4344, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4345, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4346, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4347, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4348, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4349, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4350, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4351, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4352, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4353, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4354, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4355, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4356, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4357, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4358, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4359, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4360, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4361, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4362, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4363, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4364, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4365, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4366, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4367, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4368, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4369, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4370, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4371, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4372, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4373, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4374, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4375, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4376, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4377, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4378, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4379, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4380, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4381, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4382, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4383, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4384, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4385, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4386, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4387, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4388, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4389, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4390, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4391, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4392, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4393, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4394, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4395, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4396, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4397, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4398, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4399, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4400, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4401, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4402, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4403, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4404, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4405, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4406, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4407, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4408, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4409, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4410, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4411, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4412, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4413, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4414, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4415, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4416, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4417, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4418, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4419, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4420, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4421, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4422, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4423, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4424, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4425, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4426, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4427, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4428, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4429, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4430, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4431, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4432, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4433, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4434, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4435, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4436, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4437, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4438, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4439, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4440, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4441, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4442, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4443, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4444, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4445, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4446, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4447, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4448, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4449, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4450, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4451, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4452, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4453, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4454, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4455, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4456, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4457, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4458, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4459, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4460, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4461, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4462, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4463, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4464, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4465, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4466, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4467, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4468, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4469, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4470, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4471, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4472, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4473, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4474, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4475, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4476, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4477, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4478, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4479, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4480, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4481, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4482, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4483, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4484, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4485, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4486, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4487, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4488, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4489, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4490, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4491, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4492, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4493, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4494, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4495, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4496, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4497, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4498, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4499, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4500, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4501, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4502, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4503, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4504, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4505, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4506, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4507, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4508, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4509, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4510, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4511, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4512, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4513, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4514, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4515, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4516, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4517, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4518, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4519, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4520, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4521, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4522, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4523, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4524, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4525, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4526, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4527, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4528, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4529, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4530, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4531, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4532, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4533, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4534, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4535, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4536, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4537, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4538, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4539, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4540, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4541, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4542, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4543, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4544, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4545, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4546, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4547, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4548, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4549, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4550, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4551, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4552, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4553, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4554, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4555, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4556, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4557, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4558, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4559, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4560, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4561, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4562, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4563, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4564, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4565, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4566, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4567, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4568, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4569, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4570, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4571, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4572, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4573, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4574, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4575, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4576, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4577, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4578, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4579, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4580, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4581, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4582, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4583, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4584, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4585, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4586, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4587, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4588, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4589, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4590, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4591, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4592, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4593, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4594, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4595, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4596, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4597, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4598, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4599, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4600, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4601, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4602, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4603, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4604, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4605, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4606, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4607, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4608, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4609, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4610, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4611, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4612, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4613, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4614, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4615, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4616, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4617, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4618, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4619, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4620, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4621, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4622, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4623, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4624, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4625, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4626, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4627, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4628, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4629, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4630, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4631, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4632, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4633, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4634, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4635, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4636, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4637, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4638, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4639, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4640, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4641, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4642, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4643, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4644, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4645, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4646, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4647, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4648, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4649, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4650, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4651, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4652, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4653, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4654, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4655, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4656, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4657, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4658, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4659, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4660, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4661, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4662, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4663, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4664, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4665, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4666, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4667, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4668, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4669, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4670, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4671, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4672, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4673, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4674, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4675, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4676, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4677, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4678, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4679, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4680, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4681, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4682, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4683, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4684, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4685, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4686, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4687, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4688, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4689, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4690, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4691, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4692, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4693, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4694, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4695, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4696, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4697, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4698, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4699, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4700, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4701, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4702, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4703, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4704, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4705, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4706, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4707, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4708, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4709, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4710, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4711, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4712, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4713, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4714, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4715, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4716, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4717, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4718, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4719, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4720, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4721, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4722, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4723, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4724, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4725, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4726, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4727, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4728, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4729, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4730, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4731, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4732, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4733, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4734, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4735, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4736, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4737, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4738, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4739, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4740, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4741, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4742, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4743, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4744, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4745, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4746, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4747, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4748, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4749, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4750, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4751, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4752, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4753, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4754, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4755, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4756, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4757, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4758, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4759, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4760, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4761, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4762, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4763, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4764, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4765, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4766, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4767, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4768, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4769, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4770, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4771, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4772, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4773, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4774, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4775, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4776, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4777, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4778, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4779, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4780, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4781, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4782, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4783, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4784, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4785, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4786, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4787, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4788, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4789, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4790, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4791, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4792, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4793, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4794, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4795, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4796, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4797, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4798, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4799, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4800, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4801, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4802, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4803, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4804, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4805, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4806, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4807, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4808, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4809, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4810, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4811, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4812, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4813, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4814, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4815, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4816, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4817, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4818, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4819, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4820, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4821, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4822, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4823, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4824, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4825, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4826, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4827, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4828, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4829, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4830, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4831, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4832, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4833, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4834, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4835, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4836, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4837, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4838, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4839, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4840, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4841, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4842, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4843, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4844, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4845, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4846, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4847, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4848, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4849, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4850, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4851, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4852, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4853, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4854, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4855, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4856, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4857, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4858, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4859, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4860, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4861, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4862, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4863, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4864, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4865, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4866, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4867, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4868, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4869, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4870, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4871, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4872, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4873, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4874, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4875, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4876, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4877, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4878, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4879, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4880, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4881, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4882, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4883, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4884, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4885, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4886, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4887, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4888, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4889, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4890, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4891, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4892, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4893, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4894, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4895, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4896, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4897, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4898, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4899, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4900, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4901, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4902, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4903, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4904, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4905, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4906, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4907, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4908, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4909, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4910, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4911, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4912, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4913, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4914, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4915, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4916, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4917, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4918, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4919, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4920, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4921, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4922, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4923, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4924, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4925, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4926, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4927, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4928, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4929, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4930, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4931, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4932, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4933, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4934, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4935, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4936, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4937, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4938, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4939, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4940, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4941, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4942, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4943, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4944, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4945, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4946, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4947, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4948, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4949, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4950, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4951, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4952, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4953, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4954, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4955, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4956, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4957, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4958, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4959, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4960, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4961, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4962, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4963, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4964, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4965, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4966, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4967, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4968, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4969, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4970, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4971, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4972, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4973, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4974, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4975, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4976, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4977, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4978, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4979, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4980, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4981, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4982, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4983, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4984, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4985, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4986, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4987, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4988, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4989, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4990, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4991, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4992, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4993, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4994, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4995, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4996, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4997, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4998, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 4999, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n",
      "Epoch 5000, Loss 0.262982\n",
      "\t Params:  tensor([0.8410, 0.5997])\n",
      "\t Grad:  tensor([ 2.3842e-06, -2.7493e-06])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([0.8410, 0.5997])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params = training_loop(\n",
    "    n_epochs = 5000,\n",
    "    learning_rate = 1e-2,\n",
    "    params = torch.tensor([1.0, 0.0]),\n",
    "    x = x,\n",
    "    y = y)\n",
    "params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "statutory-correspondence",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([ 3.4552,  7.4454,  8.0987,  0.3458,  5.6478,  2.6982,  9.5794, 10.6228,\n",
       "          6.1169,  0.1090,  0.8963]),\n",
       " tensor(0.8410),\n",
       " tensor(0.5997),\n",
       " tensor([3.5057, 6.8616, 7.4111, 0.8906, 5.3498, 2.8690, 8.6564, 9.5340, 5.7443,\n",
       "         0.6913, 1.3535]))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yy = model(x, *params)\n",
    "w, b = params\n",
    "x, w, b, yy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "mighty-israeli",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAADHQAAAiJCAYAAADEY8BAAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8QVMy6AAAACXBIWXMAAFxGAABcRgEUlENBAAEAAElEQVR4nOzdeZRfdWE28Of+JjOTsCQhhN0IJKxhkSVBgSAJiAso1SpibcUFq1VBrVKtClWL4lIUrIIrVbBvFdBXhQIqSyKLIATCGvYkGHZCgCBJZiYz9/0j6KstyWQmc393ls/nnDke5t7f93mukoDnzJNblGUZAAAAAAAAAAAAAAAAmqdRdwEAAAAAAAAAAAAAAICRxqADAAAAAAAAAAAAAACgyQw6AAAAAAAAAAAAAAAAmsygAwAAAAAAAAAAAAAAoMkMOgAAAAAAAAAAAAAAAJrMoAMAAAAAAAAAAAAAAKDJDDoAAAAAAAAAAAAAAACazKADAAAAAAAAAAAAAACgyQw6AAAAAAAAAAAAAAAAmsygAwAAAAAAAAAAAAAAoMkMOgAAAAAAAAAAAAAAAJrMoAMAAAAAAAAAAAAAAKDJDDoAAAAAAAAAAAAAAACazKADAAAAAAAAAAAAAACgyQw6AAAAAAAAAAAAAAAAmsygAwAAAAAAAAAAAAAAoMkMOgAAAAAAAAAAAAAAAJrMoAMAAAAAAAAAAAAAAKDJDDoAAAAAAAAAAAAAAACazKADAAAAAAAAAAAAAACgyQw6AAAAAAAAAAAAAAAAmsygAwAAAAAAAAAAAAAAoMkMOgAAAAAAAAAAAAAAAJrMoAMAAAAAAAAAAAAAAKDJDDoAAAAAAAAAAAAAAACazKADAAAAAAAAAAAAAACgyQw6AAAAAAAAAAAAAAAAmsygAwAAAAAAAAAAAAAAoMkMOgAAAAAAAAAAAAAAAJrMoAMAAAAAAAAAAAAAAKDJDDoAAAAAAAAAAAAAAACazKADAAAAAAAAAAAAAACgyQw6AAAAAAAAAAAAAAAAmsygAwAAAAAAAAAAAAAAoMkMOgAAAAAAAAAAAAAAAJrMoAMAAAAAAAAAAAAAAKDJDDoAAAAAAAAAAAAAAACazKADAAAAAAAAAAAAAACgyQw6AAAAAAAAAAAAAAAAmsygAwAAAAAAAAAAAAAAoMkMOgAAAAAAAAAAAAAAAJrMoAMAAAAAAAAAAAAAAKDJDDoAAAAAAAAAAAAAAACazKADAAAAAAAAAAAAAACgyQw6AAAAAAAAAAAAAAAAmsygAwAAAAAAAAAAAAAAoMkMOgAAAAAAAAAAAAAAAJrMoAMAAAAAAAAAAAAAAKDJDDoAAAAAAAAAAAAAAACazKADAAAAAAAAAAAAAACgyQw6AAAAAAAAAAAAAAAAmsygAwAAAAAAAAAAAAAAoMkMOgAAAAAAAAAAAAAAAJrMoAMAAAAAAAAAAAAAAKDJDDoAAAAAAAAAAAAAAACabFTdBQCGqqIoHk0y/gUudSZZ3Nw2AAAAAAAAAAAAADBkTUrS9gLff7osyy2bXaZZirIs6+4AMCQVRbEySXvdPQAAAAAAAAAAAABgmOooy3J03SWq0qi7AAAAAAAAAAAAAAAAwEhj0AEAAAAAAAAAAAAAANBkBh0AAAAAAAAAAAAAAABNZtABAAAAAAAAAAAAAADQZKPqLgAwhHUmaf+f32xvb8+UKVNqqAMAAAAAAAAAAAAAQ8/999+fjo6OF7rU2ewuzWTQAdB/i5NM/Z/fnDJlSu64444a6gAAAAAAAAAAAADA0LPbbrtl/vz5L3RpcbO7NFOj7gIAAAAAAAAAAAAAAAAjjUEHAAAAAAAAAAAAAABAkxl0AAAAAAAAAAAAAAAANJlBBwAAAAAAAAAAAAAAQJMZdAAAAAAAAAAAAAAAADSZQQcAAAAAAAAAAAAAAECTGXQAAAAAAAAAAAAAAAA0mUEHAAAAAAAAAAAAAABAkxl0AAAAAAAAAAAAAAAANJlBBwAAAAAAAAAAAAAAQJMZdAAAAAAAAAAAAAAAADSZQQcAAAAAAAAAAAAAAECTGXQAAAAAAAAAAAAAAAA0mUEHAAAAAAAAAAAAAABAkxl0AAAAAAAAAAAAAAAANJlBBwAAAAAAAAAAAAAAQJMZdAAAAAAAAAAAAAAAADSZQQcAAAAAAAAAAAAAAECTGXQAAAAAAAAAAAAAAAA0mUEHAAAAAAAAAAAAAABAkxl0AAAAAAAAAAAAAAAANJlBBwAAAAAAAAAAAAAAQJMZdAAAAAAAAAAAAAAAADSZQQcAAAAAAAAAAAAAAECTGXQAAAAAAAAAAAAAAAA0mUEHAAAAAAAAAAAAAABAkxl0AAAAAAAAAAAAAAAANJlBBwAAAAAAAAAAAAAAQJMZdAAAAAAAAAAAAAAAADSZQQcAAAAAAAAAAAAAAECTjaq7AFCtoihGJZmSZLskGyfZKMnKJMuSPJLk7rIsl9dWEAAAAAAAAAAAAABgBDLogGGoKIo9kvx1ksOT7JWkbS23l0VR3Jvkl0kuSHJFWZZl5SUBAAAAAAAAAAAAAEYwgw5IUhTFdkmm/dnXvknGr+0zZVkWlRfro6IoXpXkn5PM7MvHkuz0/NcHk9xTFMVpSb5blmX3gJcEAAAAAAAAAAAAAMCgg5GnKIoX5X+PNybWWmo9FUWxTZKvJ3nDABy3U5JvJvmHoijeW5bl7wbgTAAAAAAAAAAAAAAA/oxBB8NaURRbJJmevxxwbFFrqQFWFMVBSX6SZPMBPvolSa4qiuJDZVl+c4DPBgAAAAAAAAAAAAAY0Qw6GO5+ldXDhGGpKIq/SnJ+ktaKIlqTnFkUxbZlWf5zRRkAAAAAAAAAAAAAACNOo+4CQP8URXFYknNT3Zjjz328KIqTmpADAAAAAAAAAAAAADAiGHTAEFQUxXZJzkvSvg6335bkY0n2TzIxqwcg45PskeTvk1yWpFyHc/71+TeCAAAAAAAAAAAAAACwnkbVXQDom6IoRmX1mznG93LrY0mOL8vy/Be49szzX7cn+V5RFNOTfCvJPr2c+f2iKPYqy/L3fWsNAAAAAAAAAAAAAMCf84YO+N8WJfl13SXW4rgk+/Vyzy1J9lnDmON/KcvyhiQHJPlRL7dukuT0dTkTAAAAAAAAAAAAAIA184YORrrFSeYmufH5/5xbluWTRVFsl2RhncVeSFEUmyX5TC+33ZfksLIsn+jL2WVZdhRF8bYkGyT5q7Xc+oaiKF5RluVlfTkfAAAAAAAAAAAAAID/z6CDkeThPD/ayOoBxw19HT0MAickGbeW651J3tzf5yrLsrsoircnuTnJdmu59V+TGHQAAAAAAAAAAAAAAPSTQQfD3deTPJbVb954tO4y66MoirFJ3tvLbaeXZTlvfXLKsnymKIoPJfnFWm7bvyiKg8qyvGp9sgAAAAAAAAAAAAAARqpG3QWgSmVZnlWW5X8P9THH896etb+d4+kknx+IoLIsL0jS21jjgwORBQAAAAAAAAAAAAAwEhl0wNDxtl6uf6csy2UDmPeVXq6/riiKtQ1MAAAAAAAAAAAAAABYA4MOGAKKotgxyfRebvvuAMdemOSRtVxvT/LGAc4EAAAAAAAAAAAAABgRDDpgaHhdL9dvLMvyvoEMLMuyJ8l5vdzWWy8AAAAAAAAAAAAAAF6AQQcMDa/o5fpFFeX2du6soihaKsoGAAAAAAAAAAAAABi2DDpgkCuKYlSSl/dy22UVxV+VZOVaro9LMr2ibAAAAAAAAAAAAACAYWtU3QWAXu2WZMO1XO9Kcn0VwWVZriyKYl6S/ddy2/Qk11WRDwAAAAAAAAAAAMAw19OdLLknefjm5PH5ycqnk1UdSXdn0tKWjGpPRo9PNp+abL13MnHHpNFSc2kYGAYdMPjt08v1+WVZdlSYPzdrH3TsXWE2AAAAAAAAAAAAAMNJWSaLrk7uvjh56Kbk0VuTruXr/vnWDZMt90i22SfZ+fBkuxlJUVTXFypk0AGD3169XL+14vzezjfoAAAAAAAAAAAAAGDtVjyd3PLjZO5Zq9/I0V9dzyWLr1v9dd2ZycSdkmnHJi95SzJm/EC1haYw6IDBb6dert9bcf59vVzfseJ8AAAAAAAAAAAAAIaqpQuSq09Pbju/b2/iWFdL7kl++fHk8s8mexyVzPhwMmHywOdABRp1FwB6tX0v13sbXKyv3s7fsCiKzSruAAAAAAAAAAAAAMBQ0r0qufq05IyXJTedXc2Y4891LV+dc8bLVg9IerqrzYMBYNABg1hRFEWSbXu57eGKazyapKeXe3obnQAAAAAAAAAAAAAwUjxxd/Ifr0wu+0zS3dHc7O6O5LJPJ2e9cnUPGMQMOmBw2yTJ6F7uebTKAmVZrkryZC+3bV1lBwAAAAAAAAAAAACGgJ6e5JqvJd86KHnoxnq7PDR3dY9rvra6FwxCBh0wuG26Dvc8XnmL5LFerq9LTwAAAAAAAAAAAACGq+6u5GfvTS79l+a/lWNNujtW9/nZe1f3g0FmVN0FgLWasA73LKu8Re8Z69KzaYqi+ECS9zchakoTMgAAAAAAAAAAAAAGt66VyfnvSO65pO4mL+y285KOZ5OjfpC0jq67DfyJQQcMbpv0cn1FWZbdTejxbC/XB9WgI8lmSabWXQIAAAAAAAAAAABg2OvuGtxjjj+655LkJ+9M3nxO0tJadxtIkjTqLgCsVW8TwOea0iL5Qy/XTRUBAAAAAAAAAAAARpqenuTn7x/8Y44/uvvi1X17eupuAkkMOmCwa+vl+qqmtOg9p7eeAAAAAAAAAAAAAAw31349ue28ulv0zW3nJdd+o+4WkMSgAwY7gw4AAAAAAAAAAAAABp8n7k6u+HzdLfrnis+t7g81M+iAwa23X6PdTWnRe05LU1oAAAAAAAAAAAAAUL/uVcnP35d0d9TdpH+6O5Kfvz/padaP4sILG1V3AWCtenszRrN+DfeW09WUFuvuiSTzm5AzJUl7E3IAAAAAAAAAAAAABo9rv5E8dGPdLdbPQ3OT3349mfHhupswghl0wODW2cv1Zv0abu3lem89m6osyzOSnFF1TlEUdySZWnUOAAAAAAAAAAAAwKCxdEEy+5S6WwyM2ackU49MJkyuuwkjVKPuAsBa9fbmi7amtBhigw4AAAAAAAAAAAAAKnL16Ul3R90tBkZ3x+rngZoYdMDg9oderm/UlBbJxr1c760nAAAAAAAAAAAAAEPdiqeT286vu8XAuu38ZOUzdbdghDLogMFtaS/XW4uiGN2EHmN7ud5bTwAAAAAAAAAAAACGult+nHQtr7vFwOpavvq5oAYGHTC4PbkO94yvusQ6ZKxLTwAAAAAAAAAAAACGqrJMbvhe3S2qccP3Vj8fNJlBBwxuS9bhni0rb9F7hkEHAAAAAAAAAAAAwHC26OrkyXvrblGNJfckD1xTdwtGIIMOGMTKslye3scSW1TZoSiKDZJs3MttD1TZAQAAAAAAAAAAAICa3X1x3Q2qddcwfz4GJYMOGPwW9XJ924rz1+X8RRV3AAAAAAAAAAAAAKBOD91Ud4NqPTzMn49ByaADBr+FvVzfseL8HXq5/tjzbxIBAAAAAAAAAAAAYDjq6U4evbXuFtV65NbVzwlNZNABg98dvVzfueL83s7vrR8AAAAAAAAAAAAAQ9mSe5KuYf7nf3c9lyy5t+4WjDAGHTD49fb+pr0rzt+nl+vzKs4HAAAAAAAAAAAAoE4P31x3g+Z45Oa6GzDCGHTA4NfboONFRVFsXmH+vr1cN+gAAAAAAAAAAAAAGM4en193g+YYKc/JoGHQAYNcWZYPJnmgl9tmVpFdFMXWSXbq5barq8gGAAAAAAAAAAAAYJBY+XTdDZpjxdN1N2CEMeiAoeGyXq4fVlHuK3q5fm9Zlr2NTQAAAAAAAAAAAAAYylZ11N2gOUbKczJoGHTA0HBpL9ePLIqipYLcN/Vy/dcVZAIAAAAAAAAAAAAwmHR31t2gOboNOmgugw4YGi5Ksnwt1zdP72/T6JOiKCYkeVUvt50/kJkAAAAAAAAAAAAADEItbXU3aI6W9robMMIYdMAQUJblH5Jc0Mttxw9w7D8kWds/fRcnuXKAMwEAAAAAAAAAAAAYbEaNkKHDSHlOBg2DDhg6/qOX64cXRbHXQAQVRbFReh+InFOWZTkQeQAAAAAAAAAAAAAMYqPH192gOcaMr7sBI4xBBwwRZVlemuTWtdxSJDl9gOI+kWTLtVzvSPL1AcoCAAAAAAAAAAAAYDDbfGrdDZpjpDwng4ZBBwwtX+rl+sFFUfzj+gQURXFAko/1ctsPyrJ8bH1yAAAAAAAAAAAAABgitt6r7gbNsdVedTdghDHogKHlR0lu6OWeLxVF8br+HF4UxY5JfpJk1FpuezbJZ/pzPgAAAAAAAAAAAABD0MSdktYN6m5RrdYNk4k71t2CEcagA4aQsizLJMclKddyW2uS84uieHdfzi6K4sAkv0myVS+3frYsy0f7cjYAAAAAAAAAAAAAQ1ijJdlyz7pbVGurPVc/JzTR2v4UfhgWiqJ4eZKd+vixTdfh3D4NJp73m7Is7+3H5/6kLMvri6L4QpJPruW29iTfLYrijUn+pSzLNb7VoyiKbZN8PMnfp/ffE36T5PS+NQYAAAAAAAAAAABgyNtmn2TxdXW3qM7W+9TdgBHIoIOR4F1J3l7Bud/tx2femWS9Bh3P+5ckM5K8vJf7Xp3k1UVR3JXkquezlyXZMMmkJC9N8rIkxTpkPp7krWVZdve3NAAAAAAAAAAAAABD1M6HJ9edWXeL6uxyeN0NGIEMOmAIKsuyuyiK1yeZneQl6/CRXZ7/6q+nk7yqLMuH1+MMAAAAAAAAAAAAAIaq7WYkm+6YPDkQf7b5IDNxp2TbA+tuwQjUqLsA0D9lWT6V5LAkcyuOejyrxxw3V5wDAAAAAAAAAAAAwGBVFMn0d9fdohrT3736+aDJDDpgCCvL8okkByU5p6KIG5JMK8vy+orOBwAAAAAAAAAAAGCoeMlbktYN6m4xsFo3WP1cUAODDhjiyrJcWZbl25O8NsmCATr22SQfSbJ/WZaLB+hMAAAAAAAAAAAAAIayMeOTPY6qu8XA2uOoZPS4ulswQhl0wDBRluVFSXZJ8rasfrNGfzyQ5BNJtivL8rSyLLsHqh8AAAAAAAAAAAAAw8CMDyct7XW3GBgt7aufB2oyqu4CULWyLN+R5B0112iKsiy7kvxnkv8simJSktckmZ5kapJtk4xNskGSjqx+C8cjSe5McnOSX5VleUsNtQEAAAAAAAAAAAAYKiZMTmZ9Mrns03U3WX+zPrn6eaAmBh0wTJVluTjJd57/AgAAAAAAAAAAAICBsf9xyZ0XJA/dWHeT/ttmWnLA8XW3YIRr1F0AAAAAAAAAAAAAAIAhpGVU8vpvJi3tdTfpn5b25PVnJo2Wupswwhl0AAAAAAAAAAAAAADQN5vtnBzyqbpb9M8hJ67uDzUz6AAAAAAAAAAAAAAAoO/2Pz7Z4811t+ibPd6c7H9c3S0giUEHAAAAAAAAAAAAAAD90Wgkrz8z2ek1dTdZNzsfvrpvw4/RMzj4OxEAAAAAAAAAAAAAgP5paU2O+sHgH3XsfHjypu+v7guDhEEHAAAAAAAAAAAAAAD91zo6OfqHyR5vrrvJC9vjzcmbz1ndEwYRgw4AAAAAAAAAAAAAANZPS2vyhm8nh/1r0tJed5vVWtqTw05e3cubORiEDDoAAAAAAAAAAAAAAFh/jUZy4IeSf7gq2WbfertsM211jwM/uLoXDEL+zgQAAAAAAAAAAAAAYOBstnPyrl8nr/hs89/W0dK++i0hx/56dQ8YxEbVXQAAAAAAAAAAAAAAgGGmZVQy48PJ1COTq09Pbjs/6VpeXV7rBskeR63OnDC5uhwYQAYdAAAAAAAAAAAAAABUY8Lk5Mh/T155cnLLj5MbvpcsuWfgzp+4UzL93clL3pKMHjdw50ITGHQAAAAAAAAAAAAAAFCt0eOSl7432e89yQPXJHddnDx8U/LILX17c0frhslWeyZb75Pscniy7YFJUVTXGypk0AEAAAAAAAAAAAAAQHMURbLdjNVfSdLTnSy5N3nk5uTx+cmKp5NVHUl3R9LSnoxqT8aMTzafmmy1VzJxx6TRUl9/GEAGHQAAAAAAAAAAAAAA1KPRkmy+y+ovGGEadRcAAAAAAAAAAAAAAAAYabyhAwAAAAAAAAAAAACAWnT3dGfhMwszf+n83PfUfVnWuSwd3R3p6ulKa6M17S3tGds2NjtsskN223S3bDd2u7Q0WuquDQPCoAMAAAAAAAAAAAAAgKYoyzJzH5ubK35/Re548o7ctfSurFi1Yp0/P2bUmOwyYZfstuluOeTFh2TaFtNSFEWFjaE6Bh0AAAAAAAAAAAAAAFRqWeeyXHj/hTn37nOz8JmF/T5nxaoVmff4vMx7fF7+887/zPbjts/ROx+d1015Xca2jR3AxlA9gw4AAAAAAAAAAAAAACqxeNninHX7Wbl44cV9ehPHulr4zMJ88fov5ms3fS2Hb394jt392EwaO2nAc6AKjboLAAAAAAAAAAAAAAAwvKzqWZWzbjsrr//F6/PTe39ayZjjz61YtSI/vfenef0vXp//uP0/0t3TXWkeDASDDgAAAAAAAAAAAAAABsyCpxfkmEuOyek3nZ7Ons6mZnf2dOa0G0/LMZcckwVPL2hqNvSVQQcAAAAAAAAAAAAAAOutp+zJ92//fo668KjctuS2WrvcuuTWHHXhUfn+7d9PT9lTaxdYk1F1FwAAAAAAAAAAAAAAYGjr6unKSdeclIsWXFR3lT/p7OnMV2/8au5+6u6cfODJaW201l0J/oI3dAAAAAAAAAAAAAAA0G8d3R35yOyPDKoxx5+7aMFF+cjsj6Sju6PuKvAXDDoAAAAAAAAAAAAAAOiXrp6unDDnhMx5cE7dVdZqzoNzcsJvTkhXT1fdVeBPDDoAAAAAAAAAAAAAAOiznrInJ11z0qAfc/zRnMVzctI1J6Wn7Km7CiQx6AAAAAAAAAAAAAAAoB/OvuPsXLTgorpr9MlFCy7KOXecU3cNSGLQAQAAAAAAAAAAAABAHy14ekG+Me8bddfol6/P+3oWPL2g7hpg0AEAAAAAAAAAAAAAwLpb1bMqJ15zYjp7Ouuu0i+dPZ056ZqT0t3TXXcVRjiDDgAAAAAAAAAAAAAA1tk588/JbUtuq7vGerl1ya05e/7ZdddghDPoAAAAAAAAAAAAAABgnSxetjhnzDuj7hoD4ox5Z2TxssV112AEM+gAAAAAAAAAAAAAAGCdnHX7Wens6ay7xoDo7OnMWbefVXcNRjCDDgAAAAAAAAAAAAAAerWsc1kuXnhx3TUG1MULL86znc/WXYMRyqADAAAAAAAAAAAAAIBeXXj/hVmxakXdNQbUilUrcsH9F9RdgxHKoAMAAAAAAAAAAAAAgLUqyzI/vuvHddeoxLl3n5uyLOuuwQhk0AEAAAAAAAAAAAAAwFrNfWxuFi1bVHeNSix8ZmHmPja37hqMQAYdAAAAAAAAAAAAAACs1RW/v6LuCpWavXh23RUYgQw6AAAAAAAAAAAAAABYqzuevKPuCpW6Y8nwfj4GJ4MOAAAAAAAAAAAAAADWqLunO3ctvavuGpW6c+md6e7prrsGI4xBBwAAAAAAAAAAAAAAa7TwmYVZsWpF3TUqtWLViixatqjuGowwBh0AAAAAAAAAAAAAAKzR/KXz667QFPOfHBnPyeBh0AEAAAAAAAAAAAAAwBrd99R9dVdoinufvrfuCowwBh0AAAAAAAAAAAAAAKzRss5ldVdoimUdI+M5GTwMOgAAAAAAAAAAAAAAWKOO7o66KzRFZ3dn3RUYYQw6AAAAAAAAAAAAAABYo66errorNEVnj0EHzWXQAQAAAAAAAAAAAADAGrU2Wuuu0BRtjba6KzDCGHQAAAAAAAAAAAAAALBG7S3tdVdoirYWgw6ay6ADAAAAAAAAAAAAAIA1Gts2tu4KTTG2fWQ8J4OHQQcAAAAAAAAAAAAAAGu0wyY71F2hKXYcv2PdFRhhDDoAAAAAAAAAAAAAAFijqROm1l2hKaZuOjKek8HDoAMAAAAAAAAAAAAAgDXaftz2GTNqTN01KjVm1JhsN3a7umswwhh0AAAAAAAAAAAAAACwRi2NluwyYZe6a1Rq1wm7pqXRUncNRhiDDgAAAAAAAAAAAAAA1upFG+xUd4VK7TZxt7orMAKNqrsAAAAAAAAAAAAAAACDU8eq7rz236/Ogj+MzQbb1t2mOrMmzaq7AiOQQQcAAAAAAAAAAAAAAP/LGbPvy7/96u7n/2pyujs2S0v7E7V2qsL247bPtC2m1V2DEcigAwAAAAAAAAAAAACAP7n1wadz5Deu+R/fLdL11MvSsuWFtXSq0tE7H52iKOquwQhk0AEAAAAAAAAAAAAAQJ7rWJWDvjw7S5/rfMHrXc/sk/bNf5mi0dXkZtUZM2pMjpxyZN01GKEMOgAAAAAAAAAAAAAARriT/3t+zrp64dpv6hmTrmf2Ttsm1zenVBMcvv3h2bht47prMEIZdAAAAAAAAAAAAAAAjFC/vW9J3vq9363z/Z1PHpzWcTelaKyqsFVztDXacuzux9ZdgxHMoAMAAAAAAAAAAAAAYIR56rnO7H3ypX3+XNm1aTqfOCztW1xSQavm+sDeH8iksZPqrsEIZtABAAAAAAAAAAAAADBClGWZD597c35x88P9PqNz6YyMGnt7WsYsHsBmzbXnxD3z9qlvr7sGI1yj7gIAAAAAAAAAAAAAAFTvl7c/ku0/cfF6jTlWa8nKh49K2TM03y/Q1mjLyQeenJZGS91VGOGG5q8gAAAAAAAAAAAAAADWySPPrMj+X7hiQM/s6dw8HU8cltFbXDKg5zbD8Xsfn8njJ9ddAww6AAAAAAAAAAAAAACGo+6eMsf8x+9yzX1PVnJ+19KD0jL6kbSOu7mS86twxOQjcsxux9RdA5IkjboLAAAAAAAAAAAAAAAwsH58/e8z5ZMXVzbmWK2RX/3dmZn5opkVZgycmZNm5uQDT06j8GP0DA7e0AEAAAAAAAAAAAAAMEwseOIPOeQrv6k854y37pMj9twqSXLqzFNzwpwTMufBOZXn9tfMSTNz6sGnprXRWncV+BPTIgAAAAAAAAAAAACAIa5zVU9effqVlY85XrvnVln4hcP/NOZIkvaW9nx11ldzxOQjKs3uryMmH5Gvzvxq2lva664Cf8EbOgAAAAAAAAAAAAAAhrBvzrk/X/rlXZXn3HjiK7LpRi88imhttOaUGadk5012zjfmfSOdPZ2V9+lNW6Mtx+99fI7Z7Zg0Cu9CYPAx6AAAAAAAAAAAAAAAGIJuf+iZvPbrV1ee88Nj98tBO27W632NopF37v7OHPyig3PiNSfmtiW3Vd5tTfacuGdOPvDkTB4/ubYO0BuDDgAAAAAAAAAAAACAIWR556q8/Muzs+QP1b4F4x0HbJfPHLlbnz83efzknPOac3LO/HNyxrwzmvq2jrZGW47b+7gcM/WYtDRampYL/WHQAQAAAAAAAAAAAAAwRJxy8Z35zpULKs0YO3pUfvuJQ7NRe/9/3HxUY1Tetfu7ctiLD8tZt5+VixdenBWrVgxgy780ZtSYHL794Tl292MzaeykynJgIBl0AAAAAAAAAAAAAAAMcr+9f0ne+t3fVZ7zs/cfkL1fvMmAnTdp7KR85oDP5KPTPpoL7r8g5959bhY+s3DAzt9+3PY5euejc+SUI7Nx28YDdi40g0EHAAAAAAAAAAAAAMAg9fTyzuz1r5dWnvORw3bKBw/dsbLzN27bOH+769/mrbu8NXMfm5vZi2fnjiV35M6ld/bpzR1jRo3JrhN2zW4Td8usSbMybYtpKYqist5QJYMOAAAAAAAAAAAAAIBBpizLfOS8W/KzeQ9VmjN54oa5+EMHZXRrS6U5f1QURaZvOT3Tt5yeJOnu6c6iZYsy/8n5uffpe7OsY1k6uzvT2dOZtkZb2lraMrZ9bHYcv2Ombjo1243dLi2N5nSFqhl0AAAAAAAAAAAAAAAMIr+649G894c3Vp5z6T++PDtusXHlOWvT0mjJlPFTMmX8lFp7QB0MOgAAAAAAAAAAAAAABoFHn1mZl33h8spzTv6r3fK2/berPAdYO4MOAAAAAAAAAAAAAIAa9fSUefv3r89V9y6pNGe/7SbkR+95WVoaRaU5wLox6AAAAAAAAAAAAAAAqMl5NyzOx356a+U5V398Vl60yQaV5wDrzqADAAAAAAAAAAAAAKDJFi55LrNOnVN5ztf/Zu+87iVbV54D9J1BBwAAAAAAAAAAAABAk3Su6slfnXFN7nxkWaU5h++xZc546z4piqLSHKD/DDoAAAAAAAAAAAAAAJrg27+5P1+45K7Kc2488RXZdKP2ynOA9WPQAQAAAAAAAAAAAABQodsfeiav/frVleec/a79cvBOm1WeAwwMgw4AAAAAAAAAAAAAgAos71yVg/9tTp54tqPSnLe9bNuc/PrdK80ABp5BBwAAAAAAAAAAAADAAPvCJXfm279ZUGnGxu2j8ttPHJKNR7dWmgNUw6ADAAAAAAAAAAAAAGCAXLfgybzlO9dVnvPT9x2QfbfdpPIcoDoGHQAAAAAAAAAAAAAA6+np5Z3Z5+RL01NWm/PhV+yYD79ip2pDgKYw6AAAAAAAAAAAAAAA6KeyLHPC+bfmpzc9WGnOdptukF9++OUZ3dpSaQ7QPAYdAAAAAAAAAAAAAAD98Os7Hs17fnhj9Tn/+PLstMXGlecAzWXQAQAAAAAAAAAAAADQB48tW5mXnnJ55TmfPXK3vP2A7SrPAeph0AEAAAAAAAAAAAAAsA56esq88wc35Df3PFFpzr7bbpJz3/OyjGppVJoD1MugAwAAAAAAAAAAAACgF+fPXZx/+smtledc9bFZmTRhg8pzgPoZdAAAAAAAAAAAAAAArMGiJc9l5qlzKs/52lv2yl/ttU3lOcDgYdABAAAAAAAAAAAAAPA/dHX35A1nXpPbH1pWac6rdtsi3/q7fVMURaU5wOBj0AEAAAAAAAAAAAAA8Ge+e+WCfP7iOyvPueFTr8hmG7dXngMMTgYdAAAAAAAAAAAAAABJ7nj4mRzx71dXnvP9d07PrJ03rzwHGNwMOgAAAAAAAAAAAACAEW1FZ3dmnTonjy5bWWnO3770xfn8G/aoNAMYOgw6AAAAAAAAAAAAAIAR60u/vCvfnHN/pRljWlvyu08dmrGjWyvNAYYWgw4AAAAAAAAAAAAAYMS5fuHSvPnb11ae89P37Z99t51QeQ4w9Bh0AAAAAAAAAAAAAAAjxjPLu7Lv5y7Nqp6y0pwPHrJDPvLKnSvNAIY2gw4AAAAAAAAAAAAAYNgryzIf+8mtOf/GByvNefGEDfLrf3x5Rre2VJoDDH0GHQAAAAAAAAAAAADAsHbZ/Mfy7nPmVp7zyw8flF22HFt5DjA8GHQAAAAAAAAAAAAAAMPS48tWZr9TLq885zOvm5p3HLh95TnA8GLQAQAAAAAAAAAAAAAMKz09ZY49+4bMvvuJSnP2fvH4nP/e/TOqpVFpDjA8GXQAAAAAAAAAAAAAAMPGT298MB89/5bKc6762KxMmrBB5TnA8GXQAQAAAAAAAAAAAAAMeQ88+VwO/rc5leecfvReef3e21SeAwx/Bh0AAAAAAAAAAAAAwJDV1d2TN37zt7n1wWcqzTls6hb59t/tm0ajqDQHGDkMOgAAAAAAAAAAAACAIel7Vy3I5y66s/Kc6z91aDbfeHTlOcDIYtABAAAAAAAAAAAAAAwp8x9elsP//arKc77/jumZtcvmlecAI5NBBwAAAAAAAAAAAAAwJKzs6s4hp87Jw8+srDTnb/ablFPesEeKoqg0BxjZDDoAAAAAAAAAAAAAgEHv3351V86YfX+lGe2jGrn+U6/IuDGtleYAJAYdAAAAAAAAAAAAAMAgdsOipTnqW9dWnnP+P+yf6dtNqDwH4I8MOgAAAAAAAAAAAACAQeeZFV2Z/rnL0tndU2nOcbN2yAmv2rnSDIAXYtABAAAAAAAAAAAAAAwaZVnmn396W86du7jSnG3Gj8nlHz04o1tbKs0BWBODDgAAAAAAAAAAAABgULjirsfyrh/MrTznkg8dlF23Glt5DsDaGHQAAAAAAAAAAAAAALV6/NmV2e/zl1eec9Jrp+bYGdtXngOwLgw6AAAAAAAAAAAAAIBa9PSUec8P5+ayOx+vNOclk8bnJ/+wf1pbGpXmAPSFQQcAAAAAAAAAAAAA0HQ/m/dg/vHcWyrP+c0/zcy2m25YeQ5AXxl0AAAAAAAAAAAAAABN8/snl+fl/za78pyvvvkl+et9XlR5DkB/GXQAAAAAAAAAAAAAAJXr6u7Jm751bW5Z/HSlOYfusnm+e8y0NBpFpTkA68ugAwAAAAAAAAAAAACo1PevWZjPXji/8pzrP3loNh87uvIcgIFg0AEAAAAAAAAAAAAAVOLOR5blNV+7qvKcs94+LYfuukXlOQADyaADAAAAAAAAAAAAABhQK7u684qv/iYPPrWi0pyjp03KF9+4R4qiqDQHoAoGHQAAAAAAAAAAAADAgPnKr+/O16+4r9KM1pYic088LOPGtFaaA1Algw4AAAAAAAAAAAAAYL3NXbQ0b/rWtZXnnPfe/bPf9hMqzwGomkEHAAAAAAAAAAAAANBvy1Z2ZfrnLkvHqp5Kc943c0o+/updKs0AaCaDDgAAAAAAAAAAAAAGl57uZMk9ycM3J4/PT1Y+nazqSLo7k5a2ZFR7Mnp8svnUZOu9k4k7Jo2WmkuPPGVZ5pM/uz0/uv73leZsNW50rvjozIxp878xMLwYdAAAAAAAAAAAAABQr7JMFl2d3H1x8tBNyaO3Jl3L1/3zrRsmW+6RbLNPsvPhyXYzkqKori+ZfdfjeecPbqg856IPzshuW4+rPAegDgYdAAAAAAAAAAAAANRjxdPJLT9O5p61+o0c/dX1XLL4utVf152ZTNwpmXZs8pK3JGPGD1RbkjzxbEemf/6yynNOPGLXvPugyZXnANTJoAMAAAAAAAAAAACA5lq6ILn69OS28/v2Jo51teSe5JcfTy7/bLLHUcmMDycTjAPWR1mWec8Pb8yl8x+rNGePbcbl/77/gLS2NCrNARgMDDoAAAAAAAAAAAAAaI7uVcm1X09mfyHp7qg+r2t5ctPZq98CMuuTyQHHJ42W6nOHmZ/PeygfPvfmynPmnDAz203csPIcgMHCoAMAAAAAAAAAAACA6j1xd/Lz9yUP3dj87O6O5LJPJ3demLz+zGSznZvfYQhavHR5Dvry7Mpz/u1Ne+aoaZMqzwEYbAw6AAAAAAAAAAAAAKhOT8/qt3Jc8fnmvJVjbR6am3zroOSQTyX7H580GvX2GaRWdffkqG9fm3m/f7rSnFk7b5az3j49jUZRaQ7AYGXQAQAAAAAAAAAAAEA1uruSn78/ue28upv8f90dyaX/kjx6++q3dbS01t1oUDn7t4vy6QvuqDznd588NFuMHV15DsBgZtABAAAAAAAAAAAAwMDrWpmc/47knkvqbvLCbjsv6Xg2OeoHSathwd2PPptXnX5l5TnfPWZaDpu6ReU5AEOBQQcAAAAAAAAAAAAAA6u7a3CPOf7onkuSn7wzefM5I/ZNHSu7uvPK067M75curzTnqH1flC+/ac8URVFpDsBQYtABAAAAAAAAAAAAwMDp6Ul+/v7BP+b4o7svXt33Dd9OGo262zTVaZfek69dfm+lGS2NIjedeFjGbTAyBzMAa2PQAQAAAAAAAAAAAMDAufbryW3n1d2ib247L9lyj+TAD9bdpClufOCpvPGbv60858fveVleNnnTynMAhiqDDgAAAAAAAAAAAAAGxhN3J1d8vu4W/XPF55KdXpVstnPdTSqzbGVXXnbK5Vne2V1pznsPnpxPvGbXSjMAhgODDgAAAAAAAAAAAADWX/eq5OfvS7o76m7SP90dyc/fnxz766TRUnebAfepn92W//O731eascXY9sw+YWY2aPMjygDrwu+WAAAAAAAAAAAAAKy/a7+RPHRj3S3Wz0Nzk99+PZnx4bqbDJg5dz+ed3z/hspz/vv4Gdl9m3GV5wAMJwYdAAAAAAAAAAAAAKyfpQuS2afU3WJgzD4lmXpkMmFy3U3Wy5I/dGTa5y6rPOcTr9kl7z14SuU5AMORQQcAAAAAAAAAAAAA6+fq05PujrpbDIzujtXPc+S/192kX8qyzPv+86b88o5HK82ZutXY/PwDB6ZtVKPSHIDhzKADAAAAAAAAAAAAgP5b8XRy2/l1txhYt52fvPLkZPS4upv0yS9ufigf+vHNlefMPmFmtp+4YeU5AMOdQQcAAAAAAAAAAAAA/XfLj5Ou5XW3GFhdy1c/10vfW3eTdbJ46fIc9OXZled8+Y175s3TJ1WeAzBSGHQAAAAAAAAAAAAA0D9lmdzwvbpbVOOG7yX7vScpirqbrNGq7p685TvXZe4DT1Wa8/KdNssP3jE9jcbg/e8CYCgy6AAAAAAAAAAAAACgfxZdnTx5b90tqrHknuSBa5LtZtTd5AWdc+2i/Msv7qg857pPHJotx42uPAdgJDLoAAAAAAAAAAAAAKB/7r647gbVuuviQTfouOexZ/PK066sPOfbb9s3r9pty8pzAEYygw4AAAAAAAAAAAAA+uehm+puUK2HB8/zrezqzqtPvzKLnlxeac5f77NNvnLUS1IURaU5ABh0AAAAAAAAAAAAANAfPd3Jo7fW3aJaj9y6+jkbLbXW+Npl9+a0y+6pNKMoknknHZbxG7RVmgPA/2fQAQAAAAAAAAAAAEDfLbkn6ar2bRG163ouWXJvsvkutcTf9Pun8tdn/rbynB/9/cuy/5RNK88B4C8ZdAAAAAAAAAAAAADQdw/fXHeD5njk5qYPOp5d2ZX9v3BF/tCxqtKc97x8cj55+K6VZgCwZgYdAAAAAAAAAAAAAPTd4/PrbtAcTX7Of/nF7Tnn2gcqzZi4UXuu/NjMbNDmR4kB6uR3YQAAAAAAAAAAAAD6buXTdTdojhVPNyXmynueyDH/cX3lOf99/Izsvs24ynMA6J1BBwAAAAAAAAAAAAB9t6qj7gbNUfFzPvmHjuz7ucsqzUiSj796l7xv5pTKcwBYdwYdAAAAAAAAAAAAAPRdd2fdDZqju5pBR1mW+cB/3ZSLb3u0kvP/aJctN84Fx81I26hGpTkA9J1BBwAAAAAAAAAAAAB919JWd4PmaGkf8CMvvOXhHP+jeQN+7v90xUcPzuTNNqo8B4D+MegAAAAAAAAAAAAAoO9GDfzQYVAawOd88KnlmfGl2QN23pp86Y175OjpL648B4D1Y9ABAAAAAAAAAAAAQN+NHl93g+YYM369j+juKfM337ku1y9auv591mLGDhNz9rv2S0ujqDQHgIFh0AEAAAAAAAAAAABA320+te4GzbGez/nD6x7IST+/fYDKrNm1nzgkW40bU3kOAAPHoAMAAAAAAAAAAACAvtt6r7obNMdWe/XrY/c+9mwOO+3Kge3yAr71d/vk1btvVXkOAAPPoAMAAAAAAAAAAACAvpu4U9K6QdK1vO4m1WndMJm4Y58+0rGqO6/52lVZ8MRzFZVa7fV7bZ3Tjt4rRVFUmgNAdQw6AAAAAAAAAAAAAOi7Rkuy5Z7J4uvqblKdrfZc/Zzr6OuX35uvXHpPhYVWm3fSYdlkw7bKcwColkEHAAAAAAAAAAAAAP2zzT7De9Cx9T7rdNvNi5/O68+4puIyyX+9+6U5YIeJlecA0BwGHQAAAAAAAAAAAAD0z86HJ9edWXeL6uxy+Fov/6FjVfb/wuV5duWqSmscO2P7nPTaqZVmANB8Bh0AAAAAAAAAAAAA9M92M5JNd0yevHdAj+1OsrC1NfPb23Jfa2uWtTTSURTpStKapL0sM7a7Jzt0dWW3jo5s17UqLQPaIMnEnZJtD1zj5c9ccEd+8NtFA536FzbdsC1XfmxWNmz3I78Aw5Hf3QEAAAAAAAAAAADon6JIpr87+eXH1+uYMsnc0e25YoMxuaO9LXe1tWVFo7HOnx/T05NdOjuzW0dnDlm+ItNWdqRYr0ZZ/VzF/z7lqnufyNvOun59T+/VBccdmD1fNL7yHADqY9ABAAAAAAAAAAAAQP+95C3J5Z9Nupb3+aPLGkUu3GjDnLvxxlnY1trvCisajcwbPTrzRo/Of44bm+07u3L0s8/mdX94LmN7yr4f2LrB6uf6M0uf68w+J1/a747r6p9etXM+MGuHynMAqJ9BBwAAAAAAAAAAAAD9N2Z8ssdRyU1nr/NHFo8albPGjc3FG23QpzdxrKuFba354qYT8rVNxufwPyzPsc8sy6RVq9b9gD2OSkaPS5KUZZnjfjQvF936yID3/HM7bbFRLjx+RtpHtVSaA8DgYdABAAAAAAAAAAAAwPqZ8eHklh8n3R1rvW1VkrPHbZwzx49PZ6OovNaKRiM/HbtRLtxow3zg6afz9meeTa9ziZb21c+T5KJbH8kH/uumqmvm8o8enCmbbVR5DgCDi0EHAAAAAAAAAAAAAOtnwuRk1ieTyz69xlsWtI7KiRM3zW2j25tYbLXORpHTJmySyzfYICcveTKTu9byto5Zn8xDja1y4D9fVHmvU96wR9760hdXngPA4DTw76gCAAAAAAAAAAAAYOTZ/7hkm33/17d7knx/3MY5auutahlz/LlbR7fnqK23yvfHbZyeF7hebjMtf3PHtBz4xSsq7XHAlE1z/ymHG3MAjHDe0AEAAAAAAAAAAADA+msZlbz+m8m3Dkq6O5IkXUlO2mzTXLTRhvV2+zOdjSJfnbBJ7m5ry8lPPJnW57/f3WjLKxccnfvLZyrN/+0/H5Ktx4+pNAOAocEbOgAAAAAAAAAAAAAYGJvtnBzyqSRJR5F8ZPOJg2rM8ecu2mjDfGTziekoVv/1FzvelPvLbSrLO/Nv98miLx5hzAHAn3hDBwAAAAAAAAAAAAADZ//j0/XIrTnh8TmZs+EGdbdZqzkbbpATMjEHP7JTvtd9eCUZr3vJ1vn3t+yVoigqOR+AocugAwAAAAAAAAAAAIAB01MkJ222aeY8N7jHHH80Z8MNctlmm6Z8eODPvumkwzJhw7aBPxiAYcGgAwAAAAAAAAAAAIABc/YdZ+eiRZfUXaNPRo27Ja0rt0nX0pcPyHn/eexLM2PHiQNyFgDDl0EHAAAAAAAAAAAAAANiwdML8o1536i7Rr+0b/brdP9hl/R0bt7vM95xwHb5zJG7DWArAIYzgw4AAAAAAAAAAAAA1tuqnlU58ZoT09nTWXeVfikaqzJ66/OzfNH7kjT69NnxG7Tm6o8fko3a/WguAOvOPzUAAAAAAAAAAAAAWG/nzD8nty25re4a66VlzOK0TbgqnUsPXufP/PwDB2avSeOrKwXAsGXQAQAAAAAAAAAAAMB6Wbxscc6Yd0bdNQZE22aXpuvZ3VN2bbrW+0545U457pAdm9QKgOHIoAMAAAAAAAAAAACA9XLW7Wels6ez7hoDomisStumv0nHo3/9gtd32HyjXPTBGWkf1dLkZgAMNwYdAAAAAAAAAAAAAPTbss5luXjhxXXXGFCt4+al4/HDk57Rf/H9yz7y8uyw+cY1tQJguDHoAAAAAAAAAAAAAKDfLrz/wqxYtaLuGgOqaHSlddyN6XrqwCTJ59+we/72pdvW3AqA4cagAwAAAAAAAAAAAIB+KcsyP77rx3XXqETrJtdl7/GvzY/+fv+0NIq66wAwDBl0AAAAAAAAAAAAANAvcx+bm0XLFtVdoxIt7U/kn17VaswBQGUadRcAAAAAAAAAAAAAYGi64vdX1F2hUrMXz667AgDDmEEHAAAAAAAAAAAAAP1yx5N31F2hUncsGd7PB0C9DDoAAAAAAAAAAAAA6LPunu7ctfSuumtU6s6ld6a7p7vuGgAMUwYdAAAAAAAAAAAAAPTZwmcWZsWqFXXXqNSKVSuyaNmiumsAMEwZdAAAAAAAAAAAAADQZ/OXzq+7QlPMf3JkPCcAzWfQAQAAAAAAAAAAAECf3ffUfXVXaIp7n7637goADFMGHQAAAAAAAAAAAAD02bLOZXVXaIplHSPjOQFoPoMOAAAAAAAAAAAAAPqso7uj7gpN0dndWXcFAIYpgw4AAAAAAAAAAAAA+qyrp6vuCk3R2WPQAUA1DDoAAAAAAAAAAAAA6LPWRmvdFZqirdFWdwUAhimDDgAAAAAAAAAAAAD6rL2lve4KTdHWYtABQDUMOgAAAAAAAAAAAADos7FtY+uu0BRj20fGcwLQfAYdAAAAAAAAAAAAAPTZDpvsUHeFpthx/I51VwBgmDLoAAAAAAAAAAAAAKDPpk6YWneFppi66ch4TgCaz6ADAAAAAAAAAAAAgD7bftz2GTNqTN01KjVm1JhsN3a7umsAMEwZdAAAAAAAAAAAAADQZy2NluwyYZe6a1Rq1wm7pqXRUncNAIYpgw4AAAAAAAAAAAAA+mXL9h3rrlCp3SbuVncFAIaxUXUXAAAAAAAAAAAAAGBoWd65Ki//8pw81TMuG2xbd5vqzJo0q+4KAAxjBh0AAAAAAAAAAAAArLNTLr4z37lywfN/NTndHZulpf2JWjtVYftx22faFtPqrgHAMGbQAQAAAAAAAAAAAECvrr3/yfzNd6/7H98t0vXUy9Ky5YW1dKrS0TsfnaIo6q4BwDBm0AEAAAAAAAAAAADAGj29vDN7/eula7ze9cw+ad/8lykaXU1sVa0xo8bkyClH1l0DgGHOoAMAAAAAAAAAAACA/6Usy3z0vFvyf+c9tPYbe8ak65m907bJ9c0p1gSHb394Nm7buO4aAAxzBh0AAAAAAAAAAAAA/IVf3/Fo3vPDG9f5/s4nD07ruJtSNFZV2Ko52hptOXb3Y+uuAcAIYNABAAAAAAAAAAAAQJLksWUr89JTLu/z58quTdP5xGFp3+KSClo11wf2/kAmjZ1Udw0ARgCDDgAAAAAAAAAAAIARrqenzDt+cEOuvOeJfp/RuXRGRo29PS1jFg9gs+bac+KeefvUt9ddA4ARolF3AQAAAAAAAAAAAADqc97cxZn8yYvXa8yxWktWPnxUyp6h+eeNtzXacvKBJ6el0VJ3FQBGiKH5T0wAAAAAAAAAAAAA1svCJc9l1qlzBvTMns7N0/HEYRm9xSUDem4zHL/38Zk8fnLdNQAYQQw6AAAAAAAAAAAAAEaQzlU9ef0Z12T+I8sqOb9r6UFpGf1IWsfdXMn5VThi8hE5Zrdj6q4BwAjTqLsAAAAAAAAAAAAAAM3x3SsXZKcTL6lszLFaI7Pf/q3MfNHMCjMGzsxJM3PygSenUfixWgCayxs6AAAAAAAAAAAAAIa52x96Jq/9+tWV53z/ndMza+fNkySnzjw1J8w5IXMenFN5bn/NnDQzpx58alobrXVXAWAEMiUEAAAAAAAAAAAAGKZWdHbnpadcVvmY429f+uIs+uIRfxpzJEl7S3u+OuurOWLyEZVm99cRk4/IV2d+Ne0t7XVXAWCE8oYOAAAAAAAAAAAAgGHoC5fcmW//ZkGlGWNaW/K7Tx2asaNf+A0XrY3WnDLjlOy8yc75xrxvpLOns9I+66Kt0Zbj9z4+x+x2TBqFPxsdgPoYdAAAAAAAAAAAAAAMI79b8GSO/s51lef89H37Z99tJ/R6X6No5J27vzMHv+jgnHjNibltyW2Vd1uTPSfumZMPPDmTx0+urQMA/JFBBwAAAAAAAAAAAMAw8Mzyrux98q/TU1ab88FDdshHXrlznz83efzknPOac3LO/HNyxrwzmvq2jrZGW47b+7gcM/WYtDRampYLAGtj0AEAAAAAAAAAAAAwhJVlmX/6ya35yY0PVpozacKYXPqPB2d0a/8HEaMao/Ku3d+Vw158WM66/axcvPDirFi1YgBb/qUxo8bk8O0Pz7G7H5tJYydVlgMA/WHQAQAAAAAAAAAAADBEXTr/sfz9OXMrz/nlhw/KLluOHbDzJo2dlM8c8Jl8dNpHc8H9F+Tcu8/NwmcWDtj524/bPkfvfHSOnHJkNm7beMDOBYCBZNABAAAAAAAAAAAAMMQ8vmxl9jvl8spzPv26qXnngdtXdv7GbRvnb3f927x1l7dm7mNzM3vx7Nyx5I7cufTOPr25Y8yoMdl1wq7ZbeJumTVpVqZtMS1FUVTWGwAGgkEHAAAAAAAAAAAAwBDR01PmXWffkDl3P1Fpzt4vHp/z37t/RrU0Ks35o6IoMn3L6Zm+5fQkSXdPdxYtW5T5T87PvU/fm2Udy9LZ3ZnOns60NdrS1tKWse1js+P4HTN106nZbux2aWm0NKUrAAwUgw4AAAAAAAAAAACAIeAnNz6YE86/pfKcK/9pVl686QaV56xNS6MlU8ZPyZTxU2rtAQBVMugAAAAAAAAAAAAAGMQWLXkuM0+dU3nO6UfvldfvvU3lOQDAagYdAAAAAAAAAAAAAINQV3dP3nDmNbn9oWWV5hw2dYt8++/2TaNRVJoDAPwlgw4AAAAAAAAAAACAQeZ7Vy3I5y66s/Kc6z91aDbfeHTlOQDA/2bQAQAAAAAAAAAAADBIzH94WQ7/96sqz/n+O6Zn1i6bV54DAKyZQQcAAAAAAAAAAABAzVZ0dueQr8zJI8+srDTnb/ablFPesEeKoqg0BwDonUEHAAAAAAAAAAAAQI2+/Mu7cuac+yvNaB/VyPWfekXGjWmtNAcAWHcGHQAAAAAAAAAAAAA1uH7h0rz529dWnnP+P+yf6dtNqDwHAOgbgw4AAAAAAAAAAACAJnpmRVemfe7SdHWXleYcN2uHnPCqnSvNAAD6z6ADAAAAAAAAAAAAoAnKsszHf3przpv7YKU524wfk8s/enBGt7ZUmgMArB+DDgAAAAAAAAAAAICKXX7nYzn27LmV51zyoYOy61ZjK88BANafQQcAAAAAAAAAAABARR5/dmX2+/zlleec9NqpOXbG9pXnAAADx6ADAAAAAAAAAAAAYID19JT5+3Pm5vK7Hq805yUvGpefvO+AtLY0Ks0BAAaeQQcAAAAAAAAAAADAAPq/Nz2Yj5x3S+U5v/mnmdl20w0rzwEAqmHQAQAAAAAAAAAAADAAHnjyuRz8b3Mqz/nKUS/JG/d9UeU5AEC1DDoAAAAAAAAAAAAA1kNXd0/e9M3f5pYHn6k05xW7bp7vvG1aGo2i0hwAoDkMOgAAAAAAAAAAAAD66ayrF+bk/55fec71nzw0m48dXXkOANA8Bh0AAAAAAAAAAAAAfXTnI8vymq9dVXnOWW+flkN33aLyHACg+Qw6AAAAAAAAAAAAANbRyq7uHPqV3+Shp1dUmnP0tEn54hv3SFEUleYAAPUx6AAAAAAAAAAAAABYB6f+6u58Y/Z9lWa0thSZe+JhGTemtdIcAKB+Bh0AAAAAAAAAAAAAa3HDoqU56lvXVp5z3nv3z37bT6g8BwAYHAw6AAAAAAAAAAAAAF7AspVdmf65y9KxqqfSnPfNnJKPv3qXSjMAgMHHoAMAAAAAAAAAAADgz5RlmU/+7Lb86PrFleZsNW50rvjozIxpa6k0BwAYnAw6AAAAAAAAAAAAAJ43+67H884f3FB5zkUfnJHdth5XeQ4AMHgZdAAAAAAAAAAAAAAj3uPPrsx+n7+88pwTj9g17z5ocuU5AMDgZ9ABAAAAAAAAAAAAjFg9PWXe88Mbc9mdj1Was8c24/J/339AWlsaleYAAEOHQQcAAAAAAAAAAAAwIv183kP58Lk3V54z54SZ2W7ihpXnAABDi0EHAAAAAAAAAAAAMKL8/snlefm/za4859SjXpI37fuiynMAgKHJoAMAAAAAAAAAAAAYEVZ19+RN37o2Ny9+utKcWTtvlrPePj2NRlFpDgAwtBl0AAAAAAAAAAAAAMPe969ZmM9eOL/ynN998tBsMXZ05TkAwNBn0AEAAAAAAAAAAAAMW3c9uiyvPv2qynO+e8y0HDZ1i8pzAIDhw6ADAAAAAAAAAAAAGHZWdnXnsNN+k8VLV1Sac9S+L8qX37RniqKoNAcAGH4MOgAAAAAAAAAAAIBh5au/vjv/fsV9lWa0NIrcdOJhGbdBa6U5AMDwZdABAAAAAAAAAAAADAs3PrA0b/zmtZXn/Pg9L8vLJm9aeQ4AMLwZdAAAAAAAAAAAAABD2rKVXXnp5y/Piq7uSnPee/DkfOI1u1aaAQCMHAYdAAAAAAAAAAAAwJD1qZ/dlv/zu99XmrHF2PbMPmFmNmjzY5cAwMDxbxYAAAAAAAAAAADAkDP77sfzzu/fUHnOfx8/I7tvM67yHABg5DHoAAAAAAAAAAAAAIaMJ57tyPTPX1Z5zides0vee/CUynMAgJHLoAMAAAAAAAAAAAAY9MqyzD/854351R2PVZozdaux+fkHDkzbqEalOQAABh0AAAAAAAAAAADAoPaLmx/Kh358c+U5s0+Yme0nblh5DgBAYtABAAAAAAAAAAAADFKLly7PQV+eXXnOl9+0Z948bVLlOQAAf86gAwAAAAAAAAAAABhUVnX35OjvXJcbH3iq0pyX77RZfvCO6Wk0ikpzAABeiEEHAAAAAAAAAAAAMGic/dtF+fQFd1Sec90nDs2W40ZXngMAsCYGHQAAAAAAAAAAAEDt7n702bzq9Csrz/n22/bNq3bbsvIcAIDeGHQAAAAAAAAAAAAAtVnZ1Z1XnX5lHnhyeaU5f73PNvnKUS9JURSV5gAArCuDDgAAAAAAAAAAAKAWp116T752+b2VZhRFMu+kwzJ+g7ZKcwAA+sqgAwAAAAAAAAAAAGiqm37/VP76zN9WnvOjv39Z9p+yaeU5AAD9YdABAAAAAAAAAAAANMWzK7uy/xeuyB86VlWa856XT84nD9+10gwAgPVl0AEAAAAAAAAAAABU7qSf354fXvdApRkTN2rPlR+bmQ3a/HgkADD4+TcWAAAAAAAAAAAAoDK/ueeJvP0/rq8857+Pn5HdtxlXeQ4AwEAx6AAAAAAAAAAAAAAG3JI/dGTa5y6rPOfjr94l75s5pfIcAICBZtABw1xRFK1JtkuyVZLNkoxJ0pqkM8mKJEuSPJJkUVmWXTXVBAAAAAAAAAAAhomyLPP+/3NTLrn90Upzdtly41xw3Iy0jWpUmgMAUBWDDhhmiqLYMMnhSQ5NcmCSnbN6wNGbrqIo7kpydZLLk1xSluXyyooCAAAAAAAAAADDzoW3PJzjfzSv8pwrPnpwJm+2UeU5AABVMuiAYaIoit2TfDTJUUk27McRrUn2eP7rfUn+UBTFuUlOLcvyrgErCgAAAAAAAAAADDsPPrU8M740u/KcL71xjxw9/cWV5wAANINBBwxxRVFsmeRLSd6WpBjAozdKcmySdxVF8R9J/rksyyUDeD4AAAAAAAAAADDEdfeUect3rs0Ni56qNOegHSfmB+/cLy2NgfwRKQCAehl0wBBWFMXhSc5OMrHKmKwedhxRFMXflWV5eYVZAAAAAAAAAADAEPHD6x7IST+/vfKcaz9xSLYaN6byHACAZjPogCGqKIr3JflGkkaTIrdM8suiKI4ty/KcJmUCAAAAAAAAAACDzL2PPZvDTruy8pxv/d0+efXuW1WeAwBQF4MOGIKKonhnkjNriB6V5AdFUawsy/K8GvIBAAAAAAAAAICarOzqzuFfuyoLljxXac7r99o6px29V4qiqDQHAKBuBh0wxBRFMS3Jt/vwkblJLklyTZL7kixN8mySsUk2SbJLkgOSvDbJnutSIcnZRVHcUZblHX3oAQAAAAAAAAAADFFfu+zenHbZPZXnzDvpsGyyYVvlOQAAg4FBBwwhRVGMSnJ2ktZ1uP3qJJ8oy/LqNVxf+vzX/UkuSvKpoigOTfLFJNN6OXt0Vr+pY7+yLMt1Kg8AAAAAAAAAAAw5837/VN5w5m8rz/mvd780B+wwsfIcAIDBxKADhpZjkkxdh/tOTvLZsiy7+3J4WZaXF0VxQFaPOj7Sy+3Tkhyd5Md9yQAAAAAAAAAAAAa/P3Ssyv5fuDzPrlxVac6xM7bPSa9dlx+JAgAYfgw6YGj50Drc84WyLP+lvwFlWXYl+ejzbwP5YC+3fzgGHQAAAAAAAAAAMKx8+he35+xrH6g0Y9MN23Llx2Zlw3Y/xggAjFz+TQiGiKIodk+yZy+3XZ3kUwMU+Y9JXpZkv7Xc89KiKKaUZXn/AGUCAAAAAAAAAAA1uereJ/K2s66vPOeC4w7Mni8aX3kOAMBgZ9ABQ8eh63DPJ8qyLAcirCzLnqIo/jnJFb3c+ookBh0AAAAAAAAAADBELX2uM/ucfGnlOf/0qp3zgVk7VJ4DADBUGHTA0LFPL9fvLsvy6oEMLMtydlEU9yVZ2/+Lmpbk2wOZCwAAAAAAAAAAVK8syxz3o3m56NZHKs3ZaYuNcuHxM9I+qqXSHACAocagA4aOKb1c/3VFub/K2gcdJvMAAAAAAAAAADDEXHTrI/nAf91Uec7lHz04UzbbqPIc/h97dx6mV12YDfg5M5ksIEkIYRdNwhJI2GWVHUEhUYpWxGpdEJcqYK21yuoWgbQidQFcKlWobRHUUjEB2VUQZIeQsCdRdgghBEgyM5k53x+j/WyVrO95z7wz931d8wc5Z87znF5tSsg87w8AaEUGHdA61l/J9Xsqyl3Zc8dWlAsAAAAAAAAAADTY44uWZp/p11aec8Zbd8i79nxN5TkAAK3MoANax7CVXF9QUe6zK7k+oqJcAAAAAAAAAACgQXp6y7zrX27Ob+YtrDRnn602yIUf2DPtbUWlOQAAA4FBB7SOF1Zy/eWKclf23MUV5QIAAAAAAAAAAA3wg5t/m1MvvbfynF+feHA2G+3zYQEAVpVBB7SO51ZyfYOKclf23JX1AgAAAAAAAAAAavDwMy/mkLN/WXnON9+9aw7fYdPKcwAABhqDDmgdc5IcuoLrm1SUu7Lnzq0oFwAAAAAAAAAAWAOdy3sy5Wu/yiPPvlxpzhE7bZavvXPnFEVRaQ4AwEBl0AGt41dJ/nYF1/dL8tUKcvdfyfUbKsgEAAAAAAAAAADWwDnXPpSzrnyw8pw7Tjs0Y9YdWnkOAMBAZtABrePaJMuSDH+F6wcXRTGsLMvORgUWRTEiycEruKU3yXWNygMAAAAAAAAAANbMXY8uypHn3lh5zr9/cM/ss9XYynMAAAYDgw5oEWVZPl8Uxb8nOfYVbhmd5KNp7CkdJyQZuYLrl5Vl+VgD8wAAAAAAAAAAgNXwUufy7PuP12bRku5Kc47ZZ1w+95bJlWYAAAw2Bh3QWs5K8p4kr3RW4clFUVxSluXjaxtUFMVrk5y4ktvOXtscAAAAAAAAAABgzXzhstn53o3zK81Yf52O/OozB+dVw/y4IQBAo/k3LGghZVneXxTFF5N86RVu2TDJz4qi2L8syxfXNKcoijFJLk+y/gpu+15Zlr9c0wwAAAAAAAAAAGDN3PDQgvz1+b+pPOe/j9snO20xuvIcAIDByqADWs/0JPsneeMrXN85ya1FURxdluXdq/vwoij2TPKfScav4LZHkvzd6j4bAAAAAAAAAABYcwtf7squ066qPOdTb9wmxx+8deU5AACDnUEHtJiyLHuKojgyfSdoHPAKt01McktRFP+e5GurMuwoimL3JJ9I8o6s+PeGx5IcUpblC6vTGwAAAAAAAAAAWDNlWebjF92Vy+5+otKcrTZ6VWZ8fN8MG9JeaQ4AAH0MOqAFlWW5tCiKw5J8JcnHXuG2oUmOSXJMURRPJLkxyUNJnk/yUpL1kqyfvvHHPkk2XoXoO5IcVZbl/LV6gYoVRXFcXvl/Lo20ZRMyAAAAAAAAAAAYxC6f9WQ++u93VJ5z9Sf3z1YbrVd5DgAA/59BB7SosiyXJTmuKIqfJfnHJDus4PbNkhy1FnFdSb6e5JSyLLvW4jnNsmGSSXWXAAAAAAAAAACANfXEoqV5/fRrK885/a3b5917vrbyHAAA/pRBB7S4siwvL4riiiRHJvlAkkOSDG/Q4xcn+Y8kZ5Rl+WiDngkAAAAAAAAAALyCnt4yf/3d3+Smuc9VmrPn+DH5jw/tlfa2otIcAABemUEHDABlWZZJ/qsoivuSvDvJp7J2o47uJP+U5PSyLJc2oCIAAAAAAAAAALAS/3nL73LST2ZVnnPjiQdn89EjKs8BAGDFDDqgxRVFMSTJu5J8OsnkBj22I8kpST5YFMWlSf65LMsHGvRsAAAAAAAAAADgjzz8zEs55OxfVJ5zzrt2yZt33KzyHAAAVo1BB7SwoiimJjknybiKIjZO8pEkHyqK4idJTizL8pGKsgAAAAAAAAAAYFDpXN6TN3/9hjz0zEuV5rx5x03zjb/aJUVRVJoDAMDqMeiAFlQUxYgkX0ny0SZFtiV5e5LDiqL427Is/7VJuWvq2SRzmpCzZZJhTcgBAAAAAAAAAGCAOfe6h/Plnz9Qec7tpx6SDV7lR1wAAPojgw5oMb8fc/wsycGrcHtPkmuT/DLJjUkeS/JcksVJRiUZk2SLJPsk2f/3z2xbwfNeleT8oiheV5blcWv6DlUry/LcJOdWnVMUxewkk6rOAQAAAAAAAABg4LjnsUU54pwbK8/5t2P3yH5bb1h5DgAAa86gA1pIURRDk/w0Kx9zdCf5TpKzy7Kc+wr3PPf7r4fSN/pIURRbJvlkkg9nxb8/fKwoirIsy+NXoz4AAAAAAAAAAAxaL3cuz37/dF0WvtxVac77Xz8unz9icqUZAAA0hkEHtJYvJDlkJff8NsnRZVn+ZnUfXpblI0mOK4ri35NclL7TO17JcUVR3FuW5bdWNwcAAAAAAAAAAAaTaT+bk/NvmFdpxsjhQ/Lrk96QVw3zY4EAAK3Cv7lBiyiK4vVJPr2S2x5K8vqyLBesTVZZlr8uiuJ1SW5KsuUKbj2rKIqrfj8EAQAAAAAAAAAA/sivH16Qd313tT+XdbX918den11es37lOQAANJZBB7SO6UnaVnB9YZKpazvm+IOyLJ8timJqkpuTjH6F29ZN8uUkb2tEJgAAAAAAAAAADATPv9yVXaZdVXnOJw/dJh9/w9aV5wAAUA2DDmgBRVHsnmS/ldz2+bIsH2pkblmWDxRF8cUkZ6/gtr8oimJLp3QAAAAAAAAAADDYlWWZv/vhXbn0ricqzZmw4bqZ+fH9MryjvdIcAACqZdABreEDK7n+aJLvVJR9XpJPJnn1K1xvS/KRJJ+uKB8AAAAAAAAAAPq9K+59Mn/zgzsqz7nq7/bP1huvV3kOAADVM+iA1nDQSq7/sCzLziqCy7LsLIri4vSNOl7JG6rIBgAAAAAAAACA/u6pF5ZlrzOvqTxn2pHb5z17vbbyHAAAmsegA/q5oig2SjJxJbddWXGNK7PiQcdORVGMLMtyccU9AAAAAAAAAACgX+jpLfO+f70lNzy8oNKcPcaNyX9+eK+0txWV5gAA0HwGHdD/jV+Fe26puMNvVnK9PcnWSW6vuAcAAAAAAAAAANTuh7f+Lp/58azKc274zEF59frrVJ4DAEA9DDqg/9tgJde7yrJ8ocoCZVkuKoqiO0nHCm5bWU8AAAAAAAAAAGhpc599KQd/5ReV53zjr3bJW3barPIcAADqZdAB/d/6K7n+XFNa9OVssoLrBh0AAAAAAAAAAAxIXct7c8Q5N+T+p16sNGfKDpvk3HftmqIoKs0BAKB/MOiA/q9nJdeHNaVFMnwl18umtAAAAAAAAAAAgCb65vWP5B+vuL/ynNtOPSRjX9WsHwUCAKA/MOiA/u/llVxfvyiK9rIsVzb8WGNFUXQkGb2S25ZUlQ8AAAAAAAAAAM127+Mv5M3fuKHynAs+sEcO2GbDynMAAOh/DDqg/3tqJdeLJJsn+V2FHV69Cvc8XWE+AAAAAAAAAAA0xZKu5dn/n67Lgpe6Ks15z16vzbQjt680AwCA/s2gA/q/eatwz8FJvl9hhzeswj2r0hMAAAAAAAAAAPqtM2bel+/8cm6lGesNG5Jfn3Rw1hveUWkOAAD9n0EH9HNlWS4oiuKxrPiUjMNS7aDj8JVcf6osy2cqzAcAAAAAAAAAgMr8+pEFede//KbynJ987PXZ9TXrV54DAEBrMOiA1vDrJO9YwfW3FUUxvizLhp+SURTFtkn+YiW33dToXAAAAAAAAAAAqNqiJV3Z+YtXVZ7ziUO2zicO2abyHAAAWotBB7SGn2bFg46OJNOS/HUF2acnaV/JPZdVkAsAAAAAAAAAAJUoyzJ/f/Hd+cmdj1eaM26DdXLFJ/bP8I6V/fgNAACDkUEHtIafJnkpyatWcM+7i6L4ZVmW32lUaFEUf5/kbSu5bVmSSxuVCQAAAAAAAAAAVfr57KfykX+7vfKcK/9u/2yz8XqV5wAA0LoMOqAFlGX5YlEU/5Lk71Zy67lFUSwuy/Kitc0siuIDSf5pFW79XlmWz69tHgAAAAAAAAAAVOmpF5ZlrzOvqTzni38xOe/de1zlOQAAtD6DDmgd/5TkA0lGreCeIUn+syiK/ZL8Q1mWS1Y3pCiK9ZJ8Pcn7V+H2l5OcuboZAAAAAAAAAADQLL29Zd7//VvzywefrTRnt9eun4s+vFeGtLdVmgMAwMBh0AEtoizLp4qiODHJN1fh9o8lObooivOSfLcsy9+t7BuKohif5MNJ/ibJ6FWsdWpZlo+u4r0AAAAAAAAAANBUF9/2aD79o3sqz/nVpw/KFmPWqTwHAICBxaADWkhZlt8qimL/JH+1CrdvkOS0JKcVRTE/yQ1JHkuyMMmLSUYmGZNkiyT7JnnNatb5SZKvreb3AAAAAAAAAABA5eYteDkHnXV95Tlfe+fO+YudN688BwCAgcmgA1rPB5Ksn+Sw1fiecb//apRrk7ynLMuygc8EAAAAAAAAAIC10rW8N39x7o2578nFlea8afLG+dZfvy5FUVSaAwDAwGbQAS2mLMtlRVEcmeQ7Sd5bQ4UfJjmmLMulNWQDAAAAAAAAAMCf9e1fPJIzL7+/8pxbTzkkG643rPIcAAAGPoMOaEFlWXYmeV9RFL9K8uUko5sQuzjJiWVZfrMJWQAAAAAAAAAAsEruffyFvPkbN1Se871jds9BEzeqPAcAgMHDoANaWFmW3y2K4qdJTknyoSQjKohZluRfk0wry/KpCp4PAAAAAAAAAACrbUnX8hzw5evz7Iudlea8e8/X5PS37lBpBgAAg5NBB7S4siyfSfK3RVFMS/JXv//aI0n7Wjy2N8mtSS5K8u9lWT671kUBAAAAAAAAAKBBzrz8vnz7F3MrzVhnaHtuPvkNGTm8o9IcAAAGL4MOGCDKslyQ5BtJvlEUxagk+yfZJcnkJK9NskmS9ZMMT9KRpDt9p288n+SpJL9NMifJXUl+WZbl801+BQAAAAAAAAAAWKGb5z6Xd37n5spzfvzRvfO6146pPAcAgMHNoAMGoLIsX0hy2e+/AAAAAAAAAACgpb2wpDu7TLsyvWW1OR9/w9b55KHbVBsCAAC/Z9ABAAAAAAAAAABAv1SWZf7hR/fkR7c/VmnOa8askyv/bv8M72ivNAcAAP6YQQcAAAAAAAAAAAD9zlVzns6HLryt8pwrPrFftt1kZOU5AADwfxl0AAAAAAAAAAAA0G88vXhZ9jzjmspzPv+WSXn/PuMrzwEAgFdi0AEAAAAAAAAAAEDtenvLfOCCW3P9A89WmrPLa0bnko/snSHtbZXmAADAyhh0AAAAAAAAAAAAUKsf3f5YPnXJ3ZXn/OrTB2WLMetUngMAAKvCoAMAAAAAAAAAAIBazF/wcg486/rKc7569M45cpfNK88BAIDVYdABAAAAAAAAAABAU3X39Oat592Yex9fXGnOoZM2zrf/+nVpaysqzQEAgDVh0AEAAAAAAAAAAEDT/Msv5+b0mfdVnnPLKW/IRusNrzwHAADWlEEHAAAAAAAAAAAAlZv9xAuZ+vUbKs/53vt3z0HbblR5DgAArC2DDgAAAAAAAAAAACqztKsnB511fZ5avKzSnL/aY4uc8dYdUhRFpTkAANAoBh0AAAAAAAAAAABU4h+vuD/fvP6RSjOGd7TlNycfklEjOirNAQCARjPoAAAAAAAAAAAAoKFumbcw7/j2TZXnXPI3e2f3cWMqzwEAgCoYdAAAAAAAAAAAANAQLyzpzuu+dFWW95aV5hx/0Fb51JsmVpoBAABVM+gAAAAAAAAAAABgrZRlmc/8+J5cfNtjleZsPnpErvn7AzK8o73SHAAAaAaDDgAAAAAAAAAAANbY1XOezgcvvK3ynMv/dr9st+nIynMAAKBZDDoAAAAAAAAAAABYbc8sXpY9zrim8pzT3jwpx+47vvIcAABoNoMOAAAAAAAAAAAAVllvb5kPXXhbrrn/mUpzdtpidH70N3uno72t0hwAAKiLQQcAAAAAAAAAAACr5Cd3PJZPXnx35Tm//IeD8poN1qk8BwAA6mTQAQAAAAAAAAAAwAr99rmXc8CXr6885+x37JS37frqynMAAKA/MOgAAAAAAAAAAADgz+ru6c3bv/nr3P3YC5XmHLLdRvnOe3ZLW1tRaQ4AAPQnBh0AAAAAAAAAAAD8ifNvmJdpP5tTec4tJ78hG40cXnkOAAD0NwYdAAAAAAAAAAAA/I85TyzOlK//qvKc89+3W96w3caV5wAAQH9l0AEAAAAAAAAAAECWdffk4LOuzxMvLKs05+jdtsj0v9whRVFUmgMAAP2dQQcAAAAAAAAAAMAg9+Wf359zr3uk0oyh7W259dRDMmpER6U5AADQKgw6AAAAAAAAAAAABqlb5y/MUd+6qfKciz+yd/YYP6byHAAAaCUGHQAAAAAAAAAAAIPMC0u7s/vpV6dreW+lOR87cMt8+rBtK80AAIBWZdABAAAAAAAAAAAwSJRlmZN+MisX3fpopTmbjRqea/7+wIwY2l5pDgAAtDKDDgAAAAAAAAAAgEHg2vufzge+f1vlOTM/vl8mbTay8hwAAGh1Bh0AAAAAAAAAAAAD2DMvLssep19Tec6pU7fLB/ebUHkOAAAMFAYdAAAAAAAAAAAAA1Bvb5kP/9vtufq+pyvN2fHVo/Ljj74+He1tleYAAMBAY9ABAAAAAAAAAAAwwPzXnY/l7354d+U5v/iHA/PaDdatPAcAAAYigw4AAAAAAAAAAIAB4nfPLcn+X76u8pyzjtopb3/dqyvPAQCAgcygAwAAAAAAAAAAoMUt7+nN2791U+56dFGlOQdN3DDnv2/3tLUVleYAAMBgYNABAAAAAAAAAADQwr5347x84bI5lef85uQ3ZOORwyvPAQCAwcKgAwAAAAAAAAAAoAXd/9TiHPbVX1We8y/v3S2HTtq48hwAABhsDDoAAAAAAAAAAABayLLunhxy9i/y2PNLK8056nWvzj+9fccURVFpDgAADFYGHQAAAAAAAAAAAEl6ensy74V5mbNwTh5+/uEs7lqczp7OdPd2p6OtI8Pah2Xk0JHZav2tMnmDyRk3clza29qb2vErVz6Qb1z7cKUZ7W1F7jj10Ixap6PSHAAAGOwMOgAAAAAAAAAAgEGpLMvc9vRtufZ312b2c7Nz/8L7s3T5qp96MWLIiGw7ZttM3mByDn7Nwdlt490qO83itvkL8/Zv3VTJs//YRR/eK3tN2KDyHAAAwKADAAAAAAAAAAAYZBZ3Lc5lj1yWHz7ww8x7Yd4aP2fp8qW585k7c+czd+YH9/0g40eNz9ETj85btnxLRg4d2Ziuy7qzx+lXZ1l3b0Oe90o+csCEnHT4dpVmAAAA/5tBBwAAAAAAAAAAMCg8uvjRnH/v+Zk5b+ZqncSxqua9MC/Tb5mer93xtUwZPyXHbn9sthi5xRo/7+T/mpX/+M3vGtjwT20ycniu+9SBGTG0vdIcAADgTxl0AAAAAAAAAAAAA9ry3uW5YPYFOe+u89LV21V53tLlS/Pjh36cyx65LMftclzeN+l9aW9b9cHEdQ88k2O+d2uFDfv87IR9s/3moyrPAQAA/jyDDgAAAAAAAAAAYMCau2huTr3x1MxaMKvp2V29Xfnn2/851/z2mkzbZ1omjJ6wwvuffbEzu59+deW9TpmyXT60/4q7AAAA1TPoAAAAAAAAAAAABpzesjcXzL4g59x5TlNO5ViRexbck6MuOyrH73J83jf5fWkr2v7X9bIs85F/uz1Xznm60h6TNxuZS4/bJx3tbSu/GQAAqJxBBwAAAAAAAAAAMKB093bntBtPy4y5M+qu8j+6erty9u1n54HnH8i0faalo60jSfLfdz2ev73orsrzr/vUgRk/dt3KcwAAgFVn0AEAAAAAAAAAAAwYnT2d+dT1n8r1j11fd5U/a8bcGXm56+V8Yscv5g1f+XXlef/09h3zjt22qDwHAABYfQYdAAAAAAAAAADAgNDd292vxxx/cP1j1+eq+z6S5K+TtFeSccA2G+Z77989bW1FJc8HAADWnkEHAAAAAAAAAADQ8nrL3px242n9fszxBx3r3ZdsdkmWPfGOJG0NffbNJ70hm4wa3tBnAgAAjWfQAQAAAAAAAAAAtLwLZl+QGXNn1F1jtXSMuis9yzZL98L9G/K877zndXnj5E0a8iwAAKB6Bh0AAAAAAAAAAEBLm7tobs6585y6a6yRYRtemZ6Xtk1v10Zr/Iy37bp5vnLUTimKooHNAACAqhl0AAAAAAAAAAAALWt57/KceuOp6ertqrvKGinalmf4ZpdkyfyPJmlbve8tkjtPOzSj1xlaTTkAAKBSBh0AAAAAAAAAAEDLunDOhZm1YFbdNdZK+4hHM3TMr9K18IBV/p7//NBe2XvLDSpsBQAAVM2gAwAAAAAAAAAAaEmPLn405955bt01GmLohlel+8XtU3aveKTx4f0n5OQp2zWpFQAAUCWDDgAAAAAAAAAAoCWdf+/56ertqrtGQxRtyzN0g1+k86m3/dnrY181LL/89IFZZ6gf+QIAgIHCv90DAAAAAAAAAAAtZ3HX4sycN7PuGg3VMerOdD4zJekd/r9+/Wcn7JvtNx9VUysAAKAqBh0AAAAAAAAAAEDLueyRy7J0+dK6azRU0dadjlG3p/v5fZIkJx6+bf7mgC1rbgUAAFTFoAMAAAAAAAAAAGgpZVnmovsvqrtGJTrWvzlbDj8sPz1u3wwd0lZ3HQAAoEIGHQAAAAAAAAAAQEu57enbMn/x/LprVKJ92LOZdsQIYw4AABgE/Fs/AAAAAAAAAADQUq793bV1V6jUdY9eV3cFAACgCQw6AAAAAAAAAACAljL7udl1V6jU7AUD+/0AAIA+Bh0AAAAAAAAAAEDL6Ontyf0L76+7RqXuW3hfenp76q4BAABUzKADAAAAAAAAAABoGfNemJely5fWXaNSS5cvzfzF8+uuAQAAVMygAwAAAAAAAAAAaBlzFs6pu0JTzHlucLwnAAAMZgYdAAAAAAAAAABAy3j4+YfrrtAUDy16qO4KAABAxQw6AAAAAAAAAACAlrG4a3HdFZpicefgeE8AABjMDDoAAAAAAAAAAICW0dnTWXeFpujq6aq7AgAAUDGDDgAAAAAAAAAAoGV093bXXaEpunoNOgAAYKAz6AAAAAAAAAAAAFpGR1tH3RWaYmjb0LorAAAAFTPoAAAAAAAAAAAAWsaw9mF1V2iKoe0GHQAAMNAZdAAAAAAAAAAAAC1j5NCRdVdoipHDBsd7AgDAYGbQAQAAAAAAAAAAtIyt1t+q7gpNsfXoreuuAAAAVMygAwAAAAAAAAAAaBmTxkyqu0JTTNpgcLwnAAAMZgYdAAAAAAAAAABAyxg/anxGDBlRd41KjRgyIuNGjqu7BgAAUDGDDgAAAAAAAAAAoGW0t7Vn2zHb1l2jUtuN2S7tbe111wAAACpm0AEAAAAAAAAAALSUV2Vc3RUqNXns5LorAAAATTCk7gIAAAAAAAAAAACr4sVl3dnh81emfZ2xWee1dbepzkFbHFR3BQAAoAkMOgAAAAAAAAAAgH5v29Muz7Lu3iRJz5IJ6encMO3Dnq25VeONHzU+u228W901AACAJmiruwAAAAAAAAAAAMArufCm+Rl34oz/GXP0KdL9/F61darS0ROPTlEUddcAAACawAkdAAAAAAAAAABAv/PM4mXZ44xrXvF69wu7ZthGV6Ro625iq2qNGDIiR2x5RN01AACAJjHoAAAAAAAAAAAA+pVxJ85Y+U29I9L9wi4Zuv4t1Rdqkinjp2S9oevVXQMAAGiStroLAAAAAAAAAAAAJMn0y+9ftTHH73U9d0DK3oHxmbZD24bm2O2PrbsGAADQRAPjTzMAAAAAAAAAAEDLeviZl3LI2b9Y7e8ruzdI17OHZtjGl1fQqrmO2+W4bDFyi7prAAAATWTQAQAAAAAAAAAA1KIsy4w/aeZaPaNr4b4ZMvLetI94tEGtmm/HsTvmfZPeV3cNAACgydrqLgAAAAAAAAAAAAw+x//HHWs95ujTnmVPHJWytzU/23Zo29BM22da2tva664CAAA0WWv+KQYAAAAAAAAAAGhJt/92Yf7ymzc19Jm9XRul89lDM3zjyxv63GY4YZcTMmH0hLprAAAANTDoAAAAAAAAAAAAKtfd05utT6lucNG9cL+0D38yHaPuqiyj0aZOmJr3Tn5v3TUAAICatNVdAAAAAAAAAAAAGNiOOOeGSsccfdry/Td/JQe++sCKcxrjwC0OzLR9pqWt8CNcAAAwWPnTAAAAAAAAAAAAUIkrZz+VcSfOyD2PvVBpztQdNs386VOz14SNctaBZ/X7UceBWxyYsw44Kx1tHXVXAQAAajSk7gIAAAAAAAAAAMDA8lLn8mz/uZ83JWvemVNSFMX//POw9mE5+6Czc9qNp2XG3BlN6bA6pk6Ymmn7TDPmAAAADDoAAAAAAAAAAIDG2f5zP89Lncsrz7n6k/tnq43W+7PXOto6csa+Z2Ti+hNzzp3npKu3q/I+KzO0bWhO2OWEvHfye9NWtNVdBwAA6Af8yQAAAAAAAAAAAFhrP7j5txl34ozKxxwf2X9C5k+f+opjjj9oK9pyzPbH5JK3XJIdxu5QaaeV2XHsjrnkLZfk/du/35gDAAD4H07oAAAAAAAAAAAA1tgzLy7LHqdf05Ss+dOnrvb3TBg9IRcefmEunHNhzr3z3Kae1jG0bWiO3+X4vHfSe9Pe1t60XAAAoDUYdAAAAAAAAAAAAGtk3IkzmpLzm5PfkI1HDl/j7x/SNiQf2P4DOfQ1h+b8e8/PzHkzs3T50gY2/N9GDBmRKeOn5Njtj80WI7eoLAcAAGhtBh0AAAAAAAAAAMBqOevnD+Sc6x6uPOdzb5mUY/YZ37DnbTFyi3z+9Z/P3+/29/npIz/NDx/4Yea9MK9hzx8/anyOnnh0jtjyiKw3dL2GPRcAABiYDDoAAAAAAAAAAIBVMvfZl3LwV35ReU5He5GHTp9S2fPXG7pe3r3du/Oubd+V256+Ldc9el1mL5id+xbet1ond4wYMiLbjdkuk8dOzkFbHJTdNt4tRVFU1hsAABhYDDoAAAAAAAAAAIAVKssy40+a2ZSsez7/xowc3tGUrKIosvsmu2f3TXZPkvT09mT+4vmZ89ycPLTooSzuXJyunq509XZlaNvQDG0fmpHDRmbr0Vtn0gaTMm7kuLS3tTelKwAAMPAYdAAAAAAAAAAAAK/oby+6M/991xOV55z37l0zZYdNK89Zkfa29mw5estsOXrLWnsAAACDg0EHAAAAAAAAAADwJ+743fN523m/rjxn203WyxWf2L/yHAAAgP7GoAMAAAAAAAAAAPgf3T292fqUy5uS9eCXDs/QIW1NyQIAAOhvDDoAAAAAAAAAAIAkyVvPuzF3/m5R5TkXf2Tv7DF+TOU5AAAA/ZlBBwAAAAAAAAAADHLX3Pd0jr3gtspz3jR543z7PbtVngMAANAKDDoAAAAAAAAAAGCQerlzeSZ/7udNyZp7xpS0tRVNyQIAAGgFBh0AAAAAAAAAADAI7fzFK7NoSXflOVf93f7ZeuP1Ks8BAABoNQYdAAAAAAAAAAAwiPznLb/LST+ZVXnOB/cdn1PfPKnyHAAAgFZl0AEAAAAAAAAAAIPAgpc6s9uXrm5K1vzpU5uSAwAA0MoMOgAAAAAAAAAAYIAbd+KMpuTcfNIbssmo4U3JAgAAaHUGHQAAAAAAAAAAMECdfdWD+fo1D1Wec9qbJ+XYfcdXngMAADCQGHQAAAAAAAAAAMAAM2/ByznorOsrzymKZN6ZUyvPAQAAGIgMOgAAAAAAAAAAYIAoyzLjT5rZlKy7P/fGjBrR0ZQsAACAgcigAwAAAAAAAAAABoC/v/ju/PiOxyrPOeddu+TNO25WeQ4AAMBAZ9ABAAAAAAAAAAAt7K5HF+XIc2+sPGerjV6Vqz95QOU5AAAAg4VBBwAAAAAAAAAAtKDlPb3Z6pTLm5L1wJcOy7Ah7U3JAgAAGCwMOgAAAAAAAAAAoMW8/Zu/zm2/fb7ynIs+vFf2mrBB5TkAAACDkUEHAAAAAAAAAAC0iOvufybHfP/WynMO2W7jfPd9u1WeAwAAMJgZdAAAAAAAAAAAQD+3pGt5Jn32503JmnvGlLS1FU3JAgAAGMwMOgAAAAAAAAAAoB973bSr8tzLXZXn/PwT+2fiJutVngMAAEAfgw4AAAAAAAAAAOiHLr710Xz6x/dUnnPMPuPyubdMrjwHAACA/82gAwAAAAAAAAAA+pHnXurM6750dVOy5k+f2pQcAAAA/pRBBwAAAAAAAAAA9BPjTpzRlJxfn3hwNhs9oilZAAAA/HkGHQAAAAAAAAAAULOvX/NQzr7qwcpzTpmyXT60/4TKcwAAAFg5gw4AAAAAAAAAAKjJb597OQd8+fqmZM2fPrUpOQAAAKwagw4AAAAAAAAAAGiysiwz/qSZTcm6+7NvzKh1OpqSBQAAwKoz6AAAAAAAAAAAgCb69I/uzsW3PVZ5ztfeuXP+YufNK88BAABgzRh0AAAAAAAAAABAE9zz2KIccc6NleeMH7turvvUgZXnAAAAsHYMOgAAAAAAAAAAoELLe3qz1SmXNyXr/mmHZXhHe1OyAAAAWDsGHQAAAAAAAAAAUJGjv31TfjNvYeU5//GhPfP6LcdWngMAAEDjGHQAAAAAAAAAAECDXf/AM3n/926tPOegiRvme8fsUXkOAAAAjWfQAQAAAAAAAAAADbK0qyfbffaKpmTNPWNK2tqKpmQBAADQeAYdAAAAAAAAAADQAHuecXWeXtxZec7lf7tfttt0ZOU5AAAAVMugAwAAAAAAAAAA1sIltz2af/jRPZXnvHfv1+aLf7F95TkAAAA0h0EHAAAAAAAAAAD9Tk9vT+a9MC9zFs7Jw88/nMVdi9PZ05nu3u50tHVkWPuwjBw6Mlutv1UmbzA540aOS3tbe1M7Lny5K7tOu6opWfPOnJKiKJqSBQAAQHMYdAAAAAAAAAAAULuyLHPb07fl2t9dm9nPzc79C+/P0uVLV/n7RwwZkW3HbJvJG0zOwa85OLttvFulA4hxJ86o7Nl/7MYTD87mo0c0JQsAAIDmMugAAAAAAAAAAKA2i7sW57JHLssPH/hh5r0wb42fs3T50tz5zJ2585k784P7fpDxo8bn6IlH5y1bviUjh45sWN9zrn0oZ135YMOe90pOPHzb/M0BW1aeAwAAQH0MOgAAAAAAAAAAaLpHFz+a8+89PzPnzVytkzhW1bwX5mX6LdPztTu+linjp+TY7Y/NFiO3WOPnPbpwSfb7p+sa2PCVzZ8+tSk5AAAA1MugAwAAAAAAAACAplneuzwXzL4g5911Xrp6uyrPW7p8aX780I9z2SOX5bhdjsv7Jr0v7W3tq/z9ZVlm/EkzK2z4/9312UMzep2hTckCAACgfgYdAAAAAAAAAAA0xdxFc3Pqjadm1oJZTc/u6u3KP9/+z7nmt9dk2j7TMmH0hJV+z0k/uSf/ecujlXf76tE758hdNq88BwAAgP7FoAMAAAAAAAAAgEr1lr25YPYFOefOc5pyKseK3LPgnhx12VE5fpfj877J70tb0fYn99z7+At58zduqLzLq9cfkRs+c3DlOQAAAPRPBh0AAAAAAAAAAFSmu7c7p914WmbMnVF3lf/R1duVs28/Ow88/0Cm7TMtHW0dSZKe3jJbnjyzKR3un3ZYhne0NyULAACA/smgAwAAAAAAAACASnT2dOZT138q1z92fd1V/qwZc2fk5a6Xc9aBZ+XY792VGx5eUHnmD47dM/tuPbbyHAAAAPo/gw4AAAAAAAAAABquu7e7X485/uD6x67PTt98T5Y99tdJqjsxY7+tx+bfjt2zsucDAADQegw6AAAAAAAAAABoqN6yN6fdeFq/H3P8Qcd69yWbXZJlT7wjSVvDn//IGVPS3lY0/LkAAAC0NoMOAAAAAAAAAAAa6oLZF2TG3Bl111gtHaPuSs+yzdK9cP+GPXPGx/fN5M1GNex5AAAADCwGHQAAAAAAAAAANMzcRXNzzp3n1F1jjQzb8Mr0vLRters2WqvnvHvP1+T0t+7QoFYAAAAMVAYdAAAAAAAAAAA0xPLe5Tn1xlPT1dtVd5U1UrQtz/DNLsmS+R9N0rZGz5h35pQURdHYYgAAAAxIBh0AAAAAAAAAADTEhXMuzKwFs+qusVbaRzyaoWN+la6FB6zW9/3q0wdlizHrVNQKAACAgcigAwAAAAAAAACAtfbo4kdz7p3n1l2jIYZueFW6X9w+ZfcGK733H940MccdtFUTWgEAADDQGHQAAAAAAAAAALDWzr/3/HT1dtVdoyGKtuUZusEv0vnU21Z43/zpU5vUCAAAgIHIoAMAAAAAAAAAgLWyuGtxZs6bWXeNhuoYdWc6n5mS9A7/k2t3nnZo1l93aA2tAAAAGEgMOgAAAAAAAAAAWCuXPXJZli5fWneNhirautMx6vZ0P7/P//zaV47aKX/5ulfX2AoAAICBxKADAAAAAAAAAIA1VpZlLrr/orprVKJj/ZvT/fzrs+moEbnppDfUXQcAAIABxqADAAAAAAAAAIA1dtvTt2X+4vl116hE+7Bn8+/Hb5h9Xr1n3VUAAAAYgNrqLgAAAAAAAAAAQOu69nfX1l2hUjc++Yu6KwAAADBAGXQAAAAAAAAAALDGZj83u+4KlZq9YGC/HwAAAPUx6AAAAAAAAAAAYI309Pbk/oX3112jUvctvC89vT111wAAAGAAMugAAAAAAAAAAGCNzHthXpYuX1p3jUotXb408xfPr7sGAAAAA5BBBwAAAAAAAAAAa2TOwjl1V2iKOc8NjvcEAACguQw6AAAAAAAAAABYIw8//3DdFZrioUUP1V0BAACAAcigAwAAAAAAAACANbK4a3HdFZpicefgeE8AAACay6ADAAAAAAAAAIA10tnTWXeFpujq6aq7AgAAAAOQQQcAAAAAAAAAAGuku7e77gpN0dVr0AEAAEDjGXQAAAAAAAAAALBGOto66q7QFEPbhtZdAQAAgAHIoAMAAAAAAAAAgDUyrH1Y3RWaYmi7QQcAAACNZ9ABAAAAAAAAAMAaGTl0ZN0VmmLksMHxngAAADSXQQcAAAAAAAAAAGtkq/W3qrtCU2w9euu6KwAAADAAGXQAAAAAAAAAALBGJo2ZVHeFppi0weB4TwAAAJrLoAMAAAAAAAAAgDUyftT4jBgyou4alRoxZETGjRxXdw0AAAAGIIMOAAAAAAAAAADWSHtbe7Yds23dNSq13Zjt0t7WXncNAAAABiCDDgAAAAAAAAAA1tjkDSbXXaFSk8cO7PcDAACgPgYdAAAAAAAAAACskcvufiLfuWJE3TUqddAWB9VdAQAAgAFqSN0FAAAAAAAAAABoLS8s7c5OX7jy9/80IT2dG6Z92LO1dqrC+FHjs9vGu9VdAwAAgAHKoAMAAAAAAAAAgFU27sQZ/+dXinQ/v1faN7mslj5VOnri0SmKou4aAAAADFBtdRcAAAAAAAAAAKD/++6v5v6ZMUef7hd2Tdnb0eRG1RoxZESO2PKIumsAAAAwgDmhAwAAAAAAAACAV/TkC0uz95nXrvim3hHpfmGXDF3/luaUaoIp46dkvaHr1V0DAACAAcygAwAAAAAAAACAP+uVTuT4c7qeOyAdo+5I0ba8wkbNMbRtaI7d/ti6awAAADDAtdVdAAAAAAAAAACA/uWLl81ZrTFHkpTdG6Tr2UMratRcx+1yXLYYuUXdNQAAABjgnNABAAAAAAAAAECS5IGnXsybvvrLNf7+roX7ZsjIe9M+4tEGtmquHcfumPdNel/dNQAAABgEDDoAAAAAAAAAAAa53t4yE06e2YAntWfZE0dlnfFfT9G2vAHPa66hbUMzbZ9paW9rr7sKAAAAg4BBBwAAAAAAAADAIPahC2/LVXOebtjzers2Suezh2b4xpc37JnNcsIuJ2TC6Al116BOvT3JggeTJ+5KnpmTLFuULO9MerqS9qHJkGHJ8NHJRpOSzXZJxm6dGAABAABryKADAAAAAAAAAGAQunnuc3nnd26u5NndC/dL+/An0zHqrkqeX4WpE6bmvZPfW3cNmq0sk/k3JA/MTB6/I3nqnqR7yap/f8e6ySY7JJvvmkyckozbNymK6voCAAADikEHAAAAAAAAAMAg0rm8JxNPvaLilLYse+KovGHSqPzy8V9UnLX2DtziwEzbZ1raira6q9AsSxcld1+U3HZ+34kca6r75eTRm/u+bj4vGbtNstuxyU7vTEaMblRbAABggDLoAAAAAAAAAAAYJN74z7/Ig0+/VHnOpcftk523GJ3OnjflU9d/Ktc/dn3lmWvqwC0OzFkHnJWOto66q9AMC+cmN3w1mXXJ6p3EsaoWPJhc8Znkmi8kOxyV7PuJZMyExucAAAADgo8VAAAAAAAAAAAY4Gbc82TGnTij8jHH23bZPPOnT83OW4xOkgxrH5azDzo7UydMrTR3TU2dMDVnH3h2hrUPq7sKVetZntzwz8m5eyV3XFDNmOOPdS/pyzl3r74BSW9PtXkAAEBLckIHAAAAAAAAAMAAtXhZd3b8/JVNyZp35pQURfEnv97R1pEz9j0jE9efmHPuPCddvV1N6bMiQ9uG5oRdTsh7J783bYXPQx3wnn0gufSjyeO3Nz+7pzO5+nPJfZclR56XbDix+R0AAIB+y6ADAAAAAAAAAGAA2vLkmenpLSvPue5TB2b82HVXeE9b0ZZjtj8mB7z6gJx646mZtWBW5b1eyY5jd8y0faZlwugJtXWgSXp7k5u+kVx7et+wok6P35Z8a7/k4FOSvU9I2gyJAACAxJ8MAAAAAAAAAAAGkH+9YV7GnTij8jHHxw/eKvOnT13pmOOPTRg9IRcefmH+7nV/l6FtQyts96eGtg3NJ1/3yVx4+IXGHINBT3fyXx9Jrvps/WOOP+jp7OvzXx/p6wcAAAx6TugAAAAAAAAAABgAnnphWfY685qmZM2fPnWNv3dI25B8YPsP5NDXHJrz7z0/M+fNzNLlSxvY7n8bMWREpoyfkmO3PzZbjNyishz6ke5lySXvTx68vO4mf96si5POF5Ojvp90DK+7DQAAUKOiLKs/WhNgICqKYnaSSf/31ydNmpTZs2fX0AgAAAAAAAAYrMadOKMpObeeckg2XG9YQ5/5YteL+ekjP80PH/hh5r0wr2HPHT9qfI6eeHSO2PKIrDd0vYY9l36upzv54Xv675jjj02ckrzjwqS9o+4mAABQu8mTJ2fOnDl/7tKcsiwnN7tPszihAwAAAAAAAACgRZ0+Y07+5VeNG0G8kjPeukPetedrKnn2ekPXy7u3e3fete27ctvTt+W6R6/L7AWzc9/C+1br5I4RQ0ZkuzHbZfLYyTloi4Oy28a7pSiKSjrTT/X2Jpd+rDXGHEnywMy+vm/9dtLWVncbAACgBgYdAAAAAAAAAAAt5qGnX8yh//zLynNGDh+Sez7/pspzkqQoiuy+ye7ZfZPdkyQ9vT2Zv3h+5jw3Jw8teiiLOxenq6crXb1dGdo2NEPbh2bksJHZevTWmbTBpIwbOS7tbe1N6Uo/ddM3klkX191i9cy6ONlkh2Sfj9fdBAAAqIFBBwAAAAAAAABAi+jtLTPh5JlNyZr9hTdl3WH1/WhJe1t7thy9ZbYcvWVtHWghzz6QXHt63S3WzLVfSrZ5U7LhxLqbAAAATWbQAQAAAAAAAADQAj76g9tz+b1PVZ7zL+/dLYdO2rjyHGiYnuXJpR9NejrrbrJmejqTSz+WHHtl4pQZAAAYVAw6AAAAAAAAAAD6sVvnL8xR37qp8pydthid/z5un8pzoOFuOid5/Pa6W6ydx29Lfv2NZN9P1N0EAABoIoMOAAAAAAAAAIB+qGt5b7Y59fKmZD10+uHpaG9rShY01MK5yXVn1N2iMa47I5l0RDJmQt1NAACAJjHoAAAAAAAAAADoZw776i9z/1MvVp7z44++Pq977fqV50Blbvhq0tNZd4vG6Onse58jvl53EwAAoEl8tAIAAAAAAAAAQD9xxb1PZtyJMyofcxyx02aZP32qMQetbemiZNYldbdorFmXJMteqLsFAADQJE7oAAAAAAAAAACo2YvLurPD569sSta8M6ekKIqmZEGl7r4o6V5Sd4vG6l7S9157fqTuJgAAQBMYdAAAAAAAAAAA1GibUy9P1/LeynOu/fsDMmHDV1WeA01Rlsmt3627RTVu/W6yx4cTwysAABjwDDoAAAAAAAAA+rPenmTBg8kTdyXPzEmWLUqWdyY9XUn70GTIsGT46GSjSclmuyRjt07a2msuDayKC349P5/76ezKc447aMv8w5u2rTwHmmr+DclzD9XdohoLHkx+e2Mybt+6mwAAABUz6AAAAAAAAADoT8qy74dUH5iZPH5H8tQ9SfeSVf/+jnWTTXZINt81mTil74dBfcI39CtPL16WPc+4pilZ86dPbUoONN0DM+tuUK37Zxp0AADAIGDQAQAAAAAAANAfLF2U3H1Rctv5fZ/Mvaa6X04evbnv6+bzkrHbJLsdm+z0zmTE6Ea1BdbQuBNnNCXnllPekI3WG96ULKjF43fU3aBaTwzw9wMAAJIYdAAAAAAAAADUa+Hc5IavJrMuWb2TOFbVggeTKz6TXPOFZIejkn0/kYyZ0PgcYIXOvPy+fPsXcyvPmXbk9nnPXq+tPAdq1dvTd4LVQPbkPX3v2dZedxMAAKBCBh0AAAAAAAAAdehZntz0jeS6M5Oezurzupckd1zQdwrIQScnrz/BD4lCEzz8zEs55OxfVJ6z7tD2zP7iYZXnQL+w4MFqRpD9SffLyYKHko22rbsJAABQIYMOAAAAAAAAgGZ79oHk0o8mj9/e/OyezuTqzyX3XZYceV6y4cTmd4BBoCzLjD9pZlOy7v3Cm/KqYX4EhEHkibvqbtAcT95l0AEAAAOcP80DAAAAAAAANEtvb9+pHNee3pxTOVbk8duSb+2XHHxKsvcJSVtbvX1gADn+P+7Iz+55svKcb/3163LY9ptUngP9zjNz6m7QHIPlPQEAYBAz6AAAAAAAAABohp7u5NKPJbMurrvJ/9fTmVz12eSpe/tO62jvqLsRtLTbf7swf/nNmyrP2X7zkfnZCftVngP91rJFdTdojqWL6m4AAABUzKADAAAAAAAAoGrdy5JL3p88eHndTf68WRcnnS8mR30/6RhedxtoOd09vdn6lOb83/eDXzo8Q4c4UYdBbnnNp1w1y2B5TwAAGMQMOgAAAAAAAACq1NPdv8ccf/Dg5cmPjknecaGTOmA1vPkbv8q9jy+uPOdHf7N3dhs3pvIcaAk9XXU3aI4egw4AABjofGQDAAAAAAAAQFV6e5NLP9b/xxx/8MDMvr69vXU3gX7v57OfyrgTZ1Q+5pi646aZP32qMQf8sfahdTdojvZhdTcAAAAq5oQOAAAAAAAAgKrc9I1k1sV1t1g9sy5ONtkh2efjdTeBfumlzuXZ/nM/b0rWvDOnpCiKpmRBSxkySIYOg+U9AQBgEDPoAAAAAAAAAKjCsw8k155ed4s1c+2Xkm3elGw4se4m0K9M+uwVWdLVU3nO1Z88IFtt9KrKc6BlDR9dd4PmGDG67gYAAEDF2uouAAAAAAAAADDg9CxPLv1o0tNZd5M109OZXPqxpLf6H1yHVvBvN/82406cUfmY4yMHTMj86VONOWBlNppUd4PmGCzvCQAAg5gTOgAAAAAAAAAa7aZzksdvr7vF2nn8tuTX30j2/UTdTaA2z7y4LHucfk1TsuZPn9qUHBgQNtu57gbNsenOdTcAAAAqZtABAAAAAAAA0EgL5ybXnVF3i8a47oxk0hHJmAl1N4GmG3fijKbk/ObkN2TjkcObkgUDxthtko51ku4ldTepTse6ydit624BAABUrK3uAgAAAAAAAAADyg1fTXo6627RGD2dfe8Dg8iXf35/U8YcXzhicuZPn2rMAWuirT3ZZMe6W1Rr0x373hMAABjQnNABAAAAAAAA0ChLFyWzLqm7RWPNuiR547Rk+Ki6m0ClHnn2pbzhK7+oPGfokLY8+KXDK8+BAW/zXZNHb667RXU227XuBgAAQBMYdAAAAAAAAAA0yt0XJd1L6m7RWN1L+t5rz4/U3QQqUZZlxp80sylZsz7/xqw3vKMpWTDgTZyS3Hxe3S2qs+2UuhsAAABN0FZ3AQAAAAAAAIABoSyTW79bd4tq3PrdvveDAeZvL7qzKWOOb75718yfPtWYAxpp3L7JBlvX3aIaY7dJXrtP3S0AAIAmMOgAAAAAAAAAaIT5NyTPPVR3i2oseDD57Y11t4CGueN3z2fciTPy33c9UWnOdpuOzPzpU3P4DptWmgODUlEku3+w7hbV2P2Dfe8HAAAMeEPqLgAAAAAAAAAwIDxQ/af81+r+mX2fhg4trLunN1ufcnlTsh780uEZOsTnbEKldnpncs0Xku4ldTdpnI51+t4LAAAYFAw6AAAAAAAAABrh8TvqblCtJwb4+zHgHXnujbnr0UWV51z8kb2zx/gxlecASUaMTnY4KrnjgrqbNM4ORyXDR9XdAgAAaBKDDgAAAAAAAIC11duTPHVP3S2q9eQ9fe/Z1l53E1gtV815Oh+68LbKcw6bvEm+9Z7XVZ4D/B/7fiK5+6Kkp7PuJmuvfVjf+wAAAIOGQQcAAAAAAADA2lrwYNK9pO4W1ep+OVnwULLRtnU3gVXycufyTP7cz5uSNfeMKWlrK5qSBfwfYyYkB52cXP25upusvYNO7nsfAABg0DDoAAAAAAAAAFhbT9xVd4PmePIugw5awo6f/3kWL1teec5Vf7d/tt54vcpzgJXY+/jkvp8mj99ed5M1t/luyetPqLsFAADQZG11FwAAAAAAAABoec/MqbtBcwyW96Rl/cdvfpdxJ86ofMzxof3GZ/70qcYc0F+0D0mO/GbSPqzuJmumfVhy5HlJW3vdTQAAgCZzQgcAAAAAAADA2lq2qO4GzbF0Ud0N4M969sXO7H761U3Jmj99alNygNW04cTk4FOSqz5bd5PVd/Cpff0BAIBBx6ADAAAAAAAAYG0t76y7QXMMlvekpYw7cUZTcm4+6Q3ZZNTwpmQBa2jvE5Kn7k1mXVx3k1W3wzuSvY+vuwUAAFATgw4AAAAAAACAtdXTVXeD5ugx6KD/OPvKB/L1ax+uPOezb56UD+w7vvIcoAHa2pIjz0s6X0wevLzuNis3cUpf37a2upsAAAA1MegAAAAAAAAAWFvtQ+tu0Bztw+puAJm34OUcdNb1lee0txV55IwplecADdbekRz1/eSS9/fvUcfEKcnbv9fXFwAAGLQMOgAAAAAAAADW1pBBMnQYLO9Jv1SWZcafNLMpWfd8/o0ZOdwPWUPL6hieHP1vyaUfS2ZdXHebP7XDO/pO5jDmAACAQc+gAwAAAAAAAGBtDR9dd4PmGDG67gYMUp+8+K785I7HK8859127ZuqOm1aeAzRBe0fy1m8nm2yfXHt60tNZd6O+k64OPjXZ+/ikra3uNgAAQD9g0AEAAAAAAACwtjaaVHeD5hgs70m/cdeji3LkuTdWnrP1Rq/KVZ88oPIcoMna2pJ9/jbZ5rDk0o8mj99eX5fNd+s7lWPDifV1AAAA+h2DDgAAAAAAAIC1tdnOdTdojk13rrsBg8Tynt5sdcrlTcl64EuHZdiQ9qZkATXZcGLygSuTm85Jrjujuad1tA9LDj7l96dy+L0GAAD43ww6AAAAAAAAANbW2G2SjnWS7iV1N6lOx7rJ2K3rbsEg8PZv/jq3/fb5ynMu+vBe2WvCBpXnAP1E+5Bk308kk45IbvhqMuuSav//dsc6yQ5H9WWOmVBdDgAA0NIMOgAAAAAAAADWVlt7ssmOyaM3192kOpvu6JPFqdS19z+dD3z/tspzDp20cf7lvbtVngP0U2MmJEd8PXnjtOTui5Jbv5sseLBxzx+7TbL7B5Od3pkMH9W45wIAAAOSQQcAAAAAAABAI2y+68AedGy2a90NGKCWdC3PpM/+vClZc8+Ykra2oilZQD83fFSy50eSPT6c/PbG5P6ZyRN3JE/evXond3Ss2zd63GzXZNspyWv3SQq/zwAAAKvGoAMAAAAAAACgESZOSW4+r+4W1dl2St0NGIB2nXZVFr7cVXnOzz+xfyZusl7lOUALKopk3L59X0nS25MseCh58q7kmTnJ0kXJ8s6kpzNpH5YMGZaMGJ1sNCnZdOdk7NZOsAIAANaYQQcAAAAAAABAI4zbN9lg6+S5h+pu0nhjt+n7xHFokB/e+rt85sezKs85Zp9x+dxbJleeAwwgbe3JRtv2fQEAAFTMoAMAAAAAAACgEYoi2f2DyRWfqbtJ4+3+wb73g7W04KXO7Palq5uSNX/61KbkAAAAAKwpgw4AAAAAAACARtnpnck1X0i6l9TdpHE61ul7L1hL406c0ZScX594cDYbPaIpWQAAAABro63uAgAAAAAAAAADxojRyQ5H1d2isXY4Khk+qu4WtLCvXv1gU8Ycp07dLvOnTzXmAAAAAFqGEzoAAAAAAAAAGmnfTyR3X5T0dNbdZO21D+t7H1gDv33u5Rzw5eubkjV/+tSm5AAAAAA0kkEHAAAAAAAAQCONmZAcdHJy9efqbrL2Djq5731gNZRlmfEnzWxK1t2fe2NGjehoShYAAABAoxl0AAAAAAAAADTa3scn9/00efz2upusuc13S15/Qt0taDH/cMndueT2xyrP+cZf7ZK37LRZ5TkAAAAAVTLoAAAAAAAAAGi09iHJkd9MvrVf0tNZd5vV1z4sOfK8pK297ia0iHseW5Qjzrmx8pwJY9fNtZ86sPIcAAAAgGYw6AAAAAAAAACowoYTk4NPSa76bN1NVt/Bp/b1h5VY3tObrU65vClZ9087LMM7jIwAAACAgcOgAwAAAAAAAKAqe5+QPHVvMuviupusuh3ekex9fN0taAHv+PZNuWXewspz/uNDe+b1W46tPAcAAACg2Qw6AAAAAAAAAKrS1pYceV7S+WLyYHNOMVgrE6f09W1rq7sJ/dj1DzyT93/v1spzDpq4Yb53zB6V5wAAAADUxaADAAAAAAAAoErtHclR308ueX//HnVMnJK8/Xt9feHPWNrVk+0+e0VTsuaeMSVtbUVTsgAAAADqYtABAAAAAAAAULWO4cnR/5Zc+rFk1sV1t/lTO7yj72QOYw5ewR6nX51nXuysPOfyv90v2206svIcAAAAgP7AoAMAAAAAAACgGdo7krd+O9lk++Ta05Oe6n84fuWdhiUHn5rsfXzS1lZ3G/qhi297NJ/+0T2V57xv79fmC3+xfeU5AAAAAP2JQQcAAAAAAABAs7S1Jfv8bbLNYcmlH00ev72+Lpvv1ncqx4YT6+tAv7Xw5a7sOu2qpmTNO3NKiqJoShYAAABAf2LQAQAAAAAAANBsG05MPnBlctM5yXVnNPe0jvZhycGn/P5Ujvbm5dIyxp04oyk5N554cDYfPaIpWQAAAAD9kUEHAAAAAAAAQB3ahyT7fiKZdERyw1eTWZck3Uuqy+tYJ9nhqL7MMROqy6FlfeOah/KVqx6sPOekw7fNRw7YsvIcAAAAgP7OoAMAAAAAAACgTmMmJEd8PXnjtOTui5Jbv5ssaOAP1Y/dJtn9g8lO70yGj2rccxkwfvfckuz/5euakjV/+tSm5AAAAAC0AoMOAAAAAAAAgP5g+Khkz48ke3w4+e2Nyf0zkyfuSJ68e/VO7uhYN9l0x2SzXZNtpySv3Scpiup607LKssz4k2Y2Jeuuzx6a0esMbUoWAAAAQKsw6AAAAAAAAADoT4oiGbdv31eS9PYkCx5KnrwreWZOsnRRsrwz6elM2oclQ4YlI0YnG01KNt05Gbt10tZeX39awok/vicX3fpo5Tlfe+fO+YudN688BwAAAKAVGXQAAAAAAAAA9Gdt7clG2/Z9wVq69/EX8uZv3FB5zmvGrJNffvqgynMAAAAAWplBBwAAAAAAAAAMcD29ZbY8eWZTsu6fdliGdzglBgAAAGBlDDoAAAAAAAAAYAB793dvzo0PP1d5zr9/cM/ss9XYynMAAAAABgqDDgAAAAAAAAAYgH754LN577/eUnnO/ttsmAs/sEflOQAAAAADjUEHAAAAAAAAAAwgS7t6st1nr2hK1iNnTEl7W9GULAAAAICBxqADAAAAAAAAAAaI1595TZ54YVnlOTM+vm8mbzaq8hwAAACAgcygAwAAAAAAAABa3E/ueCyfvPjuynPevedrcvpbd6g8BwAAAGAwMOgAAAAAAAAAgBb1/Mtd2WXaVU3JmnfmlBRF0ZQsAAAAgMHAoAMAAAAAAAAAWtC4E2c0JedXnz4oW4xZpylZAAAAAIOJQQcAAAAAAAAAtJBzr3s4X/75A5XnfPqwifnYgVtVngMAAAAwWBl0AAAAAAAAAEALeHThkuz3T9c1JWv+9KlNyQEAAAAYzAw6AAAAAAAAAKAfK8sy40+a2ZSsO087NOuvO7QpWQAAAACDnUEHAAAAAAAAAPRTJ//XrPzHb35Xec7Z79gpb9v11ZXnAAAAAPD/GXQAAAAAAAAAQD8z+4kXMvXrN1Ses/noEbnxxIMrzwEAAADgTxl0AAAAAAAAAEA/0dNbZsuTZzYl6/5ph2V4R3tTsgAAAAD4UwYdAAAAAAAAANAPvOf83+RXDy2oPOfCD+yR/bfZsPIcAAAAAFbMoAMAAAAAAAAAanTjwwvy7u/+pvKcfbbaIP/+wb0qz4GG6e1JFjyYPHFX8sycZNmiZHln0tOVtA9NhgxLho9ONpqUbLZLMnbrpM2pMwAAALQOgw4AAAAAAAAAqMGy7p5se9oVTcl65IwpaW8rmpIFa6wsk/k3JA/MTB6/I3nqnqR7yap/f8e6ySY7JJvvmkyckozbNyn87z0AAAD9l0EHAAAAAAAAADTZfv90bR5duLTynJ+dsG+233xU5TmwVpYuSu6+KLnt/L4TOdZU98vJozf3fd18XjJ2m2S3Y5Od3pmMGN2otgAAANAwBh0AAAAAAAAA0CSX3vl4PvHDuyrP+as9tsiZb9ux8hxYKwvnJjd8NZl1yeqdxLGqFjyYXPGZ5JovJDsclez7iWTMhMbnAAAAwBoy6IBBpCiKSUkOTrJ9km2SjEuy3u+/2pK8nOSlJAuTzE3ySJIHktyS5N6yLHua3xoAAAAAAABa36IlXdn5i1c1JWvemVNSFEVTsmCN9CxPbvpGct2ZSU9n9XndS5I7Lug7BeSgk5PXn5C0tVefCwAAACth0AEDXFEU2yX5YJJ3JtlsJbeP/v3Xq5P834/rebkoiluSXJFkRlmWsxvbFAAAAAAAAAamcSfOaErOrz59ULYYs05TsmCNPftAculHk8dvb352T2dy9eeS+y5Ljjwv2XBi8zsAAADAH2mruwBQjaIodi2K4sokc5J8Misfc6zMukkOSvKPSe4tiuLetXweAAAAAAAADGjf+sUjTRlzfOqN22T+9KnGHPRvvb3JjV9LvrVfPWOOP/b4bX09bvxaXy8AAACoiRM6YIApimJUkq8leW+SKs9RfnWFzwYAAAAAAICW9djzS7LvP17XlKz506c2JQfWSk93cunHklkX193k/+vpTK76bPLUvX2ndbR31N0IAACAQcigAwaQoij2TfKDJK+tuwsAAAAAAAAMNmVZZvxJM5uSdcdph2bMukObkgVrpXtZcsn7kwcvr7vJnzfr4qTzxeSo7ycdw+tuAwAAwCDTVncBoDGKovirJNfEmAMAAAAAAACa7rP/fW9TxhxffvuOmT99qjEHraGnu3+POf7gwcuTHx3T1xcAAACayAkdMAAURXFckm8kKVbxW15KckuSh5L89vf/3J1k9O+/NkyyY5Ltk/gIEgAAAAAAAHgF9z25OId/7VeV52w8clh+c/IhledAw/T2Jpd+rP+POf7ggZl9fd/67aTN56MCAADQHAYd0OKKojg6qzbmWJrkP5NcmOTGsiyXr8Kz25NMSnJ4kr9Islec7AMAAAAAAADp7S0z4eTqT+RIkvu+eFhGDG1vShY0zE3fSGZdXHeL1TPr4mSTHZJ9Pl53EwAAAAYJgw5oYUVR7Ju+gcbKxhzfTfLZsiyfXJ3nl2XZk2TW77/+qSiKjZIck+Sj6TvJAwAAAAAAAAadY753S6574NnKc75/zO45cOJGledAwz37QHLt6XW3WDPXfinZ5k3JhhPrbgIAAMAgYNABLaooivXTd+LG0BXc9nySd5VleUUjMsuyfCbJPxZFcVaSQxvxTAAAAAAAAGgVv35kQd71L7+pPGfP8WPyw4/sXXkOVKJneXLpR5OezrqbrJmezuTSjyXHXpm0ORkHAACAahl0QOv6TpJXr+D6E0kOLctyTqODf39yR0NGIgAAAAAAANDfLevuybanNeevxx4+/fAMaW9rShZU4qZzksdvr7vF2nn8tuTX30j2/UTdTQAAABjgDDqgBRVFMTXJ21dwy4tJplQx5gAAAAAAAIDB5OCzrs/cBS9XnvPT4/fJjq8eXXkOVGrh3OS6M+pu0RjXnZFMOiIZM6HuJgAAAAxgPtYDWkxRFB1JvrKS2/6mLMu7m9EHAAAAAAAABqKf3v1Exp04o/Ixxzt2e3XmT59qzMHAcMNXk57Ouls0Rk9n3/sAAABAhZzQAa3n2CQTV3D9p2VZ/kezygAAAAAAAMBA8sLS7uz0hSubkjXvzCkpiqIpWVC5pYuSWZfU3aKxZl2SvHFaMnxU3U0AAAAYoAw6oIUURdGW5JMruKUnyWeaVAcAAAAAAAAGlHEnzmhKzi/+4cC8doN1m5IFTXP3RUn3krpbNFb3kr732vMjdTcBAABggGqruwCwWo5IsvUKrv+4LMv7m1UGAAAAAAAABoJ/+eXcpow5PnHI1pk/faoxBwNPWSa3frfuFtW49bt97wcAAAAVcEIHtJZjVnL9W01pAQAAAAAAAAPAE4uW5vXTr21K1vzpU5uSA7WYf0Py3EN1t6jGggeT396YjNu37iYAAAAMQAYd0CKKohid5LAV3PJkkuubUgYAAAAAAABaXDNO5EiS2089JBu8alhTsqA2D8ysu0G17p9p0AEAAEAlDDqgdbw1ydAVXP9ZWTrnFQAAAAAAAFbk8z+dne//en7lOf/4lzvk6N1fU3kO9AuP31F3g2o9McDfDwAAgNoYdEDrOHQl15tzFjQAAAAAAAC0oPufWpzDvvqrynPGrDs0d5y2sr/agwGktyd56p66W1TryXv63rOtve4mAAAADDAGHdA6DlzJ9d80owQAAAAAAAC0kt7eMhNOntmUrDlffFPWGeqv4RlkFjyYdC+pu0W1ul9OFjyUbLRt3U0AAAAYYPyXJGgBRVFslWTTFdyyqCzLeavwnCFJtk4yPsmoJMOSLEnyYpJHk8wvy/KltW8MAAAAAAAA9fvgBbfl6vuerjznX9+/Ww7eduPKc6BfeuKuuhs0x5N3GXQAAADQcAYd0Bp2Xsn1h1/pQlEUY5O8O8lbkuyXZOgKnlMWRXFfkhuS/HeSq8uy7Fq9qgAAAAAAAFCvm+c+l3d+5+bKc3Yft34u+ZvXV54D/dozc+pu0ByD5T0BAABoKoMOaA3br+T6I//3F4qi2CjJF5K8L8mIVcwpkkz6/deHkzxbFMW5Sb5eluXzq14XAAAAAAAAmq9zeU8mnnpFU7IePv3wDGlva0oW9GvLFtXdoDmWLqq7AQAAAAOQQQe0hkkruf6/zokuiuLYJGclGb2WuRsm+XyS44uiOLksy39Zy+cBAAAAAABAJQ49+xd56JmXKs+59Lh9svMWoyvPgZaxvLPuBs0xWN4TAACApjLogNawxUquP5skRVF0JPlWkg80OH9sku8URXFYkmPKslzc4OcDAAAAAADAGvnZPU/k+P+4s/Kct+26ec5+x86V50DL6emqu0Fz9Bh0AAAA0HgGHdAaNl3J9cVFUQxJ8p9J/rLCHm9LMr4oijeVZflshTlrpSiK45J8rAlRWzYhAwAAAAAAgD/jhaXd2ekLVzYla96ZU1IURVOyoOW0D627QXO0D6u7AQAAAAOQQQe0hk1Wcr0ryXmpdszxB7skubYoin368UkdGyaZVHcJAAAAAAAAqjHhpBnpLavPue5TB2b82HWrD4JWNmSQDB0Gy3sCAADQVAYd0M8VRTE8ycr+y9A7khy0gutLk1yT5L+T3JHk6STPJhmVvrHIxCRvSTI1yQarUGv7JBcVRTG1LMsm/KdyAAAAAAAASM6/YV6m/WxO5TkfP3irfPKNEyvPgQFh+Oi6GzTHiNF1NwAAAGAAMuiA/m/EKtzzSmOOMsm/JflMWZZP/Znrz/7+a1aSHxVFMSLJZ5J8ehVyD09yQpKvr0I/AAAAAAAAWGNPvbAse515TVOy5k+f2pQcGDA2mlR3g+YYLO8JAABAUxl0QP83fA2/b0mSt5Vl+fNV/YayLJcm+XxRFP+e5Mok41byLWcWRfGjsiyfWMOOAAAAAAAAsELjTpzRlJzbTj0kY181rClZMKBstnPdDZpj053rbgAAAMAA1FZ3AWClOtbge15M8sbVGXP8sbIsH0qyX5IHV3LrOkk+uyYZAAAAAAAAsCJf+tmcpow5znjrDpk/faoxB6ypsdskHevU3aJaHesmY7euuwUAAAADkBM6oP/rWYPvOaEsyxvXJrQsy8eKojgqya1Jhq7g1vcXRXFqWZYL1iavwZ5NMqcJOVsm8V/2AQAAAAAAGujBp1/MG//5l5XnjBw+JPd8/k2V58CA19aebLJj8ujNdTepzqY79r0nAAAANJhBB/R/Xat5/0/LsrygEcFlWd5TFMUXk3xpBbcNS3JMki83IrMRyrI8N8m5VecURTE7yaSqcwAAAAAAAAaD3t4yE06e2ZSs2V94U9Yd5q/LoWE233VgDzo227XuBgAAAAxQbXUXAFZqdQcdpzQ4/ytJnlvJPX/Z4EwAAAAAAAAGkY/8221NGXN89727Zf70qcYc0GgTp9TdoFrbDvD3AwAAoDb+KxX0f0tW495flWV5byPDy7JcVhTF95J8agW37V4UxdiyLBc0MhsAAAAAAICB7ZZ5C/OOb99Uec4urxmd//rYPpXnwKA1bt9kg62T5x6qu0njjd0mea3fPwAAAKiGQQf0c2VZdhdF8WKS9Vbh9u9XVGNlg462JHskac4Z2AAAAAAAALS0ruW92ebUy5uS9dDph6ejva0pWTBoFUWy+weTKz5Td5PG2/2Dfe8HAAAAFTDogNbwXFZt0HFjRfn3JVmUZPQK7tk1Bh0AAAAAAACsxGFf/WXuf+rFynN+8rHXZ9fXrF95DvB7O70zueYLSfeSups0Tsc6fe8FAAAAFfExJNAaFqzCPc8nebCK8LIsyyS3rOS2LavIBgAAAAAAYGC4fNaTGXfijMrHHEfuvFnmT59qzAHNNmJ0ssNRdbdorB2OSoaPqrsFAAAAA5gTOqA1/C7Jbiu5577fDy+qMifJG1dwfYsKswEAAAAAAGhRLy7rzg6fv7IpWfPOnJKiKJqSBfwZ+34iufuipKez7iZrr31Y3/sAAABAhQw6oDXMW4V7FlXc4fmVXB9TcT4AAAAAAAAtZptTLk9XT2/lOdf+/QGZsOGrKs8BVmLMhOSgk5OrP1d3k7V30Ml97wMAAAAVaqu7ALBK5q7CPYsq7rCy569TcT4AAAAAAAAt4oJfz8+4E2dUPuY47qAtM3/6VGMO6E/2Pj7Z/HV1t1g7m++WvP6EulsAAAAwCDihA1rDvatwz9KKO6zs+X4/AQAAAAAAGOSeXrwse55xTVOy5k+f2pQcYDW1D0mO/Gbyrf2Sns6626y+9mHJkeclbe11NwEAAGAQ8APY0BruTNKbFZ+qM6riDit7ftWDEgAAAAAAAPqxcSfOaErOLae8IRutN7wpWcAa2nBicvApyVWfrbvJ6jv41L7+AAAA0AQGHdACyrJ8sSiKB5Nsu4LbRldcY/2VXH+p4nwAAAAAAIBBqae3J/NemJc5C+fk4ecfzuKuxens6Ux3b3c62joyrH1YRg4dma3W3yqTN5iccSPHpb2Jnyx/5sz78u1fzq08Z9qR2+c9e7228hygQfY+IXnq3mTWxXU3WXU7vCPZ+/i6WwAAADCIGHRA67ghKx50bFRx/sqe/3jF+QAAAAAAAINCWZa57enbcu3vrs3s52bn/oX3Z+nyVT8sfcSQEdl2zLaZvMHkHPyag7PbxrulKIqG93z4mRdzyNm/bPhz/691h7Zn9hcPqzwHaLC2tuTI85LOF5MHL6+7zcpNnNLXt62t7iYAAAAMIgYd0Dp+nuSDK7g+qSiKdcqyXFJR/m4ruf7binIBAAAAAAAGhcVdi3PZI5flhw/8MPNemLfGz1m6fGnufObO3PnMnfnBfT/I+FHjc/TEo/OWLd+SkUNHrnXP3t4yE06eudbPWRX3fuFNedUwf60NLau9Iznq+8kl7+/fo46JU5K3f6+vLwAAADRRUZZl3R2AVVAUxegkC5Ks6HzsA8qybPjHIBVFsU6SxSvJfl9Zlhc2Ors/K4pidpJJ//fXJ02alNmzZ9fQCAAAAAAAaEWPLn405997fmbOm7laJ3GsrhFDRmTK+Ck5dvtjs8XILdboGcf9+x2ZMevJBjf7U995z+vyxsmbVJ4DNElPd3Lpx5JZF9fd5E/t8I6+kzmMOQAAAGo1efLkzJkz589dmlOW5eRm92kWH2UCLaIsy0VFUVyZ5PAV3PbGJFWca/2GrHjMkSS/qSAXAAAAAABgwFreuzwXzL4g5911Xrp6uyrPW7p8aX780I9z2SOX5bhdjsv7Jr0v7W0r+yugPrfNX5i3f+umihsmO2w+KpedsG/lOUCTtXckb/12ssn2ybWnJz2ddTdK2oclB5+a7H180tZWdxsAAAAGKSd0QAspiuLoJBet4JankrymLMvuBufOzIqHJPPLshzfyMxW4IQOAAAAAABgTc1dNDen3nhqZi2YVVuHHcfumGn7TMuE0RNe8Z6u5b3Z5tTLm9LnodMPT0e7H6qGAe/Z/8fefYdZWhbmA37ema0gS0cEkaHDIlUsCEgTlZ1oTBQ1iWKN/hS7JlmKFYVNNNhrNLYUa6IxsyAQQAVBBaX3MooU6Sxl2TLz/v6YHZldZpad3TnfmXLf13Wunfm+M9/znJjkj12fea9Jfvjm5JaL2tdh6/0GTuXYfJf2dQAAAGAlU/WEDn8bBhPLj5LctZr7WyY5aiwDSyk7JXn+47zth2OZCQAAAAAAMFn11/587fKv5agfH9XWMUeSXHrXpTnqx0fla5d/Lf21/zH3uz/980bGHD948/7pXdBtzAFTxea7JK87PXnuhwZOyWhS58zkiA8nrz/dmAMAAIBxwd+IwQRSa30kyace520fL6VsPBZ5pZSS5Mt5/P9f8S9jkQcAAAAAADCZLetfluPOPS6nXHRKlvYvbXedJMnS/qU55aJTcty5x2VZ/8Ah8Kddfnu65vfkilsXtTT7hXttld4F3Xnatpu0NAcYhzqnJQe+MznmgmTfVyfT12tt3vT1BnKOuSA54B1JR2dr8wAAAGANTWt3AWDUPpvkvUk2HOH+k5J8PslfjUHWO5Ic8jjvOb3WOuz5RgAAAAAAAAxY0rck7z3nvTnnD+e0u8qwem7syf2PPJBTzzoiqdNbnnfTyfMy8LvFgCltk+2TF306ed6JySXfTn79leSua8fu+ZvtnDz9Dcler0hmjfRP7AAAANA+Bh0wwdRa7yulvD+rP6njFaWUe5McU2uta5NTSnl9kn9+vDpJ5q/N8wEAAAAAAKaKZf3LxvWYY9C5t/4ss7a+M4/84ZVJWvPb689898HZcYsntOTZwAQ2a8PkmW9KnvHG5HfnJVcvTG79TXLbJcmyh9f8OdPXT560Z7LVvsmu85JtD0iMxwAAABjHDDpgYvpcklcn2Xc173lzkk1LKW+ttd65pg8upcxMcmyS9yd5vL/Z+mKt9bdr+mwAAAAAAICppr/2533nvW/cjzkGTd/gqmSr7+WRW1+WpGPMnvv/Dt4h84/cdcyeB0xSpSRdBw68kqS/L7nruuS2i5M7rkwW35csX5L0LUk6ZybTZiazN0q2mJs8ae9ks52SjtYM0gAAAKAVDDpgAqq19pVSXpnkV0lW9yuMXpbkeaWUjyb5t1rr7SO9sZTyhCQvTHJikh3WoMY1Sd675q0BAAAAAACmnm9c8Y303NjT7hqjMn3Di9P3yFZZds9zxuR5vQu6x+Q5wBTU0ZlssevACwAAACYhgw6YoGqtV5VSXpfkO1n9SRobJflYkn8qpVyQ5DdJ/pjk7iRzkjwxya5JDk0ycw3j70rywlrrKM62BQAAAAAAmFpuvO/GfPa3n213jbUyc/PT0/fgrulfusVaP+NXxx2eLebMGsNWAAAAADC5GHTABFZr/V4pZfMkn1uDt5ck+694rYt7k3TXWq9bx+cAAAAAAABMWsv7l+eE807I0v6l7a6yVkrH8sza6nt5uPfNSTpG9bMf/vPdc/T+XS3pBQAAAACTiUEHTHC11s+XUpZlYNQxvcVxNyeZV2u9vMU5AAAAAAAAE9o3r/xmLrvrsnbXWCeds2/OjE1+nqX3HLxG7585rSPXfOTIFrcCAAAAgMnDoAMmgVrrv5RSrkjynSRPblHMj5K8rtZ6T4ueDwAAAAAAMCncvOjmfO63a3LA+vg3Y/MzsuyBp6Yu23S177vsg8/LBrNa/bvHAAAAAGByGd3ZuMC4VWv9RZLdkvxjkrE8u/vaJH9ea32xMQcAAAAAAMDj++rlX83S/rH855r2KR3LM2PTn454/4uv3De9C7qNOQAAAABgLRh0wCRSa32w1jo/SVeSDyX5w1o+ammShUlenGS3Wuv/jElBAAAAAACASW7R0kVZeNPCdtcYU9M3/G3S8chK13Z70pz0LujOC576pDa1AgAAAICJb1q7CwBjr9Z6W5IPJvlgKWWvJEck2SvJrkm2TrJBkvWSLEvyUJLbk9yU5PIk5yc5p9Z6f/PNAQAAAAAAJrYf3/DjLF6+uN01xlTpWJbpG16UZfcekCS59iNHZsY0vzsQAAAAANbVlBp0lFJubHeHNqq11h3aXYLm1VovSXJJu3sAAAAAAABMdrXWfPvqb7e7RktM3/iC/PvL3pNnbLdpu6sAAAAAwKQxpQYdSbqS1CSlzT3aoba7AAAAAAAAAExmF/7xwvQu6m13jZbonHlnyuwbkxh0AAAAAMBYmarn4NYp9gIAAAAAAABa7Kzfn9XuCi119s1nt7sCAAAAAEwqU3XQAQAAAAAAADCmrrj7inZXaKkr7prcnw8AAAAAmjat3QXapLS7QIOc0AEAAAAAAAAt1tffl6vvubrdNVrqqnuuSl9/Xzo7OttdBQAAAAAmhal6QkedQi8AAAAAAACgxW66/6YsXr643TVaavHyxeld1NvuGgAAAAAwaUzFQUeZgi8AAAAAAACgha6858p2V2jElXdPjc8JAAAAAE2Y1u4CDftGuwsAAAAAAAAAk8/1917f7gqNuO6+69pdAQAAAAAmjSk16Ki1vrbdHQAAAAAAAIDJZ9HSRe2u0IhFS6bG5wQAAACAJnS0uwAAAAAAAADARLekb0m7KzRiad/SdlcAAAAAgEnDoAMAAAAAAABgHS3rX9buCo1Y2m/QAQAAAABjxaADAAAAAAAAYB1N75je7gqNmNExo90VAAAAAGDSMOgAAAAAAAAAWEczO2e2u0IjZnQadAAAAADAWDHoAAAAAAAAAFhHc2bMaXeFRsyZOTU+JwAAAAA0waADAAAAAAAAYB3tuPGO7a7QiJ022qndFQAAAABg0jDoAAAAAAAAAFhHczeZ2+4KjZi76dT4nAAAAADQBIMOAAAAAAAAgHW03YbbZfa02e2u0VKzp81O15yudtcAAAAAgEnDoAMAAAAAAABgHXV2dGbXTXZtd42W2m2T3dLZ0dnuGgAAAAAwaRh0AAAAAAAAAKyjux5ckl9etV67a7TU7pvt3u4KAAAAADCpTGt3AQAAAAAAAICJrGt+T5Kkc725mbHpeW1u0zqHbnNouysAAAAAwKRi0NGQUsqcJAcm2SfJHkmenGTrJHOSzE4yc8jba63VfzYAAAAAAAAwjn3ijGvzqf+77k/f9z28ffqWbJ7OmXe2sVVrbLfhdtnvifu1uwYAAAAATCpGAy1UStkkySuTvDzJ05N0rvqWMcrpSrL9CLd7a603jkUOAAAAAAAAkPzu7ody8MfOGeZOybJ7n5XOLX/cdKWWe/kuL08pY/LPmwAAAADACgYdLVBK2SbJ8UlenWTG4OVh3lqH+/G1iNwwyZkjPO+8JM9Zi2cCAAAAAAAAQ9Ras92xC1f7nmX375uZW5yW0rGsoVatN3va7Lxohxe1uwYAAAAATDod7S4wmZRSOkspJyS5NsnfJpmZgYFGycDYYtXXSj++trm11kuSLBySNfR1QCllpNM7AAAAAAAAgDXw3u9d8rhjjiRJ/+wsu3+f1hdq0Lzt5mWDGRu0uwYAAAAATDoGHWOklPLkJBck+VAeHXKsOt4YbnAx+FpXp6z4c7jByNFj8HwAAAAAAACYci65+b50ze/J9y/6wxr/zNK7D07tn9bCVs2Z0TEjr3/q69tdAwAAAAAmJYOOMVBKeVqSi5Lsm5WHHMnKo43HO6VjrdVaz0pyzaqXV+QadAAAAAAAAMAoLO/rT9f8nvz5584b9c/WZZtm6Z1HtKBV847Z55hsM2ebdtcAAAAAgEnJoGMdrRhznJ5k8zw62kiGH3EMvT5WJ3MM9c0hzxz67G1LKXuMcRYAAAAAAABMSi/74vnZ8fhT1+kZS+85MH2LJ/YQYs/N9syr57663TUAAAAAYNIy6FgHpZQtk/xPko3z6Ghj6FBj1RHH8iQXJPlOks8nWbjK+9bVv6/m3nPHKAMAAAAAAAAmpbOvuSNd83vyq957xuBpnXnk1qNS+6eNwbOaN6NjRk484MR0dnS2uwoAAAAATFoT828Px49vJ3lSVh5uZJXv+5P8IMmXk5xXa108+MOllNcnmTdWZWqtvy+lXJxk7zx2JHJ4kk+MVRYAAAAAAABMFg8vXZ657//JmD+3f+kWWXLnEZn1xHU77aMd3rbP27L9Rtu3uwYAAAAATGoGHWtpxRjjOVn9mOMnSd5Wa72+wWqnZmDQMWjw1JDnlFI6a619DXYBAAAAAACAce3pHz0zdz6wpGXPX3bPQemcdVumb3hxyzLGWvf23Tl696PbXQMAAAAAJr2OdheYiEopM5OcmJXHGzWPjidqkvclmdfwmCNJfjrk6zLk6/WT7N5wFwAAAAAAABiXvnvhzema39PSMUeSvObZ2+eyt/5rDnnyIS3NGSuHbHNITjzgxHQU/5QMAAAAAK3mhI6189okW+bRAcegwTHH39VaP9GOYkl+mUeHJnWVe7slubTZOgAAAAAAADB+3P3gkjztI2c2ktW7oPtPX3/8kI/nvee8N+f84ZxGstfGIdscko8f/PFM75je7ioAAAAAMCUYdKyd16zy/dCTOb7cxjFHaq33l1J+n+Qpw9zetek+AAAAAAAAMF50ze9pJOe8+Ydl641mr3RtZufMnHLoKXnfee9Lz43N9BiN7u27c+IBJxpzAAAAAECDDDpGqZSybZJnZOURx6B7kvx9O3qt4pok2+axJ3QYdAAAAAAAADDlfOb/rss/n3Fty3OOPXLXvOngHUa8P71jek468KTssvEu+exvP5ul/Utb3unxzOiYkbft87YcvfvR6Sgd7a4DAAAAAFOKQcfoHTrMtcFhx8m11gca7jOc341wvavJEgAAAAAAANBOv7/74TznY2c3ktW7oHuN3tdROvLap742Bz/54Jxw3gm57K7LWtxsZHtutmdOPODEbL/R9m3rAAAAAABTmUHH6O0/5Ou6ytf/1nCXkdw+zLWSZE7TRQAAAAAAAKBptdZsd+zCRrIuef/zsuF600f9c9tvtH2+eeQ3880rv5nP/fZzjZ7WMaNjRt66z1tz9Nyj09nR2VguAAAAALAyg47R23mV78uKPy+ptd7RdJkR3LvK9zUGHQAAAAAAAEwB//D9S/OdC29uec6nXrF3/nzvrdfpGdM6puV1T31djnjKEfnq5V/NwpsWZvHyxWPU8LFmT5udedvNy+uf+vpsM2ebluUAAAAAAGvGoGP0urLyyRxZ8f0FzVcZ0SMjXN+g0RYAAAAAAADQkMv+cH9e+NlzW56z7abr5ad/d+iYPnObOdvkg8/+YN6z33vyPzf8T75zzXdy0/03jdnzt9twu7x8l5fnRTu8KBvM8E+GAAAAADBeGHSM3kYjXB8vp3Mkjx2cDFq/0RYAAAAAAADQYsv7+rPj8ac2knX1iS/IrOmdLXv+BjM2yN/s9jf5613/Ohf+8cKcffPZueKuK3LVPVeN6uSO2dNmZ7dNdsvum+2eQ7c5NPs9cb+UUlrWGwAAAABYOwYdozfSKGI8DTo2HuH6skZbAAAAAAAAQAu94svn54Ib72l5zr+/4Zk5YMfNWp4zqJSSp2/59Dx9y6cnSfr6+9K7qDdX3n1lrrvvuixasihL+5Zmaf/SzOiYkRmdMzJn5pzstNFOmbvp3HTN6UpnR+uGJwAAAADA2DDoGL3+JMP97eeMpousxkiDjocbbQEAAAAAAAAt8NNr78yr//VXLc85eOfN843XPaPlOY+ns6MzO2y0Q3bYaId2VwEAAAAAxpBBx+g9lGSjYa5v2nCP1dlkhOv3N9oCAAAAAAAAxtDipX3Z7f2nNZJ1w0nz0tlRGskCAAAAAKYmg47Ruz/jf9CxxyrflyQ1yc1t6AIAAAAAAADrbP+T/y+33f9Iy3MWvv2gzN1qTstzAAAAAAAMOkbvd0m6MjCQGGqf5qs8VillvST75rH9kqS32TYAAAAAAACwbn5w0R/ynu9d0vKcVz1r25z44qe2PAcAAAAAYJBBx+jdmOTgId/XDJyAsW8pZVattfW/Fmj19s/Af66DvYYOOy5tSyMAAAAAAAAYpXsfWpp9TjyjkaybTp6XUkojWQAAAAAAgww6Ru/CJK9d8fXQwcT0JIckOa0NnYb6q9Xc+1VjLQAAAAAAAGAtdc3vaSTn3H84NE/eeL1GsgAAAAAAVmXQMXrnrebeO9LGQUcp5YlJ/iaPjkyGns7xcJJfN14KAAAAAAAA1tDnzr4+H/vJNS3P+YcX7Jo3H7JDy3MAAAAAAFbHoGP0LktyS5KtMjCYKEP+fF4pZY9a62Vt6vbOJDOH6VWTnF5rXdKmXgAAAAAAALRJX39fbrr/plx5z5W5/t7rs2jpoizpW5Jl/csyvWN6ZnbOzJwZc7Ljxjtm9013T9ecrnR2dDba8eZ7Hs5B/3R2I1m9C7obyQEAAAAAeDwGHaNUa62llO9n4DSOusrtkuSrpZQDaq3LmuxVSnlWkvcM02nQdxqsAwAAAAAAQJvUWnPhHy/MWb8/K1fcfUWuvufqLF6+eI1/fva02dl1k12z+6a757CnHJb9nrhfSikt67rdsQtb8uxV/fZ9R2Tj9Wc0kgUAAAAAsCYMOtbOvyR5+4qvVz0N42lJ/inJu5oqU0rZJAODjWmr9Bl0e5L/aqoPAAAAAAAAzVu0dFF+fMOP851rvpOb7r9prZ+zePni/PaO3+a3d/w2/3bVv2W7DbfLy3d5eV64wwszZ8acMet73H9flv/45e/H7Hkj+cTL98pf7PPklucAAAAAAIyWQcdaqLVeWUr53yQvzKPDiaGjjreXUh6ptR7b6i6llI2S9CTZZkj+n26vuPa5WuvyVncBAAAAAACgeTcvujlfvfyrWXjTwlGdxLGmbrr/piz41YJ86jefyrzt5uX1T319tpmzzVo/7/Jb7s+ffebcMWw4vK03mp3z5h/W8hwAAAAAgLVl0LH2TkhyZJLOPDqkGDrq+PtSylOSvKXWen8rCpRSnpbkP5PskJVP5Bj69a1JTmlFPgAAAAAAAO2zvH95vnHFN/L5iz+fpf1LW563ePni/OC6H+THN/w4x+xzTF4999Xp7Ohc45/v66/Z4biFLWz4qKtPfEFmTV/zbgAAAAAA7dDR7gITVa31siSfysonYiQrjzpekeT6UspbSiljNp4ppWxXSvlSkguS7Dikw3Cnc7yn1vrIWGUDAAAAAADQfjfed2OOPvXofPI3n2xkzDHU0v6l+cRFn8jRpx6dG++7cY1+5lVf/WUjY45vvf4Z6V3QbcwBAAAAAEwIBh3r5oQkv13x9dBTMYaOOjZN8pkkd5RSvlpKObKUssVoQkopnaWUvUopx5RSTk9ybZI35NHTQQazMuTrmuTfa63fXbuPBgAAAAAAwHjTX/vztcu/lqN+fFQuu+uytna59K5Lc9SPj8rXLv9a+mv/sO/5+XV3pmt+T35+3V0t7XLgjpuld0F3Dtpp85bmAAAAAACMpTE7NWIqqrUuKaW8JMkvk2yWlYcVg6OKwa83SvKaFa+UUu5J8sBIzy6l/CzJrCRbJNkqA+ONP90erDDC9zXJpUnePOoPBQAAAAAAwLi0rH9Z3nfe+9JzY0+7q/zJ0v6lOeWiU3LNvdfkxANOzPSO6UmSR5b1Zdf3ndZIhxtOmpfOjvL4bwQAAAAAGGcMOtZRrbW3lHJ4krMycBrHcKOOVYcXWfHeTVe5PvTPA1Z5/0qxQ74ebtxxU5Ija60PjerDAAAAAAAAMC4t6VuS957z3pzzh3PaXWVYPTf25KGlD+Xjh3w8h33svNxy3+KWZ/7v2w7MU7fesOU5AAAAAACt0tHuApNBrfXyJIcm+X2GH3EMHV3UVe6PZKSfGem5g9euTHJwrfX2tfw4AAAAAAAAjCPL+peN6zHHoHP+cE72+sKrcst9D7Y056+e8ZT0Lug25gAAAAAAJjwndIyRWusVpZT9knw7yeEZeXzxpx/Jyqd5POaRq3w/3PtWPanj1CR/U2u9b82bAwAAAAAAMF711/6877z3jfsxx6DpG1yVbPW9PHLry9KK3y1308nzUspI/7wGAAAAADCxOKFjDNVa7661HpHkLUkWZfhTNgYNN/LIMPdXNwYZfN9DSd5Ra+025gAAAAAAAJg8vnHFN9JzY0+7a4zK9A0vzvRNzh3TZ/787w9N74JuYw4AAAAAYFIx6GiBWusXk2yf5GNJHs7Ko4ya4QceIz5uhJ8pSZYn+UqSnWqtnxmr/gAAAAAAALTfjffdmM/+9rPtrrFWZm5+ejpm3LHOz/m75++S3gXd2WaT9cagFQAAAADA+GLQ0SK11ntrrf+QZKsMnNhxQQbGGKueujHcYGN1J3qUJDcm+XCSbWutb6y1/rHlHwgAAAAAAIDGLO9fnhPOOyFL+5e2u8paKR3LM2ur7yXpX+tn9C7ozjGH7jh2pQAAAAAAxplp7S4w2dVaH0jyxSRfLKVsluR5SZ6ZZJ8kuybZ7HEesTRJb5LfJvllkjNrrZe3rDAAAAAAAABt980rv5nL7rqs3TXWSefsmzNjk59n6T0Hj+rnfvO+I7LJ+jNa1AoAAAAAYPww6GhQrfWuJP+x4pUkKaXMzMApHhskmZ1kepIlSR5Ocnet9fY2VAUAAAAAAKBNbl50cz7328+1u8aYmLH5GVn2wFNTl236uO/92Ev3zFH7bdNAKwAAAACA8cGgo81qrUuS3NTuHgAAAAAAAIwPX738q1nav7TdNcZE6VieGZv+NEtu/8sR37PlnFm54LjDG2wFAAAAADA+GHQAAAAAAADAOLFo6aIsvGlhu2uMqekb/jZL7piX9M96zL2rPvyCzJ7R2YZWAAAAAADtZ9ABAAAAAAAA48SPb/hxFi9f3O4aY6p0LMv0DS/KsnsP+NO1r7/26Tlkly3a2AoAAAAAoP0MOgAAAAAAAGAcqLXm21d/u901WmL6xhdk2b3PzrO23zTffuP+7a4DAAAAADAuGHQAAAAAAADAOHDhHy9M76Ledtdoic6Zd+bbb98iz9rqGe2uAgAAAAAwbnS0uwAAAAAAAACQnPX7s9pdoaV+dss57a4AAAAAADCuGHQAAAAAAADAOHDF3Ve0u0JLXXHX5P58AAAAAACjZdABAAAAAAAAbdbX35er77m63TVa6qp7rkpff1+7awAAAAAAjBvT2l1gKiqlzE4yN8kOSbZM8sQk6yeZlaQzySMrXncnuT3JzUmurLXe1pbCAAAAAAAAtNRN99+UxcsXt7tGSy1evji9i3qzw0Y7tLsKAAAAAMC4YNDRgFLK+kn+LMlhSQ5OsmOSshbPuTfJL5Kck+R/a63XjmFNAAAAAAAA2uTKe65sd4VGXHn3lQYdAAAAAAArGHS0UCnl4CTHJOnOwOkbyVoMOYbYZMWzupN8rJRySZKvJvlarfXhdekKAAAAAABA+1x/7/XtrtCI6+67rt0VAAAAAADGjY52F5iMSimHlVJ+neSsJC9JMjsDQ46SpK7jqwx57Z3k00luLqUcX0oZHI0AAAAAAAAwgSxauqjdFRqxaMnU+JwAAAAAAGvCoGMMlVK2KKX8d5Izkuyb4Uccf3r7WrwyzLNKko2TfDjJNaWUF7To4wEAAAAAANAiS5Y/0u4KjVjat6TdFQAAAAAAxg2DjjGyYkhxWZIX5bFDjmT4ccaoY1Z5rXpyxzZJekopny6ldK5lBgAAAAAAAA1btviedldoxNKH7253BQAAAACAccOgYwyUUt6S5H+SbJ6VhxbDDTjqOr5Wis5jT+4oSY5JcmopZc4YfkwAAAAAAABaZPqSB9pdoREzljzY7goAAAAAAOOGQcc6KqW8M8lnkkzLY4ccQ606yFj1tI01eQ19znDPGrxfkhye5LRSyvrr+BEBAAAAAABosZmPTJVBx9T4nAAAAAAAa2JauwtMZKWUlyf55zx6Kkcy/JBj6PX+JNcluTTJJUmuTXJ/kkUrXsuSzBnyelKSvVa8nppkgyHPXfXZq446npnk+0mOXPtPCQAAAAAAQKvN6e9vd4VGzOmbGp8TAAAAAGBNGHSspVLKjkm+nJHHHKteuyDJfyT5Tq31zrXM7ExyRJK/TvLnGRh3DD0V5E9vHXLteaWU42utH12bTAAAAAAAAFpvxzKz3RUasVOZ0e4KAAAAAADjRke7C0xgX8ijg4qSkcccP02yT6312bXWz67tmCNJaq19tdbTaq1HJ9kqyclJlmTlUclKP7Li3vtKKTutbS4AAAAAAACtNTdTY+gwVT4nAAAAAMCaMOhYC6WUeUkOz2NHFENPy7g7yUtrrYfWWi8Z6w611odqrccnmZvkJ3nsqGPowGR6kgVj3QEAAAAAAICxsd20OZnd39/uGi01u78/XdM2bHcNAAAAAIBxw6Bj7bxryNeDw4mhJ3Vcm2S/Wut/tbpIrbU3ybwkn8zwJ3UM9vrzUsr2re4DAAAAAADA6HVOn5Vdly5td42W2m3p0nROn9XuGgAAAAAA44ZBxyiVUp6SR0/nGDrmGPSHJAfXWn/fVKc64N1JvpiVRx1DT+koSV7bVCcAAAAAAABGYdZG2X3J5B507L5kaTJ7o3bXAAAAAAAYNww6Rm/eKt8PHU/0J3l5rfWPzVb6k7cn+e2Kr1c9qaPksd0BAAAAAAAYB75543o57OHF7a7RUoc+vDjZYm67awAAAAAAjBsGHaN34DDXBk/F+Fat9fyG+/xJrXV5kndk5ZM5kkfHHXuVUtZrthUAAAAAAAAjuf6OB9I1vyff+t0m2e+RJelauqzdlVpiu6XLst8jS5In7d3uKgAAAAAA44ZBx+it7tcGfbyxFiOotZ6b5Jd5dGQydNxRkuzWjl4AAAAAAACsrGt+T557ys+SJDfUrbK4zswrHnigza1a4+UPPJAyff1ks53aXQUAAAAAYNww6Bi9J+fREy/qkOvX1VqvaEOf4Xx/Nfe2bqwFAAAAAAAAj/H8T/wsXfN7VrrWn45cUbfNCx98KLP7+9vUrDVm9/fnRQ88lDxpz6Sjs911AAAAAADGDYOO0dtgle8HT8L4eRu6jOTc1dxbtT8AAAAAAAANOP+Gu9M1vyfX/HH4Uzgu7d8hc/pr5j34cMPNWmvegw9ng1qTrfZtdxUAAAAAgHHFoGP0ygjXr2u0xeqtrstI/QEAAAAAAGiB/v6arvk9+at/uWC17zuj/2lJktffvygz+utq3ztRzOivef39iwa+2XVee8sAAAAAAIwz09pdYAJ6IMkmw1y/v+kiq7G6LsP/yicAAAAAAADGXNf8njV+7wX9u+WG/idlh+W35Zj77ssnNtm4hc2accx992Wb5cuTzXZOtj2g3XUAAAAAAMYVJ3SM3r0jXO9stMXqra7LPY21AAAAAAAAmKJ++NtbRjXmGFDyrb4jkiRH3/9A9nhkydgXa9CejyzJq+9f8bvGnv6GpDhIHgAAAABgKIOO0bs6yXB/27xZ00VWY9PV3LumsRYAAAAAAABTzCPL+tI1vyfv/M7Fa/Xz/9V3UB6uMzMtyUfuujsz+uuY9mvKjP6aE++6e+C3kE1fL9nrFe2uBAAAAAAw7hh0jN7lI1zfrdEWqzd3yNdD/5b/rlrrHU2XAQAAAAAAmAq65vdk1/edtk7PWJT188O+ZydJtl+2PG+9774xaNa8t917X7Zftnzgmz2OSmZt2N5CAAAAAADjkEHH6J2xyvc1Ayd2HNJ8lREdtsr3JQM9z2xDFwAAAAAAgEnt0/93Xbrm94zZ877Y96IsqdOTJK++/4F0P/jQmD27Cd0PPpSjFz0w8E3nzOTAd7a1DwAAAADAeGXQMXo/T3LfMNe3KKUc0XCXxyillCR/lZVP5hj0Pw3XAQAAAAAAmLTuenBJuub35JQzrh3T5/6+PjGnLH9pkoF/zDvxzrtzyEMPj2lGqxzy0MM58c67H/1HyEOPSzbZvp2VAAAAAADGLYOOUaq1Lk/ylQycerGq4xquM5y/SdI1zPXbk/xXs1UAAAAAAAAmp675PdnvI607HP0rffNycf8OSZLpST5+513jftRxyEMP5+N33pXpgxe23i959tvaWQkAAAAAYFwz6Fg7n0iyeMj3NQMDj+eUUl7bnkpJKWXzJP+UlU/nKCu+/0StdVlbigEAAAAAAEwS7/rOxema39PynH223Sx7v+0/k86ZSZKZNTnljrvS/eBDLc9eG90PPpRT7rgrMwf/lapzZvLizycdnW3tBQAAAAAwnk1rd4GJqNZ6Wynl/Uk+lkfHE4Ojjs+WUq6utZ7fZKdSyqwk30+y5ZBOg2OOy5N8ssk+AAAAAAAAk8kNdz6Yw//5p41k9S7ofvSbw45Pznh/koGTOk668+7ssnRpPrvRRlnaMdyB8s2a0V/ztnvvy9GLHlj5N8kddkKy+S7tqgUAAAAAMCEYdKy9U5LMS3JoVh51zE5yWinlqFrr6U0UKaVsnOQHSQ7KyqdzJMlDSV5Va13eRBcAAAAAAIDJpokTOZLkZ393aJ6y6XorX9z/bcntlyeXfTdJ0pHktfc/kIMfXpwTNts0l82a2Ui34ez5yJKceNfd2X7ZKv8MtcfLkv3f2p5SAAAAAAATSMfjv4Xh1FprkhcnuTQDJ2H86VaSDZL0lFI+XkqZ08oepZS/SPKbJAeveivJ0iQvrbVe2soOAAAAAAAAk9G8T/28kTHHK56+TXoXdD92zJEkHR3Jiz+f7HzkSpe3X7Y837ztj3nXPfdmRv+qv++rtWb017z7nnvzzdv++Ngxxy7zBvp2+GdIAAAAAIDH429S10Gt9YEMnNDx0zx21NGZ5F1JriulvLmUsv5YZpdS9i+lnJXk+0m2XZE/+Lf1Jcn9SV7Y1CkhAAAAAAAAk8WvbronXfN7cuVti1qe1bugOwtesufq39Q5PTnq648ZdUxL8rr7H8gPb7ktL1n0YGb397esZ5LM7u/PSxY9mB/ecltee/8D6Vz1DbvMS176tYG+AAAAAAA8rjJw0ATropQyLck/JnlHVh52DH5dkyxO8r9Jvp3k3FrrXaPMmJ5kzyR/keQVSbYbkjF0yJEkFyZ5Za312tF9EmA0SilXJJm76vW5c+fmiiuuaEMjAAAAAADWRX9/zfbHLWwk67fvOyIbrz9jdD/Utyz54VuSy7477O0HSsn/bLB+vrPBBrlpxtiNKrZbuiwvf+CBvOiBh7LBSP+2uMfLBk7mMOYAAAAAANbC7rvvniuvvHK4W1fWWndvuk9TptSgo5RydIsjnpnk/w0XveLPof/D/mOSS5Jcm4HTNB5IsijJsiQbJJmz4rVVBoYcuyR/+kVHq54GMnitJrk4yWeT9K1aotb6zVF+HmA1DDoAAAAAACaP7Y7tSRP/bHbskbvmTQfvsPYP6O9Pzv9MctZHk74lw76lJrlw1sycvd7sXDFzRq6aMSOLOzrWOGJ2f392W7o0uy9ZmkMfXpz9Hlmy0j9OraRzZnLYCcn+b01GkQEAAAAAMJRBxxRQSunPyqOKlkUN+bqOcH3Ve2v6vLV+Zq31MSdfA2vPoAMAAAAAYOL7n0tuzdv/87eNZPUu6B67h915TfLDNye3XPS4b+1L0jt9Wq6cOSPXTZ+RRR0dWVpKlpZkRk1m1Jo5/f3ZadnSzF2yNF3LlmeN/lFp6/0GTuXYfJd1/TQAAAAAwBQ3VQcd09pdoE1G/CVCLcyqeezYYjQ91vRnR7o+dZY7AAAAAAAAj+ORZX3Z9X2nNZJ19YkvyKzpY/x7tzbfJXnd6cn5n03OPmnE0zqSgSPgd1i2PDssW57k4XXP7pyZHHb8ilM5/D4xAAAAAIC1NVUHHa0eNww3qhjuJI3R9liTAchwz2xywAIAAAAAADCudc3vaSTns3+9T/5sz61aF9A5LTnwncncFyXnfjK57HvJsjEYbIxk+nrJHkcNZG6yfetyAAAAAACmiKk66BgPA4dWdRhuOAIAAAAAADDlfe7s6/Oxn1zT8pwZnR259qNHtjznTzbZPnnRp5PnnZhc8u3k119J7rp27J6/2c7J09+Q7PWKZNaGY/dcAAAAAIApbqoOOgAAAAAAAJgi7nloafY98YxGsm48aV46Otr0u8VmbZg8803JM96Y/O685OqFya2/SW67ZHQnd0xfP3nSnslW+ya7zku2PSAp4+H3pQEAAAAATC5TddDh1AoAAAAAAIApoGt+TyM5P3jz/nnatps0kvW4Skm6Dhx4JUl/X3LXdcltFyd3XJksvi9ZviTpW5J0zkymzUxmb5RsMTd50t7JZjslHZ3t6w8AAAAAMEVMxUGHXx8EAAAAAAAwyb33e5fk+xf9oeU5ez15w/zorQe2PGeddHQmW+w68AIAAAAAYNyYaoOO7dpdAAAAAAAAgNa56a6HcujHz2kkq3dBdyM5AAAAAABMTlNq0FFr/V27OwAAAAAAANAaXfN7Gsk5+72HZLvN1m8kCwAAAACAyWtKDToAAAAAAACYfP78c+flkpvva3nOS5/25Hz8qL1angMAAAAAwNRg0AEAAAAAAMCEdNHv7slLvnB+I1m9C7obyQEAAAAAYOow6AAAAAAAAGBC6e+v2f64hY1k/eZ9R2ST9Wc0kgUAAAAAwNRi0AEAAAAAAMCEsfPxp2ZpX3/Lc/7u+bvkmEN3bHkOAAAAAABTl0EHAAAAAAAA417PpbflmP/4TSNZvQu6G8kBAAAAAGBqM+gAAAAAAABg3FqyvC+7nHBaI1lXn/iCzJre2UgWAAAAAAAYdAAAAAAAADAudc3vaSTnU6/YO3++99aNZAEAAAAAwCCDDgAAAAAAAMaVL/30hpx86tWNZPUu6G4kBwAAAAAAVmXQAQAAAAAAwLhw70NLs8+JZzSSdeNJ89LRURrJAgAAAACA4Rh0AAAAAAAA0HZd83sayfnum/bPM7bbpJEsAAAAAABYHYMOAAAAAAAA2mb+Dy7Nt399c8tzdt9qTnreflDLcwAAAAAAYE0ZdAAAAAAAANC43939UA7+2DmNZPUu6G4kBwAAAAAARsOgAwAAAAAAgEZ1ze9pJOes9xyc7Td/QiNZAAAAAAAwWgYdDSmlbJJkjyRdSZ6UZNMks5PMTNLZQIWbaq0nNpADAAAAAAAwrJd84Re56Hf3tjznL/bZOp94+d4tzwEAAAAAgHVh0NEipZRZSV6c5AVJDkuydVsLJRclMegAAAAAAAAa95vf35u//PwvGsnqXdDdSA4AAAAAAKwrg44xVkp5cpL5SV6ZZIPBy+1rBAAAAAAA0B611mx37MJGsi484bnZ7AkzG8kCAAAAAICxYNAxRkop05O8L8nfJ5melUcctS2lAAAAAAAA2mTu+0/Lw0v7Wp7z7iN2ztsP36nlOQAAAAAAMNYMOsbAilM5fpxkzzw65Fh1xNGOUzpqm3IBAAAAAIAp6rTLb8v/+7ffNJLVu6C7kRwAAAAAAGgFg451VErZOck5SZ6YgfHE0CHHqmOK1d0b7j2re9+a/OxI1wAAAAAAAMbUkuV92eWE0xrJuvrEF2TW9M5GsgAAAAAAoFUMOtZBKeWJSc5IsmUGhhOD44mhI4x1HVQM9/MjjTxGOh0EAAAAAACgZbrm9zSS84mX75W/2OfJjWQBAAAAAECrGXSsm28k2SarH3IMvdaf5K4knUk2XfGessqfv1/x9YZJ5mT4Uz6GPnvo18uT3DJC11vX8DMBAAAAAACska/8/MZ8pOeqRrJ6F3Q3kgMAAAAAAE0x6FhLpZRXJXleRh5zDH5/dpLvJTk1yc211v5SyuuT/Mtwz621bjckoyTZOMn2SQ5Y8TosySZZedgxqDPJz5K8tdb6wFp/OAAAAAAAgNW4/+Fl2evDpzeSdeNJ89LRMdLh5QAAAAAAMHEZdKyFUsq0JB/JY8ccQ7/vTfK2WutanzFea61J7lnxujDJp0ops5IcneStSZ46JHNwRPLKJAeVUrprrc38SiwAAAAAAGDK6Jq/1v/0MSr/+bfPyv47bNpIFgAAAAAAtENHuwtMUC9Jss2Kr4eOOcqK11VJnr0uY46R1FofqbV+uda6Z5J3JHlk6O0V+V1Jzi2lPGus8wEAAAAAgKnphB9e1siYY5cnbpDeBd3GHAAAAAAATHpO6Fg7b1jl+zrk6zuTHFprvaPVJWqtnymlnJHku3n0tI7BLhsn6SmlPLPWen2ruwAAAAAAAJPTzfc8nIP+6exGsnoXdDeSAwAAAAAA44FBxyiVUp6Q5DlZecSRDJyMUZO8q4kxx6Ba69WllOckOTPJvkN61QyMOv63lLJ3rfWRkZ4BAAAAAAAwnCZO5EiSM9/9nOy4xQaNZAEAAAAAwHjR0e4CE9AhSaav+HpwxFFWfP+bWut/NF2o1npfkj9Lcuswt3dK8oFGCwEAAAAAABPay790fiNjjhfutVV6F3QbcwAAAAAAMCU5oWP0njbC9Zrkq00WWSm81ttLKW9K8uOsfEpHSfLuUsoXa62/a1c/AAAAAABg/Lvk5vvy5587r5Gs3gXdjeQAAAAAAMB4ZdAxensM+bqu8vV/NtxlJbXWnlLKWUkOy8rdpiU5Jsnft6UYAAAAAAAwrtVas92xCxvJ+vXxz83mG8xsJAsAAAAAAMYzg47Re8oI16+vtd6/rg8vpXTWWvvW4RGfyMCgY9DgKR2vLaX8Q621Dv9jAAAAAADAVLTXh07P/YuXtTznHYfvlHcdsXPLcwAAAAAAYKIw6Bi9rbLy6Rdlxfe/HqPnT0+yLoOO05Lcn2TOKtc3SfKMJL9ch2cDAAAAAACTxOlX3J43fuuiRrJ6F3Q3kgMAAAAAABOJQcfobTDC9d+P4hn9q7n3hCSPjOJZK6m19pVSzk3SnZWHJ0lyeAw6AAAAAABgSlu6vD87n3BqI1lXfvj5WW+Gf44CAAAAAIDh+Bv00Zs1wvX7R/GMpau594Qkd43iWcO5MgODjlU9dR2fCwAAAAAATGBd83sayfnYS/fMUftt00gWAAAAAABMVAYdo7fqqReDRjPoWLKae1sk6R3Fs4Zz6zDXSpJd1vG5AAAAAADABPS1827Kh358ZSNZvQuG+51TAAAAAADAqgw6Rm9Rkk2HuT5jFM9Y3fjjSaOrM6zFq3xfMzDoGItnAwAAAAAAE8T9i5dlrw+d3kjWDSfNS2dHaSQLAAAAAAAmA4OO0Rtp0LHhKJ5x52rubTe6OsNaf5TXAQAAAACASaZrfk8jOf/+hmfmgB03ayQLAAAAAAAmE4OO0VuUgdMu6irXRzPouHU193YfdaPH2mSE67PH4NkAAAAAAMA49sH/uSJf/0Vvy3O232z9nPXeQ1qeAwAAAAAAk5VBx+jdlGTvYa5vvKYPqLXeUUpZlGSDrDwMKUn2W6d2A/Ya4foDY/BsAAAAAABgHPrDvQ/nwH88u5Gs3gXdjeQAAAAAAMBkZtAxelePcH3XUT7nqiTPzKODjpqBQccepZTNaq13rU25UkpHkmfnsSeIJMnda/NMAAAAAABgfOua39NIzunvek52fuIGjWQBAAAAAMBkZ9AxeqsOOv40xBjlc36VgUFHVvx8HfL1S5N8cS37vTjJJkN6Df3ToAMAAAAAACaRV37llzn3+rX6HVGjcuRTt8wXXvm0lucAAAAAAMBUYtAxelcM+XroEOMJpZQdaq03rOFzzk3ytlWuDY4v3lFK+VKtdbhTNkZUSulMctxq3nL5aJ4HAAAAAACMT5f94f688LPnNpLVu6C7kRwAAAAAAJhqDDpG75IkDyR5Qh4dcwzaL8maDjp+kmRZBv4zGHqKRpLsnOSkJMeOstuHkuw75HmrOnuUzwMAAAAAAMaRWmu2O3ZhI1m/Ou7wbDFnViNZAAAAAAAwFXW0u8BEU2vtS/LzDD+YeNEonrMoA6OOoc8ZHHWUJH9fSln1BI8RlVI+kIHTOUYacyQGHQAAAAAAMGE97cQzGhlzHHPoDuld0G3MAQAAAAAALWbQsXZWHUYMjiiOLKV0juI5Xxzm2tBRxydLKQtLKYeWUh4z0iildJRSnltK+UWS9w/zrKEnf/yk1nrbKLoBAAAAAADjwP9d9cd0ze/J3Q8tbXlW74Lu/N3zd215DgAAAAAAkExrd4EJ6swhXw8OJpJkwySHJTljTR5Sa11YSvltkr2z8skaQ0cdz1/xuruUck2S25P0Jdlixc9tOMzPDOekNekEAAAAAACMD8v6+rPT8ac2knXFh56f9Wf6ZyMAAAAAAGiSv5lfC7XWS0op1yXZMY+OOQa9Oms46FjhvXl0IDLSqCNJNkuy6So/O3S8seqYY+jpHGfUWs8dRScAAAAAAKCNuub3NJLzjy/ZIy9/+lMayQIAAAAAAFbW0e4CE9h3M/yA4mWllG3W9CG11rOTfD7Dn6wxOMgYfJVVXqveG9pl8M9bk7xqTfsAAAAAAADt883zexsbc/Qu6DbmAAAAAACANnJCx9r7jyTH57FDjGlJ3pXk3aN41ruS7JbksDw6xiir/Fnz2NNAht7PkPcNXn8kyctqrXeOogsAAAAAANCwBx5Zlj0+eHojWdd/9MhM6/Q7vwAAAAAAoN0MOtZSrfWqUsrxSeYMc/uBUT5reSnlhUm+l2ReVh5vrDrsWO2jhrx3UZK/rLWeP5ouAAAAAABAs5o6keObr3tGnrPz5o1kAQAAAAAAj8+gYx3UWk8ew2ctXjHq+IckH0gyMyOfyjGSwdHHz5O8ptZ601j1AwAAAAAAxtaJ/3tlvnpu6/8qf5tNZufnf39Yy3MAAAAAAIDRMegYR2qtNcmCUsq3ksxP8tdJNl71bUO+XvXUjl8kOaXW+l+tawkAAAAAAKyLW+9bnGcvOKuRrJtOnpdS1uQQcAAAAAAAoGkGHeNQrfWWJG8rpbwnyXOSHJhkbpJtk2yQZEaSxUnuTHJDkl8nOb3W2tuWwgAAAAAAwBrpmt/TSM6p7zgouz1pTiNZAAAAAADA2jHoGMdqrUuTnLniBQAAAAAATFCv/tdf5afX3tnynCPmPjH/cvR+Lc8BAAAAAADWnUEHAAAAAABAi1x+y/35s8+c20hW74LuRnIAAAAAAICxYdABAAAAAAAwxmqt2e7YhY1k/fK4w/PEObMayQIAAAAAAMaOQQcAAAAAAMAYeuZJZ+aPi5a0POf/HbxD5h+5a8tzAAAAAACA1jDoAAAAAAAAGANnX3NHXvu1XzeS1bugu5EcAAAAAACgdQw6AAAAAAAA1sHyvv7sePypjWRd/qHn5wkz/fMOAAAAAABMBv7GHwAAAAAAYC11ze9pJOekv9gjf/3MpzSSBQAAAAAANMOgAwAAAAAAYJT+/Ze/y/H/fXkjWb0LuhvJAQAAAAAAmmXQAQAAAAAAsIYeXLI8T/3ATxrJuv6jR2ZaZ0cjWQAAAAAAQPMMOgAAAAAAANZA1/yeRnK+9tqn59BdtmgkCwAAAAAAaB+DDgAAAAAAgNU4eeFV+dLPbmx5zpZzZuWC4w5veQ4AAAAAADA+GHQAAAAAAAAM4/b7H8mzTv6/RrJuOnleSimNZAEAAAAAAOPDlBp0lFKe0+4O7VRr/Vm7OwAAAAAAwETQNb+nkZyetx+Y3bfasJEsAAAAAABgfJlSg44k5ySp7S7RJjVT7z9vAAAAAAAYlTd849c586o7Wp5z6C6b52uvfUbLcwAAAAAAgPFrqv4X/J1ZDgAAAAAA/MlVty3KkZ/6eSNZvQu6G8kBAAAAAADGt6k66Jhqp3QYsAAAAAAAwDBqrdnu2IWNZJ1/7GF50oazG8kCAAAAAADGv6k66JhKA4epNl4BAAAAAIA1cuA/npU/3Lu45Tl/e9B2Ob57bstzAAAAAACAiWWqDjoAAAAAAIAp6mfX3pmj//VXjWT1LuhuJAcAAAAAAJh4puqgw6kVAAAAAAAwxSzv68+Ox5/aSNZlH3xeNpg1vZEsAAAAAABgYpqKg47S7gIAAAAAAECzuub3NJJz4oufmlc9a9tGsgAAAAAAgIltqg06Dm13AQAAAAAAoDnf+fXv8w8/uKyRrN4F3Y3kAAAAAAAAk8OUGnTUWn/a7g4AAAAAAEDrPbRkeXb/wE8aybruo0dmemdHI1kAAAAAAMDkMaUGHQAAAAAAwOTXNb+nkZyvvnq/HL7bExvJAgAAAAAAJh+DDgAAAAAAYFL4p9OuzufPuaHlOZs9YUYuPOGIlucAAAAAAACTm0EHAAAAAAAwod2x6JE846T/ayTrppPnpZTSSBYAAAAAADC5GXQAAAAAAAATVtf8nkZyfvzWA7PHkzdsJAsAAAAAAJgaDDoAAAAAAIAJ5/9966KcdsXtLc85aKfN8q3XP7PlOQAAAAAAwNRj0AEAAAAAAEwY19z+QJ7/yZ81ktW7oLuRHAAAAAAAYGoy6AAAAAAAACaErvk9jeScN/+wbL3R7EayAAAAAACAqcugAwAAAAAAGNcO/fg5uemuh1qe85pnd+WDL9q95TkAAAAAAACJQQcAAAAAADBOnXf9Xfmbr/yykazeBd2N5AAAAAAAAAwy6AAAAAAAAMaVvv6aHY5b2EjWpR98XubMmt5IFgAAAAAAwFAGHQAAAAAAwLjRNb+nkZwPvnBuXnPAdo1kAQAAAAAADMegAwAAAAAAaLvvXXhz/u77lzaS1bugu5EcAAAAAACA1THoAAAAAAAA2ubhpcsz9/0/aSTruo8ememdHY1kAQAAAAAAPB6DDgAAAAAAoC265vc0kvPlVz0tz9t9y0ayAAAAAAAA1pRBBwAAAAAA0KhTTr8mnz7r+pbnbLTe9Fz8/ue1PAcAAAAAAGBtGHQAAAAAAACNuOOBR/KMj/5fI1k3nTwvpZRGsgAAAAAAANaGQQcAAAAAANByXfN7Gsn50TEHZK9tNmokCwAAAAAAYF0YdAAAAAAAAC1zzH/8Jj2X3tbynGdtv0m+/cb9W54DAAAAAAAwVgw6AAAAAACAMXf9HQ/kuaf8rJGs3gXdjeQAAAAAAACMJYMOAAAAAABgTHXN72kk5+d/f2i22WS9RrIAAAAAAADGmkEHAAAAAAAwJp73iZ/m2j8+2PKcVz1r25z44qe2PAcAAAAAAKCVDDoAAAAAAIB18osb7spf/8svG8nqXdDdSA4AAAAAAECrGXQAAAAAAABrpa+/ZofjFjaSdckHnpcNZ09vJAsAAAAAAKAJBh0AAAAAAMCodc3vaSTnhO7d8oaDtm8kCwAAAAAAoEkGHQAAAAAAwBr7r9/8Ie/+7iWNZPUu6G4kBwAAAAAAoB0MOgAAAAAAgMe1eGlfdnv/aY1kXfuRIzNjWkcjWQAAAAAAAO1i0AEAAAAAAKxW1/yeRnK++Mqn5QVP3bKRLAAAAAAAgHYz6AAAAAAAAIb1qTOvyyfOvLblOevP6MwVH35By3MAAAAAAADGE4MOAAAAAABgJXc9uCT7feTMRrJuOnleSimNZAEAAAAAAIwnBh0AAAAAAMCfdM3vaSTnv97y7Oz7lI0byQIAAAAAABiPDDoAAAAAAIC849u/zY8uvrXlOfttu3G+/+ZntzwHAAAAAABgvDPoAAAAAACAKeyGOx/M4f/800ayehd0N5IDAAAAAAAwERh0AAAAAADAFNU1v6eRnJ/93aF5yqbrNZIFAAAAAAAwURh0AAAAAADAFHPkp36eq25b1PKcv3rGNjn5L/dseQ4AAAAAAMBEZNABAAAAAABTxK9uuicv+9L5jWT1LuhuJAcAAAAAAGCiMugYpVLKG5JsmeSrtdbb2t0HAAAAAAAeT39/zfbHLWwk6+L3H5GN1pvRSBYAAAAAAMBEZtAxelsn+UCS95dSepJ8Oclptdba3loAAAAAAPBYXfN7Gsk5bt6ueeNzdmgkCwAAAAAAYDIw6Fh705K8aMXr5lLKV5P8a631lvbWAgAAAACA5EcX35J3fPviRrJ6F3Q3kgMAAAAAADCZGHSsvZqkrPj6KUk+mOR9pZRTM3Bqx0KndgAAAAAA0LRHlvVl1/ed1kjWNR95QWZO62wkCwAAAAAAYLIx6Fg3QwcbJQP/8/yzFa9bVpza8dVa6x/aUQ4AAAAAgKmla35PIzmf++t9073nkxrJAgAAAAAAmKw62l1ggit59JSOmkdP7ShJnpzk/UluKqX8uJTywlKK/3kDAAAAADDmPnf29Y2MOWZM60jvgm5jDgAAAAAAgDHghI6xseqoY+j1ziTzVrxuLaX8awZO7fh9sxUBAAAAAJhs7nloafY98YxGsm48aV46OsrjvxEAAAAAAIA1YtCxblYdb5RV7tUh95Jk6yQnJDmulHJGki8l+XGttb/VRQEAAAAAmFyaOJEjSX7w5v3ztG03aSQLAAAAAABgKjHoGL1PJ3kgyeuT7Lbi2nDjjcc7teP5K163rzi14yu11t+1sDcAAAAAAJPAe757SX7wmz+0PGevbTbKj445oOU5AAAAAAAAU5VBxyjVWu9NckqSU0opByZ5Y5KXJJk9+JYhb1+TUzuelOS4JMeWUs7MwKkd/1Nr7WvNJwAAAAAAYCK66a6HcujHz2kkq3dBdyM5AAAAAAAAU5lBxzqotZ6b5NxSytuTvCrJG5LsMXg7ozu1oyQ5YsXrj6WUr2Xg1I6bWvcJAAAAAACYCLrm9zSSc/Z7D8l2m63fSBYAAAAAAMBU19HuApNBrfW+Wutnaq17Jdk/ydeTPJxHhxo1jx14DD29Y/De4LUtk8xPcl0p5fRSyktKKcY3AAAAAABTzIs+e24jY46XPu3J6V3QbcwBAAAAAADQICOBMVZr/WWSX5ZS3pHklRk4tWOfwdsZ/akdh6943Tnk1I4bWvcJAAAAAABotwt778lLv3h+I1m9C7obyQEAAAAAAGBlBh0tUmt9IMkXknyhlPK0JG9M8ookGwy+Zcjbh57WMXhv1eHHFkn+PsnflVLOSfKlJP9da13ekg8AAAAAAEDj+vtrtj9uYSNZv3nfEdlk/RmNZAEAAAAAAPBYBh0NqLVelORNpZR3J/mrJH+b5OmDtzP6UzsOXfG6q5Ty9ST/Umu9vmUfAAAAAACAltvp+IVZ1lcf/43r6O9fsEvecsiOLc8BAAAAAABg9TraXWAqqbU+VGv9Sq31mUn2zsAJHovy6FCj5rEDj6GndwzeG7y2eZL3JrmmlHJWKeXlpZTpDX0cAAAAAADGwP9eemu65vc0MuboXdBtzAEAAAAAADBOOKGjTWqtlyY5ppTy3iQvT/KGJM8evJ3RndqRJAeveN1dSvlGBk7tuLZF9QEAAAAAWEePLOvLru87rZGsq098QWZN72wkCwAAAAAAgDVj0NFmtdbFSb6e5OullLlJ3pjklUk2GXzLkLcPPa1j8N6qw4/Nkrw7ybtLKT9P8qUkP6i1Lm3JBwAAAAAAYNS65vc0kvPpv9onL9prq0ayAAAAAAAAGB2DjnGk1nplkneWUv4hyVEZOLXjOYO38/jjjqH3kuSgFa9Pl1K+mYFTO65uRXcAAAAAGtDfl9x1bXLrxckdVyaP3JcsX5L0LU06ZyTTZiazNkq2mJtstU+y2U5Jh9/ID+PJF396Qxac2vq/pu0oyY0nd7c8BwAAAAAAgLVn0DEO1VqXJPm3JP9WStk5A6d2HJ2B0zeS4U/mKKu5t2mSd2ZgLHJeBk7t+P6KHAAAAADGq1qT3nOTaxYmt/wmuf3SZNnDa/7z09dPttwj2XrfZJd5SdeBSSmP/3PAmLv3oaXZ58QzGsm68aR56ejwf+sAAAAAAADjXam1Pv67aLtSyvQkf5nkb5McmoGxxqrDjVUNd2rH4LX7knwjyZed2gFrp5RyRZK5q16fO3durrjiijY0AgAAYNJYfF9yybeTC786cCLHWNls52S/1yd7vSKZvdHYPRdYra75PY3kfO//7Z+nd23SSBYAAAAAAMBY2n333XPllVcOd+vKWuvuTfdpikHHBFRK2T4Dp3a8OskTMzDSWN2vWxtu2DH0+hlJTqm1nj6WPWGyM+gAAABgzN1zY3LuJ5PLvje6kzhGa/p6yR5HJQe+M9lk+9blwBT3D9+/NN+58OaW5+y+1Zz0vP2glucAAAAAAAC0ylQddExrdwFGr9Z6Yynl+CQXJ/lMkk2y+lHHcCOOodePSHJEKeWiJB+stS4c28YAAAAArFbf8uT8zyRnn5z0LWl93rKHk998Y+AUkEOPS579tqSjs/W5MEX87u6HcvDHzmkkq3dBdyM5AAAAAAAAjD2DjgmmlLJDkjckeU2SLQYvj+YRK/6seXTcMXhtvyQ/LqWck+SYWuvV61QWAAAAgMd35zXJD9+c3HJR89l9S5IzP5Bc9ePkxZ9PNt+l+Q4wyXTN72kk56z3HJztN39CI1kAAAAAAAC0hkHHBFBKmZ7kL5P8bZJDMjDAWJMRRx3h+qo/X1e5fmiSi0spx9da/3ltOgMAAADwOPr7B07lOOujzZzKsTq3XJh88aDksOOT/d+WdHS0tw9MQH/5+fPym9/f1/Kcv9hn63zi5Xu3PAcAAAAAAIDWM+gYx0opOyd5Y5Kjk2w6eHnFn0PHGquOOx7v3qonc6z6zJJkRpJ/KqXsm+RVtdb+UX8AAAAAAIbXtyz54VuSy77b7iaP6luSnPH+5PbLB07r6Jze7kYwIVz0u3vzki/8opGs3gXdjeQAAAAAAADQDIOOcaaUMiPJURkYchw4eHnIW0Yaawx3/e4kX0/ytSR7J3lDkoNX3F/TYccrktyf5C2j/SwAAAAADGPZI8n3XpNce2q7mwzvsu8mSx5Ijvp6Mn1Wu9vAuFVrzXbHLmwk66ITnptNnzCzkSwAAAAAAACaY9AxTpRS5mZgxPHKJBsPXl7xZ1317UO+Hm7I8YskX0jyvVrr0hXXrkzyH6WUHTIw7Hhtki1WecbQYcfg4KMkeVMp5b9rrWesxUcDAAAAYFDfsvE95hh07anJ91+bvOybTuqAYez2vtOyeFlfy3Pec8TOedvhO7U8BwAAAAAAgPboaHeBqayUMquU8upSynlJLkvytiSbZGBEMdwpGqter0OuPZjk80n2rLUeWGv99yFjjj+ptd5Qaz02yTZJ/ibJxRl+ODL0WkmyYCw+MwAAAMCU1d+f/PAt43/MMeiahQN9+/vb3QTGjVMvuy1d83saGXP0Lug25gAAAAAAAJjknNDRBqWUPTNwGsffJJkzeHnFn8ONKrKaexdn4DSO/6i1PrSmHWqty5L8Z5L/LKX8eZJ/TrJ9Hh1wDGYMZu5dSnlWrfWCNc0AAAAAYIjzP5Nc9t12txidy76bbLlHcsDb290E2mrJ8r7scsJpjWRdfeILMmt6ZyNZAAAAAAAAtJdBR0NKKesl+eskf5tkv8HLQ94y0pBj6PXBe4uTfCfJF2utv1rXbrXWH5VSfpLkc0lem5VHHUN1JzHoAAAAABitO69Jzvpou1usnbM+kuz8/GTzXdrdBNqia35PIzmffPneefE+WzeSBQAAAAAAwPhg0NFipZSnZeA0jlckeULWbMQx0r2rk3wpyTdqrfeNZc9a6yNJXl9Kmb2i63CjjmeOZSYAAADAlNC3PPnhm5O+Je1usnb6liQ/fEvy+tOTDqcGMHX8y89uzEcXXtVIVu+C7kZyAAAAAAAAGF8MOlqglLJBkr/JwGkcew9eHvKW0ZzGsSzJf2fgNI5zxrTo8N6R5C+TTB/SZ3DcsVMD+QAAAACTy/mfTW65qN0t1s0tFya/+Exy4Dvb3QRa7v6Hl2WvD5/eSNaNJ81LR8dwhyUDAAAAAAAwFRh0jKFSyrMyMOJ4WZL1sm6ncfwuyZeTfLXWescYVx1RrfXOUsoZSbrz2IHJRk31AAAAAJgU7rkxOfukdrcYG2eflMx9UbLJ9u1uAi3TNb+nkZxvv/FZedb2mzaSBQAAAAAAwPhl0LGOSikbJjk6A0OO3QcvD3nLaE7j6E/Sk+SLSU6tta76nqZcmIFBx6o2aLoIAAAAwIR27ieTviXtbjE2+pYMfJ4XfbrdTWDMHfffl+U/fvn7lufsuuUGOe2dz2l5DgAAAAAAABODQcdaKqUcmOSNSV6SZFbW7TSO25N8NcmXa603j3HVtXHnkK9LHu286mcBAAAAYCSL70su+167W4yty76XPO/EZNaG7W4CY+Lmex7OQf90diNZvQuG+x06AAAAAAAATGUGHaNUSjkiyaeS7DJ4acjt0Qw5apKzMnAaxw9rrcvHuOq6mCS/NhIAAACgjS75drLs4Xa3GFvLHh74XM98U7ubwDrrmt/TSM6Z7z44O27xhEayAAAAAAAAmFgMOkbv2Ul2HfL9aE/juCfJN5J8sdZ63djXAwAAAKDtak1+/ZV2t2iNX38lecYbk+IwVyaml33x/Pyq956W57xwr63ymb/ap+U5AAAAAAAATFwGHWtvtEOOCzJwGsd3aq0T6QSM+vhvAQAAAGAlvecmd0/S3+Vx17XJ785Lug5sdxMYlYtvvi8v/tx5jWT1LuhuJAcAAAAAAICJzaBj3Qwdcgw34ngwyb8n+UKt9dLGWo0dv2YRAAAAYG1cs7DdDVrr6oUGHUwYtdZsd2wz/zf56+Ofm803mNlIFgAAAAAAABOfQce6G27IcWmSLyT591rrg81XWmc/TfLadpcAAAAAmLBu+U27G7TWrZP88zFp7PHBn+SBR5a3POcdh++Udx2xc8tzAAAAAAAAmFwMOtbN4JijJHkkyfcycBrHBe2rtO5qrdcnub7dPQAAAAAmpP6+5PaJeFjrKNx26cDn7OhsdxMY1k+uuD1v+tZFjWT1LuhuJAcAAAAAAIDJx6Bj7Q2exnFtki8l+Xqt9d429gEAAABgPLjr2mTZw+1u0VrLHkruui7ZYtd2N4GVLF3en51POLWRrKs+/ILMnmHUBAAAAAAAwNoz6Fg7y5P8KAOncZzV7jIAAAAAjCO3XtzuBs247WKDDsaVrvk9jeT881F75SVPe3IjWQAAAAAAAExuBh2j96MkX6q13t7uIgAAAACMQ3dc2e4GzZgqn5Nx71/PvSkf/t9m/vexd0F3IzkAAAAAAABMDQYdo1RrvbjdHQAAAAAYxx65r90NmrH4vnY3YIq7f/Gy7PWh0xvJuuGkeensKI1kAQAAAAAAMHUYdAAAAADAWFq+pN0NmjFVPifjUtf8nkZy/uMNz8yzd9yskSwAAAAAAACmHoMOAAAAABhLfUvb3aAZfQYdNO8DP7o83zj/dy3P2X7z9XPWew5peQ4AAAAAAABTm0EHAAAAAIylzhntbtCMzpntbsAU8od7H86B/3h2I1m9C7obyQEAAAAAAACDDgAAAAAYS9OmyNBhqnxO2q5rfk8jOae/6znZ+YkbNJIFAAAAAAAAiUEHAAAAAIytWRu1u0EzZm/U7gZMcn/zlQty3vV3tzxn3h5b5vN/87SW5wAAAAAAAMCqDDoAAAAAYCxtMbfdDZoxVT4njbv0D/flRZ89r5Gs3gXdjeQAAAAAAADAcAw6AAAAAGAsbbV3uxs040l7t7sBk0ytNdsdu7CRrF8dd3i2mDOrkSwAAAAAAAAYiUEHAAAAAIylzXZOpq+XLHu43U1aZ/r6yWY7tbsFk8i+J56Rex5a2vKctx66Y977/F1angMAAAAAAABrwqADprBSysZJrkryxDV4+zdqra9pbSMAAACYBDo6ky33TG6+oN1NWudJew58TlhHZ175x7zhmxc2ktW7oLuRHAAAAAAAAFhTBh0wtf1z1mzMAQAAAIzG1vtO7kHHVvu2uwET3LK+/ux0/KmNZF354ednvRn+KhwAAAAAAIDxx79iwRRVSjksyWvb3QMAAAAmpV3mJRd8vt0tWmfXee1uwATWNb+nkZx/eumeedl+2zSSBQAAAAAAAGvDoAOmoFLK7CRfbncPAAAAmLS6Dkw23Sm5+7p2Nxl7m+2cbHtAu1swAX3z/N68/0dXNJLVu6C7kRwAAAAAAABYFwYdMDV9KMkO7S4BAAAAk1YpydPfkJz2D+1uMvae/oaBzwdraNEjy7LnB09vJOuGk+als8P/fgIAAAAAADAxGHTAFFNK2SfJu9rdAwAAACa9vV6R/N+HkmUPt7vJ2Jm+3sDngjXUNb+nkZxvvf4ZOWinzRvJAgAAAAAAgLHS0e4CQHNKKZ1JvhpjLgAAAGi92RslexzV7hZja4+jklkbtrsFE8CHf3xlI2OObTddL70Luo05AAAAAAAAmJD8l7phanlPkn1GuHdjku0b7AIAAACT34HvTC75dtK3pN1N1l3nzIHPA6tx632L8+wFZzWSddPJ81JKaSQLAAAAAAAAWsGgA6aIUsoOST44wu1fJDkzyfsbKwQAAABTwSbbJ4cel5z5gXY3WXeHHjfweWAETZzIkSSnvfOg7LrlnEayAAAAAAAAoJU62l0AaMyXkswe5vqyJG9KUputAwAAAFPE/m9Ntn5au1usm633S579tna3YJw6+l9/1ciY44i5T0zvgm5jDgAAAAAAACYNJ3TAFFBKeV2Sw0e4/c+11stLKS9tshMAAABMGZ3Tkhd/IfniQUnfkna3Gb3OmcmLP590dLa7CePM5bfcnz/7zLmNZPUu6G4kBwAAAAAAAJpk0AGTXCnliUk+PsLtG5N8uME6AAAAMDVtvkty2PHJGe9vd5PRO+yEgf6wQq012x27sJGsXx53eJ44Z1YjWQAAAAAAANA0gw6Y/D6dZOMR7r2l1rq4yTIAAAAwZe3/tuT2y5PLvtvuJmtuj5cl+7+13S0YR57x0TNzxwOtP2nm/x28Q+YfuWvLcwAAAAAAAKCdDDpgEiulvDDJy0a4/Z1a60+a7AMAAABTWkdH8uLPJ0seSK49td1tHt8u8wb6dnS0uwnjwNlX35HXfv3XjWT1LuhuJAcAAAAAAADazaADJqlSygZJPj/C7fuSvLOxMgAAAMCAzunJUV9Pvvea8T3q2GVe8tKvDfRlSlve158dj2/mf1ev+NDzs/5Mf2UNAAAAAADA1OFfx2DyWpDkySPcO7bWenuTZQAAAIAVps9KXv6t5IdvSS77brvbPNYeLxs4mcOYY8rrmt/TSM7Jf7lH/uoZT2kkCwAAAAAAAMYTgw6YhEopz07y5hFun5/kSw3WAQAAAFbVOT35iy8lWz41OeujSd+SdjdKOmcmh52Q7P/WpKOj3W1oo3+74Hc54YeXN5LVu6C7kRwAAAAAAAAYjww6YJIppcxI8pUkZZjby5O8qdZam20FAAAAPEZHR3LAO5KdX5D88M3JLRe1r8vW+w2cyrH5Lu3rQNs9uGR5nvqBnzSSdf1Hj8y0TsMhAAAAAAAApjaDDph8jk+y2wj3Tqm1XtZkGQAAAOBxbL5L8rrTk/M/m5x9UrOndXTOTA47fsWpHJ3N5TLudM3vaSTn6699eg7ZZYtGsgAAAAAAAGC8M+iASaSUMjfJ/BFu9yb5UHNtAAAAgDXWOS058J3J3Bcl534yuex7ybKHW5c3fb1kj6MGMjfZvnU5jHsnLbwqX/7ZjS3P2WrDWfnFsYe3PAcAAAAAAAAmEoMOmCRKKR1JvpJkxghveUuttYX/TRAAAABgnW2yffKiTyfPOzG55NvJr7+S3HXt2D1/s52Tp78h2esVyawNx+65TDi33b84+598ViNZN508L6WURrIAAAAAAABgIjHogMnjmCT7j3Dvu7XWU5ssAwAAAKyDWRsmz3xT8ow3Jr87L7l6YXLrb5LbLhndyR3T10+etGey1b7JrvOSbQ9I/Bfrp7yu+T2N5Cx8+0GZu9WcRrIAAAAAAABgIjLogEmglLJNko+OcPv+JO9srg0AAAAwZkpJug4ceCVJf19y13XJbRcnd1yZLL4vWb4k6VuSdM5Mps1MZm+UbDE3edLeyWY7JR2d7evPuPK6r/86Z119R8tzDtt1i/zra57e8hwAAAAAAACY6Aw6YHL4fJINRrh3XK31tibLAAAAAC3S0ZlssevAC9bQlbcuyrxP/7yRrN4F3Y3kAAAAAAAAwGRg0AETXCnlFUn+bITbFyT5YoN1AAAAABgnaq3Z7tiFjWSdf+xhedKGsxvJAgAAAAAAgMnCoAMmsFLKJkk+NcLt5UneVGvtb7DSuFBKOSbJWxqI2qGBDAAAAIBRO2DBWbnlvsUtz/nbg7bL8d1zW54DAAAAAAAAk5FBB0xspyTZYoR7n6i1XtpkmXFk8yT+mwQAAADAlPPTa+/Mq//1V41k9S7obiQHAAAAAAAAJiuDDpigSinPTfLqEW7/LskHm2sDAAAAQDst7+vPjsefwblPpAAAz09JREFU2kjWZR98XjaYNb2RLAAAAAAAAJjMDDpgAiqlrJfkS6t5yzG11oeb6gMAAABA+3TN72kk58QXPzWveta2jWQBAAAAAADAVGDQARPTh5NsP8K979dam/lXfAAAAADa5tu/+n3m/9dljWT1LuhuJAcAAAAAAACmEoMOmGBKKU9L8s4Rbi9K8vbm2gAAAADQtIeWLM/uH/hJI1nXffTITO/saCQLAAAAAAAAphqDDphASinTknwlSecIbzmu1npbg5XGqzuTXNlAzg5JZjaQAwAAAJAk6ZrfzMGsX3vN03Porls0kgUAAAAAAABTlUEHTCzvTbL3CPd+leQLzVUZv2qtn0vyuVbnlFKuSDK31TkAAAAA/3ja1fnCOTe0PGfzDWbm18c/t+U5AAAAAAAAgEEHTBillB2TfGCE28uTvKnW2t9gJQAAAABa7I+LHskzT/q/RrJuOnleSimNZAEAAAAAAAAGHTCRfDnJrBHufarWenGDXQAAAABosa75PY3k/O/bDsxTt96wkSwAAAAAAADgUQYdMAGUUl6f5NARbv8uI5/cAQAAAMAE88ZvXpjTr/xjy3MO2mmzfOv1z2x5DgAAAAAAADA8gw4Y50opT0zysdW85a211oea6gMAAABAa1x9+6K84JM/bySrd0F3IzkAAAAAAADAyAw6YPz7bJKNR7j3g1rr/zZZBgAAAICxVWvNdscubCTrvPmHZeuNZjeSBQAAAAAAAKyeQQeMY6WUFyV56Qi3FyV5e4N1AAAAABhjB3/s7Pzu7odbnvPaA7rygRfu3vIcAAAAAAAAYM0ZdMD4dspq7p1Qa721sSYAAAAAjJlzr7srr/zqLxvJ6l3Q3UgOAAAAAAAAMDoGHTC+bTbC9UVJlpRS3jCGWfs+zv2d1iDvp7XW68aqEAAAAMBk09dfs8NxCxvJuvSDz8ucWdMbyQIAAAAAAABGz6ADJqY5Sb7UcOazV7xW57VJDDoAAAAAhtE1v6eRnA+9aPe8+tldjWQBAAAAAAAAa8+gAwAAAACghb574c35++9f2khW74LuRnIAAAAAAACAdWfQAQAAAADQAg8vXZ657/9JI1nXffTITO/saCQLAAAAAAAAGBsGHQAAAAAAY6xrfk8jOV85er88d+4TG8kCAAAAAAAAxpZBBwAAAADAGDnl9Gvy6bOub3nOxutNz2/f/7yW5wAAAAAAAACtY9ABAAAAALCO7njgkTzjo//XSNZNJ89LKaWRLAAAAAAAAKB1DDpgHKu1btRUVinlg0k+sJq3fKPW+ppm2gAAAABMHF3zexrJ+Z+3HpA9n7xRI1kAAAAAAABA6xl0AAAAAACshWP+/Tfpuey2lufsv/2m+c83PqvlOQAAAAAAAECzDDoAAAAAAEbhuj8+kCM+8bNGsnoXdDeSAwAAAAAAADTPoAMAAAAAYA11ze9pJOfnf39ottlkvUayAAAAAAAAgPYw6AAAAAAAeByH//M5ueHOh1qec/T+2+bDf/7UlucAAAAAAAAA7WfQAQAAAAAwgl9cf1f++iu/bCSrd0F3IzkAAAAAAADA+GDQAQAAAACwir7+mh2OW9hI1iUfeF42nD29kSwAAAAAAABg/DDoAAAAAAAYomt+TyM5J3TvljcctH0jWQAAAAAAAMD4Y9ABAAAAAJDkv37zh7z7u5c0ktW7oLuRHAAAAAAAAGD8MugAAAAAAKa0xUv7stv7T2sk69qPHJkZ0zoayQIAAAAAAADGN4MOAAAAAGDK6prf00jOl171tDx/9y0byQIAAAAAAAAmBoMOAAAAAGDK+eSZ1+aTZ17X8pwNZk3LZR98fstzAAAAAAAAgInHoAMAAAAAmDLuenBJ9vvImY1k3XTyvJRSGskCAAAAAAAAJh6DDgAAAABgSuia39NIzn+/5dnZ5ykbN5IFAAAAAAAATFwGHcCgcx7n/sUNdAAAAOD/s3ffYXbVdf7AP+dOZiY9IaFDJA0ICS00gdA7iaCyIpYVVPzZEHUVV3oLhKyyKCuwNlTQVZB1V41J6AQhdAgQEkJLQpWShGQgZdo9vz8QVySk3u+5c2der+fhgbn3e877ff8Lk3nPASruq7+ZEX985KXkOXsMHhC//eJeyXMAAAAAAACAzsGgA4iIiDzPp8XqRx0AAAAANePpV9+MQy65vZCs+RPHFZIDAAAAAAAAdB4GHQAAAABApzP41MmF5Pz5WwfG+wb2LCQLAAAAAAAA6FwMOgAAAACATuOI7/855rz8RvKcj+/xvrjomB2S5wAAAAAAAACdl0EHAAAAAFDz7pm7MD7243sKyZo/cVwhOQAAAAAAAEDnZtABAAAAANSscjmPoadPKSTr4bMPjf49GwrJAgAAAAAAADo/gw4AAAAAoCYNPnVyITmnjx0Rn99vWCFZAAAAAAAAQNdh0AEAAAAA1JQ/PPxifO2ahwvJmj9xXCE5AAAAAAAAQNdj0AEAAAAA1IQVre0x4qzrC8l64oIjorFbXSFZAAAAAAAAQNdk0AEAAAAAdHiDT51cSM4Vn9wlxu6wWSFZAAAAAAAAQNdm0AEAAAAAdFiX3fpUXHzjk8lzGruV4okLjkyeAwAAAAAAAPA2gw4AAAAAoMNZ+GZz7HrBzYVkzbtobGRZVkgWAAAAAAAAwNsMOgAAAACADmXwqZMLyfndl/aOXbfaoJAsAAAAAAAAgH9k0AEAAAAAdAjf+O3D8T8PvZg8Z/T7+sf/fnlM8hwAAAAAAACAVTHoAAAAAACqau5rb8ZB/357IVnzJ44rJAcAAAAAAABgdQw6AAAAAICqGXzq5EJybv/WAbHVwF6FZAEAAAAAAACsCYMOAAAAAKBwH/jBHfHYi03Jcz6625bxnY/slDwHAAAAAAAAYG0ZdAAAAAAAhbl//qI49od3F5I1f+K4QnIAAAAAAAAA1oVBBwAAAACQXLmcx9DTpxSSNeOsQ2ODXg2FZAEAAAAAAACsK4MOAAAAACCpoadNjnKePudfj9g2vnzA8PRBAAAAAAAAABVg0AEAAAAAJDHpkZfi5N/MKCRr/sRxheQAAAAAAAAAVIpBBwAAAABQUSta22PEWdcXkjVn/BHRvb6ukCwAAAAAAACASjLoAAAAAIAa0V5uj3lL5sXsRbPj6defjqaWpmhub47WcmvUl+qjsa4x+jb0jeEbDI9RA0fF4L6Do65U7Nhh8KmTC8n5wcdHx1E7bV5IFgAAAAAAAEAKBh0AAAAA0EHleR4PvPJA3PrcrTFr4ayYs2hOLG9bvsbX9+jWI0YMGBGjBo6Kg953UOy2yW6RZVmSrv857Zn4t+vnJLn336srZfHMhLHJcwAAAAAAAABSM+gAAAAAgA6mqaUpJj0zKa594tqYt2TeOt9nedvymPHqjJjx6oz41eO/iiH9hsRx2x4XRw07Kvo29K1I19eXtsTo8TdV5F6rM3fC2CiV0gxSAAAAAAAAAIpm0AEAAAAAHcTzTc/HlY9dGVPmTVmrJ3GsqXlL5sXE+ybGpQ9dGmOHjI0Ttz8xBvUdtM73G3zq5Aq2e2///cW9YrfBAwrJAgAAAAAAACiKQQcAAAAAVFlbuS2umnVVXPHwFdFSbkmet7xtefzuqd/FpGcmxUmjT4oTRp4QdaW6Nb7+X//7kfjtAy8kbPiWHbboF5NO3id5DgAAAAAAAEA1GHQAAAAAQBXNXTw3zpx+ZsxcMLPw7JZyS3zvwe/FLc/eEuPHjI+h/Yeu8vz8BUvjgIunFdJt/sRxheQAAAAAAAAAVItBBwAAAABUQTkvx1WzrorLZlxWyFM5VuXRBY/GsZOOja+M/kqcMOqEKGWld50ZfOrkQrrc+s39Y+hGvQvJAgAAAAAAAKgmgw4AAAAAKFhruTXOmn5WTJ5bzEhiTbSUW+KSBy+JJ15/IsaPGR/1pfqIiPjwFdNjxnOLk+cfs8sWcclHd06eAwAAAAAAANBRGHQAAAAAQIGa25vjlGmnxLQXplW7ykpNnjs5lrYsjU8MOSM+/uMHC8mcP3FcITkAAAAAAAAAHYlBBwAAAAAUpLXc2qHHHG+b9sK0uOnxVyLinyOiLlnOg2ceEgN7Nya7PwAAAAAAAEBHVqp2AQAAAADoCsp5Oc6aflaHH3O8rb7P49F98+siolzxe3/z0G1i/sRxxhwAAAAAAABAl+YJHQAAAABQgKtmXRWT506udo21Ut/v4WhfsXm0LtqvYvecP3Fcxe4FAAAAAAAAUMsMOgAAAAAgsbmL58ZlMy6rdo110rjRjdH+5ogot2y8XveZM/6I6F5fV6FWAAAAAAAAALWvVO0CAAAAANCZtZXb4szpZ0ZLuaXaVdZJVmqL7ptfFxHldbr+0o/tHPMnjjPmAAAAAAAAAPgHntABAAAAAAldPfvqmLlgZrVrrJe6Hs9Hw4A7omXR/mt13fyJ4xI1AgAAAAAAAKh9Bh0AAAAAkMjzTc/H5TMur3aNimjY6KZofWP7yFsHrvbs3Aljo1TKCmgFAAAAAAAAULsMOgAAAAAgkSsfuzJayi3VrlERWaktGgbeHs0vH/OeZ675/J6x59DVDz4AAAAAAAAAiChVuwAAAAAAdEZNLU0xZd6UateoqPp+MyJKK971+nab9Y35E8cZcwAAAAAAAACsBU/oAAAAAIAEJj0zKZa3La92jYrKSq1R3+/BaH19zN9emz9xXBUbAQAAAAAAANQugw4AAAAAqLA8z+OaOddUu0YS9RvcE62v7x03f+OAGL5x72rXAQAAAAAAAKhZpWoXAAAAAIDO5oFXHoj5TfOrXSOJusbX4rqvb2LMAQAAAAAAALCeDDoAAAAAoMJufe7WaldI6rbnb6t2BQAAAAAAAICaZ9ABAAAAABU2a+GsaldIataCzv35AAAAAAAAAIpg0AEAAAAAFdRebo85i+ZUu0ZSjy96PNrL7dWuAQAAAAAAAFDTDDoAAAAAoILmLZkXy9uWV7tGUsvblsf8pvnVrgEAAAAAAABQ0ww6AAAAAKCCZi+aXe0KhZi9sGt8TgAAAAAAAIBUDDoAAAAAoIKefv3palcoxFOLn6p2BQAAAAAAAICaZtABAAAAABXU1NJU7QqFaGruGp8TAAAAAAAAIBWDDgAAAACooOb25mpXKERLe0u1KwAAAAAAAADUNIMOAAAAAKig1nJrtSsUoqVs0AEAAAAAAACwPgw6AAAAAKCC6kv11a5QiIZSQ7UrAAAAAAAAANQ0gw4AAAAAqKDGusZqVyhEQ51BBwAAAAAAAMD6MOgAAAAAgArq29C32hUK0bexa3xOAAAAAAAAgFQMOgAAAACggoZvMLzaFQqxdf+tq10BAAAAAAAAoKYZdAAAAABABY0cMLLaFQoxcmDX+JwAAAAAAAAAqRh0AAAAAEAFDek3JHp061HtGkn16NYjBvcdXO0aAAAAAAAAADXNoAMAAAAAKqiuVBcjBoyodo2kthuwXdSV6qpdAwAAAAAAAKCmGXQAAAAAQIUN6Das2hWSGrXhqGpXAAAAAAAAAKh53apdAAAAAAA6i9b2cmx9xtSo67lB9Nyq2m3SOXDQgdWuAAAAAAAAAFDzDDoAAAAAoAJ2OOeGeKO5LSIi2pcNjfbmjaKu8bUqt6q8If2GxG6b7FbtGgAAAAAAAAA1r1TtAgAAAABQy/7w8Isx+NTJfxtzvCWL1tf3rFqnlI7b9rjIsqzaNQAAAAAAAABqnid0AAAAAMA6eLO5LbY/54b3fL91yS7RuPH1kZVaC2yVVo9uPeLoYUdXuwYAAAAAAABAp2DQAQAAAABrafCpk1d/qNwjWpeMjoYN7ktfqCBjh4yNPg19ql0DAAAAAAAAoFMoVbsAAAAAANSKK6Y9vWZjjr9qWbh/5OXO8TtVGkoNceL2J1a7BgAAAAAAAECn0Tn+NhkAAAAAEnq1aUXsMeGWtb4ubx0YLa8dGo2bTE3QqlgnjT4pBvUdVO0aAAAAAAAAAJ2GQQcAAAAArMLaPJFjZVoW7RPd+j4WdT2er1Cj4u244Y5xwsgTql0DAAAAAAAAoFMpVbsAAAAAAHREp/3Po+s95nhLXax46djIy7X5u1UaSg0xfsz4qCvVVbsKAAAAAAAAQKdSm3+LDAAAAACJPPXKG3Ho9/5c0XuWWzaO5tcOje6bTK3ofYtw8uiTY2j/odWuAQAAAAAAANDpGHQAAAAAQETkeR5DTpuS7P6ti/aNuu5/ifp+DyfLqLRxQ8fF8aOOr3YNAAAAAAAAgE6pVO0CAAAAAFBtH/3R3UnHHG8pxTdGnx0HbHlA4pzKOGDQATF+zPgoZb6FCAAAAAAAAJCCJ3QAAAAA0GXdO3dhHPfjewrJmj9xXERENLdfHKdMOyWmvTCtkNx1ccCgA+Li/S+O+lJ9tasAAAAAAAAAdFoGHQAAAAB0OW3t5Rh+xtRCsh4//4jo0VD3t68b6xrjkgMvibOmnxWT504upMPaGDd0XIwfM96YAwAAAAAAACAxgw4AAAAAupRdxt8Ui5a2JM+54pO7xNgdNlvpe/Wl+piwz4TYdoNt47IZl0VLOX2f1WkoNcTJo0+O40cdH6WsVO06AAAAAAAAAJ2eQQcAAAAAXcIfH3kpvvqbGclzNu7TGPedcchqz5WyUnxm+8/E/lvuH2dOPzNmLpiZvNt72XHDHWP8mPExtP/QqnUAAAAAAAAA6GoMOgAAAADo1JY2t8Woc24oJOuZCWOjrpSt1TVD+w+Nq4+8Oq6efXVcPuPyQp/W0VBqiK+M/kocP/L4qCvVFZYLAAAAAAAAgEEHAAAAAJ3Y4FMnF5Jz3Rf3it0HD1jn67uVusVnt/9sHPq+Q+PKx66MKfOmxPK25RVs+E49uvWIsUPGxonbnxiD+g5KlgMAAAAAAADAezPoAAAAAKDT+fGfn4kJU+Ykz9ln+Ibxq8+9v2L3G9R3UJy797nxzd2+GX985o9x7RPXxrwl8yp2/yH9hsRx2x4XRw87Ovo09KnYfQEAAAAAAABYewYdAAAAAHQar73RHLtfeHMhWfMuGhtZliW5d5+GPvHJ7T4ZnxjxiXjglQfitudvi1kLZsXjix5fqyd39OjWI7YbsF2M2nBUHDjowNhtk92SdQYAAAAAAABg7Rh0AAAAANApDD51ciE5t3xz/xi2Ue9CsrIsi9033T1233T3iIhoL7fH/Kb5MXvh7Hhq8VPR1NwULe0t0VJuiYZSQzTUNUTfxr6xdf+tY+TAkTG47+CoK9UV0hUAAAAAAACAtWPQAQAAAEBNO/P3M+NX9zyXPOdTe24V4z+0ffKcVakr1cWw/sNiWP9hVe0BAAAAAAAAwPoz6AAAAACgJj3z2ptx8L/fXkjW/InjCskBAAAAAAAAoOsw6AAAAACgpuR5HkNOm1JI1v1nHBIb9WksJAsAAAAAAACArsWgAwAAAICa8cmf3hPTn16YPOeMsdvF/9tvaPIcAAAAAAAAALougw4AAAAAOrz75y+KY394dyFZ8yeOKyQHAAAAAAAAgK7NoAMAAACADqu9nMew06cUkjX7/MOjZ4NvlwEAAAAAAABQDH9DDQAAAECHtMeFN8erbzQnz/nBx0fHUTttnjwHAAAAAAAAAP6eQQcAAAAAHcqUmX+JL//XQ8lzBvZqiAfPOjR5DgAAAAAAAACsjEEHAAAAAB3Cspa2GHn2DYVkPX3hkdGtrlRIFgAAAAAAAACsjEEHAAAAAFU3+NTJheRc+/k94/1DBxaSBQAAAAAAAACrYtABAAAAQNX89I65ccHkx5Pn7Dl0QFzz+b2S5wAAAAAAAADAmjLoAAAAAKBwC95sjt0uuLmQrHkXjY0sywrJAgAAAAAAAIA1ZdABAAAAQKEGnzq5kJybv7FfDN+4TyFZAAAAAAAAALC2DDoAAAAAKMQ5f3gsrrr72eQ5H9/jfXHRMTskzwEAAAAAAACA9WHQAQAAAEBSc197Mw7699sLyZo/cVwhOQAAAAAAAACwvgw6AAAAAEgiz/MYctqUQrLuO+Pg2LhP90KyAAAAAAAAAKASDDoAAAAAqLjjf3Zf/PnJ15LnfPuIEfGlA4YlzwEAAAAAAACASjPoAAAAAKBiHnz29fin/7yrkKz5E8cVkgMAAAAAAAAAKRh0AAAAALDe2st5DDt9SiFZs847PHo1+rYWAAAAAAAAALXN33wDAAAAsF72vuiWeGnJiuQ5l35s5/jgzlskzwEAAAAAAACAIhh0AAAAALBOrn/sL/HFXz2UPKdv927x6LmHJ88BAAAAAAAAgCIZdAAAAACwVpa3tMd2Z19fSNbTFx4Z3epKhWQBAAAAAAAAQJEMOgAAAABYY4NPnVxIzm/+356x17CBhWQBAAAAAAAAQDUYdAAAAACwWj+fPi/OmzQ7ec5uW20Q//2lvZPnAAAAAAAAAEC1GXQAAAAA8J4WLW2JXcbfVEjWvIvGRpZlhWQBAAAAAAAAQLUZdAAAAACwUoNPnVxIzo3/sl9ss0mfQrIAAAAAAAAAoKMw6AAAAADgHc6fNDt+Nn1e8pyP7rZlfOcjOyXPAQAAAAAAAICOyKADAAAAgIiImL9gaRxw8bRisiaOKyQHAAAAAAAAADoqgw4AAAAAYvCpkwvJuff0g2OTvt0LyQIAAAAAAACAjsygAwAAAKAL++wv7o9b57yaPOdbh28bJx04PHkOAAAAAAAAANQKgw4AAACALmjGc6/Hh6+4q5Cs+RPHFZIDAAAAAAAAALXEoAMAAACgCymX8xh6+pRCsh477/Do3ejbTwAAAAAAAACwMv5GHQAAAKCL2P+7t8WzC5clz7nkozvFMbtsmTwHAAAAAAAAAGqZQQcAAADQ+ZTbIxY8GfHSwxGvzo5YsTiirTmivSWiriGiW2NE9/4RG4+M2Hx0xIZbR5Tqqlw6nRtnvRyf/+WDyXN6NdTFrPOPSJ4DAAAAAAAAAJ2BQQcAAABQ+/I8Yv6dEU9MiXjxoYiXH41oXYsnUdT3ith0h4gtdonYdmzE4H0isixd34KsaG2PEWddX0jWUxceGfV1pUKyAAAAAAAAAKAzMOgAAAAAatfyxRGPXBPxwJVvPZFjXbUujXj+nrf+ueeKiA23idjtxIidPhbRo3+l2hZq8KmTC8n5r8+9P8YM37CQLAAAAAAAAADoTAw6AAAAgNqzaG7End+PmHnd2j2JY00teDLi+m9H3HJexA7HRuzz9YgBQyufk8DVd8+Ps/8wK3nOzoP6x+9PGpM8BwAAAAAAAAA6K4MOAAAAoHa0t0Xc/YOI2y6KaG9On9e6LOKhq956CsiBp0fsfXJEqS597jp4fWlLjB5/UyFZ8y4aG1mWFZIFAAAAAAAAAJ2VQQcAAABQG157IuL3X4p48cHis9ubI24+J+LxSREfuiJio22L77AKg0+dXEjO9V/fN0Zs2reQLAAAAAAAAADo7ErVLgAAAACwSuVyxPRLI364b3XGHH/vxQfe6jH90rd6VdmEKY8XMuY4ZpctYv7EccYcAAAAAAAAAFBBntABAAAAdFztrRG//3LEzN9Wu8n/aW+OuOnsiJcfe+tpHXX1hVd4buGy2O+7txWSNX/iuEJyAAAAAAAAAKCrMegAAAAAOqbWFRHXfTriyanVbrJyM38b0fxGxLG/iKjvXlhsEU/kiIi4+7SDYrN+PQrJAgAAAAAAAICuqFTtAgAAAADv0t7ascccb3tyasR/f+atvol9/uoHChlz/Msh28T8ieOMOQAAAAAAAAAgMU/oAAAAADqWcjni91/u+GOOtz0x5a2+H/5RRKnyvzvjkecXxwcvn17x+67M/InjCskBAAAAAAAAAAw6AAAAgI7m7h9EzPxttVusnZm/jdh0h4gxX63YLcvlPIaePqVi91uVmeceFn261xeSBQAAAAAAAAC8xaADAAAA6DheeyLi1gur3WLd3HpBxDaHR2y07Xrf6qB/nxZzX1tagVKrdvGxO8VHdt0yeQ4AAAAAAAAA8G4GHQAAAEDH0N4W8fsvRbQ3V7vJumlvjvj9lyNOvDGiVLdOt7jl8VfixKseqHCxd2voVoonLzgyeQ4AAAAAAAAA8N4MOgAAAICO4e7LIl58sNot1s+LD0Tc9YOIfb6+VpetaG2PEWddn6bTP3jygiOjoVupkCwAAAAAAAAA4L0ZdAAAAADVt2huxG0Tqt2iMm6bEDHy6IgBQ9fo+OBTJycu9JarP7tH7LfNRoVkAQAAAAAAAACrZ9ABAAAAVN+d349ob67Y7dojYl59fcxubIin6+ujqa4UzVkWrRFRHxGNeR5928sxvLU1RjU3x+DWtqirWHjzW5/n6P9Y5bFf3fNsnPn7xyqV+p6236Jv/OnkfZPnAAAAAAAAAABrx6ADAAAAqK7liyNmXrdet8gj4oHujXFrzx4xq7Eh5jQ0xPJSaY2v71Eux4iWlhjV3BIHLVseu61ojmx9Cs28LuKw8RHd+73rrcXLWmLn829an7uvsXkXjY0sW69PAgAAAAAAAAAkYtABAAAAVNcj10S0LlunS5tKWUzq3Suu7dMn5jXUr3OF5aVSzOjePWZ07x6/6tc3hrS0xnFvvBFHvbk0+pbztb9h67K3Ptf7v/COlwefOnmdO66NKV/dN0Zu3reQLAAAAAAAAABg3Rh0AAAAANWT5xH3/3StL3u+W7e4sl/fmNK751o9iWNNzWuoj4kDB8SlG/SPsW8uixOXNMWgtra1u8n9P43Y4/MRWRYTp86JH97+TMV7/qMP7rx5XPqx0clzAAAAAAAAAID1Z9ABAAAAVM/8OyMWPrXGx9si4qp+feKK/v2jpZSl6/VXy0ul+F3f3jGpd684afHiOGHJG1G3phcveDJenXlL7PHr5pQV/2b+xHGF5AAAAAAAAAAAlWHQAQAAAFTPE1PW+Ojc+m5x5oYDY2b3xoSFVq6llMX3BmwQt/TsGeMXLIyhrWv2tI4//vanEfGppN3uOvWg2Lx/j6QZAAAAAAAAAEDllapdAAAAAOjCXnxotUfKEfHzfn3i2M03q8qY4+892r0xjt18s/h5vz5RXoPzO5bmJuvy1YOGx/yJ44w5AAAAAAAAAKBGeUIHAAAAUB3l9oiXH13lkdaIOGujgTG5d69iOq2BllIWlwzYIJ5oaIjxry2M+lWcHZXNj1KUo1zh36kxf+K4it4PAAAAAAAAACieQQcAAABQHQuejGhd9p5vN2cRp2y0YUzr1bPAUmtucu9esTTL4uLXFkRjvvIzvbLmGJq9FE/nW1Yk85FzDot+PVY1IQEAAAAAAAAAakVlfz0kAAAAwJp66eH3fKs1OvaY423TevWMUzbaMFpXcWaHbN5653znn3aM+RPHGXMAAAAAAAAAQCfiCR0AAABAdbw6e6UvlyPirI0Gdvgxx9um9eoZZ200MCa8tnClvzlj29ILb32odVDKIuZeNG69+gEAAAAAAAAAHZNBBwAAAFAdKxav9OWr+vWJyb17FdtlPU3u3StGNLfEp5veeNd7fePNdbrnkxccGQ3dPFwVAAAAAAAAADorgw4AAACgOtqa3/XS3PpucVn//sV3qYAfbNA/9lu+PIa2tr3j9casda3u84vP7B4HbLtxJasBAAAAAAAAAB2QX/MIAAAAVEd7yzu+bIuIMzccGC2lrDp91lNLKYuzNhwY7f/wemO0rfT8PxqxaZ+YP3GcMQcAAAAAAAAAdBGe0AEAAABUR13DO768ul+fmNm9sUplKuPR7o1xVb8+8dklb/ztteY1+PbL3Aljo1SjQxYAAAAAAAAAYN0YdAAAAADV0e3/xhvPd+sWl/fvX70uFXR5//5x6NLlMajtrSdzNOf173n2TyfvE9tv0a+oagAAAAAAAABAB1KqdgEAAACgi+re/2//eWW/vtHSSZ5Q0VLK4sp+ff/2dVP0fteZcTtsFvMnjjPmAAAAAAAAAIAuzBM6AAAAgOrYeGRERDSVspjSu2eVy1TWlN4945uLXo8+eR5PlLd8x3vzJ46rUisAAAAAAAAAoCMx6AAAAACqY/OdIyJiUu9esbzUuR4iurxUij/26RWfbHozZuZDIiLizm8fGFtu0LmGKwAAAAAAAADAuutcPy0BAAAA1I4Nt4m8vmdc06dPtZskcW2fPvFm3hhH7L9PzJ84zpgDAAAAAAAAAHgHgw4AAACgOkp18cBm28b8hvpqN0liXkN9PL7VqDjliFHVrgIAAAAAAAAAdEAGHQAAAEDV3Nqnb7UrJHVbJ/98AAAAAAAAAMC6M+gAAAAAqmZWt879rYlZ3bJqVwAAAAAAAAAAOqjO/VMTAAAAQIfVXm6POcterHaNpB5f9lK0l9urXQMAAAAAAAAA6IAMOgAAAICqmLdkXixvW1HtGkktb1se85vmV7sGAAAAAAAAANABGXQAAAAAVTF70exqVyjE7IVd43MCAAAAAAAAAGvHoAMAAACoiqdff7raFQrx1OKnql0BAAAAAAAAAOiADDoAAACAqmhqaap2hUI0NXeNzwkAAAAAAAAArB2DDgAAAKAqmtubq12hEC3tLdWuAAAAAAAAAAB0QAYdAAAAQFW0llurXaEQLWWDDgAAAAAAAADg3Qw6AAAAgKqoL9VXu0IhGkoN1a4AAAAAAAAAAHRABh0AAABAVTTWNVa7QiEa6gw6AAAAAAAAAIB3M+gAAAAAqqJvQ99qVyhE38au8TkBAAAAAAAAgLVj0AEAAABUxfANhle7QiG27r91tSsAAAAAAAAAAB2QQQcAAABQFSMHjKx2hUKMHNg1PicAAAAAAAAAsHYMOgAAAICqGNJvSPTo1qPaNZLq0a1HDO47uNo1AAAAAAAAAIAOyKADAAAAqIq6Ul2MGDCi2jWS2m7AdlFXqqt2DQAAAAAAAACgAzLoAAAAAKpm5IBR1a6Q1KgNO/fnAwAAAAAAAADWnUEHAAAAUBVn/+Gx+MkNPapdI6kDBx1Y7QoAAAAAAAAAQAfVrdoFAAAAgK7lwWdfj3/6z7v++tXQaG/eKOoaX6tqpxSG9BsSu22yW7VrAAAAAAAAAAAdlEEHAAAAUIgVre0x4qzr/+HVLFpf3zPqNp1UlU4pHbftcZFlWbVrAAAAAAAAAAAdlEEHAAAAkNznrnogbn78lZW+17pkl2jc+PrISq0Ft0qnR7cecfSwo6tdAwAAAAAAAADowAw6AAAAgGRunfNKfPYXD6z6ULlHtC4ZHQ0b3FdMqQKMHTI2+jT0qXYNAAAAAAAAAKADM+gAAAAAKm7J8tbY6bwb1/h8y8L9o77fQ5GV2hK2KkZDqSFO3P7EatcAAAAAAAAAADo4gw4AAACgoo689I54/C9Na3VN3jowWl47NBo3mZqoVXFOGn1SDOo7qNo1AAAAAAAAAIAOrlTtAgAAAEDncN0Dz8fgUyev9ZjjbS2L9on25bU9hNhxwx3jhJEnVLsGAAAAAAAAAFADPKEDAAAAWC8vL1kRe150SwXuVBcrXjo2eg75j8hKbRW4X7EaSg0xfsz4qCvVVbsKAAAAAAAAAFADDDoAAACAdZLneWx/zg2xtKW9Yvcst2wcza8dGt03mVqxexbl5NEnx9D+Q6tdAwAAAAAAAACoEaVqFwAAAABqzxXTno4hp02p6Jjjba2L9o3WJTtX/L4pjRs6Lo4fdXy1awAAAAAAAAAANcQTOgAAAIA19vSrb8Yhl9yeOKUUR2zytWjf8KqY9sK0xFnr74BBB8T4MeOjlPm9GQAAAAAAAADAmjPoAAAAAFarvZzHsNOnFJL15AVHRkO3UjS37xCnTDulQ486Dhh0QFy8/8VRX6qvdhUAAAAAAAAAoMb41ZEAAADAKp37x1mFjDkmf3WfmD9xXDR0e+vbFY11jXHJgZfEuKHjkmevi3FDx8UlB1wSjXWN1a4CAAAAAAAAANQgT+gAAAAAVmrGc6/Hh6+4K3nOSQcOi28dPmKl79WX6mPCPhNi2w22jctmXBYt5ZbkfVanodQQJ48+OY4fdXyUMr8rAwAAAAAAAABYNwYdAAAAwDusaG2PEWddX0jW3Aljo1TKVnmmlJXiM9t/Jvbfcv84c/qZMXPBzEK6rcyOG+4Y48eMj6H9h1atAwAAAAAAAADQORh0AAAAAH/zhV8+EDfMeiV5zp+/dWC8b2DPtbpmaP+hcfWRV8fVs6+Oy2dcXujTOhpKDfGV0V+J40ceH3WlusJyAQAAAAAAAIDOy6ADAAAAiNueeDU+8/P7k+ece9TI+PSYIet8fbdSt/js9p+NQ993aFz52JUxZd6UWN62vIIN36lHtx4xdsjYOHH7E2NQ30HJcgAAAAAAAACArsegAwAAALqwJctbY6fzbkyes1m/7nH3aQdX7H6D+g6Kc/c+N7652zfjj8/8Ma594tqYt2Rexe4/pN+QOG7b4+LoYUdHn4Y+FbsvAAAAAAAAAMDbDDoAAACgi/rAD+6Ix15sSp7zwJmHxIa9G5Pcu09Dn/jkdp+MT4z4RDzwygNx2/O3xawFs+LxRY+v1ZM7enTrEdsN2C5GbTgqDhx0YOy2yW6RZVmSzgAAAAAAAAAAEQYdAAAA0OX87sEX4pvXPZI85wcfHx1H7bR58pyIiCzLYvdNd4/dN909IiLay+0xv2l+zF44O55a/FQ0NTdFS3tLtJRboqHUEA11DdG3sW9s3X/rGDlwZAzuOzjqSnWFdAUAAAAAAAAAiDDoAAAAgC7jlaYV8f4JtyTP2WPwgPjtF/dKnrMqdaW6GNZ/WAzrP6yqPQAAAAAAAAAA3otBBwAAAHRyeZ7HTufdGE0r2pJnPXbe4dG70bcbAAAAAAAAAABWx09YAAAAQCf2o9ufiYumzkme81+fe3+MGb5h8hwAAAAAAAAAgM7CoAMAAAA6obmvvRkH/fvtyXM+tPPm8f2PjU6eAwAAAAAAAADQ2Rh0AAAAQCfSXs5j2OlTCsl68oIjo6FbqZAsAAAAAAAAAIDOxqADAAAAOonzJ82On02flzznTyfvE9tv0S95DgAAAAAAAABAZ2bQAQAAADXukecXxwcvn54854v7D4tTjxyRPAcAAAAAAAAAoCsw6AAAAIAataK1PUacdX0hWXMnjI1SKSskCwAAAAAAAACgKzDoAAAAgBr05f96MKbMfDl5zu3fOiC2GtgreQ4AAAAAAAAAQFdj0AEAAAA15PYnX4sTfnZf8pyzPzAyPrvPkOQ5AAAAAAAAAABdlUEHAAAA1ICmFa2x47k3Js/ZuE9j3Hv6wZFlWfIsAAAAAAAAAICuzKADAAAAOrgPXj49Hnl+cfKc+884JDbq05g8BwAAAAAAAAAAgw4AAADosH4/48X4+rUPJ8+59GM7xwd33iJ5DgAAAAAAAAAA/8egAwAAADqYV5tWxB4Tbkmes8v7+sf/fHlM8hwAAAAAAAAAAN7NoAMAAAA6iDzPY9cLbo5FS1uSZ80897Do070+eQ4AAAAAAAAAACtn0AEAAAAdwE/+PDcunPJ48pxfnrhH7Lv1RslzAAAAAAAAAABYNYMOAAAAqKJ5C5bGgRdPS55z1E6bxw8+Pjp5DgAAAAAAAAAAa8agAwAAAKqgXM5j6OlTCsl64oIjorFbXSFZAAAAAAAAAACsGYMOAAAAKNiFk2fHT+6Ylzznj18ZEztu2T95DgAAAAAAAAAAa8+gAwAAAAry6AuL4+jLpifP+fx+Q+P0sdslzwEAAAAAAAAAYN0ZdAAAAEBizW3tse2Z1xeSNXfC2CiVskKyAAAAAAAAAABYdwYdAAAAkNBXfv1Q/OnRvyTPmXbKATF4w17JcwAAAAAAAAAAqAyDDgAAAEjgjqdei09deV/ynDPHbRef23do8hwAAAAAAAAAACrLoAMAAAAq6I0VrbHDuTcmz9mwd0Pcf8YhkWVZ8iwAAAAAAAAAACrPoAMAAAAq5JgrpsdDzy1OnnPfGQfHxn26J88BAAAAAAAAACAdgw4AAABYT394+MX42jUPJ8/5/nE7x4dGb5E8BwAAAAAAAACA9Aw6AAAAYB299kZz7H7hzclzdhrUP/5w0pjkOQAAAAAAAAAAFMegAwAAANZSnuex+4W3xII3m5NnzTz3sOjTvT55DgAAAAAAAAAAxTLoAAAAgLXwszvnxfl/mp0856rP7hH7b7NR8hwAAAAAAAAAAKrDoAMAAADWwLMLl8b+352WPGfcDpvF5Z/cJXkOAAAAAAAAAADVZdABAAAAq1Au5zH09CmFZD1xwRHR2K2ukCwAAAAAAAAAAKrLoAMAAADew0VTH48f3T43ec4fThoTOw3qnzwHAAAAAAAAAICOw6ADAAAA/sFjLy6JD/zgzuQ5nx0zJM4+amTyHAAAAAAAAAAAOh6DDgAAAPirlrZybHPm1EKynpkwNupKWSFZAAAAAAAAAAB0PAYdAAAAEBFfu2ZG/OHhl5Ln3PrN/WPoRr2T5wAAAAAAAAAA0LEZdAAAANClTX96QXzyp/cmzzntyBHxhf2HJc8BAAAAAAAAAKA2GHQAAADQJb3Z3Bbbn3ND8pz+PetjxlmHRpZlybMAAAAAAAAAAKgdBh0AAAB0Ocf+8K64f/7ryXPuO/3g2Lhv9+Q5AAAAAAAAAADUHoMOAAAAuoxJj7wUJ/9mRvKcSz66Uxyzy5bJcwAAAAAAAAAAqF0GHQAAAHR6C95sjt0uuDl5zvZb9I0/nbxv8hwAAAAAAAAAAGqfQQcAAACd2p4TbomXm1Ykz3nknMOiX4/65DkAAAAAAAAAAHQOBh0AAAB0Sr+YPi/OnTQ7ec7PP7N7HLjtxslzAAAAAAAAAADoXAw6AAAA6FSeW7gs9vvubclzDh+1SfzoU7slzwEAAAAAAAAAoHMy6AAAAKBTKJfzGHr6lEKy5ow/IrrX1xWSBQAAAAAAAABA52TQAQAAQM37zvVz4oppzyTP+d8v7x2j37dB8hwAAAAAAAAAADo/gw4AAABq1qyXlsS4/7gzec6n9x4c5x49KnkOAAAAAAAAAABdh0EHAAAANaelrRzbnDm1kKxnJoyNulJWSBYAAAAAAAAAAF2HQQcAAAA15RvXPhz/M+PF5Dk3f2P/GL5x7+Q5AAAAAAAAAAB0TQYdAAAA1IS7nlkQn/jJvclzvn3EiPjSAcOS5wAAAAAAAAAA0LUZdAAAANChLW1ui1Hn3JA8p09jt3j03MMiy7LkWQAAAAAAAAAAYNABAABAh/WxH98d98xdlDznntMOjk37dU+eAwAAAAAAAAAAbzPoAAAAoMOZ/Ohf4qRfP5Q857sf2TGO3W1Q8hwAAAAAAAAAAPhHBh0AAAB0GAvfbI5dL7g5ec52m/WNqV/bN3kOAAAAAAAAAAC8F4MOAAAAOoQxE2+NFxcvT57zyNmHRb+e9clzAAAAAAAAAABgVQw6AAAAqKqr754fZ/9hVvKcn316tzhoxCbJcwAAAAAAAAAAYE0YdAAAAFAVzy9aFvt+57bkOYdst0n89ITdkucAAAAAAAAAAMDaMOgAAACgUOVyHkNPn1JI1pzxR0T3+rpCsgAAAAAAAAAAYG0YdAAAAFCYf7/xifjBrU8nz/ndl/aOXbfaIHkOAAAAAAAAAACsK4MOAAAAknv8L01x5KV3JM85fq+t4vwPbp88BwAAAAAAAAAA1pdBBwAAAMm0tpdj6zOmFpL1zISxUVfKCskCAAAAAAAAAID1ZdABAABAEt+67pG47sEXkufc/I39YvjGfZLnAAAAAAAAAABAJRl0AAAAUFH3zF0YH/vxPclzvnX4tnHSgcOT5wAAAAAAAAAAQAoGHQAAAFTEspa2GHn2DclzejbUxazzDo8sy5JnAQAAAAAAAABAKgYdAAAArLdP/vSemP70wuQ5d592UGzWr0fyHAAAAAAAAAAASM2gAwAAgHU2deZf4kv/9VDynO/8047x0d0HJc8BAAAAAAAAAICiGHQAAACw1hYtbYldxt+UPGebTXrHjf+yf/IcAAAAAAAAAAAomkEHAAAAa2X/794Wzy5cljznkbMPi34965PnAAAAAAAAAABANRh0AAAAsEZ+dc+zcebvH0ue89Pjd4tDRm6SPAcAAAAAAAAAAKrJoAMAAIBVeuH1ZbHPv92WPOegERvHzz69e/IcAAAAAAAAAADoCAw6AAAAWKk8z2PIaVMKyZoz/ojoXl9XSBYAAAAAAAAAAHQEBh0AAAC8y/duejIuveWp5Dm/+9JesetWA5LnAAAAAAAAAABAR2PQAQAAwN/Mebkpjvj+Hclz/nnP98UFH9oheQ4AAAAAAAAAAHRUBh0AAABEa3s5tj5jaiFZT194ZHSrKxWSBQAAAAAAAAAAHZVBBwAAQBd36u8ejWvufz55zo3/sl9ss0mf5DkAAAAAAAAAAFALDDoAAAC6qHvnLozjfnxP8pxvHLpNfPXgrZPnAAAAAAAAAABALTHoAAAA6GKWt7THdmdfnzynoa4UT1xwRGRZljwLAAAAAAAAAABqjUEHAABAF3L8z+6LPz/5WvKcu049KDbv3yN5DgAAAAAAAAAA1CqDDgAAgC7ghlkvxxd++WDynIuO2SE+vsf7kucAAAAAAAAAAECtM+gAAADoxF5f2hKjx9+UPGfYRr3ilm8ekDwHAAAAAAAAAAA6C4MOAACATuqgi6fF3AVLk+c8fPah0b9nQ/IcAAAAAAAAAADoTAw6AAAAOplf3/tcnP6/M5Pn/PhTu8ZhozZNngMAAAAAAAAAAJ2RQQcAAEAn8eLi5TFm4q3Jc/bfZqO46rN7JM8BAAAAAAAAAIDOzKADAACgxuV5HsPPmBrt5Tx51uPnHxE9GuqS5wAAAAAAAAAAQGdn0AEAAFDDLr35qfjezU8mz/ntF/aKPYYMSJ4DAAAAAAAAAABdhUEHAABADXrylTfisO/9OXnOx/cYFBcds2PyHAAAAAAAAAAA6GoMOgAAAGpIW3s5hp8xtZCspy88MrrVlQrJAgAAAAAAAACArsagAwAAoEac/r8z49f3Ppc854av7xfbbtoneQ4AAAAAAAAAAHRlBh0AAAAd3APzF8VHfnh38pyvH7J1fP2QbZLnAAAAAAAAAAAABh3QKWRZVh8RIyJi+4gY9dd/bxkR/f/6T7+IaI+IFRGxKCJeioh5EfFoRNwfEXfled5SdG8AAFZteUt7bHf29clz6kpZPH3hkZFlWfIsAAAAAAAAAADgLQYdUIOyLCtFxOiIOCgiDo6IfSOi52ou6xYRjfHWuGNIRIz5u/eWZVl2Y0RcFRF/yvO8reKlAQBYK5/++X0x7YnXkufc+e0DY8sNVvdHSQAAAAAAAAAAoNIMOqBGZFnWLd4abxwXER+MiAEVvH3PiPjQX/+Zl2XZxIi4Ms/z9gpmAACwBm6a/Ur8v6sfSJ5z4Ye3j0++f6vkOQAAAAAAAAAAwMoZdEAHl2XZqIj4ekR8OCIGFhA5JCJ+FBFfyLLsc3mezyggEwCgy1u8rCV2Pv+m5DmDB/aMad86MHkOAAAAAAAAAACwagYd0PEdFRGfq0LuLhFxd5ZlX8vz/EdVyAcA6DIOueT2ePrVN5PnzDjr0NigV0PyHAAAAAAAAAAAYPUMOoBVaYyIH2ZZtnme5+dUuwwAQGdz7f3Pxbd/NzN5zg//eZc4YvvNkucAAAAAAAAAAABrzqADOp/2iJgVEY9HxLyIWBARSyOie0QMjIjNImKfiNh2Le55dpZly/I8/7cKdwUA6JJeWrw89p54a/KcfYZvGL/63PuT5wAAAAAAAAAAAGvPoAM6hzkRMSkipkbEvXmeL1vdBVmWbRYRn4+Ik+OtocfqXJRl2cw8z6esV1MAgC4sz/MYcdb10dxWTp41+/zDo2eD/+UDAAAAAAAAAICOyk/3QO1aHBG/iIhf5nn+0NpenOf5XyLivCzLLo6I70fE51ZzSRYRP82ybGSe54vXNg8AoKu77Nan4uIbn0yec83n94w9h67JXhcAAAAAAAAAAKgmgw6oPU9HxHcj4ldr8iSO1cnzfGlE/L8sy+6IiJ9FRN0qjm8WEd+OiNPWNxcAoKt46pU34tDv/Tl5zkd32zK+85GdkucAAAAAAAAAAACVYdABtePJiDg/Iq7J87y90jfP8/zqLMt6RcQVqzl6cpZlF+V53lTpDgAAnUlbezmGnzG1kKynLjwy6utKhWQBAAAAAAAAAACVYdABHd8rEfHliPhJnudtKYPyPP/PLMv2jIjjV3GsV0R8NCJ+mrILAEAtO+v3j8Uv73k2ec7Ur+0b223WN3kOAAAAAAAAAABQeQYd0MHlef7zgiNPj4iPRETPVZz5UBh0AAC8y4PPvh7/9J93Jc/56kHD4xuHbZs8BwAAAAAAAAAASMegA3iHPM9fzLLsNxFx4iqO7ZtlWSnP83JRvQAAOrIVre0x4qzrC8mad9HYyLKskCwAAAAAAAAAACAdgw5gZf4Uqx509I2IrSJiXjF1AAA6rhN/cX/cMufV5Dl3/OuBMWjAqh6iBgAAAAAAAAAA1BKDDmBl/rwGZ4aGQQcA0IXd8vgrceJVDyTPGf/BUfGpvQYnzwEAAAAAAAAAAIpl0AG8S57ni7Isa4mIhlUc619QHQCADmXJstbY6fwbk+dsuUGPuPPbByXPAQAAAAAAAAAAqsOgA3gvCyJi81W836OoIgAAHcUR3/9zzHn5jeQ5D555SAzs3Zg8BwAAAAAAAAAAqB6DDuC99FzN+ysKaQEA0AH89oHn41//+9HkOVd8cpcYu8NmyXMAAAAAAAAAAIDqM+gA3iXLsj4R0W81x14vogsAQDW9vGRF7HnRLclz9ho6MH7z+T2T5wAAAAAAAAAAAB2HQQewMqMjIlvNmWeKKAIAUA15nseoc26IZS3tybNmnXd49Gr0v2YAAAAAAAAAANDV+KkhYGXGreb9poh4rogiAABFu2La0/Gd659InvOb/7dn7DVsYPIcAAAAAAAAAACgYzLoAN4hy7K6iDhuNcfuzPO8XEQfAICiPP3qm3HIJbcnzzlmly3iko/unDwHAAAAAAAAAADo2Aw6gH/0oYjYajVn/lhADwCAQrSX8xh2+pRCsp668MiorysVkgUAAAAAAAAAAHRsBh3A3/z16Rznr+ZYS0RcV0AdAIDkzv3jrPjFXfOT50z56r4xcvO+yXMAAAAAAAAAAIDaYdAB/L0vRcTI1Zy5Ks/zRUWUAQBIZcZzr8eHr7grec5XDhwepxy+bfIcAAAAAAAAAACg9hh0ABERkWXZ4Ii4aDXHWiPi39K3WT9Zlp0UEV8uIGpYARkAQAWtaG2PEWddX0jW3Aljo1TKCskCAAAAAAAAAABqj0EHEFmW1UXEVRHRezVHv5/n+TMFVFpfG8XqnzQCAHQxn7/6gbhx9ivJc+741wNj0ICeyXMAAAAAAAAAAIDaZtABRESMj4j9VnPm+b+eAwCoKbfNeTU+84v7k+ecd/SoOGHvwclzAAAAAAAAAACAzsGgA7q4LMuOiohTV3Msj4jP5nn+RgGVAAAqYsny1tjpvBuT52zer3vcddrByXMAAAAAAAAAAIDOxaADurAsy7aPiP+KiGw1Ry/L8/zmAioBAFTEuP+4I2a91JQ854EzD4kNezcmzwEAAAAAAAAAADofgw7oorIs2zgiJkVEn9UcvT8iTknfCABg/f3uwRfim9c9kjznsk+Mjg/suHnyHAAAAAAAAAAAoPMy6IAuKMuy3hExJSIGr+bowog4Ns/zluSlKuu1iJhdQM6wiPAruQGgA3ilaUW8f8ItyXP2GDIgfvuFvZLnAAAAAAAAAAAAnZ9BB3QxWZY1RMT/RsSuqzm6PCI+mOf5s+lbVVae55dHxOWpc7IsmxURI1PnAADvLc/z2PHcG+ON5rbkWbPOOzx6NfpfKAAAAAAAAAAAoDL8NBJ0IVmW1UXEbyLikNUcbY23nswxPX0rAIB188Pbn4mJU+ckz/n1594few/fMHkOAAAAAAAAAADQtRh0QBeRZVkWET+NiGNWc7QcEcfneT45fSsAgLX3zGtvxsH/fnvynA+P3iK+d9zOyXMAAAAAAAAAAICuyaADuo5LI+LTa3Dui3meX5O4CwDAWmsv5zHs9CmFZD15wZHR0K1USBYAAAAAAAAAANA1GXRAF5Bl2YSIOHkNjn4zz/OfpO4DALC2zps0K34+fX7ynD+dvE9sv0W/5DkAAAAAAAAAAAAGHdDJZVl2ekSctgZHz8nz/JLUfQAA1sbDzy+OD10+PXnOF/cfFqceOSJ5DgAAAAAAAAAAwNsMOqATy7LsaxFx4Roc/W6e5+en7gMAsKZWtLbHiLOuLyRr7oSxUSplhWQBAAAAAAAAAAC8zaADOqksyz4fEd9fg6OX5Xn+r4nrAACssS/96sGY+tjLyXNu/9YBsdXAXslzAAAAAAAAAAAAVsagAzqhLMs+FRE/XIOjV0bEVxPXAQBYI7c/+Vqc8LP7kuecc9TI+MyYIclzAAAAAAAAAAAAVsWgAzqZLMuOjYifR0S2mqO/iYjP53mep28FAPDemla0xo7n3pg8Z5O+jXHPaQdHlq3uj0kAAAAAAAAAAADpGXRAJ5Jl2dER8V8RUbeao/8bEcfneV5O3woA4L0dfdmd8egLS5Ln3H/GIbFRn8bkOQAAAAAAAAAAAGvKoAM6iSzLDo+I30ZE/WqOTo2Ij+V53pa+FQDAyv3vjBfiX659JHnOf3x8dBy90+bJcwAAAAAAAAAAANaWQQd0AlmWHRBvPXVjdb92+taIOCbP85bUnQAAVubVphWxx4RbkufsutUG8bsv7Z08BwAAAAAAAAAAYF0ZdECNy7Jsr4iYFBE9VnP0zog4Os/zFelbAQC8U57nscv4m+L1Za3Jsx477/Do3eh/dQAAAAAAAAAAgI7NTzlBDcuybNeImBoRvVdz9P6IGJfn+dL0rQAA3uknf54bF055PHnOr058f+yz9YbJcwAAAAAAAAAAACrBoANqVJZlO0TEDRHRbzVHH4mIw/M8b0rfCgDg/8xbsDQOvHha8pyjdto8fvDx0clzAAAAAAAAAAAAKsmgA2pQlmXbRMRNETFwNUdnR8SheZ6/nr4VAMBb2st5DDt9SiFZT1xwRDR2qyskCwAAAAAAAAAAoJIMOqDGZFk2OCJuiYhNVnP0qYg4JM/z15KXAgD4qwsnz46f3DEvec6kr+wTO2y5ugeVAQAAAAAAAAAAdFwGHVBDsizbPN4ac2y5mqPzI+KgPM//krwUAEBEPPrC4jj6sunJc76w39A4bex2yXMAAAAAAAAAAABSM+iAGpFl2Ubx1phj6GqOvhBvjTleSN8KAOjqmtvaY9szry8ka+6EsVEqZYVkAQAAAAAAAAAApGbQATUgy7L+EXFjRIxYzdGX460xx7zkpQCALu+kXz8Ukx9N/0CwaaccEIM37JU8BwAAAAAAAAAAoEgGHdDBZVnWOyKmRsTOqzm6ICIOzvP8qeSlAIAu7Y6nXotPXXlf8pwzx20Xn9t3dQ8nAwAAAAAAAAAAqE0GHdDx/SYi9lyDc9dGxN5Zlu2duM/b/pLn+eSCsgCADuCNFa2xw7k3Js/ZsHdj3H/GwZFlWfIsAAAAAAAAAACAajHogI5vhzU8d1LSFu92e0QYdABAF/HhK6bHjOcWJ8+574yDY+M+3ZPnAAAAAAAAAAAAVJtBBwAA8J7+8PCL8bVrHk6ec+nHdo4P7rxF8hwAAAAAAAAAAICOwqADAAB4l1ffWBF7XHhL8pydB/WP3580JnkOAAAAAAAAAABAR2PQAQAA/E2e57H7hTfHgjdbkmfNPPew6NO9PnkOAAAAAAAAAABAR2TQAQAARETET++YGxdMfjx5ztWf3SP222aj5DkAAAAAAAAAAAAdmUEHAAB0cfMXLI0DLp6WPGfcDpvF5Z/cJXkOAAAAAAAAAABALTDogA4uz/PB1e4AAHRO5XIeQ0+fUkjWExccEY3d6grJAgAAAAAAAAAAqAUGHQAA0AVdNOXx+NGf5ybP+cNJY2KnQf2T5wAAAAAAAAAAANQagw4AAOhCHntxSXzgB3cmz/ncPkPizA+MTJ4DAAAAAAAAAABQqww6AACgC2hpK8c2Z04tJOuZCWOjrpQVkgUAAAAAAAAAAFCrDDoAAKCT+9o1M+IPD7+UPOe2Uw6IIRv2Sp4DAAAAAAAAAADQGRh0AABAJ3XnUwvin6+8N3nO6WNHxOf3G5Y8BwAAAAAAAAAAoDMx6AAAgE7mzea22P6cG5LnDOjVEA+eeUhkWZY8CwAAAAAAAAAAoLMx6AAAgE7kI/95Vzzw7OvJc+47/eDYuG/35DkAAAAAAAAAAACdlUEHAAB0An985KX46m9mJM+55KM7xTG7bJk8BwAAAAAAAAAAoLMz6AAAgBq24M3m2O2Cm5Pn7LBFv5h08j7JcwAAAAAAAAAAALoKgw4AAKhBeZ7HnhfdEq80NSfPevTcw6Jv9/rkOQAAAAAAAAAAAF2JQQcAANSYn0+fF+dNmp0+5zO7x4Hbbpw8BwAAAAAAAAAAoCsy6AAAgBrx3MJlsd93b0uec8SoTeOHn9o1eQ4AAAAAAAAAAEBXZtABAAAdXLmcx9DTpxSSNWf8EdG9vq6QLAAAAAAAAAAAgK7MoAMAADqwf7t+TvzntGeS5/z+pDGx86D+yXMAAAAAAAAAAAB4i0EHAAB0QI+9uCQ+8IM7k+d8ZszgOOeoUclzAAAAAAAAAAAAeCeDDgAA6EBa2sqxzZlTC8l6ZsLYqCtlhWQBAAAAAAAAAADwTgYdAADQQXzj2ofjf2a8mDznlm/uH8M26p08BwAAAAAAAAAAgPdm0AEAAFV21zML4hM/uTd5zqlHjogv7j8seQ4AAAAAAAAAAACrZ9ABAABVsrS5LUadc0PynD7du8Wj5xwWWZYlzwIAAAAAAAAAAGDNGHQAAEAVHPeju+PeeYuS59x7+sGxSd/uyXMAAAAAAAAAAABYOwYdAABQoMmP/iVO+vVDyXMuPnan+MiuWybPAQAAAAAAAAAAYN0YdAAAQAEWvtkcu15wc/KckZv1jSlf2zd5DgAAAAAAAAAAAOvHoAMAABIbM/HWeHHx8uQ5j5xzWPTrUZ88BwAAAAAAAAAAgPVn0AEAAIlcddf8OOePs5Ln/PzTu8eBIzZOngMAAAAAAAAAAEDlGHQAAECFPb9oWez7nduS5xw6cpP4yfG7Jc8BAAAAAAAAAACg8gw6AACgQsrlPIaePqWQrDnjj4ju9XWFZAEAAAAAAAAAAFB5Bh0AAFABF9/wRFx229PJc/7ny3vHLu/bIHkOAAAAAAAAAAAAaRl0AADAepj9UlOM/Y87kuecsNdWcd4Ht0+eAwAAAAAAAAAAQDEMOgAAYB20tpdj6zOmFpL1zISxUVfKCskCAAAAAAAAAACgGAYdAACwlk657pH47wdfSJ5z8zf2j+Eb906eAwAAAAAAAAAAQPEMOgAAYA3d/czC+PhP7kme869HbBtfPmB48hwAAAAAAAAAAACqx6ADAABWY1lLW4w8+4bkOb0a6uKx8w6PLMuSZwEAAAAAAAAAAFBdBh0AALAKn/jJPXHXMwuT59x92kGxWb8eyXMAAAAAAAAAAADoGAw6AABgJabO/Et86b8eSp7znY/sGB/dbVDyHAAAAAAAAAAAADoWgw4AAPg7i5a2xC7jb0qes+0mfeKGf9kveQ4AAAAAAAAAAAAdk0EHAAD81X7fuS2eW7Qsec4jZx8W/XrWJ88BAAAAAAAAAACg4zLoAACgy/vlPc/GWb9/LHnOT4/fLQ4ZuUnyHAAAAAAAAAAAADo+gw4AALqs5xcti32/c1vynINHbBxXfnr35DkAAAAAAAAAAADUDoMOAAC6nDzPY8hpUwrJmjP+iOheX1dIFgAAAAAAAAAAALXDoAMAgC7lkhufiP+49enkOb/70l6x61YDkucAAAAAAAAAAABQmww6AADoEh7/S1MceekdyXP+ec/3xQUf2iF5DgAAAAAAAAAAALXNoAMAgE6ttb0cW58xtZCspy88MrrVlQrJAgAAAAAAAAAAoLYZdAAA0Gl9+78fjWsfeD55zo3/sl9ss0mf5DkAAAAAAAAAAAB0HgYdAAB0OvfOXRjH/fie5DmnHLZNfOWgrZPnAAAAAAAAAAAA0PkYdAAA0Gksa2mLkWffkDynsVsp5ow/IrIsS54FAAAAAAAAAABA52TQAQBAp/CpK++NO55akDznrlMPis3790ieAwAAAAAAAAAAQOdm0AEAQE27YdbL8YVfPpg8Z+IxO8TH9nhf8hwAAAAAAAAAAAC6BoMOAABWqr3cHvOWzIvZi2bH068/HU0tTdHc3hyt5daoL9VHY11j9G3oG8M3GB6jBo6KwX0HR12prrB+ry9tidHjb0qeM3zj3nHzN/ZPngMAAAAAAAAAAEDXYtABAEBEROR5Hg+88kDc+tytMWvhrJizaE4sb1u+xtf36NYjRgwYEaMGjoqD3ndQ7LbJbpFlWZKuB148LeYtWJrk3n/v4bMPjf49G5LnAAAAAAAAAAAA0PUYdAAAdHFNLU0x6ZlJce0T18a8JfPW+T7L25bHjFdnxIxXZ8SvHv9VDOk3JI7b9rg4athR0behb0W6/vre5+L0/51ZkXutyo8/tWscNmrT5DkAAAAAAAAAAAB0XQYdAABd1PNNz8eVj10ZU+ZNWasncaypeUvmxcT7JsalD10aY4eMjRO3PzEG9R20Tvd6cfHyGDPx1go3fLcDtt0ofvGZPZLnAAAAAAAAAAAAgEEHAEAX01Zui6tmXRVXPHxFtJRbkuctb1sev3vqdzHpmUlx0uiT4oSRJ0RdqW6Nrs3zPIadPiXKeeKSEfH4+UdEj4Y16wUAAAAAAAAAAADry6ADAKALmbt4bpw5/cyYuWBm4dkt5Zb43oPfi1uevSXGjxkfQ/sPXeX579/8ZHz/5qeS97rui3vF7oMHJM8BAAAAAAAAAACAv2fQAQDQBZTzclw166q4bMZlhTyVY1UeXfBoHDvp2PjK6K/ECaNOiFJWesf7T7z8Rhz+/T8n7/HxPQbFRcfsmDwHAAAAAAAAAAAAVsagAwCgk2stt8ZZ08+KyXMnV7vK37SUW+KSBy+JJ15/IsaPGR/1pfpoay/H8DOmFpL/9IVHRre60uoPAgAAAAAAAAAAQCIGHQAAnVhze3OcMu2UmPbCtGpXWanJcyfH0pal0WPxCfHb+19JnnfD1/eLbTftkzwHAAAAAAAAAAAAVsegAwCgk2ott3boMcfbpr0wLVrfeCUi/jki6pJk/Msh28TXDtk6yb0BAAAAAAAAAABgXRh0AAB0QuW8HGdNP6vDjzneVt/n8YjNr4sVL300IkoVu2+3UhZPXXhkZFlWsXsCAAAAAAAAAABAJRh0AAB0QlfNuiomz51c7Rprpb7fw9G+YvNoXbRfRe43/dSDYov+PSpyLwAAAAAAAAAAAKg0gw4AgE5m7uK5cdmMy6pdY500bnRjtL85IsotG6/zPSZ8eIf4xPvfV8FWAAAAAAAAAAAAUHkGHQAAnUhbuS3OnH5mtJRbql1lnWSltui++XWxbP6XIqK0VtcO3bBX3HrKAUl6AQAAAAAAAAAAQKUZdAAAdCJXz746Zi6YWe0a66Wux/PRMOCOaFm0/xpfM+OsQ2ODXg0JWwEAAAAAAAAAAEBlGXQAAHQSzzc9H5fPuLzaNSqiYaObovWN7SNvHbjKcz/8513jiO03LagVAAAAAAAAAAAAVI5BBwBAJ3HlY1dGS7ml2jUqIiu1RcPA26P55WNW+v6+W28Yvzzx/QW3AgAAAAAAAAAAgMox6AAA6ASaWppiyrwp1a5RUfX9ZkTzq2Mjyt3f8frs8w+Png3+GAsAAAAAAAAAAEBt85NwAACdwKRnJsXytuXVrlFRWak16vs9GK2vj4mIiGs/v2e8f+jAKrcCAAAAAAAAAACAyjDoAACocXmexzVzrql2jSTqN7gnPjz0uPjOsTtVuwoAAAAAAAAAAABUVKnaBQAAWD8PvPJAzG+aX+0aSdQ1vhbH7ttS7RoAAAAAAAAAAABQcQYdAAA17tbnbq12haRue/62alcAAAAAAAAAAACAijPoAACocbMWzqp2haRmLejcnw8AAAAAAAAAAICuyaADAKCGtZfbY86iOdWukdTjix6P9nJ7tWsAAAAAAAAAAABARRl0AADUsHlL5sXytuXVrpHU8rblMb9pfrVrAAAAAAAAAAAAQEUZdAAA1LDZi2ZXu0IhZi/sGp8TAAAAAAAAAACArsOgAwCghj39+tPVrlCIpxY/Ve0KAAAAAAAAAAAAUFEGHQAANayppanaFQrR1Nw1PicAAAAAAAAAAABdh0EHAEANa25vrnaFQrS0t1S7AgAAAAAAAAAAAFSUQQcAQA1rLbdWu0IhWsoGHQAAAAAAAAAAAHQuBh0AADWsvlRf7QqFaCg1VLsCAAAAAAAAAAAAVJRBBwBADWusa6x2hUI01Bl0AAAAAAAAAAAA0LkYdAAA1LC+DX2rXaEQfRu7xucEAAAAAAAAAACg6zDoAACoYcM3GF7tCoXYuv/W1a4AAAAAAAAAAAAAFWXQAQBQw0YOGFntCoUYObBrfE4AAAAAAAAAAAC6DoMOAIAaNqTfkOjRrUe1ayTVo1uPGNx3cLVrAAAAAAAAAAAAQEUZdAAA1LC6Ul2M2GBEtWsktd2AEVFXqqt2DQAAAAAAAAAAAKgogw4AgBr38jMt1a6Q1Kj6AdWuAAAAAAAAAAAAABVn0AEAUKOuvf+5GHzq5Ni7aWm1qyR14LJl1a4AAAAAAAAAAAAAFdet2gUAAFg7C99sjl0vuPlvX49t/ks82NIa8xvqq9gqjSEtrbHbgmerXQMAAAAAAAAAAAAqzqADAKCGDD518ju+LkU5ts+ei4+9UR8TBw6oUqt0jnvjjciWz4wot0eU6qpdBwAAAAAAAAAAACqmVO0CAACs3qU3P/WuMUdExLDspeiZNcdRby6NHuVyFZql06NcjqPfWBrRujRiwVPVrgMAAAAAAAAAAAAV5QkdAAAd2LMLl8b+3532nu/vkM2NiIi+5TzGvrksfte3d0HN0hv75rLok+dvffGXhyM2HlHVPgAAAAAAAAAAAFBJBh0AAB1Qnucx5LQpqz23TemFv/33iUuaYlLvXtFSylJWK0RDOY8TlzT93wuvzq5eGQAAAAAAAAAAAEigVO0CAAC807eue2SNxhwREf1i6d/+e1BbW5y0eHGiVsU6afHiGNTW9n8vLF9ctS4AAAAAAAAAAACQgid0AAB0EI++sDiOvmz6Wl3TmLW+4+vjl7wRN/fsGTO7N1ayWqF2XNEcJyx5450vtjVXpwwAAAAAAAAAAAAkYtABAFBlbe3lGH7G1HW6tiHa3vF1t4i4YMHCOHbzzaKllFWgXbEaynmMX7Aw6v7xjXaDDgAAAAAAAAAAADoXgw4A4C3l9ogFT0a89HDEq7MjVix+66kI7S0RdQ0R3RojuveP2HhkxOajIzbcOqL0rh+7Zy0d96O74955i9b5+paV/HFuaGtbfGXx4rhkwAbrU60qTn59cQxtbXv3G3W1+8QRAAAAAAAAAAAAWBmDDgDoqvI8Yv6dEU9MiXjxoYiXH41oXbbm19f3ith0h4gtdonYdmzE4H0istp7IkS1THvi1fj0z+9f7/s05/Urff2EJW/EEw0NMbl3r/XOKMq4N5fG8U1vrPzNbgYdAAAAAAAAAAAA/5+9Ow+zrKzvBP59q7qraZamWRWUCI0ssgkIbmAGUKJCXGKCODpxw3FDEpckGqNGw6gk4xCNSjaJQpKJSpwYFTQuoCO4IqjI2iKtqKhsTQPd9FL1zh+3nWhC16muuueeurc+n+epx4d633p/3+t2aznfcxgtCh0AsNCsW51864PJ5ef2nsgxWxvvTW7+Su/jK+cku+6fHHVa8vBnJUuX9yvtyFm3YTIPe9On+nbeXbn/wsZYkjNvvT33lpLPb7dt3+a15bh71+bMW2/P2JY2+O8UAAAAAAAAAAAAI0ahAwAWiju+l1z6zuSqC7buSRwzddsNyadem3zuLcmhpyTHvjLZeUX/5wyxR73ts/npmvV9PfOGqQdvcW1xknfcelt+L7vO61LHcfeuzTtuvS33/6yRzXY/aFBxAAAAAAAAAAAAYCC2eBNkAGBETG5KLv3z5L2PTq44r50yxy/auLY3572P7hVIpibbnTcELrj85uz9ugv7XuZIkqvq9KWZJTU5+2e35eR77u377H44+Z57c/bPbsuS2rBxj8MHEQcAAAAAAAAAAAAGxhM6AGCU3Xp98tGXJT/6xuBnT65PPvvHybUfT55+TrLbAYPP0LE77t2QI8/8TKszbqx7Zm1dkm3Llssii5O87dbbc8CGDXnP8uXZMFZazTQTE1M1Z9y5Os9dc3dzw3jxdsmu+w0iFgAAAAAAAAAAAAyMJ3QAwCiamkoue1fyV4/rpszxi350eS/HZe/q5Vog9n7dha2XOZLki697Qrb9lSMa940lecFdd+eCH9+SQ+/r/5NCtsZh963PBT++Jc+fSZkjSfY4LBkbbzsWAAAAAAAAAAAADJRCBwCMmsmNyb+8JPnMm3pPyZgPJtf38vzLS3r5Rth7Ll6ZvV93Yetz/vDJB2bVWSfnQcuXJg86csZft2Ljppx/y0/zqjvuzMRUbTHhfzYxVfPqO+7M+bf8NCs2bpr5F+4589cHAAAAAAAAAAAAw2JR1wEAgD7aeF9ywfOTGz7ZdZL7d9WHk/V3J6d8IFm8Tddp+urmO9bmcX92yUBmrTrr5F/+xAEnJV85Z8ZfvyjJC++6Oyfeuy7n7rgsF22/bdaNtdfzXTo1lZPuWZvT7lqTvTZtRZHj5w48qf+hAAAAAAAAAAAAoGMKHQAwKiY3zu8yx8/d8Mnkn1+QPPP8ZHxx12nmrNaaff7wooHM+tabfi07bns//57tfWyyy37J7Su36ry9Nm3Km2+/I6+54858bIft8qEddshNE/37z2SfDRtz6t1356l335sd6iyfBrLr/slDjulbJgAAAAAAAAAAAJgvFDoAYBRMTSUfffn8L3P83PUX9fL+xl8nLT4Zom1/+H++nX/62s2tz3nXsw7P0w5/0JY3lJIc/aLkU6+d1fk71JrnrLknz15zTy7fZkku2XZprl4ykWsnJrbqyR1Lp6bysA0bcvD6DTl+7bocdd/6lFkl+gVHv6j3+gAAAAAAAAAAAGDEKHQAwCj48ruTqz7cdYqtc9WHkwcemhzzO10n2Wrf+dFd+fV3X9r6nIfssm2+8PvHz2zzw5+VfO4tyca1s55Xkhx93/ocfd/6JMlkklWLF+WaJRNZuXgia8bGsqGUbCjJRE0mas2yqanst3FDDlq/IXtv3JTxWU+/H4u37b0uAAAAAAAAAAAAGEEKHQAw7G69Prn4rV2nmJ2L/0ey/xOT3Q7oOsmMTE7V7Pv6iwYy67ozn5RtFm9FPWLp8uTQU5IrzutbhvEk+27clH03bkoy+6LIrB16SrLNjoOfCwAAAAAAAAAAAAMw1nUAAGAOJjclH31ZMrm+6ySzM7k++ejLk6nJrpM0es77vjKQMsc/vuhRWXXWyVtX5vi5Y1+ZjC/pe6ZOjC/pvR4AAAAAAAAAAAAYUQodADDMvvye5Eff6DrF3Pzo8uRL7+46xRb93xtuzd6vuzCXfff2Vuf86v67ZdVZJ+eYh+46+0N2XpEc//r+herS8a/vvR4AAAAAAAAAAAAYUYu6DgAAzNId30sueVvXKfrjkrclBz11Xl3Af9/GyRz4xk8NZNaNbzsp42OlP4c95hXJtR8b7qLPg45KHntG1ykAAAAAAAAAAACgVZ7QAQDD6tJ3JpPru07RH5Pre69nnjjmrIsHUua48HeOzaqzTu5fmSNJxhclT//LZHxJ/84cpPElydPPScbGu04CAAAAAAAAAAAArVLoAIBhtG51ctUFXafor6suSO67q9MI/+eKH2bv112YH61e1+qc//boX8mqs07OwXvu2M6A3Q5ITvijds5u2wlv6OUHAAAAAAAAAACAEbeo6wAAwCx864PJxrVdp+ivjWt7r+tRLxn46Dvv3ZAjzvzMQGbd9PaTUkofn8ixJY85I/nJd5KrPtz+rH459JnJY17RdQoAAAAAAAAAAAAYCIUOABg2tSZff1/XKdrx9fclj3xxMojCw2Z7v+7Cgcy59LXH58E7bTuQWUmSsbHk6eck6+9Obvjk4ObO1gEn9fKOeYAcAAAAAAAAAAAAC4Mr5gBg2Ky6NLl9Zdcp2nHbDcn3LxvIqHM+/92BlDle+6QDs+qskwdb5vi58cXJKR9I9n/y4GdvjQNOSn7r/b28AAAAAAAAAAAAsEB4QgcADJvrL+o6QbuuuyjZ+9jWjv/hnWtz7J9e0tr5v2jVWScPZM60Fm+TnPr3yUdfnlz14a7T/GeHPrP3ZA5lDgAAAAAAAAAAABYYhQ4AGDY/uqLrBO36cTuvr9aaff5wMGWYK994YnbabmIgs2ZkfHHyG3+dPPCQ5OK3JpPru06UjC9JTnhD8phXJGMeGgcAAAAAAAAAAMDCo9ABAMNkajL5ybe7TtGuW77de51j43078g0fvSr/8JUf9O28LfnzUx+e3zjiwa3PmZWxseSY3032f1Ly0ZclP/pGd1kedFTvqRy7HdBdBgAAAAAAAAAAAOiYQgcADJPbbkg2ru06Rbs23pvctjLZ/cA5H3X1j+/KyX9xaR9CTe9By5fmsted0PqcvtjtgOSFn06+/J7kkrcN9mkd40uSE/5o81M5+lfYAQAAAAAAAAAAgGGk0AEAw+TH3+w6wWDc8s05FTomp2r2ff1F/cszjevOfFK2WTxk5YTxRcmxr0wOempy6TuTqy5otyi0eNvk0FN6M3de0d4cAAAAAAAAAAAAGCIKHQAwTH52TdcJBmMOr/O3z/1qvrjytj6GuX9/f9oj87j9dmt9Tqt2XpE89S+SXzsz+dYHk6+/r/cUmH7Zdf/k6BclD39Wss2O/TsXAAAAAAAAAAAARoBCBwAMk/tWd51gMNat3uovuXTlbflv5361/1n+g2Mfumv+4UWPan3OQG2zY/KolySPfHHy/cuS6y5KfnxFcsu3tu7JHYu3S/Y4LNnzyOTAk5KHHJOU0l5uAAAAAAAAAAAAGGIKHQAwTDat7zrBYGzF67xv42QOfOOnWgzz725820kZHxvhgkIpyd7H9j6SZGoyuW1lcss3e09NWbe695/N5PpkfEmyaEmydHmy+0HJHocnu+6XjI13lx8AAAAAAAAAAACGiEIHAAyTyQ1dJxiMyZkVOn71zy7JD+7YiidIzNInzjg2hzxox9bnzDtj48nuB/Y+AAAAAAAAAAAAgL5S6ACAYTI+0XWCwRhfMu3yv37zR/ndD36z9Rj/9ZF75e3POKz1OQAAAAAAAAAAAMDCo9ABAMNk0fRFh5Gxhde5eu2GHP4nnxlIhJveflJKKQOZBQAAAAAAAAAAACw8Ch0AMEy2Wd7a0ZNJblq8ONcsmch3Fy/OmvGxrC8lG5MsTrKk1iybnMpDN27MwevXZ++NmzLeVpily//Tp/Z+3YVtTfslX/yD47PXztsOZBYAAAAAAAAAAACwcCl0AMAw2f2gvh1Vk1y+zZJcvO3SXL1kItdNTGTd2NiMv37p1FQO3LAhB6/fkBPWrstR961P355n8Quv86+/cGPe/snr+nXyFv3+Ew/I6cc/tPU5AAAAAAAAAAAAAIlCBwAMlz0Pn/MRa8ZKPr79dvnQDjvkponFsz5n3dhYrtxmm1y5zTb5hx2XZZ8NG3Pq3XfnKffcm2VTdW4h9zg8P1q9LsecdfHczpmhVWedPJA5AAAAAAAAAAAAAD+n0AEAw2TX/ZPF2yYb1271l968aFHO3XFZLtp+2616EsdM3TSxOGftsnPetdPynHTP2px215rstWnTVp9TF2+Xfc9emanc2PeM/9EVbzwxO2830focAAAAAAAAAAAAgP9IoQMAhsnYePLAw5KbvzLjL9mU5Lwdd8g5y5dnw1hpL9tm68bG8pFl2+fj22+X01evzvPuujvjW/H1X1//4Eyl/4WTX/SOUx6e33rEg1udAQAAAAAAAAAAADAdhQ4AGDYPOnLGhY7vLV6UN+y6S67aZknLof6zDWMlf77zTvncttvmzNtuz4qNM3tax7enVrSW6YHLtslXXv/41s4HAAAAAAAAAAAAmKl2b38NAPTfASc1bplK8v4dd8gpe+7RSZnjF317myU5Zc898v4dd8jUDPZ/ZvKoVnJc+ydPUuYAAAAAAAAAAAAA5g1P6ACAYbP3scku+yW3r7zf5Y1J3rjbLrlw++0Gm2saG8ZKzt55p1w/MZEzb709i7ew77tTe+ar9cC+zv7AC47OcQfs3tczAQAAAAAAAAAAAObKEzoAYNiUkhz9ovtdWl+SV+++67wqc/yiC7ffLq/efdesL/e//veTJybZwuJWevSKnbPqrJOVOQAAAAAAAAAAAIB5SaEDAIbRw5+VLN72lz61Mcnv7bZrPr/dtvf/NfPE57fbNr+3267Z+B8+v7Yuyf+ZfFxfZnz3rU/OB1/8mL6cBQAAAAAAAAAAANAGhQ4AGEZLlyeHnvL//3EqyRt322Xelzl+7vPbbZs37rZLpn7hcx+dfGzuztzyf+wVx2TVWSdn0bhvcQAAAAAAAAAAAID5zdWOADCsjn1lMr4kSXLejjvkwu236zbPVrpw++1y/rIdkiTr6+L81eRTZ33WM496cFaddXIOe/DyPqUDAAAAAAAAAAAAaNeirgMAALO084rk+Nfne184M+9ZvrzrNLPy7p2W51fXrcuH1v5WflAfMKszbnr7SSml9DkZAAAAAAAAAAAAQLsUOgBgiG161EvzhpXnZcPYpq6jzMqGsZJX7/qgfPumJ2/1137h94/LQ3YZrqeSAAAAAAAAAAAAAPzcWNcBAIDZO//6/52rhrTM8XM3blOzaOfLZrz/VU/YP6vOOlmZAwAAAAAAAAAAABhqntABAEPq5jU3571XvrfrGH0xsdtnsvHuQ1I37jLtvlVnnTygRAAAAAAAAAAAAADtUugAgCF17nfOzYapDV3H6IsytikTu3wh63/yjPtd/8YbnpBdtl8y4FQAAAAAAAAAAAAA7RnrOgAAsPXWbFiTi266qOsYfbV4xyuTsft+6XN/9puHZdVZJytzAAAAAAAAAAAAACPHEzoAYAh9/MaPZ92mdV3H6KsytjGLd/xGNt55THbdfiKXv+HEriMBAAAAAAAAAAAAtEahAwCGTK01H7zug13HaMXinb6Sb77yLdluyeKuowAAAAAAAAAAAAC0aqzrAADA1rn8p5dn1ZpVXcdoxfiSW3PNnd/sOgYAAAAAAAAAAABA6xQ6AGDIXPyDi7uO0KpLbr6k6wgAAAAAAAAAAAAArVPoAIAhc/XtV3cdoVVX3zbarw8AAAAAAAAAAAAgUegAgKEyOTWZ6+64rusYrbr2jmszOTXZdQwAAAAAAAAAAACAVil0AMAQuemum7Ju07quY7Rq3aZ1WbVmVdcxAAAAAAAAAAAAAFql0AEAQ+SaO67pOsJAXHP7wnidAAAAAAAAAAAAwMKl0AEAQ+S7d3636wgDsXL1yq4jAAAAAAAAAAAAALRKoQMAhsiaDWu6jjAQa9YvjNcJAAAAAAAAAAAALFwKHQAwRNZPru86wkBsmNzQdQQAAAAAAAAAAACAVil0AMAQ2Ti1sesIA7FhSqEDAAAAAAAAAAAAGG0KHQAwRBaPLe46wkBMjE10HQEAAAAAAAAAAACgVQodADBElowv6TrCQEyMK3QAAAAAAAAAAAAAo02hAwCGyLKJZV1HGIhlSxbG6wQAAAAAAAAAAAAWrkVdBwCAYTA5NZmb7rop19xxTb5753ezZsOarJ9cn41TG7N4bHGWjC/JsolleehOD83BuxycvZftnfGx8b7neOhOD+37mfPRfsv36zoCAAAAAAAAAAAAQKsUOgDgftRac/lPL8/FP7g4V99+da6747qs27Ruxl+/dNHSHLjzgTl4l4Nzwq+ckKMecFRKKXPOddDOB835jGFw0C4L43UCAAAAAAAAAAAAC5dCBwD8gjUb1uTjN348H7r+Q7nprptmfc66Tety5c+uzJU/uzL/cO0/ZJ8d98mpB5yap+z7lCybWDbrc/fZcZ8sXbR0q8olw2bpoqXZe9neXccAAAAAAAAAAAAAaJVCBwAkuXnNzTn3O+fmopsuaqUscdNdN+Wsr52Vd13xrpy0z0k57ZDTsteyvbb6nPGx8Ry484G58mdX9j3jfPGwnR+W8bHxrmMAAAAAAAAAAAAAtGqs6wAA0KVNU5ty7lXn5un/+vR8ZOVHWn/yxbpN6/KRlR/J0//16fm77/xdJqcmt+rrv/uzu/PVa7dtKd38cPCuB3cdAQAAAAAAAAAAAKB1ntABwIL1vdXfyxsue0Ouuu2qgc/eMLUhf/6NP8/nvv+5nHnMmVmxfMW0+9dvmsxJ7/pibrz13oxve1AmdrlsQEkH7/i9ju86AgAAAAAAAAAAAEDrPKEDgAVnqk7l/d95f075+CmdlDl+0bdv+3ZO+fgpef933p+pOnW/e95z8coc8IZP5cZb702STK5dkcn1uw0y5sDss+M+OeoBR3UdAwAAAAAAAAAAAKB1ntABwIKycWpj3njZG3Ph9y7sOsr/t2FqQ87+xtm5/s7rc+YxZ2bx2OIkyTdvXp2nv/f+nsRRsvHOR2f8gR8fbNABOPWAU1NK6ToGAAAAAAAAAAAAQOsUOgBYMNZPrs/vff738vkffr7rKPfrwu9dmHs33Js3P+asPP4dl2X12o1b3LvxriOzZPdPpYxtec+wWbpoaZ6671O7jgEAAAAAAAAAAAAwEGNdBwCAQdg4tXFelzl+7vM//HyO+bsXZPXa+6bfOLU0G+86YjChBuSkfU7KDhM7dB0DAAAAAAAAAAAAYCAUOgAYeVN1Km+87I3zvszxc4t3uDbb7HlBkqlp9224/b+kTo3Gw7YmxiZy2iGndR0DAAAAAAAAAAAAYGAUOgAYeeddfV4u/N6FXcfYKot3/GYW73zptHvqxl2y4dYTB5SoXacfcXr2WrZX1zEAAAAAAAAAAAAABkahA4CR9r3V38t7rnxP1zFmZclun87YxM+m3bPhjmMzuW64ixCH7XpYnnfQ87qOAQAAAAAAAAAAADBQCh0AjKxNU5vyhsvekA1TG7qOMitlbFO22fOCJFPT7BrPfT8+JXVq0aBi9dXE2ETOPObMjI+Ndx0FAAAAAAAAAAAAYKAUOgAYWedfc36uuu2qrmPMyfjSmzOx8xen3TO1Yfesv/XEASXqrzOOOCMrlq/oOgYAAAAAAAAAAADAwCl0ADCSbl5zc9575Xu7jtEXE7t9JmXx7dPu2XjH47J86lEDStQfJ684Oc89+LldxwAAAAAAAAAAAADohEIHACPp3O+cmw1TG7qO0RdlbFMmdvnCtHu+9Lon5OLn/WWOe/Bxgwk1R8ftdVzOPObMjBXfigAAAAAAAAAAAAALk6soARg5azasyUU3XdR1jL5avOOVydh9/+nz5zznyKw66+TsuXxpFo8tzjuOe8e8L3Uct9dxecd/eUcWjy3uOgoAAAAAAAAAAABAZxQ6ABg5H7/x41m3aV3XMfqqjG3M4h2/8f//+akP3zM3vf2knHToHr+0b8n4kpx9/Nk5ecXJg444IyevODlnH3d2lowv6ToKAAAAAAAAAAAAQKcWdR0AAPqp1poPXvfBrmO0YvFOX8nGOx+bK974a9l5u4kt7xtbnLcd+7YcsNMBec+V78mGqQ0DTHn/JsYmcsYRZ+S5Bz83Y0WfFAAAAAAAAAAAAMAVlQCMlMt/enlWrVnVdYxWjC+5NRe88gHTljl+bqyM5QWHvCAXPOWCHLrroQNIt2WH7XpYLnjKBXn+Ic9X5gAAAAAAAAAAAADYzFWVAIyUi39wcdcRWnXJzZds1f4Vy1fk/Cefn1c94lWZGGsugvTTxNhEXv2IV+f8J5+fFctXDHQ2AAAAAAAAAAAAwHy3qOsAANBPV99+ddcRWnX1bVv/+haNLcoLD3lhTvyVE3Pud87NRTddlHWb1rWQrmfpoqU5aZ+Tctohp2WvZXu1NgcAAAAAAAAAAABgmCl0ADAyJqcmc90d13Udo1XX3nFtJqcmMz42vtVfu9eyvfLmx745rznqNfnYjR/Lh67/UG6666a+Zdtnx31y6gGn5qn7PjU7TOzQt3MBAAAAAAAAAAAARpFCBwAj46a7bmr1yRPzwbpN67Jqzarsu3zfWZ+xw8QOec7DnpNnH/jsXP7Ty3PJzZfk6tuuzrV3XLtV//4tXbQ0D9v5YTl414Nz/F7H56gHHJVSyqxzAQAAAAAAAAAAACwkCh0AjIxr7rim6wgDcc3t18yp0PFzpZQc/cCjc/QDj07Se8LJqjWrcs3t12Tl6pVZs35NNkxuyIapDZkYm8jE+ESWLVmW/Zbvl4N2OSh7L9t7Vk8KAQAAAAAAAAAAAEChA4AR8t07v9t1hIFYuXplK+eOj41n3+X79qUsAgAAAAAAAAAAAMD0xroOAAD9smbDmq4jDMSa9QvjdQIAAAAAAAAAAACMMoUOAEbG+sn1XUcYiA2TG7qOAAAAAAAAAAAAAMAcKXQAMDI2Tm3sOsJAbJhS6AAAAAAAAAAAAAAYdgodAIyMxWOLu44wEBNjE11HAAAAAAAAAAAAAGCOFDoAGBlLxpd0HWEgJsYVOgAAAAAAAAAAAACGnUIHACNj2cSyriMMxLIlC+N1AgAAAAAAAAAAAIwyhQ4ARsZDd3po1xEGYr/l+3UdAQAAAAAAAAAAAIA5UugAYGQctPNBXUcYiIN2WRivEwAAAAAAAAAAAGCUKXQAMDL22XGfLF20tOsYrVq6aGn2XrZ31zEAAAAAAAAAAAAAmCOFDgBGxvjYeA7c+cCuY7TqYTs/LONj413HAAAAAAAAAAAAAGCOFDoAGCkH73Jw1xFadfCuo/36AAAAAAAAAAAAABYKhQ4ARsoJv3JC1xFadfxex3cdAQAAAAAAAAAAAIA+UOgAYKQc9YCjsveyvbuO0Yp9dtwnRz3gqK5jAAAAAAAAAAAAANAHCh0AjJRSSp514LO6jtGKUw84NaWUrmMAAAAAAAAAAAAA0AcKHQCMnKfs+5QsXbS06xh9tXTR0jx136d2HQMAAAAAAAAAAACAPlHoAGDkLJtYlpP2OanrGH110j4nZYeJHbqOAQAAAAAAAAAAAECfKHQAMJJOO+S0TIxNdB2jLybGJnLaIad1HQMAAAAAAAAAAACAPlLoAGAk7bVsr5x+xOldx+iL0484PXst26vrGAAAAAAAAAAAAAD0kUIHACPruQc9N4fuemjXMebksF0Py/MOel7XMQAAAAAAAAAAAADoM4UOAEbWorFF+R/H/I9MjE10HWVWJsYmcuYxZ2Z8bLzrKAAAAAAAAAAAAAD0mUIHACNtxfIVecURr+g6xqycccQZWbF8RdcxAAAAAAAAAAAAAGiBQgcAI+95Bz8vJ684uesYW+XkFSfnuQc/t+sYAAAAAAAAAAAAALREoQOAkTdWxnLmMWfmuAcf13WUGTlur+Ny5jFnZqx4mwYAAAAAAAAAAAAYVa4UBWBBWDy2OO847h3zvtRx3F7H5R3/5R1ZPLa46ygAAAAAAAAAAAAAtEihA4AFY8n4kpx9/Nk5ecXJXUe5XyevODlnH3d2lowv6ToKAAAAAAAAAAAAAC1b1HUAABikxWOL87Zj35YDdjog77nyPdkwtaHrSJkYm8gZR5yR5x783IwVXUsAAAAAAAAAAACAhcBVowAsOGNlLC845AW54CkX5NBdD+00y2G7HpYLnnJBnn/I85U5AAAAAAAAAAAAABYQV44CsGCtWL4i5z/5/LzqEa/KxNjEQGdPjE3k1Y94dc5/8vlZsXzFQGcDAAAAAAAAAAAA0L1FXQcAgC4tGluUFx7ywpz4Kyfm3O+cm4tuuijrNq1rbd7SRUtz0j4n5bRDTstey/ZqbQ4AAAAAAAAAAAAA85tCBwAk2WvZXnnzY9+c1xz1mnzsxo/lQ9d/KDfddVPfzt9nx31y6gGn5qn7PjU7TOzQt3MBAAAAAAAAAAAAGE4KHQDwC3aY2CHPedhz8uwDn53Lf3p5Lrn5klx929W59o5rt+rJHUsXLc3Ddn5YDt714By/1/E56gFHpZTSYnIAAAAAAAAAAAAAholCBwDcj1JKjn7g0Tn6gUcnSSanJrNqzapcc/s1Wbl6ZdasX5MNkxuyYWpDJsYmMjE+kWVLlmW/5fvloF0Oyt7L9s742HjHrwIAAAAAAAAAAACA+UqhAwBmYHxsPPsu3zf7Lt+36ygAAAAAAAAAAAAAjICxrgMAAAAAAAAAAAAAAAAsNAodAAAAAAAAAAAAAAAAA6bQAQAAAAAAAAAAAAAAMGAKHQAAAAAAAAAAAAAAAAOm0AEAAAAAAAAAAAAAADBgCh0AAAAAAAAAAAAAAAADptABAAAAAAAAAAAAAAAwYAodAAAAAAAAAAAAAAAAA6bQAQAAAAAAAAAAAAAAMGAKHQAAAAAAAAAAAAAAAAOm0AEAAAAAAAAAAAAAADBgCh0AAAAAAAAAAAAAAAADptABAAAAAAAAAAAAAAAwYAodAAAAAAAAAAAAAAAAA6bQAQAAAAAAAAAAAAAAMGAKHQAAAAAAAAAAAAAAAAOm0AEAAAAAAAAAAAAAADBgCh0AAAAAAAAAAAAAAAADptABAAAAAAAAAAAAAAAwYAodAAAAAAAAAAAAAAAAA6bQAQAAAAAAAAAAAAAAMGAKHQAAAAAAAAAAAAAAAAOm0AEAAAAAAAAAAAAAADBgCh0AAAAAAAAAAAAAAAADptABAAAAAAAAAAAAAAAwYAodAAAAAAAAAAAAAAAAA6bQAQAAAAAAAAAAAAAAMGCLug4AAENhajK57Ybkx99MfnZNct/qZNP6ZHJDMj6RLFqSbLM82f2gZM8jkl33S8bGOw4NAAAAAAAAAAAAwHyl0AEA96fWZNWlyfUXJT+6IvnJt5ONa2f+9Yu3Sx54aPKgI5MDTkr2PjYppb28AAAAAAAAAAAAAAwVhQ4A+EXrViff+mBy+bm9J3LM1sZ7k5u/0vv4yjnJrvsnR52WPPxZydLl/UoLAAAAAAAAAAAAwJBS6ACAJLnje8ml70yuumDrnsQxU7fdkHzqtcnn3pIcekpy7CuTnVf0fw4AAAAAAAAAAAAAQ2Gs6wAA0KnJTcmlf56899HJFee1U+b4RRvX9ua899G9AsnUZLvzAAAAAAAAAAAAAJiXFDoAWLhuvT75u19LPvvmZHL9YGdPrk8++8fJub/WywEAAAAAAAAAAADAgqLQAcDCMzWVXPau5K8el/zoG91m+dHlvRyXvauXCwAAAAAAAAAAAIAFYVHXAQBgoCY3Jh99eXLVh7tO8u8m1yefeVPyk+8kTz8nGV/cdSIAAAAAAAAAAAAAWuYJHQAsHBvvSz702/OrzPGLrvpwL9/G+7pOAgAAAAAAAAAAAEDLFDoAWBgmNyYXPD+54ZNdJ5neDZ9M/vkFvbwAAAAAAAAAAAAAjCyFDgBG39RU8tGXz/8yx89df1Ev79RU10kAAAAAAAAAAAAAaIlCBwCj78vvTq76cNcpts5VH06+/J6uUwAAAAAAAAAAAADQEoUOAEbbrdcnF7+16xSzc/H/6OUHAAAAAAAAAAAAYOQodAAwuiY3JR99WTK5vuskszO5Pvnoy5Opya6TAAAAAAAAAAAAANBnCh0AjK4vvyf50Te6TjE3P7o8+dK7u04BAAAAAAAAAAAAQJ8pdAAwmu74XnLJ27pO0R+XvK33egAAAAAAAAAAAAAYGQodAIymS9+ZTK7vOkV/TK7vvR4AAAAAAAAAAAAARoZCBwCjZ93q5KoLuk7RX1ddkNx3V9cpAAAAAAAAAAAAAOgThQ4ARs+3PphsXNt1iv7auLb3ugAAAAAAAAAAAAAYCQodAIyWWpOvv6/rFO34+vt6rw8AAAAAAAAAAACAoafQAcBoWXVpcvvKrlO047Ybku9f1nUKAAAAAAAAAAAAAPpAoQOA0XL9RV0naNd1I/76AAAAAAAAAAAAABYIhQ4ARsuPrug6Qbt+POKvDwAAAAAAAAAAAGCBUOgAYHRMTSY/+XbXKdp1y7d7rxMAAAAAAAAAAACAoabQAcDouO2GZOParlO0a+O9yW0ru04BAAAAAAAAAAAAwBwpdAAwOn78za4TDMYt3+w6AQAAAAAAAAAAAABzpNABwOj42TVdJxiMhfI6AQAAAAAAAAAAAEaYQgcAo+O+1V0nGIx1q7tOAAAAAAAAAAAAAMAcKXQAMDo2re86wWAslNcJAAAAAAAAAAAAMMIWdR0AaF8pZUmS/ZM8OMkOSbZNsjbJ3Ul+mOT6WuuG7hJCn0wukP8aTyp0AAAAAAAAAAAAAAw7hQ4YUaWURyd5epInJzk4yfg02ydLKVcnuSjJv9Zav9J+QmjB+ETXCQZjfEnXCQAAAAAAAAAAAACYI4UOGDGllGcl+f0kR27Fl40nOWzzx+tKKd9I8j9rrR9qISK0Z9ECKToslNcJAAAAAAAAAAAAMMLGug4A9Ecp5cBSyheS/FO2rsxxfx6R5IOllEtKKQfMPR0MyDbLu04wGEuXd50AAAAAAAAAAAAAgDlS6IARUEp5RpKvJ/nVPh99XJLLSym/0edzoR27H9R1gsFYKK8TAAAAAAAAAAAAYIQpdMCQK6WcnuSfk2zf0ojtk3yklPLyls6H/tnz8K4TDMYeh3edAAAAAAAAAAAAAIA5UuiAIVZKeV6SdycpbY9K8p5SynNbngNzs+v+yeJtu07RrsXbJbvu13UKAAAAAAAAAAAAAOZIoQOGVCnlkUn+NjMrc3wpySuSHJlk5ySLN//rUUl+J8lXZzIyyd+WUo6eVWAYhLHx5IGHdZ2iXXsc1nudAAAAAAAAAAAAAAw1hQ4YQqWUZUk+mF4xYzorkzyh1npMrfW9tdYra6131lo3bf7Xb9Ra311rfXSSJya5seG8iSQf2jwf5qcHHdl1gnbtOeKvDwAAAAAAAAAAAGCBUOiA4fQnSfZp2PPZJEfXWj83kwNrrZ9O74kdlzRs3SfJm2dyJnTigJO6TtCuA0f89QEAAAAAAAAAAAAsEAodMGRKKQclOb1h25eTPK3WetfWnF1rXZ3kKUm+1rD1jFLKw7bmbBiYvY9Ndtmv6xTt2HX/5CHHdJ0CAAAAAAAAAAAAgD5Q6IDh88dJFk2zfkeSU2uta2dzeK313iTPTLJ6mm2LkrxpNudD60pJjn5R1ynacfSLeq8PAAAAAAAAAAAAgKGn0AFDpJSyIslvNmx7Q6315rnMqbV+P73iyHROKaXsPZc50JqHPytZvG3XKfpr8ba91wUAAAAAAAAAAADASFDogOFyepLxadZXJvmbPs06J8n3plkf35wH5p+ly5NDT+k6RX8dekqyzY5dpwAAAAAAAAAAAACgTxQ6YEiUUsaT/NeGbX9ea53sx7xa66Ykf9Gw7dmlFP8/wvx07CuT8SVdp+iP8SW91wMAAAAAAAAAAADAyHAhNgyPE5LsMc36fUn+oc8zz0uyYZr1PZMc1+eZ0B87r0iOf33XKfrj+Nf3Xg8AAAAAAAAAAAAAI0OhA4bHUxrWL6y13t3PgbXW1Uk+2bCtKRd05zGvSB70iK5TzM2Djkoee0bXKQAAAAAAAAAAAADoM4UOGB5PaFi/sKW5Teee2NJcmLvxRcnT/zIZX9J1ktkZX5I8/ZxkbLzrJAAAAAAAAAAAAAD0mUIHDIFSyh5JHtaw7bMtjf9Mw/rBpZQHtjQb5m63A5IT/qjrFLNzwht6+QEAAAAAAAAAAAAYOQodMBwe2bB+c6315jYG11pXJbmlYdvRbcyGvnnMGcmhz+w6xdY59JnJY17RdQoAAAAAAAAAAAAAWqLQAcPhyIb1K1qef3nD+hEtz4e5GRtLnn5Osv+Tu04yMwec1Ms75m0aAAAAAAAAAAAAYFS5UhSGw+EN699ueX7T+QodzH/ji5NTPjD/Sx0HnJT81vt7eQEAAAAAAAAAAAAYWQodMBz2b1hf2fL87zas79fyfOiPxdskp/59cugzu05y/w59ZvLM83s5AQAAAAAAAAAAABhpCh0wz5VSSpK9G7Y1FS7mqun8vVueD/0zvjj5jb9OTvyTZHxJ12l6xpckJ57Zy+XJHAAAAAAAAAAAAAALgkIHzH8PSNJ0u/4ft5yh6fztSim7t5wB+mdsLDnmd5OXfjF50CO6zfKgo3o5jvmdXi4AAAAAAAAAAAAAFgRXjsL8t+cM9vyk5QwzOX8mOWF+2e2A5IWfTp7wlsE/rWN8Se8pIad9upcDAAAAAAAAAAAAgAVFoQPmv10a1tfUWte3GaDWujbJPQ3bmnLC/DS+KDn2lcnpX0mOfF6yeNt25y3etjfn9K/0nhIyNt7uPAAAAAAAAAAAAADmpUVdBwAa7dywvmYgKXpztp9mvSknzG87r0ie+hfJr52ZfOuDydffl9x2Q//O33X/5OgXJQ9/VrLNjv07FwAAAAAAAAAAAIChpNAB899ODet3DyRF85x5U+gopZye5OUDGLXvAGYwaNvsmDzqJckjX5x8/7LkuouSH1+R3PKtZOPamZ+zeLtkj8OSPY9MDjwpecgxSSnt5QYAAAAAAAAAAABgqCh0wPy3TcP6vQNJkdzTsN6Uc5B2S3JQ1yEYcqUkex/b+0iSqcnktpXJLd9MfnZNsm51sml9Mrk+GV+SLFqSLF2e7H5Qssfhya77JWPj3eUHAAAAAAAAAAAAYF5T6ID5b6JhfdNAUjTPacoJw21sPNn9wN4HAAAAAAAAAAAAAMzRWNcBgEYKHQAAAAAAAAAAAAAAI0ahA+a/pv+dTg4kRfOc8YGkAAAAAAAAAAAAAAAYAQodMP81PRlj0UBSNM/ZOJAUAAAAAAAAAAAAAAAjYFAXggOzt6FhfVD/O17csN6Uc5BuTXLNAObsm2TJAOYAAAAAAAAAAAAAACNGoQPmv6YnX0wMJMUQFTpqre9N8t6255RSrk5yUNtzAAAAAAAAAAAAAIDRM9Z1AKDRPQ3r2w8kRbJDw3pTTgAAAAAAAAAAAAAANlPogPnvjob1ZQNJ0TynKScAAAAAAAAAAAAAAJspdMD8d3vD+vJBhEiyY8N6U04AAAAAAAAAAAAAADZT6ID577aG9SWllOVtBiil7JxkomGbQgcAAAAAAAAAAAAAwAwpdMD894MZ7HlAyxlmcv5McgIAAAAAAAAAAAAAEIUOmPdqrfek+ekXD2k5xt4N6z+rtd7bcgYAAAAAAAAAAAAAgJGh0AHD4aaG9f1anv/QhvWmfAAAAAAAAAAAAAAA/AKFDhgOVzesH9Dy/Kbzm/IBAAAAAAAAAAAAAPALFDpgOFzRsH5Ey/OPbFi/suX5AAAAAAAAAAAAAAAjRaEDhkNToePwUsp4G4NLKYuSPLxhm0IHAAAAAAAAAAAAAMBWUOiA4XB5kvumWd8+ySNamv3IJNtOs35fkm+0NBsAAAAAAAAAAAAAYCQpdMAQqLXel+Syhm0ntjT+CQ3rX9ycDwAAAAAAAAAAAACAGVLogOHxmYb1Z7Q097ca1j/d0lwAAAAAAAAAAAAAgJGl0AHD458b1o8spRzQz4GllEOSHDrNlprmXAAAAAAAAAAAAAAA/AcKHTAkaq03JvlKw7Yz+jz2dxrWv1RrXdXnmQAAAAAAAAAAAAAAI0+hA4bL3zWsv6CUskc/BpVSHpzktxu2faAfswAAAAAAAAAAAAAAFhqFDhguf5/kZ9Osb5vkrD7N+tMk20yz/tPNeQAAAAAAAAAAAAAA2EoKHTBEaq33JXlXw7bnllJ+Yy5zSinPTPLshm3vrLWun8scAAAAAAAAAAAAAICFSqEDhs87k9zcsOe8UsojZ3N4KeXRSc5t2Pb9NBdLAAAAAAAAAAAAAADYAoUOGDK11rVJXt2wbYckny6l/PrWnF1KeVqSf0uyfcPW19Ra123N2QAAAAAAAAAAAAAA/DuFDhhCtdZ/TvK/G7btmORjpZR/LKUcON3GUspBpZQPJvlokmUN5/5jrfUjMw4LAAAAAAAAAAAAAMB/sqjrAMCsvSTJI5IcMM2ekuTZSZ5dSrkyyZeS3JTknvSe4rFPkmOSPHyGM69L8tLZBgYAAAAAAAAAAAAAoEehA4ZUrfWeUsoTk3wxyV4z+JIjNn/M1g+SPLHWes8czgAAAAAAAAAAAAAAIMlY1wGA2au1fj/JCUlubHnUd5OcUGv9QctzAAAAAAAAAAAAAAAWBIUOGHK11u8mOTrJv7U04lNJjq61tl0aAQAAAAAAAAAAAABYMBQ6YATUWu+stT4pyfOT/KxPx/4syfNqrU+uta7u05kAAAAAAAAAAAAAAEShA0ZKrfW8JCuSnJ7k2lkec83mr9+n1np+v7IBAAAAAAAAAAAAAPDvFnUdAOivWuu9Sc5Jck4pZf8kT0pyZJKDkzwoyQ5Jtk2yNsndSX6YXonjiiSfrLWu7CI3AAAAAAAAAAAAAMBCotABI6zWekOSG7rOAQAAAAAAAAAAAADALxvrOgAAAAAAAAAAAAAAAMBCo9ABAAAAAAAAAAAAAAAwYAodAAAAAAAAAAAAAAAAA6bQAQAAAAAAAAAAAAAAMGAKHQAAAAAAAAAAAAAAAAOm0AEAAAAAAAAAAAAAADBgCh0AAAAAAAAAAAAAAAADptABAAAAAAAAAAAAAAAwYAodAAAAAAAAAAAAAAAAA6bQAQAAAAAAAAAAAAAAMGAKHQAAAAAAAAAAAAAAAAOm0AEAAAAAAAAAAAAAADBgCh0AAAAAAAAAAAAAAAADptABAAAAAAAAAAAAAAAwYKXW2nUGgKFUSlmTZIf/+PklS5Zk33337SARAAAAAAAAAAAAAAyfG2+8MevXr7+/pbtrrcsGnWdQFDoAZqmUcl+SJV3nAAAAAAAAAAAAAIARtb7Wuk3XIdoy1nUAAAAAAAAAAAAAAACAhUahAwAAAAAAAAAAAAAAYMAUOgAAAAAAAAAAAAAAAAZMoQMAAAAAAAAAAAAAAGDAFnUdAGCIrU6y/H4+vyHJzQNNQpLsm2TJ/Xx+fZIbB5wFABYi78UA0B3vwwDQLe/FANAd78MA0C3vxUA/7ZVk4n4+v3rAOQZKoQNglmqtD+w6A/+ulHJ1koPuZ+nGWuvBg84DAAuN92IA6I73YQDolvdiAOiO92EA6Jb3YoC5G+s6AAAAAAAAAAAAAAAAwEKj0AEAAAAAAAAAAAAAADBgCh0AAAAAAAAAAAAAAAADptABAAAAAAAAAAAAAAAwYAodAAAAAAAAAAAAAAAAA6bQAQAAAAAAAAAAAAAAMGAKHQAAAAAAAAAAAAAAAAOm0AEAAAAAAAAAAAAAADBgCh0AAAAAAAAAAAAAAAADptABAAAAAAAAAAAAAAAwYAodAAAAAAAAAAAAAAAAA6bQAQAAAAAAAAAAAAAAMGAKHQAAAAAAAAAAAAAAAAOm0AEAAAAAAAAAAAAAADBgCh0AAAAAAAAAAAAAAAADptABAAAAAAAAAAAAAAAwYAodAAAAAAAAAAAAAAAAA6bQAQAAAAAAAAAAAAAAMGAKHQAAAAAAAAAAAAAAAAOm0AEAAAAAAAAAAAAAADBgCh0AAAAAAAAAAAAAAAADptABAAAAAAAAAAAAAAAwYAodAAAAAAAAAAAAAAAAA6bQAQAAAAAAAAAAAAAAMGCLug4AAH1yTpLd7ufztw46CAAsUN6LAaA73ocBoFveiwGgO96HAaBb3osB5qjUWrvOAAAAAAAAAAAAAAAAsKCMdR0AAAAAAAAAAAAAAABgoVHoAAAAAAAAAAAAAAAAGDCFDgAAAAAAAAAAAAAAgAFT6AAAAAAAAAAAAAAAABgwhQ4AAAAAAAAAAAAAAIABU+gAAAAAAAAAAAAAAAAYMIUOAAAAAAAAAAAAAACAAVPoAAAAAAAAAAAAAAAAGDCFDgAAAAAAAAAAAAAAgAFT6AAAAAAAAAAAAAAAABgwhQ4AAAAAAAAAAAAAAIABU+gAAAAAAAAAAAAAAAAYMIUOAAAAAAAAAAAAAACAAVPoAAAAAAAAAAAAAAAAGDCFDgAAAAAAAAAAAAAAgAFT6AAAAAAAAAAAAAAAABgwhQ4AAAAAAAAAAAAAAIABU+gAAAAAAAAAAAAAAAAYMIUOAAAAAAAAAAAAAACAAVPoAAAAAAAAAAAAAAAAGDCFDgAAAAAAAAAAAAAAgAFT6AAAAAAAAAAAAAAAABgwhQ4AAAAAAAAAAAAAAIABU+gAAAAAAAAAAAAAAAAYMIUOAAAAAAAAAAAAAACAAVPoAAAAAAAAAAAAAAAAGDCFDgAAAAAAAAAAAAAAgAFT6AAAAAAAAAAAAAAAABgwhQ4AAAAAAAAAAAAAAIABW9R1AACYq1LKkiT7J3lwkh2SbJtkbZK7k/wwyfW11g3dJQQAAAAAAACgX0opi5PsnWSPJLslWZpkcZINSdYluS3JLUlW1Vo3dhQTAJijUsqiJPum976/Q5Ltk9yXZE167/XX11rXdhYQoA9KrbXrDACw1Uopj07y9CRPTnJwkvFptk8muTrJRUn+tdb6ldYDAsAI2fyHsQOTHJLe++4h6RUpl2/+2DG999v7ktyR5MdJbkry7SRfT/Il5UoAAABGWSnloCQnpPcz8/759wtNdkgyluTeJPek93Pz95LcmOT6JF9L8p1a6+TgUwPA8CilbJfkpCSPT3JMkgPSK3A02ZjkuiSXJvlckk+66BMA5rdSyqFJnpHee//hSSam2V6TrEzyqSQfS3JxdWE0MGQUOgAYKqWUZyX5/SRHzuGYbyT5n7XWD/UnFQCMllLKWJIj0rsQ5fFJHpfeE7Bma22STyc5L8knaq2b5hwSAAAAOlZKeViSFyV5VpI953DUvekVOz6V5MJa69V9iAcAI6GUckiS1yQ5Jcl2fTjyniQfSvKOWut1fTgPADpXStk7yVG/8PGI9G7Mt0W11tJ6sK1USnliktclOW4Ox9yQ5M+T/K2bJwDDQqEDgKFQSjkwyV8n+dU+Hvv5JC+ttV7fxzMBYChtflTt45OcmuRpSXZuadRNSc5Kcq5foAFAe0opOyW5NskDZrD9vFrr89tNBACjo5RyZHo/257Y0oira62HtHQ2AAyFUsoDk/xpkt9O0sYFpzXJ3yV5Xa31thbOB4BWlFIenP9c3th1a8+ZT4WOUsqDkrw7yW/08dhvJXlJrfWrfTwToBUKHQDMe6WUZ6R3R+/tWzj+niTPrbX+SwtnA8C8V0o5OMkr0/vl2C4DHH1FkhfVWq8c4EwAWDBKKX+X5AUz3K7QAQAzUErZMcm7kjw37VxY+nN31VqXt3g+AMxrpZST0vv78FZfnDoLP0ny32qtnxvALADYKqWUByQ5Or9c4JjJTXwazZdCRynlcUn+OcnuLRy/Mcnv1lr/soWzAfpmrOsAADCdUsrp6X3T3kaZI5vP/Ugp5eUtnQ8A891Tkrwogy1zJMmRSb5cSnnJgOcCwMgrpZyQmZc5AIAZKKUcm97dPZ+XdsscALCglVJeluTjGUyZI0kemORTpZTnDmgeAGyNf0vvffGPk5ycPpU55otSytOSfC7tlDmSZHGSc0opZ7V0PkBfKHQAMG+VUp6X3uP02v7jWEnyHr+kA4CBW5Lkr0opb+k6CACMilLK0iR/03UOABglpZT/mt4FJg/pOgsAjLJSyguSnJPBX8+0KMkHSinPHPBcAFiwSiknJvlQeqWLtr22lPLGAcwBmJVFXQcAgPtTSnlkkr/NzMocX0ryvzf/66okdyfZIcmKJI9N8pwkj2oameRvSynX1lq/PsvYALAQTCa5Osm1SW5KcluSe5Nsk95TPvZIcmySA7bizDeVUtbWWv+0z1kBYCF6S5J9uw4BAKNi81Okt+bGQ/ck+VqSlUm+v/mfNyZZvvljtySHJTkkvZ+lAYAkpZSjkvz1VnzJ5Uk+meSyJN9Nckd6fydelmSnJAem97fiX0/vvbcxQpLzSilX11qv3oocAMBWKqXsneTD6d0AsMlVSf4+yRfT+1n7riTbJdkryaOTnJrk8Wn+uf1PSinfrrX+6yxjA7Sm1Fq7zgAAv6SUsizJN5Ps07B1ZZKX1Vo/N4Mzfy29u7k0XdRyU5LDa61rZhAVAIZeKeV1Sd7esO269B7l+8kkX621rp3BuXskeXGSM9IrejSpSX691nrRDPYCAPejlHJEeheQbu2NfM6rtT6//4kAYLiVUk5N8k9pvihk3eZ95ye5rNa6aQZnjyc5KMmTkzwtvYtQfn438rtqrctnGRsAhk4pZVGSb6X33tjk0iR/WGu9dCvOf3ySs5IcNYPtlyd5ZHVBFQDzQCnlm0ke3sbZtdaZ3rigrza/71+W5JENW3+a5Ixa6wUzOPPoJH+V5MiGrXemd13YD2aSFWBQBv2IQgCYiT9Jc5njs0mOnkmZI0lqrZ9O7xd0lzRs3SfJm2dyJgCMuNVJ3pnkEbXWh9Va/6DWeslMyhxJUmu9pdb6liQPSfK+GXxJSfK+UsryWeYFgAVt80Wh58ZTmQGgL0opx6ZX0Gi6wOV9SfattZ5Wa/3CTMocSVJrnay1XlVr/bNa6zHpPfHydek91QMAFprnZmZljjOTHLc1ZY4k2fw35ccmOXsG249K707fADCsViX5dNchpvGKNJc5vpXkyJmUOZKk1vr19N7r/6lh607p/Q0cYF5R6ABgXimlHJTk9IZtX07ytFrrXVtzdq11dZKnpHe30umcUUp52NacDQAj5LtJXpLkQbXWV9Var5jLYbXWe2ut/z3J85JMNmzfI8lr5zIPABaw1yQ5Ygtr3xtkEAAYdqWUndK7CGRimm13JnlyrfW/11pvmevMWuvPaq1/mt5Tpp811/MAYMj87gz2vL3W+qZaa9Pvme9XrXVjrfU1Sf5iBttfOZsZANCBm5P8S5I3JHlSkl1rrfuk9/feeaeUsluab7T73SQn1lp/vDVn11rXJ/ntJP/asPU3SilP2JqzAdqm0AHAfPPHmf5uonckOXWmdwf/j2qt9yZ5Znp3Hd+SRUneNJvzAWCI3ZDkvyU5sNb6N7N9r92SWuv5Sc6YwdYzSinL+jkbAEZdKWXfbPmPYF9K8g+DSwMAI+Fvkjx4mvUfJzm21vqpfg/e/OSOvp8LAPNVKeWQJIc1bLs0yR/1aeSr0nwDwEdt/lkbAOaTHyf5WHrXNJ2cZPda66/UWp9Ra31rrfXfaq23dxux0e8l2XGa9Q1JnllrvXU2h28ufj4vvaeUTOdPZnM+QFsUOgCYN0opK5L8ZsO2N9Rab57LnFrr99MrjkznlFLK3nOZAwBD4qdJXp7k4FrrP8727mYzUWv9yyTnN2zbLr3yJQAwc3+dZOn9fH5jendiq4ONAwDDq5RycpLfmmbL3UlOqrVeM6BIADDqHj+DPX9Ya+3Lz7a11qkkr5vBVnfuBmA+eHeSpyTZo9b6oFrr02qtZ9ZaL5pt6aErm2/q1/TkkHfWWq+cy5xa611pfvrXY0opj5vLHIB+UugAYD45Pcn4NOsr07szWj+ck+R706yPb84DACOt1vr+Wutf1lo3DWjk65M0Pf3j6QPIAQAjoZTywmz54pf/VWv9ziDzAMAwK6UsTvK/Gra9tNb6rUHkAYAF4siG9etrrZf2c2Ct9ZIk323YdlQ/ZwLAbNRaz621fqLW+pOus/TB8zL90zlWJ3lrPwbVWj+W5IsN236nH7MA+kGhA4B5oZQynuS/Nmz7837dNXzzRat/0bDt2aUU75UA0Ee11h8l+aeGbY/zHgwAzUopD0jyji0sfy8eGw8AW+u0JAdMs/6xWuv/HlQYAFgg9m1Y/3RLc/+tYf2hLc0FgIXqtxvW/6bWuqaP85pu2PCUUsp0BROAgXGBDADzxQlJ9phm/b4k/9Dnmecl2TDN+p5JjuvzTAAg+UTD+rIkDxlEEAAYcn+RZKctrL281rpukGEAYJhtvrHAq6fZMpnktQOKAwALyZZ+rv25b7c0t+ncXVuaCwALTillvyRHN2z72z6P/XiSW6ZZX5LkN/s8E2BWFDoAmC+e0rB+Ya317n4OrLWuTvLJhm1NuQCArfd/Z7BnRespAGCIlVKekuSZW1j+UK216U6jAMAve2qS/aZZ/0it9bpBhQGABWRJw/ptLc29tWF9aUtzAWAharr+6hu11u/2c2CtdSrJhxu2uS4MmBcUOgCYL57QsH5hS3Obzj2xpbkAsGDVWu/I9E/JSpLlA4gCAEOplLJDknO2sLw6ySsHFgYARscLGtb/aiApAGDhuath/d6W5jadu6aluQCwEM3X68KOL6WMtzQbYMYUOgDoXClljyQPa9j22ZbGf6Zh/eBSygNbmg0AC1nTXdXc/QwAtuysJA/ewtof1lp/MsgwADDsSinLkzxpmi23JPn8QMIAwMJze8P6Li3NbTq3KRcAMAOllEVJfrVhW1vXhX0xyX3TrO+Y5OiWZgPMmEIHAPPBIxvWb6613tzG4FrrqvT+GDcd37gDQP9t27A+3S/WAGDBKqU8NsnLtrD85SR/PcA4ADAqfiPJxDTrn6i11kGFAYAF5pqG9bZuvtd07vdamgsAC83BSbabZn1jkq+1MbjWel+SKxu2uS4M6JxCBwDzwZEN61e0PP/yhvUjWp4PAAtKKWWH9O52Mp07B5EFAIZJKWUiyfuSlPtZ3pTkJS42BYBZObFh/eKBpACAhemLDeuPa2lu053CL21pLgAsNE3XhV1Ta13f4nzXhQHznkIHAPPB4Q3r3255ftP5vnEHgP46Ivd/IeovunEQQQBgyPxRkodtYe3sWutVgwwDACPkuIb1rw4iBAAsUBdn+ic2n1BKWdLPgaWUpUlOmGbLVJJL+jkTABawwxvWXRcGLHiLug4AAEn2b1hf2fL87zas79fyfABYaE5uWF+T5AeDCAIAw6KUclCS121heVWStwwuDQCMjlLKQ5PsMc2W1bXWm2ZwzqL0fpe8T3pPpVySZG2Su5PcnGRVrfWeuScGgNFSa72zlPKPSU7bwpblSV6W5J19HHtGkmXTrH+81vrDPs4DgIXMdWEADRQ6AOhUKaUk2bthW9M31nPVdP7eLc8HgAWjlDKe5NSGbZfWWqcGkQcAhkEpZSzJ+5JMbGHLy2utawcYCQBGyeEN61v8/XEpZdckz0nylCSPy5bfq5OkllKuTXJpkn9N8tla64atiwoAI+sdSX47W34vfX0p5YJa64/mOqiU8pBs+YYJP3f2XOcAAP/fPg3rXV8Xtl0pZbda660t5wDYorGuAwCw4D0gyTYNe37ccoam87crpezecgYAWCienuQhDXs+NoAcADBMTk/ymC2sfbjW+slBhgGAEXNIw/qN//ETpZTdSyl/md7TJd+Z5PGZvsyRJCXJQUlenOTCJD8spfxxKWWnrU4MACOm1npdkj+ZZstuST5RStlhLnNKKTsn+WSS6d5/319r/b9zmQMA9Gy+0W/T34bbvi7sJ0mabibYVDoBaJVCBwBd23MGe37ScoaZnD+TnADANDY/nWO6P8olyYYkFwwgDgAMhVLKXkneuoXlu5K8cnBpAGAkHdSw/tNf/IdSymlJrk/y0iRL5zB3tyRvTnJDKeW/z+EcABgVZyX59DTrhyf5einl4bM5vJTyqCSXJ3nYNNtuTPKq2ZwPANyvndJ8o99WrwurtW5KcnvDNteFAZ1S6ACga7s0rK+pta5vM0CtdW2Sexq2NeUEAJq9LM0XypxXa71jEGEAYEick2RLdyB9fa31lkGGAYARtFfD+q1JUkpZXEo5N8n7kizv4/xdk/xNKeUjpZRlfTwXAIZKrXUyvSc8f2GabQck+Vop5e9mWuwopRxdSvnHJJdm+rtv/zDJE2qtd80wMgDQbCbXW/2s9RT/4WYN98N1YUCnFnUdAIAFb+eG9TUDSdGbs/006005AYBplFL2TvL2hm0bk/xp+2kAYDiUUp6V5Ne3sPyVJH81wDgAMKr2aFhfU0pZlOSfkvxmizmekWSfUsoTa623tjgHAOatWuu6UsqTkvyvJC/fwraJJC9I8oJSyo+TXJZkZZI707uJ3w7p3Q38gCTHJHnADEZfkeSUWuuqOb0AAOA/msn1VoO4NqxphuvCgE4pdADQtZ0a1u8eSIrmOb5xB4BZKqWMJzkv05cnk+SdtdYbBxAJAOa9UsrOSd61heVNSV5Sa50aYCQAGFUPbFjfkN4Ts9osc/zcEUkuLqUcU2sd1M2OAGBeqbXel+T0Uson0rsB0KHTbN8zySlzGLchyV8k+aNa64Y5nAMA3L+m68LWbX5KV9tcFwbMa2NdBwBgwdumYf3egaTo3a1lOk05AYAtOzPJrzbsuXnzPgCg5+wku29h7c9rrd8eZBgAGEWllG2SLGnY9swk/32a9XVJPrF5zyOSPHjzmbsnOSy9i0zPT3L7DGMdkuSDpZQyw/0AMJJqrZ9M8vD0nmL1iST39fH4Nek99fKhtdbfV+YAgNa4LgxgBjyhA4CuTTSsbxpIiuY5TTkBgPtRSnlKktc1bKtJXlhrHdSTuQBgXiulPCHJ87aw/P0kbx5cGgAYaUtnsOf4LXy+Jvn7JK+ttf7kftZv3fxxVZJ/LqUsTfLaJH8wg7lPTnJGencMB4AFq9Zak/xLKeXaJM9J8nuZ2wWXG5P8WZK31lrX9SEiADA914UBzIAndADQNd+4A8CIKqUckuQfkzTdVfQ9tdbPDiASAMx7pZRtk/z1NFtOr7WuHVQeABhxs70gdG2SJ9dan7eFMsd/UmtdV2t9c3p3Gl81gy95eyllz1nmA4ChV0pZVEp5binlO0muTfKGzP3u2YuT/FGSm0opf1VKOWCuOQGAabkuDGAGFDoA6FrTe9HkQFI0zxkfSAoAGBGllN2TfDzJDg1bv57eXdUAgJ4/SbJiC2v/XGu9cJBhAGDELZ7F19yd5Ndqrf82m4G11pVJHpfkhoat2yZ502xmAMCwK6WcnGRlkvOSHNzCiAckeUmSa0opF5RS9m1hBgDgujCAGVHoAKBrTQ3oRQNJ0Txn40BSAMAIKKVsn+SiJHs3bL09ySm11g2thwKAIVBKeUSSV25heU2S3xlcGgBYEGZz4cgZtdbL5jK01vrDJKckafp5+PmllF3nMgsAhkkpZWkp5Zwkn0jz75f7YSzJbyX5ZinlhQOYBwALjevCAGZAoQOArjX9wWpQ37g33YnNhaYAMAOllIkk/5LkEQ1b1yV5Wq31++2nAoD5r5SyKMn7suU7gb2+1nrLACMBwEKwtb/3/Vit9bx+DK61fju9J3NNZ0mSF/RjHgDMd6WUpekVOV42g+2TST6T5I1JTkiyf5Jd0vub766b//nx6T3t6rNJphrO2z7JuaWU984qPACwJa4LA5iBQf2fIQBsSVPDeWIgKXzjDgBzVkoZT/JPSZ7QsHVjek/mmNMdTQFgxPxeksO3sPa1JH85uCgAsGBs7e99/6jP8/9XkleldwHqlvxmkv/Z57kAMK9svlHQx9IrZ0xnY5K/SXJ2rfV7W9hz++aPlUku3nz+vkleneTFmf5aqZeXUmqt9RVbER8A2DLXhQHMgCd0ANC1exrWtx9IimSHhvWmnACwoJVSSnp3FX9Gw9apJM+ttV7YfioAGA6llIcm+eMtLG9K8pJaa9PdRAGArbd2K/Z+sdb6nX4Or7Xel+T9DduOLqXs2s+5ADAPvSXNNwr6fpLH1VpfMU2Z437VWm+stZ6e5L8kublh++mllJduzfkAwBa5LgxgBhQ6AOjaHQ3rywaSonlOU04AWOjeleT5M9j30lrrB1vOAgDD5m+SbLOFtXfVWr85wCwAsGDUWjcmuXuG2z/QUoymQsdYkke2NBsAOldKeWySP2jYtjLJUbXWr85lVq31S0kekeTGhq3v2PxUDwBgbpqut1pcStnS78b7yXVhwLym0AFA125vWF8+iBBJdmxYb8oJAAtWKeVtSc6YwdbX1Fr/tu08ADBMSimnJTl+C8vfz5af3AEA9MdMf/d7WUvzr02yumHPkS3NBoD54KxMf/3SHUlOrrXe1o9htdZbk5yc6d9/t0vyP/sxDwAWuJn8zL287RAzmOG6MKBTCh0AdK3pF29LSinL2wxQStk5yUTDNt+4A8D9KKW8PskfzmDrH9daz247DwAMk1LKAzL9BSKvqLXeO6g8ALBAzeTi0DuT3NDG8FprTfK1hm3uEA7ASCqlHJ3kcQ3b3lxrXdnPubXW65P8ScO2p3lKBwDM2Ux+5n5g6ymaZ7guDOiUQgcAXfvBDPY8oOUMMzl/JjkBYEEppfxukrfOYOv/rLU2/XEMABai9yTZaQtrH6m1fmKQYQBggZrJ736v3Vy8aMs1Det7tTgbALr0wob1m5P8TUuzz0nyw2nWx5K8pKXZALAg1FrXprks0ep1YaWUbZPs0LDt+21mAGii0AFAp2qt96T5G/eHtBxj74b1n7kjKgD8slLKi5O8cwZb31Nr/YOW4wDA0CmlPDXJb21heU2S3xlgHABYyG6awZ7VLWe4s2F955bnA0BXjm9Y/1CtdX0bgzef++GGbY9vYzYALDCrGtbbvi5sJuevajkDwLQUOgCYD5r+YLZfy/Mf2rA+kz/oAcCCUUr57SR/NYOt58bFqACwJWdPs/aGWuuPB5YEABa2781gz+qWMzSdv23L8wFg4Eopuyc5oGHbp1uO0XT+w0spy1rOAACjbr5fF/bTzU8SAejMoq4DAECSq5McNc160y/y5qrp/Ktbng8AQ6OUckqS9ycpDVv/KcmLa621/VQAMJR23cLn1yRZX0p5UR9nHdmwvt8M5n2h1rqyX4EAYB75zgz2rGs5Q9P5/qYLwCjaZwZ7vtZyhq82rI+nd5HpN1rOAQCj7Ops+WnVievCAPzyD4B54Yokz5tm/YiW5zdd2HJly/MBYCiUUp6a5B/T+yPWdP4lyXNrrVPtpwKAkbMsyV8PeOZjN39M5wVJFDoAGEVXJplKMjbNnh1bztB0ftuFEgDowi4N6xtqrXe1GaDWurqUsjHJ4mm2NeUEAKZ3RcO668KABW+6X0wCwKA0feN+eCml6cLRWSmlLEry8IZtvnEHYMErpTwxyYcz/R+2kuSTSZ5Va93UfioAAACYm1rr3UluaNi2vOUYOzWs39PyfADoQtP73+0DSdE8R6EDAOam6bqwB5dSdm9x/iMa1l0XBnROoQOA+eDyJPdNs759mr+5nq1HJtl2mvX74hG6ACxwpZTj0nvqxpKGrRcneUatdUPbmQAAAKCPLm1Yb/PCkpmc/6OW5wNAFyYb1pt+H90v2zSs14GkAIARVWv9YZLvN2w7ro3ZpZQ9k+zfsK3pdwIArVPoAKBztdb7klzWsO3ElsY/oWH9i5vzAcCCVEp5TJKPJ1nasPXSJE/1vgkAAMAQ+reG9YNKKdPdGGiujmpYb7rwBQCG0b0N6zuVUsbbDFBKWZzmJ3GtbTMDACwQn21Y7+q6sJW1Vj9zA51T6ABgvvhMw/ozWpr7Ww3rn25pLgDMe6WURyT5ZHpPy5rO15OcXGtt+gMcAAAAzEefzfR3CV+U5tLFrGwuihzasO1bbcwGgI79pGG9JHlQyxkePIM9P205AwAsBE3XhT21pSKn68KAoaDQAcB88c8N60eWUg7o58BSyiGZ/g9lNc25AGAklVIOTe8OpTs2bP1WkifWWte0nwoAAAD6r9a6Os0XcfxaS+Mfn6TpopWvtjQbALp00wz2nNByhsfPYM9McgIA07sw0z/1avc0P01jq5RSdk7yxIZtF/RzJsBsKXQAMC/UWm9M8pWGbWf0eezvNKx/qda6qs8zAWDeK6Xsn95dUnZp2HpNkhNrrXe2nwoARkutdXmttQziI8lbGuKcN4NzPjCAf1sAoEvnNayfVkpZ3MLclzWsr6q1Xt/CXADoVK31tiQ/bNj2pJZjPLlh/Se11p+1nAEARl6t9Z4kH2vY1u/rwl6aZGKa9ZuT/N8+zwSYFYUOAOaTv2tYf0EpZY9+DCqlPDjJbzds+0A/ZgHAMCml7J3kc0ke0LB1ZZIn1FpvbT0UAAAAtO9fk9w2zfoDk5zSz4GllP3SfLfQj/ZzJgDMM19qWH9GKWWfNgaXUg5M8rSGbV9uYzYALFBN14WdVEo5vB+DSinbp7kgcn6ttfZjHsBcKXQAMJ/8fZLp7nCybZKz+jTrT5NsM836TzfnAYAFo5SyZ3pljgc3bF2V5IRa6y2thwIAAIABqLXel+RdDdveUUrZqR/zSiklyd+k+e+1f9uPeQAwTzXdqXtxkjNbmv3WJOMNez7e0mwAWHBqrZ9J8u1ptpQk7+zTuD9M78YMW7I+ybv7NAtgzhQ6AJg3ZvgHs+eWUn5jLnNKKc9M8uyGbe+sta6fyxwAGCallN3SK3OsaNj6w/TKHD9sPxUAAAAM1HuS3DXN+h5JzunTrN9NclzDnk/XWq/p0zwAmI8+luSehj3PKaW8uJ9DSymvSfKMhm33xZOyAKDf/rRh/b+UUl41lwGllMcm+YOGbR+otf50LnMA+kmhA4D55p1Jbm7Yc14p5ZGzObyU8ugk5zZs+36aiyUAMDJKKcuTfDrJgQ1bf5JemeOm1kMBAADAgNVaVyd5U8O2Z5VSztn8hI1ZKaWcluR/NcVJ8rrZzgCAYVBrvTszexrVe0spz+rHzFLKC5P82Qy2vr/Wemc/ZgIA/98/Jfl6w54/LaU8ZTaHl1L2S/LPSRZNs+3uJG+ezfkAbVHoAGBeqbWuTfLqhm07JPl0KeXXt+bsUsrTkvxbku0btr6m1rpua84GgGFVStk+ySeTHN6w9bYkj6+1rmw9FAAAAHTnvUmuaNjzsiQf3Py0yxkrpSwppbw5vQtXm/5O+1e11iu35nwAGFJ/lumfkJX0Lsr8p1LKe0sp285mSCllh1LK+9O7+V/T+/C9Sd4+mzkAwJbVWmuSV6R3E4MtWZzkglLKi7bm7FLKMUm+kN7TNafzllrrT7bmbIC2ld7/PwLA/FJK+cckz27YVtNrbp9Za71umrMOSu+uaqfOYPQ/1lr/24yDAsCQK6V8PMlMSpLvTfLNdtP8kltqrRcOcB4AjKzNF47+8TRbzqu1Pn8waQBg/iulPCzJ19J8c6DVSd6a5B+muxhk880UnpLkzCT7ziDC9UmO3HwDJAAYeaWUlyb5yxluvz3JOUneV2v9wQzO3ifJi5O8NMnyGc54Va31nTPcCwCtKqX8apL9t/LLdklyVsOe/z6LOF/oxw0ASylvTfL6GWz9VJI31Vq3+FSPUspDkrw2vdcz3ZM5kl7h4/G11smZZgUYBIUOAOalzX/gujzJATP8kiuTfCnJTUnuSe8pHvskOSbJw2d4xnVJjq613rN1aQFgeJVSViV5SNc57scXaq3HdR0CAEaBQgcAbL1SyilJPpSkzGB7TfKV9J7s8dP0LjRdluQBSQ5McnySJTMcfVuSx3pCJgALTSnlfyf5r1v5ZauSXJrkh0nuSHJ3eu/BOyfZK8mxSX5lK8/8P0l+q7qgCoB5opTygSTP6zrHZi+otX5groeUUsaTXJzkV2f4Jdcl+WKSlUnWJNkuvff6RyV5dGb2s/vPkhxRa/3xVgcGaFlTGw0AOlFrvaeU8sT0vhnfawZfcsTmj9n6QZInKnMAAAAAAFBrvaCUslt6T6xsUpI8ZvPHXNyZ5GRlDgAWqBcm2SnJk7bia/be/NEvFyf5bWUOAGhXrfX/tXfv0baXZb3Av8+GLaABgmiIIXgLQ1HCIDveELxgetSTJ0VCcZiSncxb5t28lKZhanoGnsSj4vEClXYRRRLTLC01BfJGXhDyRqRcRC6y3Tznj7k6cXTv35xr7Tl/a++1Pp8x5mCM/T7zeZ611mRsZc3vfDdX1cOTfDizfVDvHZceK3V5Ju8LE+YAtksbVnsBANia7r4oyVFJvrrgUV9JctQsV/ICAAAAALA+dPfJSU5MsmmEcV9Pcu/u/uQIswBgu9Pd1yZ5eJK3rdIKpyd5SHdfvUrzAWBd6e7Lktw/yT8teNQlmYQ5zl3wHIAVE+gAYLvW3V9JcniSsxY04gNJDu/uRYdGAAAAAADYwXT3KUmOTPKNBY75yySHdvfnFjgDALZ73f2D7j4hyRMz+STtMXwvyf/o7mO7+5qRZgIASbr735PcK4sLdH4qyc/58ARgeyfQAcB2r7sv6+5jkjwuk9T0PFyS5ITuflB3Xz6nngAAAAAArDHd/fEkP5PklUmum2PrLyV5WHc/vLsvnWNfANihdfebkhyU5HVJFhWyuDbJyUkO6u43LGgGADBFd1+7FOh8SJIL5tT2yiTPSPIL3f31OfUEWBiBDgB2GN19apLbJvmNJF9cYZsvLD3/Nt29Wtf1AgAAAACwA+nu73f3c5IcmOQlWfmNHdcleX+Shyf5me7+q7ksCABrTHdf0t1PTXLrJE9J8g9JNm9j2+uTfCLJ05Pcurt/o7sv3saeAMAcdPf7ktwxyWMyuVljJS5K8twkB3b3a7p7W/+3A8AoqrtXewcAWJGq+ukkxyQ5LMmdktwqye5Jbpzk6kzS1t/IJMTxmSRndveXV2dbAAAAWB1V9eIkLxooObW7HzfONgCwdlTVXZPcP8ldM3nTyQ3/G/WmJFcluTjJ15J8LpM3on6ku69YlYUBYAdXVXsmuXeSn83k98MHJNk3yV5Jdk2yMZO/g69Nclkmfw9flMnvi89N8tHuvmz0xQGAZauq/ZM8KMnhSQ7O5O/9PTL5/9w/yOR9Yd/O5EOBz01yVneftyrLAmwjgQ4AAAAAgDWsqo5McuRAybnd/Rdj7AIAAAAAAAD8J4EOAAAAAAAAAAAAAACAkW1Y7QUAAAAAAAAAAAAAAADWG4EOAAAAAAAAAAAAAACAkQl0AAAAAAAAAAAAAAAAjEygAwAAAAAAAAAAAAAAYGQCHQAAAAAAAAAAAAAAACMT6AAAAAAAAAAAAAAAABiZQAcAAAAAAAAAAAAAAMDIBDoAAAAAAAAAAAAAAABGJtABAAAAAAAAAAAAAAAwMoEOAAAAAAAAAAAAAACAkQl0AAAAAAAAAAAAAAAAjEygAwAAAAAAAAAAAAAAYGQCHQAAAAAAAAAAAAAAACMT6AAAAAAAAAAAAAAAABiZQAcAAAAAAAAAAAAAAMDIBDoAAAAAAAAAAAAAAABGJtABAAAAAAAAAAAAAAAwMoEOAAAAAAAAAAAAAACAkQl0AAAAAAAAAAAAAAAAjEygAwAAAAAAAAAAAAAAYGQCHQAAAAAAAAAAAAAAACMT6AAAAAAAAAAAAAAAABiZQAcAAAAAAAAAAAAAAMDIBDoAAAAAAAAAAAAAAABGJtABAAAAAAAAAAAAAAAwMoEOAAAAAAAAAAAAAACAkQl0AAAAAAAAAAAAAAAAjEygAwAAAAAAAAAAAAAAYGQCHQAAAAAAAAAAAAAAACMT6AAAAAAAAAAAAAAAABiZQAcAAAAAAAAAAAAAAMDIBDoAAAAAAAAAAAAAAABGJtABAAAAAAAAAAAAAAAwMoEOAAAAAAAAAAAAAACAkQl0AAAAAAAAAAAAAAAAjEygAwAAAAAAAAAAAAAAYGQCHQAAAAAAAAAAAAAAACMT6AAAAAAAAAAAAAAAABiZQAcAAAAAAAAAAAAAAMDIBDoAAAAAAAAAAAAAAABGJtABAAAAAAAAAAAAAAAwMoEOAAAAAAAAAAAAAACAkQl0AAAAAAAAAAAAAAAAjEygAwAAAAAAAAAAAAAAYGQCHQAAAAAAAAAAAAAAACMT6AAAAAAAAAAAAAAAABiZQAcAAAAAAAAAAAAAAMDIBDoAAAAAAAAAAAAAAABGJtABAAAAAABAkqSqjqyqHngcudo7bg+q6sAp36fHrfaO611VvXXg53Phau8HAAAAAJAIdAAAAAAAAAAAAAAAAIxOoAMAAAAAAAAAAAAAAGBkAh0AAAAAAAAAAAAAAAAjE+gAAAAAAAAAAAAAAAAYmUAHAAAAAAAAAAAAAADAyAQ6AAAAAAAAAAAAAAAARibQAQAAAAAAAAAAAAAAMDKBDgAAAAAAAAAAAAAAgJEJdAAAAAAAAAAAAAAAAIxMoAMAAAAAAAAAAAAAAGBkAh0AAAAAAAAAAAAAAAAjE+gAAAAAAAAAAAAAAAAYmUAHAAAAAAAAAAAAAADAyAQ6AAAAAAAAAAAAAAAARibQAQAAAAAAAAAAAAAAMDKBDgAAAAAAAAAAAAAAgJHtvNoLAAAAAAAAQFXtnOSuSe6U5I5Lj9sk2eMGj0pybZIrk3wryb8m+WySTyf5SHdfOf7my1dVG5PcK8n9k9w5k691ryS7J+lMvr5vJPlCkr9P8v7uvmh1tt2yqtqQ5JAk90jyc0lum+SAJHsmuUkmHyx3VZLLk1yY5EtJ/iHJh7v7wtEXBgAAAADYDgl0AAAAAAAAa0ZVVZIzkzxwoOzqJHfr7vMXtMMzkvzhlLITu/uURczfkVTVoUnul+S+mQQcdp/haRuX6vbLJEjwS0t/vqmqPprkTUne3d2b5r7wNqqq/ZI8LckTk9x0oHSXJPskOTTJcUvP/VCS13T3+xa65BRVdackT0jyy0luNaV8z6XHAUnuk8nXnar6ZJK3JnlLd1+7sGUBAAAAALZzG1Z7AQAAAAAAgHnp7k7y2CTfHii7cZLTq2rXec+vqsOTvGJK2enrOcxRVQdX1e9W1ZeSnJPkpCS/mNnCHEM2Jjk6ybuSfLWqHrsU8Fl1VbWhqn4ryVeS/HaGwxxbc3SSM6rq7Kq6wzz3m0VVHVJVf5nJjShPy/Qwx5Ajkpyc5GtV9YTt5ecEAAAAADA2gQ4AAAAAAGBN6e5Lkhyf5PqBsrskec0851bVHklOyyRYsDUXJDlxnnN3JFX1vCSfT/KCJIsMJeyf5NQkH6iqfRc4Z6ql18UHk7wqyW5zaHl0kk9X1bFz6DVVVe1aVScl+UyShyaZZ/hi3ySnJPlIVd1yjn0BAAAAAHYIAh0AAAAAAMCa091/k+RlU8qeVFWPmOPYU5LcduB8U5Jju/t7c5y5o9lj5HkPSPLJqjpo5LlJkqraO8lHkxw159a7J3lnVT19zn3/P1V1YJKPJXlmkp0XOOremYRUfm6BMwAAAAAAtjsCHQAAAAAAwFr1kiR/N6XmTUtvWt8mVXVikkdOKXtOd39qW2exbPsn+duquvWYQ6tqY5J3J7nrokYkeXVVPXkhzavunOQTSQ5bRP8tuGWSDwp1AAAAAADrySI/SQcAAAAAAGDVdPfmqjouyblJbraVspsmeVdV3au7f7iSOUtvfH/tlLL3J3nNSvqvQ99P8tkk5ye5LMkVS4/rkuy59Lh1ksOT3D6TYMM0P5nkvVV1RHf/YBFLb8Erkhw5cH5Jko8k+WKS7yTZnMnXdock91r65yxeW1UXdPf7V7zpj6iqOyX5cJJ9ZnzKpiSfSXJOku8uPa5Lcoulx90zCbZM+1ndNMmZVXW37v7X5W8OAAAAALBjEegAAAAAAADWrO7+RlU9Lsl7B8runuRlSZ693P5VdeMkpyfZbaDsW0lO6O5ebv914ookZyb5qyT/lOQrs36vqmqvJI9O8quZfpPEXZK8MMkLVr7qzI5I8mtbOfu7JC9PctbQ11lVhyR5ZpLHZDgIsVOSU6vqkO6+eIX73nDuPknOyGxhjg8m+aMkH+7uq6f03TfJ8Umem2TvgdJ9kvx5Vd29uzfNtjUAAAAAwI5pw2ovAAAAAAAAsEjdfUam347x21X1wBW0f32SgwfONyc5rru/s4Lea9nmJKclOSbJzbv70d39ru7+8nKCL919WXef3N13yyT4MO37/KyquvXK157Zk/Ljv4e7NskTu/ve3f2BaV9nd3+2u0/I5LaOb06Zt0+SP17xtkuqqpK8K8mBU0o/k+Rnu/sB3f2+aWGOJOnui7v7VUlum+SNU8oPyyT4AQAAAACwpgl0AAAAAAAA68FzMrn9YWsqyduq6pazNqyqRyd5/JSy3+vuv5215zpwZSY3OtxuKcRx1rxuYejutyc5JMn5A2UbM3ktLNqP3qjx/ST37+43LbdRd38syeFJvjSl9KFVdb/l9v8Rv5ZkWo/XJ/kv3X3uSgZ09xXd/WuZ3D5y/UDp86vqgJXMAAAAAADYUQh0AAAAAAAAa153X5fk2CTfGyi7RZL/U1VTf39SVbfP9BsR/jbJ78685DrQ3S/r7qd190UL6n9xkqOSXDhQdnxV3XgR87fi+iSP7O6/X2mD7v52kgdk+g0kr1zpjKraO8krppSd1N1P6e4frHTOf+juP0zyooGSGyV5/rbOAQAAAADYngl0AAAAAAAA60J3fzXJiVPKjk7yvKGCqrpRktOS7D5Q9p0kv9Ldm5e1JNtsKfzwmwMluyd5+DjbJEle191nbmuTpRDMk6aUHVZV913hiGcn2XPg/PSlmnl6eZKzBs5PqKp95jwTAAAAAGC7IdABAAAAAACsG919epJTppS9uKruOXD+B0nuNjQmyeO6+5vL3Y/56O4zknx4oOSYkVb5TpKXzKtZd787k5tfhvzGcvtW1R5Tnndpkl/v7l5u7yHdfX0m4Zvrt1JyoyTHz3MmAAAAAMD2RKADAAAAAABYb56a5HMD5zsleWdV7f2jB1X1X5eeP+Q13f2+bdiP+fjTgbOjRtrhJd19+Zx7Pm3K+YOrauj2mC05LslNBs5f3N2XLbPnTLr7y0neM1DyqEXMBQAAAADYHgh0AAAAAAAA60p3X5PJm8SvHijbP8lbbvgHVbV/krdOaf+pJM/Zlv2YmzMHzm5VVfsseP41SU6dd9PuPjfJPw6U7JrkIctse8LA2RVJ/niZ/ZbrjQNnh1fVTRc8HwAAAABgVQh0AAAAAAAA6053fyHJb04pe2hVPTVJqmqnJO9M8mO3dtzA95Ic292b5rMl2+jrSa4fOD9kwfPP6O4rF9T7nVPOZ76BpKpunuSIgZI/7+7rZu23Qh9LsrV/b3ZKcs8FzwcAAAAAWBUCHQAAAAAAwLrU3W/O9DfG/0FVHZbkJZn+pvITu/uCuSzHNuvuzUm+M1By4IJXOG2BvU/PcFjlPsvo9YAM/87wz5bRa0W6++oknx4o+dlF7wAAAAAAsBp2Xu0FAAAAAAAAVtGTMrmd4PZbOb9Rkvcm2XdKn1O6+/R5LkZSVT+ZZL8kN0+yZ5JdMvmZzPqhZTsNnN1y27ab6u8X1bi7L6mqLyc5aCslt6+qm3T3VTO0+/kp50NBi3m6KMndt3K26NtUAAAAAABWhUAHAAAAAACwbnX3lVX1qCT/kElQYEv2m9Lm80meOtfF1qGq2jPJAzO5CeWIJD+TZI8FjrzZAnv/W3dfssD+SfLP2Xqgo5IcnORTM/QZuv3i0u6+eLmLrdB3B85+aqQdAAAAAABGJdABAAAAAACsa939mar67SR/tIKnX5PkUd19zZzXWheqqpI8KJObUo5JsnHE8bstsPd5C+x9wxm/PHB+u8wW6LjjwNmVVfWEZW21cgcOnN1qpB0AAAAAAEYl0AEAAAAAAKx73f26qjoqycOW+dSndPfnF7HTWldV903yqiSHrdIKuyyw95cW2HvWGftOa1BVuybZZ6DkgCSnLGepBVnkTS0AAAAAAKtmw2ovAAAAAAAAsJ14fJKvL6P+9O5+06KWWauqamNVvT7Jh7J6YY4k2WmBva9YYO9ZZ/zkDD32m8ciI1jkbSoAAAAAAKtGoAMAAAAAACBJd1+a5NFJNs9QfkGSExe70dpTVTdOcmaSJyepVV5nka4cYcb3ppzfZIYeu89jkREs8jYVAAAAAIBVI9ABAAAAAADwn/bJbDc3vKG7p72hnhuoqg1J/iTJ0au9ywjGeG1Mm7HrDD3cfAEAAAAAsIp2Xu0FAAAAAAAAtgdVtX+SN89Y/sKqek93X7DIndaYZyV58Iy1neSzSf4pyeeTfC3Jt5NckuT7S49NSX7Y3b21JlV1YZIDVr7yim3aDmbMEkzaOI9FAAAAAABYGYEOAAAAAABg3auqnZK8M8neMz5ljySnVdU9unuMN+/v0Krqp5K8eIbSLyV5XZI/7e5L5jF6Dj1WYvcRZuwx5fzaGXr8YB6LAAAAAACwMgIdAAAAAAAAk7DBPZf5nMOT/H6SZ859m7Xn2Ul2mVJzUpLndvfmOc7dc469lmNa2GKMGbMEOq6ecv6x7l7uvxcAAAAAAMxow2ovAAAAAAAAsJqq6qgkz1vh059RVQ+a5z5rTVXtkuRXppQ9vbufNc8wR1VtyDjBii3ZHgId352hx7SaXWfcBQAAAACAFRDoAAAAAAAA1q2qunmSt2f4dyY91CLJqVV1y7kutrbcO8leA+d/3d2vXcDcvTL5+ayG/UaYMe01d/EMPS5Oct3A+S1mXwcAAAAAgOUS6AAAAAAAANalqqokb8vwG+OvTPKAJJcP1Nw8yduXboTgx91zyvlJC5p72wX1ncVdRphx1ynnX5/WoLs7yYUDJftV1cblLAUAAAAAwOz8YgEAAAAAAFivnpnkmCk1v97dZyd54pS6o5I8by5brT0HD5x9N8mHFzT3HgvqO4uDqmqXBc+YFuj4wox9zhk42ynJnWfsAwAAAADAMgl0AAAAAAAA605VHZHkZVPK3trd70iS7v6zJH88pf7FVTXtNor16ICBs3/p7s0LmruagY6dssBbOqpq5wwHLS7r7m/O2O4TU869pgEAAAAAFkSgAwAAAAAAWFeqas8kpyXZOFB2fpIn/8ifPS3J5waes1OSd1bV3tu04Nqz+8DZxYsYWFW7ZXJrymr6bwvsff8Mf18/voxeZ085f9gyegEAAAAAsAwCHQAAAAAAwHrzxiS3GTi/Nsmx3X3VDf+wu69N8qgk1ww8d/8kb97mDdeWGw2cLep2jsckWe1gzbEL7H3clPOPzNqouz+b5KsDJfetqtvN2g8AAAAAgNkJdAAAAAAAAOtGVZ2Y5JFTyn6ru8/b0kF3fyHJU6c8/2FV9Zsr2W+NGgrA3GLew6qqkmwP3//bVNU95t20qn4i02/NOGOZbd8xcLYhyXOW2Q8AAAAAgBkIdAAAAAAAAOtCVd0pyWunlL2nu08eKujuU5L8yZQ+J1XVobNvt6b9+8DZoVW185znPTnJnefcc6VetoCez02y+8D5ed19/jJ7/q8kmwbOH19Vhy+zJwAAAAAAUwh0AAAAAAAAa15V7ZZJCGO3gbKLkjxhxpYnJvnawPkuSU6vqpvM2G8t++rA2Z5JjprXoKq6Y5JXzqvfHNynqh4xr2ZVdWCSZ0wpe/Ny+3b3t5OcOlCyIcm7qmrv5fYGAAAAAGDrBDoAAAAAAID14HVJDh44/2GS47r7slmadfcVSY7N8K0GP51k8LaPdeJTU85fWlW1rUOqaq9MD+2shldX1c22tUlVbUjyhiS7DpRdnhUEOpa8MMn3B85vl+TMeXwts6qqQ6vqNmPNAwAAAAAYm0AHAAAAAACwplXVozL95o0XdffHl9O3uz+Z5PlTyh5bVccvp+8a9NdTzn8+yYu3ZUBV7Zvkg0kO2ZY+C3LrJO9euiVmW7wqyTFTal7T3UOhjK3q7osz/fV8RJJzquoXVjJjVlV1dFWdkeScTIIkAAAAAABrkkAHAAAAAACwZlXVbZO8cUrZh5K8YoUjXpXkA1Nq3lBVd1hh/x1ed38tybSwzO9U1cuqaufl9q+qByf5dJK7beF483L7Lch9kpxVVbdY7hOramNVnZzk6VNKv5HJ63FbvD7JmVNq9k/y0ap6xVKQZi6q6oCqem5VnZ/k7CQPnldvAAAAAIDtlUAHAAAAAACwJlXVxiSnJdljoOySJMd39/UrmdHdneSEJBcPlP1EktOq6kYrmbFGvHqGmucl+URVPaKqdhoqrKqdq+rBVXV2kjOS7LeFsrdnEnJYDedu4c/uleS8qnpMVc30O7qqumeSf0zy6zOUP6m7r559xR+39Hr+lSRfmFK6c5JnJ7mwqv53Vd2vqm6ynFlVddOqemBVvbyqzklyYZKXJzloBasDAAAAAOyQlv0pRwAAAAAAADuI309y+MB5J3lsdw+FMabq7kuq6jFJzsrWP0zrsCQnJXnqtszaUXX3u6vq7zIJNQw5LMmfJbm0qj6e5Lwklya5KslumQQ3Dk5yzyR7DvT51yRPXnr+avifmYQwfvTWkH2TvC3JS6vqXUn+JpPwxHczuU1kzyR3yOTr++9Jfn7GeSd39/vmsHe6+7KqOibJR5McOKV8lySPX3r8sKo+k+SzmfzMLlt6VJJdk+yVydd/QCahjVsvnQEAAAAArFsCHQAAAAAAwJpTVb+Y5BlTyl7V3WfNY153n11Vr0zy3IGyp1TV2d393nnM3AE9Lsk5Gb4x5T/sneQhS4/lujzJg7v7iqpVywtsTnJ8JrdrbCl4cmAmr5Wh18us/ibJ0+bQ5//p7q9X1T2SnJnkLjM+beckRyw9AAAAAACYwUzXOQMAAAAAAOwoquqWSd6a4U///0SS58959O8k+fiUmrdU1U/Nee4OobsvSPJLSX6wwDGXJ3lId39ugTNm0t3nJ3lkkk0LHPPJJI/o7rnP6O5vZXJTyDvm3RsAAAAAgAmBDgAAAAAAYM2oqg2ZvAH95gNlVyR59LzfBN/dP0xyXCahgq25WZJ3VNVO85y9o+juDyU5Osm3FtD+K0nu3t0fW0DvFenuv07y0CRXLaD9B5Pcr7svX0DvJEl3X9ndxyd5dBbzM5vm40kuXIW5AAAAAACjEOgAAAAAAADWkhckue+UmhO7+2uLGN7dFyX51Sll987kNo91aSlwcVgmgYR5+GGSk5Lctbv/ZU4956a7P5DJTRfzujVkU5IXJTmmu6+cU89B3X1akjskeWGSixc87qIkv5fkDt19j+7+yoLnAQAAAACsGoEOAAAAAABgTaiqe2V6UOKU7v6TRe7R3e9J8oYpZS+oqiMXucf2rLv/rbsfkOQ+Sc5aYZtLk7w6yU9397O6++ot1Pxzkk9v5bGQUM+WdPe5Se6W5NlZeSCik/xlkkO6+6Xdff2c1pttePfV3f17SQ7I5MaOv0iype/5cl2V5Mwkz0xyaJLbdPcLBTkAAAAAgPWgunu1dwAAAAAAAGAdq6pbZXKzyn2T3DnJzZYeuye5JpM3/X87yb9kctPFh5J8srs3r8rC26CqdknyS0kemuR+SfYZKO8k5yV5f5K3bG8hh6raLcnhSY5IcpckBybZP8meSW6cZGMmP7srk3xv6XFhkvOXHl9M8vnu3jTy6gAAAAAA2wWBDgAAAAAAAFglVbVvkoOS7J1JgKUzCUB8M8kXu/v7q7geAAAAAAALJNABAAAAAAAAAAAAAAAwsg2rvQAAAAAAAAAAAAAAAMB6I9ABAAAAAAAAAAAAAAAwMoEOAAAAAAAAAAAAAACAkQl0AAAAAAAAAAAAAAAAjEygAwAAAAAAAAAAAAAAYGQCHQAAAAAAAAAAAAAAACMT6AAAAAAAAAAAAAAAABiZQAcAAAAAAAAAAAAAAMDIBDoAAAAAAAAAAAAAAABGJtABAAAAAAAAAAAAAAAwMoEOAAAAAAAAAAAAAACAkQl0AAAAAAAAAAAAAAAAjEygAwAAAAAAAAAAAAAAYGQCHQAAAAAAAAAAAAAAACMT6AAAAAAAAAAAAAAAABiZQAcAAAAAAAAAAAAAAMDIBDoAAAAAAAAAAAAAAABGJtABAAAAAAAAAAAAAAAwMoEOAAAAAAAAAAAAAACAkQl0AAAAAAAAAAAAAAAAjEygAwAAAAAAAAAAAAAAYGQCHQAAAAAAAAAAAAAAACMT6AAAAAAAAAAAAAAAABiZQAcAAAAAAAAAAAAAAMDIBDoAAAAAAAAAAAAAAABGJtABAAAAAAAAAAAAAAAwMoEOAAAAAAAAAAAAAACAkQl0AAAAAAAAAAAAAAAAjEygAwAAAAAAAAAAAAAAYGQCHQAAAAAAAAAAAAAAACMT6AAAAAAAAAAAAAAAABiZQAcAAAAAAAAAAAAAAMDIBDoAAAAAAAAAAAAAAABGJtABAAAAAAAAAAAAAAAwMoEOAAAAAAAAAAAAAACAkQl0AAAAAAAAAAAAAAAAjEygAwAAAAAAAAAAAAAAYGQCHQAAAAAAAAAAAAAAACMT6AAAAAAAAAAAAAAAABiZQAcAAAAAAAAAAAAAAMDIBDoAAAAAAAAAAAAAAABGJtABAAAAAAAAAAAAAAAwMoEOAAAAAAAAAAAAAACAkQl0AAAAAAAAAAAAAAAAjEygAwAAAAAAAAAAAAAAYGQCHQAAAAAAAAAAAAAAACMT6AAAAAAAAAAAAAAAABiZQAcAAAAAAAAAAAAAAMDIBDoAAAAAAAAAAAAAAABGJtABAAAAAAAAAAAAAAAwMoEOAAAAAAAAAAAAAACAkQl0AAAAAAAAAAAAAAAAjEygAwAAAAAAAAAAAAAAYGQCHQAAAAAAAAAAAAAAACMT6AAAAAAAAAAAAAAAABiZQAcAAAAAAAAAAAAAAMDIBDoAAAAAAAAAAAAAAABGJtABAAAAAAAAAAAAAAAwMoEOAAAAAAAAAAAAAACAkQl0AAAAAAAAAAAAAAAAjEygAwAAAAAAAAAAAAAAYGQCHQAAAAAAAAAAAAAAACMT6AAAAAAAAAAAAAAAABiZQAcAAAAAAAAAAAAAAMDIBDoAAAAAAAAAAAAAAABGJtABAAAAAAAAAAAAAAAwMoEOAAAAAAAAAAAAAACAkQl0AAAAAAAAAAAAAAAAjEygAwAAAAAAAAAAAAAAYGQCHQAAAAAAAAAAAAAAACMT6AAAAAAAAAAAAAAAABiZQAcAAAAAAAAAAAAAAMDIBDoAAAAAAAAAAAAAAABG9n8BLbFGkPgkw1wAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 3600x2400 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "fig = plt.figure(dpi=600)\n",
    "plt.xlabel(\"x label\")\n",
    "plt.ylabel(\"y label\")\n",
    "plt.plot(x.numpy(), yy.detach().numpy())\n",
    "plt.plot(x.numpy(), y.numpy(), 'o')\n",
    "plt.plot(x.numpy(), yy.detach().numpy(),'o')\n",
    "\n",
    "plt.savefig(\"dl_ch1t2_plot01.png\", format=\"png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nervous-blowing",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
